accepted_at,created_at,current_state,deadline,description,estimate,id,kind,labels,name,owned_by_id,owner_ids,project_id,requested_by_id,story_type,updated_at,url
2013-11-15T18:41:07Z,2013-10-16T20:57:37Z,accepted,,"follow-on from #56532652

the new box is ready to go, just waiting on services to finish their deploy and give us the go ahead. 

AZ1: i-e7e1319f (block from pointing at jb-z1 until services is ready to let the old jb go)
",,59004602,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",roll the 'old' prod jumpbox to the new instance,756869.0,[756869],956238,1338772,chore,2013-11-15T18:41:22Z,https://www.pivotaltracker.com/story/show/59004602
2013-11-15T21:35:47Z,2013-10-24T22:02:23Z,accepted,,"ci build was aborted and didnt have a chance to teardown_microbosh in the runner.rb. we should do something similar to what we do in aws (before test and tags) or use resource pools (or folders) to isolate bats from everything else.

error that might be seen if clean up was not done before:

** Execute spec:system:micro
I, [2013-10-24T12:57:01.888887 #9026]  INFO -- : CANDIDATE_BUILD_NUMBER is 1161. Using candidate build.
I, [2013-10-24T12:57:01.966151 #9026]  INFO -- : Downloading http://bosh-ci-pipeline.s3.amazonaws.com/1161/bosh-stemcell/vsphere/bosh-stemcell-1161-vsphere-esxi-ubuntu.tgz to /tmp/ci-artifacts/vsphere/ubuntu/deployments/bosh-stemcell-1161-vsphere-esxi-ubuntu.tgz
bosh -v -n -P 10 --config '/tmp/bosh_config20131024-9026-gbh6v8' micro deployment microbosh
WARNING! Your target has been changed to `https://172.16.69.6:25555'!
Deployment set to '/tmp/ci-artifacts/vsphere/ubuntu/deployments/microbosh/micro_bosh.yml'
bosh -v -n -P 10 --config '/tmp/bosh_config20131024-9026-gbh6v8' micro deploy /tmp/ci-artifacts/vsphere/ubuntu/deployments/bosh-stemcell-1161-vsphere-esxi-ubuntu.tgz
at depth 1 - 19: self signed certificate in certificate chain
at depth 0 - 20: unable to get local issuer certificate
at depth 1 - 19: self signed certificate in certificate chain
at depth 1 - 19: self signed certificate in certificate chain
at depth 1 - 19: self signed certificate in certificate chain
at depth 1 - 19: self signed certificate in certificate chain
at depth 1 - 19: self signed certificate in certificate chain
/var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/agent_client/lib/agent_client/base.rb:21:in `method_missing': {""message""=>""Unknown persistent disk: disk-6396dd2b-ec98-40d8-95b7-4dfbddd46c08"", ""backtrace""=>[""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/platform/linux/disk.rb:52:in `lookup_disk_by_cid'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/platform/linux/adapter.rb:24:in `lookup_disk_by_cid'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/message/mount_disk.rb:29:in `setup_disk'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/message/mount_disk.rb:20:in `mount'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/message/mount_disk.rb:8:in `process'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/handler.rb:265:in `process'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/handler.rb:250:in `process_long_running'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/handler.rb:177:in `block in process_in_thread'"", ""<internal:prelude>:10:in `synchronize'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/handler.rb:175:in `process_in_thread'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.1155/lib/bosh_agent/handler.rb:155:in `block in handle_message'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `call'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `block in spawn_threadpool'""]} (Bosh::Agent::HandlerError)
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/agent_client/lib/agent_client/base.rb:12:in `run_task'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:255:in `block in mount_disk'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:79:in `step'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:254:in `mount_disk'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:314:in `attach_disk'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:350:in `update_persistent_disk'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:138:in `block in create'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:79:in `step'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:137:in `create'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli_plugin_micro/lib/bosh/cli/commands/micro.rb:182:in `perform'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli/lib/cli/command_handler.rb:57:in `run'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli/lib/cli/runner.rb:59:in `run'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli/lib/cli/runner.rb:18:in `run'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/bosh_cli/bin/bosh:7:in `<top (required)>'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/tmp/ruby/1.9.1/bin/bosh:23:in `load'
	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu/workspace/tmp/ruby/1.9.1/bin/bosh:23:in `<main>'",,59523646,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",vsphere microbosh bats should clean up before running tests,1068489.0,[1068489],956238,81882,chore,2013-11-15T21:35:48Z,https://www.pivotaltracker.com/story/show/59523646
2013-11-15T21:36:01Z,2013-10-29T19:00:55Z,accepted,,,,59778912,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",Rebuild Jumpboxes in A1 & Prod,756869.0,[756869],956238,81882,chore,2013-11-15T21:36:03Z,https://www.pivotaltracker.com/story/show/59778912
2013-11-19T21:45:03Z,2013-11-19T01:45:09Z,accepted,,"Automation creates a release and uploads it. Seeing an issue with checksums going into the db, but failing before the blob is uploaded. 


+-------+-------+-------------------------+-------+------------------------------------------------+----------------------------------------------------------+
| 32628 | error | 2013-11-18 19:26:23 UTC | admin | create release                                 | `cloud_controller_ng/29' checksum mismatch, expected...  |
| 32627 | error | 2013-11-18 18:51:34 UTC | admin | create deployment                              | Can't find template `nats_stream_forwarder'              |
| 32626 | error | 2013-11-18 18:51:01 UTC | admin | create release                                 | `cloud_controller_ng/29' checksum mismatch, expected...  |
| 32625 | error | 2013-11-18 18:45:26 UTC | admin | create deployment                              | Can't find template `nats_stream_forwarder'              |
| 32624 | error | 2013-11-18 18:44:50 UTC | admin | create release                                 | `buildpack_cache/5' checksum mismatch, expected...       |
| 32623 | error | 2013-11-18 18:40:46 UTC | admin | create deployment                              | Can't find template `nats_stream_forwarder'              |
| 32622 | error | 2013-11-18 18:40:14 UTC | admin | create release                                 | `buildpack_cache/5' checksum mismatch, expected...       |
| 32621 | error | 2013-11-18 18:31:23 UTC | admin | create deployment                              | Can't find template `nats_stream_forwarder'              |
| 32620 | error | 2013-11-18 18:22:33 UTC | admin | create release                                 | `buildpack_cache/5' checksum mismatch, expected...       |",,61008990,story,[],Investigate AWS bosh partial release upload,1068489.0,[1068489],956238,1068489,chore,2013-11-19T21:45:15Z,https://www.pivotaltracker.com/story/show/61008990
2013-11-20T07:23:56Z,2013-11-19T04:57:12Z,accepted,,"see details: https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/_wgYmTzYTtc/eYWbu8rmrgMJ

see if there is any troubleshooting steps we can advise or ask for more information that would help.",0.0,61014318,story,[],review and advise on bosh director template error on vsphere from bosh-users,1068489.0,[1068489],956238,494053,feature,2013-11-20T07:23:56Z,https://www.pivotaltracker.com/story/show/61014318
2013-11-20T07:24:42Z,2013-11-18T15:25:33Z,accepted,,"Since these have been combined with the regular bosh stemcells. E.g.

http://docs.cloudfoundry.com/docs/running/deploying-cf/vsphere/deploying_bosh_with_micro_bosh.html",1.0,60955736,story,[],Docs at cloudfoundry.org should no longer refer to microbosh stemcells,1068489.0,[1068489],956238,5637,feature,2013-11-20T07:24:42Z,https://www.pivotaltracker.com/story/show/60955736
2013-11-20T19:28:11Z,2013-11-04T22:31:25Z,accepted,,,,60125634,story,[],Setup local vSphere environment for embarcadero to iterate on BATs,506755.0,[506755],956238,5637,chore,2013-11-20T19:28:11Z,https://www.pivotaltracker.com/story/show/60125634
2013-11-20T23:42:43Z,2013-11-15T17:29:56Z,accepted,,change --points-at to --contains to be backwards compatible with slightly older Git,1.0,60860812,story,[],GitPromoter should not promote the same commit twice,1338772.0,[1338772],956238,1338772,feature,2013-11-20T23:42:44Z,https://www.pivotaltracker.com/story/show/60860812
2013-11-21T00:56:29Z,2013-10-07T21:20:17Z,accepted,,"acceptance is running ""bosh public stemcells --all"" and seeing them all.",2.0,58381628,story,"[{'name': 'release', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034564, 'updated_at': '2013-11-14T20:34:18Z'}]","Publish CI promoted stemcells as ""public stemcells""",5637.0,[5637],956238,14062,feature,2013-11-21T00:56:29Z,https://www.pivotaltracker.com/story/show/58381628
2013-11-21T01:27:25Z,2013-11-08T23:59:39Z,accepted,,,,60449810,story,"[{'name': 'release', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034564, 'updated_at': '2013-11-14T20:34:18Z'}]",Tell community about published stemcells,494053.0,[494053],956238,14062,chore,2013-11-21T01:36:18Z,https://www.pivotaltracker.com/story/show/60449810
2013-11-22T01:20:30Z,2013-10-14T21:59:22Z,accepted,,,2.0,58846844,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Build Centos/AWS Stemcell once,506755.0,[506755],956238,14062,feature,2013-11-22T01:20:31Z,https://www.pivotaltracker.com/story/show/58846844
2013-11-22T01:20:35Z,2013-10-14T21:59:29Z,accepted,,demo a running centos stemcell on AWS,2.0,58846852,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]","Boot a Centos/AWS Stemcell
",506755.0,[506755],956238,14062,feature,2013-11-22T01:20:35Z,https://www.pivotaltracker.com/story/show/58846852
2013-11-22T01:24:52Z,2013-10-24T18:36:22Z,accepted,,"After some discussion with @ajackson @tlabeeuw & @mattr we agreed that each team's acceptance environment should only ever have release-candidate (~ deployed to A1) versions of the other team's artifacts.

For instance for Dijon this means a bleeding edge Services release but a release-candidate cf-release.

In keeping with this convention the BOSH team should have its own environment for auto-deploys, package cache population and general acceptance. BOSH CI should no longer automatically deploy to Tabasco & Dijon.

The BOSH team should instead publish a ""release-candidate"" branch like Services & Runtime that the other teams can use to update BOSH on their respective acceptance environments. This is similar to how the PRISM environment is managed currently.",4.0,59507586,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","BOSH's CI should auto-deploy to its own acceptance environment on vsphere instead of the other teams' acceptance environment (i.e. Dijon, Tabasco & PRISM)",81882.0,[81882],956238,5637,feature,2013-11-22T01:24:53Z,https://www.pivotaltracker.com/story/show/59507586
2013-11-22T02:10:40Z,2013-11-14T19:11:33Z,accepted,,http://aws.amazon.com/about-aws/whats-new/2013/11/13/announcing-aws-cloudtrail/,1.0,60793894,story,[],Investigate and enable aws cloudtrail,637633.0,[637633],956238,14062,feature,2013-11-22T02:10:41Z,https://www.pivotaltracker.com/story/show/60793894
2013-11-22T04:55:14Z,2013-11-21T18:36:15Z,accepted,,"One implementation might be:

`rake sandbox:director:start` which starts a director like the integration tests",,61231410,story,[],As a BOSH developer I would like the ability to start a director on my development machine with a dummy CPI so I can iterate faster on director improvments,5637.0,[5637],956238,5637,chore,2013-11-22T04:55:14Z,https://www.pivotaltracker.com/story/show/61231410
2013-11-22T22:41:59Z,2013-11-22T20:05:30Z,accepted,,"The autodeploy is currently failing as the latest build number is 1394, but the sha has not changed since 1391 (the first build number for this sha). This is because promote artifacts sees that the sha has not changed, and therefore doesn't need to promote any new artifacts to production.

The autodeploy script should either deploy with the latest promoted artifacts, or exit early when it detects that the deployed sha has not changed,",,61313558,story,[],Autodeploy should be graceful when git sha has not changed,506755.0,[506755],956238,506755,chore,2013-11-22T22:41:59Z,https://www.pivotaltracker.com/story/show/61313558
2013-11-27T18:56:53Z,2013-10-01T00:44:02Z,accepted,,"During a deploy, vsphere ran out of resources and when tempest retried executing bosh, it ended with out of resource errors. This happened with a previous story so it seems like the issue is back. Enclosed is the bo task * --debug logs for all of the tasks that ran.",,57964476,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",out of sync error,81882.0,[81882],956238,81882,bug,2013-11-27T18:56:53Z,https://www.pivotaltracker.com/story/show/57964476
2013-11-27T19:42:29Z,2013-11-20T20:38:51Z,accepted,,"the current implementation creates the blobs for all to-be-imported packages, and then in a transaction save them to the database.
on an execption, the database records are rolled back, but the blobstore is left as is. this will prevent subsequent imports to proceed without manual intervention (because on import we blindly take the blobstore_id of the export side)",,61162324,story,"[{'name': 'compiled-packages', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-09T18:12:51Z', 'id': 7435282, 'updated_at': '2014-07-31T04:56:46Z'}]",bosh import compiled packages should remove the new blobs on error,1338772.0,[1338772],956238,1338772,chore,2014-07-31T04:56:46Z,https://www.pivotaltracker.com/story/show/61162324
2013-11-27T23:13:41Z,2013-11-27T18:05:00Z,accepted,,"http://bosh-jenkins.cf-app.com:8080/job/bat_micro_vsphere_centos/212/

Shows a run that had leftover VMs sitting around.  They weren't cleaned up as expected because the leftover VMs were not powered off.  Need to power them off first, then remove them.",,61568252,story,[],Investigate BATs vSphere VM cleanup failures,1068489.0,[1068489],956238,1068489,chore,2013-11-27T23:13:41Z,https://www.pivotaltracker.com/story/show/61568252
2013-11-28T01:51:12Z,2013-11-19T21:21:59Z,accepted,,Broken out of #58381628,1.0,61074790,story,[],Update docs to reflect new style public stemcells,1068489.0,[1068489],956238,5637,feature,2013-11-28T01:51:13Z,https://www.pivotaltracker.com/story/show/61074790
2013-11-28T02:04:26Z,2013-11-19T23:09:47Z,accepted,,https://github.com/cloudfoundry/bosh/pull/453,1.0,61084540,story,[],Accept or otherwise comment on this syslog pull request from Kibana,1068489.0,[1068489],956238,1408486,feature,2013-11-28T02:04:26Z,https://www.pivotaltracker.com/story/show/61084540
2013-12-02T22:24:50Z,2013-11-25T18:26:17Z,accepted,,,1.0,61412354,story,[],Update to latest runtimes,1068489.0,[1068489],956238,1068489,feature,2013-12-02T22:24:25Z,https://www.pivotaltracker.com/story/show/61412354
2013-12-02T22:49:56Z,2013-11-21T01:03:02Z,accepted,,"Uploading a non-light AWS stemcell and then trying to delete it gives this error:

ubuntu@ip-10-194-45-189:/bosh$ bundle exec bosh delete stemcell bosh-aws-xen-ubuntu 1365
Checking if stemcell exists...
You are going to delete stemcell `bosh-aws-xen-ubuntu/1365'
Are you sure? (type 'yes' to continue): yes

Director task 14

Deleting stemcell from cloud
  delete stemcell: unable to find the image (00:00:01)
Error                   1/1 00:00:01

Error 100: unable to find the image

The AMI does get deleted, but the database row does not get deleted.

Traceback:

E, [2013-11-25T18:14:46.532857 #28429] [task:398] ERROR -- : unable to find the image
/var/vcap/packages/director/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/resource.rb:238:in `rescue in block in define_attribute_getter'
/var/vcap/packages/director/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/resource.rb:234:in `block in define_attribute_getter'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/resource_wait.rb:157:in `call'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/resource_wait.rb:157:in `block in for_resource'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/retryable.rb:23:in `block in retryer'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/retryable.rb:21:in `loop'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/retryable.rb:21:in `retryer'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/common.rb:119:in `retryable'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/resource_wait.rb:154:in `for_resource'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/resource_wait.rb:66:in `for_image'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/stemcell.rb:26:in `delete'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/cloud.rb:421:in `block in delete_stemcell'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.961/lib/cloud/aws/cloud.rb:419:in `delete_stemcell'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/jobs/delete_stemcell.rb:55:in `block in delete_from_cloud'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/event_log.rb:58:in `track'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/jobs/delete_stemcell.rb:54:in `delete_from_cloud'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/jobs/delete_stemcell.rb:33:in `block in perform'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/lock_helper.rb:35:in `block in with_stemcell_lock'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/lock_helper.rb:34:in `with_stemcell_lock'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/jobs/delete_stemcell.rb:27:in `perform'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/job_runner.rb:98:in `perform_job'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.961/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/lib/director/jobs/base_job.rb:12:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
/var/vcap/packages/director/gem_home/gems/director-1.5.0.pre.961/bin/worker:76:in `<top (required)>'
/var/vcap/packages/director/bin/worker:23:in `load'
/var/vcap/packages/director/bin/worker:23:in `<main>'",,61179184,story,[],Non-light AWS stemcells cannot be deleted.,756869.0,[756869],956238,756869,bug,2013-12-02T22:49:31Z,https://www.pivotaltracker.com/story/show/61179184
2013-12-03T23:07:19Z,2013-12-03T22:56:12Z,accepted,,,,61857938,story,[],Investigate Travis test failures due to debugger gem,1068489.0,[1068489],956238,1068489,chore,2013-12-03T23:07:20Z,https://www.pivotaltracker.com/story/show/61857938
2013-12-04T01:19:53Z,2013-12-03T19:59:27Z,accepted,,"In preparation for a deploy after #61412354.

Micro and full BOSH on Dijon and Tabasco should be updated.",,61843830,story,[],Update pipeline bosh instances with new stemcell,1068489.0,[1068489],956238,1068489,chore,2013-12-04T01:19:53Z,https://www.pivotaltracker.com/story/show/61843830
2013-12-04T22:24:05Z,2013-11-21T01:04:29Z,accepted,,It would be useful for testing AWS stemcells (and because deleting heavy AWS stemcells doesn't actually work yet).,,61179350,story,[],User should be able to create an AWS light stemcell without publishing,637633.0,[637633],956238,756869,chore,2013-12-22T07:23:46Z,https://www.pivotaltracker.com/story/show/61179350
2013-12-06T00:22:26Z,2013-12-04T18:02:50Z,accepted,,Update micro and full BOSH on A1,,61916422,story,[],Update acceptance environment with new stemcell,1068489.0,[1068489],956238,1068489,chore,2013-12-06T00:22:26Z,https://www.pivotaltracker.com/story/show/61916422
2013-12-06T17:37:15Z,2013-12-03T18:19:20Z,accepted,,,,61835376,story,[],BOSH lite repo should have a license file,1408486.0,[1408486],956238,1408486,bug,2013-12-06T17:37:11Z,https://www.pivotaltracker.com/story/show/61835376
2013-12-06T17:38:53Z,2013-11-27T00:21:38Z,accepted,,"Hi. I would like to know the answer to the following:

Does BOSH support...

Networks in vCenter folders and subfolders?
Resource Pools in vCenter folders and subfolders?
Clusters in vCenter folders and subfolders?
Data Centers in vCenter folders and subfolders?
",1.0,61519862,story,[],Does BOSH support folders for different things?,506755.0,[506755],956238,548715,feature,2013-12-06T17:38:49Z,https://www.pivotaltracker.com/story/show/61519862
2013-12-06T18:35:06Z,2013-11-26T01:25:08Z,accepted,,"https://github.com/cloudfoundry/bosh/issues/460#issuecomment-29250004

Deployment `cf-c3692b2ed65396f3add9'

Director task 33

Error 100: PG::Error: ERROR:  invalid input syntax for integer: """"
LINE 1: ...yment_id"" = 1) AND (""job"" IS NULL) AND (""index"" = '')) LIMIT...

Task 33 error
Failed to fetch VMs information from director",,61442960,story,[],bosh vms should not error out on a postgres failure,1068489.0,[1068489],956238,494053,bug,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61442960
2013-12-06T18:37:26Z,2013-11-26T23:15:11Z,accepted,,See this story for details #61516544,1.0,61516610,story,[],Update configuration of bosh director for run.pivotal.io,756869.0,[756869],956238,285989,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61516610
2013-12-06T18:40:05Z,2013-11-20T17:44:26Z,accepted,,"cloud_controller_ng/29.1-dev                     |ooooooooooooooooo           | 5/8 00:05:29  ETA: 00:00:10                                                                                                                        
cloud_controller_ng/29.1-dev (00:05:31) |ooooooooooooooooooooo  | 6/8 00:05:46  ETA: 00:00:48
                                                                                                                        
Error                                       6/8 00:05:31

Error 100: Unable to access file [onion-ds] sc-9842e78b-aa68-40d4-8b3a-421a1672afa4 / datastore-24/sc-9842e78b-aa68-40d4-8b3a-421a1672afa4 / datastore-24.vmdk since it is locked",,61148420,story,[],Investigation: Bosh deploy is failing on vsphere with a locked vmdk },506755.0,[506755],956238,163328,chore,2013-12-06T18:40:05Z,https://www.pivotaltracker.com/story/show/61148420
2013-12-06T22:02:29Z,2013-11-26T17:51:55Z,accepted,,See #61492572,1.0,61492730,story,[],Fix cloudability integration,1068489.0,[1068489],956238,353433,feature,2014-01-08T18:20:56Z,https://www.pivotaltracker.com/story/show/61492730
2013-12-06T22:25:41Z,2013-11-15T23:52:19Z,accepted,,"upload_and_track could return :unknowon, :task_timeout, etc.return statuses. We should probably raise a cli error if return status is not success.",,60884440,story,"[{'name': 'compiled-packages', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-09T18:12:51Z', 'id': 7435282, 'updated_at': '2014-07-31T04:56:46Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",user should see a cli error if task that is created by `bosh compiled_packages import ...` does not succeed.,1068489.0,[1068489],956238,81882,bug,2014-07-31T04:56:46Z,https://www.pivotaltracker.com/story/show/60884440
2013-12-06T22:35:59Z,2013-10-07T21:26:34Z,accepted,,Acceptance:  Tammer can look at the release/index.yml directory on github and see the recent releases.,4.0,58382172,story,"[{'name': 'release', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034564, 'updated_at': '2013-11-14T20:34:18Z'}]",Publish all BOSH releases as final releases to blobs.cfblobs.com after green CI,1068489.0,[1068489],956238,14062,feature,2013-12-06T22:36:00Z,https://www.pivotaltracker.com/story/show/58382172
2013-12-06T22:51:29Z,2013-11-07T23:17:22Z,accepted,,"Current implementation assumes a tight succession of (upload-release, import compiled packages, deploy).
An operator should be able to import the same exported compiled packages more than once and proceed.",1.0,60373962,story,"[{'name': 'compiled-packages', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-09T18:12:51Z', 'id': 7435282, 'updated_at': '2014-07-31T04:56:46Z'}]",Make import compiled packages idempotent.,1338772.0,[1338772],956238,81882,feature,2014-07-31T04:56:46Z,https://www.pivotaltracker.com/story/show/60373962
2013-12-09T19:29:01Z,2013-12-05T19:02:22Z,accepted,,"Can be seen in:
http://bosh-jenkins.cf-app.com:8080/job/bosh_integration/1137/console

Error:
     Failure/Error: Unable to find matching line from backtrace
     Errno::ECONNREFUSED:
       Connection refused - connect(2)



",,62001564,story,[],Investigate flakey integration test failures,1068489.0,[1068489],956238,1068489,chore,2015-11-03T00:03:54Z,https://www.pivotaltracker.com/story/show/62001564
2013-12-10T03:32:05Z,2013-12-10T03:25:56Z,accepted,,See #62208000,,62208014,story,[],Update artifact tgzs,1068489.0,[1068489],956238,1068489,chore,2013-12-10T03:32:06Z,https://www.pivotaltracker.com/story/show/62208014
2013-12-10T20:33:04Z,2013-12-10T19:25:36Z,accepted,,The BOSH Jenkins CI requires additional executors to accommodate the incorporation of AWS CentOS stemcell creation into our CI,,62258414,story,[],Increase number of Jenkins's executors on slave nodes,1068489.0,[1068489],956238,1068489,chore,2013-12-10T20:33:05Z,https://www.pivotaltracker.com/story/show/62258414
2013-12-10T21:33:10Z,2013-12-10T00:49:29Z,accepted,,BAT's is testing against the wrong bits. That's embarrassing. Until this is fixed we can't even trust our CI ...,,62202440,story,[],final releases published in CI should contain the right bits,506755.0,[506755],956238,506755,chore,2013-12-10T21:33:11Z,https://www.pivotaltracker.com/story/show/62202440
2013-12-10T22:10:22Z,2013-12-02T08:47:26Z,accepted,,https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/sQp8e2YAJT4/Qz2L_akbwOkJ,1.0,61708894,story,[],answer bosh-users question about whether autoconf is appropriate addition to the stemcell,506755.0,[506755],956238,494053,feature,2013-12-10T22:10:22Z,https://www.pivotaltracker.com/story/show/61708894
2013-12-10T22:10:45Z,2013-11-27T20:01:06Z,accepted,,"PR 461: https://github.com/cloudfoundry/bosh/pull/461

New AT&T OpenStack environment requires to use 'omit_default_port: true' option when connecting to the OpenStack API.",1.0,61575288,story,[],Allow optional OpenStack API connection params,756869.0,[756869],956238,1015645,feature,2013-12-10T22:10:44Z,https://www.pivotaltracker.com/story/show/61575288
2013-12-10T22:11:36Z,2013-12-02T08:53:16Z,accepted,,https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/uzrAP0Vtpv8/ez9J_FDBeFgJ,1.0,61709144,story,[],answer bosh-users question about final releases requiring a blob store sync,1338772.0,[1338772],956238,494053,feature,2013-12-10T22:11:36Z,https://www.pivotaltracker.com/story/show/61709144
2013-12-10T22:21:49Z,2013-11-27T20:36:22Z,accepted,,Running 'bosh stemcells' could show which are in use by adding a star.,1.0,61577422,story,[],A user can easily find out which stemcells on a director are in use.,1068489.0,[1068489],956238,1406536,feature,2013-12-10T22:21:49Z,https://www.pivotaltracker.com/story/show/61577422
2013-12-11T16:38:03Z,2013-10-14T21:59:42Z,accepted,,"Includes light stemcell
",1.0,58846872,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Build Centos/AWS Stemcells in CI,1068489.0,[1068489],956238,14062,feature,2013-12-11T16:38:04Z,https://www.pivotaltracker.com/story/show/58846872
2013-12-11T16:40:36Z,2013-10-14T21:59:35Z,accepted,,,2.0,58846858,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Run BATS against Centos/AWS once using a Centos/AWS Micro,1406536.0,[1406536],956238,14062,feature,2013-12-11T16:40:37Z,https://www.pivotaltracker.com/story/show/58846858
2013-12-11T16:41:27Z,2013-10-28T21:54:53Z,accepted,,"default should be 30sec

duration between iterations - not total time. 

(also, not in retryable)",2.0,59709584,story,[],All exponential backoffs should cap for 32 seconds for AWS cpi,81882.0,[81882],956238,14062,feature,2013-12-11T16:41:28Z,https://www.pivotaltracker.com/story/show/59709584
2013-12-11T21:20:01Z,2013-12-09T20:16:04Z,accepted,,,1.0,62181872,story,[],check nested resources for vcenter 5.1,81882.0,[81882],956238,637633,feature,2013-12-11T21:20:01Z,https://www.pivotaltracker.com/story/show/62181872
2013-12-11T22:24:26Z,2013-12-11T20:37:01Z,accepted,,See #62341856,,62342136,story,[],Fix CI blobstore,1068489.0,[1068489],956238,1068489,chore,2013-12-11T22:24:27Z,https://www.pivotaltracker.com/story/show/62342136
2013-12-11T23:50:57Z,2013-12-05T18:13:46Z,accepted,,This story is an internal request from PM. See https://www.pivotaltracker.com/story/show/61997058,1.0,61997592,story,[],Stemcell component list,637633.0,[637633],956238,548715,feature,2013-12-12T17:39:13Z,https://www.pivotaltracker.com/story/show/61997592
2013-12-12T18:57:37Z,2013-11-21T23:13:34Z,accepted,,"The following error can be reproduced by setting the Microbosh's CPU count to more than the number of any host in the vSphere cluster while DRS is enabled. This error also happens when cluster DRS is set to manual.

Creating VM from...                 |oooo                    | 2/11 00:00:49  ETA: 00:01:34at depth 0 - 20: unable to get local issuer certificate
Creating VM from...                 |oooo                    | 2/11 00:00:54  ETA: 00:01:29/Users/pivotal/workspace/bosh/bosh_vsphere_cpi/lib/cloud/vsphere/client.rb:161:in `power_on_vm': undefined method `task' for nil:NilClass (NoMethodError)
	from /Users/pivotal/workspace/bosh/bosh_vsphere_cpi/lib/cloud/vsphere/cloud.rb:258:in `block in create_vm'
	from /Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_formatter.rb:46:in `with_thread_name'
	from /Users/pivotal/workspace/bosh/bosh_vsphere_cpi/lib/cloud/vsphere/cloud.rb:164:in `create_vm'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:244:in `create_vm'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:123:in `block in create'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:79:in `step'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:122:in `create'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
	from /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/cli/commands/micro.rb:182:in `perform'
	from /Users/pivotal/workspace/bosh/bosh_cli/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/workspace/bosh/bosh_cli/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/workspace/bosh/bosh_cli/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/workspace/bosh/bosh_cli/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/workspace/bosh/bin/bosh:16:in `load'
	from /Users/pivotal/workspace/bosh/bin/bosh:16:in `<main>'",2.0,61252764,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",As a BOSH operator I would like to see better error messages when VM poweron fails due to vSphere DRS placement issues,506755.0,[506755],956238,568617,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61252764
2013-12-12T21:45:35Z,2013-12-12T20:25:34Z,accepted,,Required for #58846902 to be run in parallel,,62417266,story,[],Set up route53 for ci3 account,1183746.0,[1183746],956238,1183746,chore,2013-12-12T21:45:36Z,https://www.pivotaltracker.com/story/show/62417266
2013-12-13T02:36:21Z,2013-12-12T18:59:08Z,accepted,,,,62409566,story,[],create bosh_environment for embarcadero,1406536.0,[1406536],956238,1406536,chore,2013-12-13T02:36:22Z,https://www.pivotaltracker.com/story/show/62409566
2013-12-13T21:58:13Z,2013-11-20T19:25:08Z,accepted,,ssh-keygen -R exits non zero if the file isn't there.,,61155830,story,[],ssh bats test fails if there is no known-hosts file,1406536.0,[1406536],956238,46031,bug,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61155830
2013-12-13T21:58:28Z,2013-11-25T17:32:21Z,accepted,,"Currently BATs tries to delete the light stemcell which was used to create a currently running micro. This is a problem if you attempt to run BATs against an existing installation because the AMI used in your deployment will no longer exist

To accept:
1. Tammer takes aws stemcell 1525 and deploys a micro with that
2. Tammer uploads a light stemcell to micro director:
bosh upload stemcell https://s3.amazonaws.com/bosh-jenkins-artifacts/bosh-stemcell/aws/light-bosh-stemcell-1525-aws-xen-ubuntu.tgz
3. Tammer deletes the light stemcell:
bosh delete stemcell  bosh-aws-xen-ubuntu 1525
4. Tammer sees the special string NoOP appears in the debug task log:
bosh task last --debug",,61408126,story,[],BATs Deletes Light Stemcells,1406536.0,[1406536],956238,1406536,bug,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61408126
2013-12-18T22:03:11Z,2013-12-16T18:51:12Z,accepted,,,,62578118,story,[],On board Rob as anchor,1068489.0,[1068489],956238,1068489,chore,2013-12-18T22:03:12Z,https://www.pivotaltracker.com/story/show/62578118
2013-12-18T23:52:25Z,2013-12-13T22:03:30Z,accepted,,See: https://www.pivotaltracker.com/story/show/62572390 for acceptance,,62492004,story,[],Update Load Balancing configuration in some envs,1338772.0,[1338772],956238,1408486,bug,2014-01-08T18:20:56Z,https://www.pivotaltracker.com/story/show/62492004
2013-12-20T19:43:18Z,2013-11-18T18:11:31Z,accepted,,"We need this for our deployment scripts

-Runtime",2.0,60972862,story,[],Things that need to be installed on the jumpbox,1068489.0,[1068489],956238,553935,feature,2013-12-20T19:43:18Z,https://www.pivotaltracker.com/story/show/60972862
2013-12-20T19:43:39Z,2013-12-18T19:23:28Z,accepted,,"As a result of this, we are currently unable to `bosh ssh` into any job on a1 or dijon (I'd also assume tabasco) without checking out an older version of deployments-aws first and ssh-adding that key.

We talked briefly with Jesse and Caleb, who suggested that we could use the microbosh on all CI environments to redeploy the bosh director with the new key.

I would categorize this as urgent, since no one is currently able to bosh ssh into any of the CI environments without this special knowledge/hack.",,62729052,story,[],BOSH director ssh keys in deployments-aws do not match actual ssh keys on the director,506755.0,[506755],956238,285989,bug,2013-12-20T19:43:40Z,https://www.pivotaltracker.com/story/show/62729052
2013-12-21T00:00:33Z,2013-10-14T22:00:06Z,accepted,,"BATS must pass (duh), and do not promote it.",1.0,58846902,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Run BATS against Centos/AWS stemcell in CI,1068489.0,[1068489],956238,14062,feature,2013-12-21T00:00:35Z,https://www.pivotaltracker.com/story/show/58846902
2013-12-26T17:34:33Z,2013-10-14T22:00:12Z,accepted,,,1.0,58846912,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Promote the Centos/AWS Stemcell in CI,1068489.0,[1068489],956238,14062,feature,2013-12-26T17:34:28Z,https://www.pivotaltracker.com/story/show/58846912
2013-12-26T17:36:10Z,2013-10-14T22:03:56Z,accepted,,,2.0,58847170,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",Build Centos/OpenStack Stemcell once,,[],956238,14062,feature,2013-12-26T17:36:05Z,https://www.pivotaltracker.com/story/show/58847170
2013-12-26T17:36:18Z,2013-12-09T22:16:55Z,accepted,,Bringing up a CentOS/AWS stemcell consumes 100% of CPU using a m1.small AWS instance.,,62192238,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",high resource consumption on an CentOS stemcell for AWS,1183746.0,[1183746],956238,1406536,bug,2013-12-26T17:36:13Z,https://www.pivotaltracker.com/story/show/62192238
2013-12-26T18:14:42Z,2013-11-20T19:21:16Z,accepted,,"https://gist.github.com/mreider/82021ab8e94d60e022d5

The bottom line is that if you look at line 91 here: https://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/lib/cloud/vsphere/cloud.rb

You will see that we are raising an error using the variable ""state"" - which contains nothing more than the word ""error."" - we need to include the entire object so we can get useful information and debug a core issue.",,61155578,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Dump an entire object (not just 'error') when logging vSphere errors,506755.0,[506755],956238,548715,bug,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/61155578
2013-12-26T22:16:28Z,2013-10-04T17:25:36Z,accepted,,"When using src_alt, files are picked up from both src and src_alt.  This is extremely confusing when you delete a file in your local copy you've symlinked into src_alt, build a bosh release, deploy it, and find that the file you deleted is still being deployed.
",2.0,58255776,story,[],"Make src_alt override, not merge src files",81882.0,[81882],956238,46031,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/58255776
2013-12-26T22:16:47Z,2013-12-19T00:34:59Z,accepted,,Lucas Marks and I discussed this with Dmitriy. It seems that there's a check in the BOSH agent that will short circuit setup if the disk is not a block device. One consequence of this is that the /var/vcap/data/sys/run directory is never created. This is causing our cf-mysql-release mysql job to fail to start.,1.0,62749248,story,[],Bosh Agent does not create /var/vcap/data/sys/run directory on bosh-lite (warden-cpi),1406536.0,[1406536],956238,285989,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/62749248
2013-12-26T22:25:15Z,2013-12-12T22:37:00Z,accepted,,,,62427108,story,[],Account maintenance,1068489.0,[1068489],956238,1068489,chore,2013-12-26T22:25:15Z,https://www.pivotaltracker.com/story/show/62427108
2013-12-26T22:35:56Z,2013-11-27T00:42:40Z,accepted,,"When deploying a deployment with ""parallel_job_deoloy"" set to true, a job can specify that it should be deployed serially.

then this job must complete before proceeding to other jobs further below in the deployment manifest.",4.0,61520702,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]",user should be able to specify `serial: true` in job/deployment update section to deploy job/deployment serially,81882.0,[81882],956238,668075,feature,2013-12-26T22:36:09Z,https://www.pivotaltracker.com/story/show/61520702
2013-12-26T22:36:06Z,2013-12-20T22:03:51Z,accepted,,"...for now!

If a deployment that bosh manages says ""serial: false"" in the ""deploy"" section, then it will deploy in parallel.

This is intended to allow release deployers to test out parallel deployments to find issues and bugs before we make it the default across the board.",0.0,62867494,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]",Bosh should default to deploying jobs in serial,81882.0,[81882],956238,14062,feature,2013-12-26T22:36:09Z,https://www.pivotaltracker.com/story/show/62867494
2013-12-26T23:26:44Z,2013-12-19T22:00:01Z,accepted,,"When running bosh deploy, if I also run ""watch 'bosh vms'"", once the director hits 500 tasks it cleans up the earliest one i.e. the deploy.  And the deploy stops. ",1.0,62807592,story,[],Bosh should never garbage collect a running task,756869.0,[756869],956238,1097582,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/62807592
2013-12-27T19:03:24Z,2013-12-23T17:36:10Z,accepted,,It would be nice to be able to actually debug problems on instances after a failed run and just clean them up before a test run as we do on AWS and vSphere.  This would allow us to stop tearing down VMs in an :after hook across the board.,,62920930,story,[],Clean up Instances before BATs tests on OpenStack,81882.0,[81882],956238,1068489,chore,2013-12-27T19:03:24Z,https://www.pivotaltracker.com/story/show/62920930
2013-12-28T00:37:49Z,2013-12-23T17:37:39Z,accepted,,,,62920990,story,[],update to rspec3,81882.0,[81882],956238,81882,chore,2013-12-28T00:37:49Z,https://www.pivotaltracker.com/story/show/62920990
2013-12-28T00:37:56Z,2013-12-27T18:49:29Z,accepted,,"After #62920930, all infrastructures clean up instances before runs.  This will give us the ability to trouble shoot failed directors between runs",,63021050,story,[],Stop cleaning up instances after BATs runs,81882.0,[81882],956238,81882,chore,2013-12-28T00:37:57Z,https://www.pivotaltracker.com/story/show/63021050
2013-12-31T19:59:49Z,2013-12-30T19:18:29Z,accepted,,,,63079696,story,[],Investigate RSpec3 custom matcher changes,756869.0,[756869],956238,1068489,chore,2013-12-31T20:00:55Z,https://www.pivotaltracker.com/story/show/63079696
2014-01-02T21:49:59Z,2013-10-14T22:03:50Z,accepted,,,2.0,58847158,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",Boot a Centos/OpenStack Stemcell,1068489.0,[1068489],956238,14062,feature,2014-05-21T21:56:09Z,https://www.pivotaltracker.com/story/show/58847158
2014-01-02T21:51:52Z,2013-12-19T18:20:48Z,accepted,,see #62791468,,62792078,story,[],Rotate access keys,1068489.0,[1068489],956238,1338772,chore,2014-01-02T21:51:53Z,https://www.pivotaltracker.com/story/show/62792078
2014-01-02T21:55:26Z,2013-12-24T01:23:28Z,accepted,,,1.0,62938840,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Investigate migrating from Ubuntu to CentOS on bosh micro deploy --update,756869.0,[756869],956238,756869,feature,2014-01-02T21:55:27Z,https://www.pivotaltracker.com/story/show/62938840
2014-01-03T00:11:10Z,2013-12-03T17:32:08Z,accepted,,"Travis is failing to download the debugger gem for ruby 1.9.3.

Figure out why we have this for rubies < 2.0.0 and if it's not actually important, remove it.",,61831052,story,[],Figure out why we're including the debugger gem for ruby < 2.0,81882.0,[81882],956238,506755,chore,2014-01-03T00:11:11Z,https://www.pivotaltracker.com/story/show/61831052
2014-01-03T18:30:33Z,2013-10-14T22:03:45Z,accepted,,,2.0,58847150,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",Run BATS against Centos/OpenStack once using a Centos/AWS Micro,1406536.0,[1406536],956238,14062,feature,2014-01-03T18:30:33Z,https://www.pivotaltracker.com/story/show/58847150
2014-01-03T22:07:04Z,2013-10-14T22:03:39Z,accepted,,,1.0,58847144,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]","Build Centos/OpenStack Stemcells in CI
",1068489.0,[1068489],956238,14062,feature,2014-01-03T22:07:04Z,https://www.pivotaltracker.com/story/show/58847144
2014-01-03T22:27:39Z,2013-12-11T17:47:03Z,accepted,,Record as a comment here any stories that would need to be written to support nested resources under 5.0.,2.0,62328190,story,[],check nested resources for vcenter 5.0,81882.0,[81882],956238,14062,feature,2014-01-03T22:27:40Z,https://www.pivotaltracker.com/story/show/62328190
2014-01-06T18:17:24Z,2013-12-12T19:34:31Z,accepted,,"Local build stemcells (without candidate build number) use the ReleaseCreator class, which automatically attempts to create a final release.",,62412334,story,[],Local stemcell builds should not create final release,756869.0,[756869],956238,1183746,chore,2014-01-06T18:17:25Z,https://www.pivotaltracker.com/story/show/62412334
2014-01-06T23:09:17Z,2013-12-04T04:41:43Z,accepted,,"i.e. 1.5.0.pre.1478 would become 1.1478.0

This leaves with the flexibility to:
- Make backwards incompatible releases by manually bumping the MAJOR version
- Use the PATCH version to make bug/security fixes retroactively (especially useful for on-prem?)

From semver.org:

Given a version number MAJOR.MINOR.PATCH, increment the:

MAJOR version when you make incompatible API changes,
MINOR version when you add functionality in a backwards-compatible manner, and
PATCH version when you make backwards-compatible bug fixes.
Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.",2.0,61872478,story,[],BOSH's gems should use the build number as the MINOR version instead of a pre suffix to more accurately describe our workflow,1068489.0,[1068489],956238,5637,feature,2014-01-06T23:09:17Z,https://www.pivotaltracker.com/story/show/61872478
2014-01-09T17:41:44Z,2014-01-03T00:20:27Z,accepted,,,,63182482,story,[],cache rsyslog package in centos mirror instead of relying on external mirror,81882.0,[81882],956238,81882,chore,2014-01-09T17:41:44Z,https://www.pivotaltracker.com/story/show/63182482
2014-01-09T22:06:58Z,2014-01-03T19:32:31Z,accepted,,P/C has added neutron networking to our cluster. Manual networking should work now. Test it out.,1.0,63220956,story,[],make PistonCloud manual networking work on CentOS,1068489.0,[1068489],956238,637633,feature,2014-01-09T23:54:44Z,https://www.pivotaltracker.com/story/show/63220956
2014-01-09T22:17:35Z,2013-10-14T22:03:34Z,accepted,,,1.0,58847140,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",Run BATS against Centos/OpenStack stemcell in CI,1068489.0,[1068489],956238,14062,feature,2014-01-09T22:17:36Z,https://www.pivotaltracker.com/story/show/58847140
2014-01-10T18:25:43Z,2014-01-03T18:31:54Z,accepted,,Improve acceptance auto deploy in CI,1.0,63217652,story,[],Deploy full CentOS BOSH on AWS via CI,1068489.0,[1068489],956238,1068489,feature,2014-01-10T18:25:44Z,https://www.pivotaltracker.com/story/show/63217652
2014-01-10T23:36:38Z,2014-01-09T21:45:57Z,accepted,,"Spend the day with Greg, helping him deploy a micro bosh *without* using the bootstrap tools.",0.0,63572812,story,[],Teach Greg how to deploy a micro BOSH,637633.0,[637633],956238,14062,feature,2014-01-10T23:36:39Z,https://www.pivotaltracker.com/story/show/63572812
2014-01-13T23:31:45Z,2014-01-06T23:39:49Z,accepted,,"Acceptance is to see BATS run against a micro that was upgraded from ubuntu to cents.
",2.0,63343886,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Investigate migrating from ubuntu to centos in production AGAIN.,1338772.0,[1338772],956238,14062,feature,2014-01-13T23:31:58Z,https://www.pivotaltracker.com/story/show/63343886
2014-01-13T23:32:38Z,2014-01-10T23:37:05Z,accepted,,without using the bootstrap tools,1.0,63652524,story,[],Show Greg how to deploy a full bosh,14062.0,[14062],956238,756869,feature,2014-01-13T23:32:51Z,https://www.pivotaltracker.com/story/show/63652524
2014-01-13T23:32:51Z,2014-01-10T23:37:29Z,accepted,,from scratch and include deploying to full bosh,1.0,63652544,story,[],show greg how to create a simple release,756869.0,[756869],956238,14062,feature,2014-01-13T23:33:04Z,https://www.pivotaltracker.com/story/show/63652544
2014-01-13T23:34:23Z,2013-10-14T22:03:29Z,accepted,,,1.0,58847130,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",Promote the Centos/OpenStack Stemcell in CI,1068489.0,[1068489],956238,14062,feature,2014-01-13T23:34:37Z,https://www.pivotaltracker.com/story/show/58847130
2014-01-13T23:34:27Z,2013-11-13T22:41:24Z,accepted,,,,60733634,story,"[{'name': 'centos on openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034894, 'updated_at': '2013-11-14T21:07:12Z'}]",CentOS on OpenStack finished,,[],956238,14062,release,2014-01-13T23:34:40Z,https://www.pivotaltracker.com/story/show/60733634
2014-01-14T00:03:24Z,2013-12-10T19:25:49Z,accepted,,"if the manifest does not have root properties specified, bosh deploy diffing will blow up with calling 'NoMethodError: calling summary on nil...' Quite few people run into this scenario.",,62258438,story,[],bosh deploy should not raise an error when manifest does not have root properties,81882.0,[81882],956238,81882,bug,2014-01-14T00:03:37Z,https://www.pivotaltracker.com/story/show/62258438
2014-01-14T00:37:40Z,2014-01-09T17:53:06Z,accepted,,"A job was deleted upstream. When we deploy from the new manifest, we get a stack trace from inside the bosh-director gem:

E, [2014-01-09T17:38:39.938733 #2976] [task:3088] ERROR -- : wrong number of arguments (1 for 2)
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/instance_deleter.rb:17:in `delete_instances'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/deployment_plan/assembler.rb:428:in `delete_unneeded_instances'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/deployment_plan/updater.rb:38:in `update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/jobs/update_deployment.rb:55:in `update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/jobs/update_deployment.rb:81:in `block in perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/jobs/update_deployment.rb:74:in `perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/job_runner.rb:98:in `perform_job'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.1750.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1750.0/bin/bosh-director-worker:76:in `<top (required)>'",,63553464,story,[],Error when deleting instances during deployment,81882.0,[81882],956238,5637,bug,2014-01-14T00:37:57Z,https://www.pivotaltracker.com/story/show/63553464
2014-01-14T17:42:25Z,2014-01-13T18:49:49Z,accepted,,,,63742270,story,[],Yulia deploys a dummy release,1183746.0,[1183746],956238,1183746,chore,2014-01-14T17:42:26Z,https://www.pivotaltracker.com/story/show/63742270
2014-01-14T19:19:49Z,2013-10-31T01:14:16Z,accepted,,To avoid a situation where BOSH deploys a new stemcell and an on-call person accidentally inherits a stemcell deploy at 3am!,0.0,59881688,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]","Other teams' deployments should not refer to ""latest"" stemcell, use a specific version instead",,[],956238,5637,feature,2014-01-14T19:19:49Z,https://www.pivotaltracker.com/story/show/59881688
2014-01-14T20:07:57Z,2013-11-04T18:09:20Z,accepted,,,0.0,60100820,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",Audit load balancers in prod and add any that are missing to https://github.com/cloudfoundry/prod-aws/wiki/Load-Balancers,,[],956238,5637,feature,2014-01-14T20:07:57Z,https://www.pivotaltracker.com/story/show/60100820
2014-01-15T00:28:03Z,2014-01-06T23:37:48Z,accepted,,"This is required to migrate from ubuntu to cents.

To accept, see story #63343886 (previously the upgrade would fail due to postgresql's data directory having incorrect permissions).


To accept:
- Tammer deploys a new Ubuntu AWS micro version 1750:
cd tammer/deployments
bosh micro deployment micro
bosh micro delete
bosh micro deploy ~/Downloads/light-bosh-stemcell-1750-aws-xen-ubuntu.tgz

- (When commit 65e7c29 passes CI) Tammer migrates the ubuntu micro to centos:
cd tammer/deployments
bosh micro deployment micro
bosh micro deploy --update ~/Downloads/light-bosh-stemcell-<TBD>-aws-xen-centos.tgz

- Tammer runs BAT against his new micro:
cd bosh
bundle exec rake spec:system:existing_micro[aws,centos]",1.0,63343746,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",postgresql job on microbosh should chown -R its data directory on start,1338772.0,[1338772],956238,14062,feature,2014-01-15T00:28:04Z,https://www.pivotaltracker.com/story/show/63343746
2014-01-15T19:45:42Z,2014-01-15T00:38:15Z,accepted,,OpenStack BATs flakily fails with and Excon::ConnectionTimeout error when clusters are busy.  Allow setting this property in the MicroBOSH manifest when running BATs.,,63854098,story,[],Allow configuration for Excon connection timeout in Openstack BATs,1068489.0,[1068489],956238,1068489,chore,2014-01-15T19:45:43Z,https://www.pivotaltracker.com/story/show/63854098
2014-01-15T22:38:58Z,2014-01-09T20:00:49Z,accepted,,"see the following shell session:
$ bosh upload stemcell bosh-stemcell-1750-vsphere-esxi-ubuntu.tgz --bad-option
invalid option: --bad-optionUsage: bosh        upload stemcell <stemcell_location>
$ echo $?
0

This should be returning a non-zero exit code",,63565060,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",bosh stemcell upload command returns 0 exit status even when there is an error,756869.0,[756869],956238,786819,bug,2014-01-15T22:38:56Z,https://www.pivotaltracker.com/story/show/63565060
2014-01-15T23:29:44Z,2014-01-15T20:03:48Z,accepted,,This was missed out with #60722770. Backfilling.,,63916864,story,[],Deployment op can omit a job-level release if all templates specify a release,506755.0,[506755],956238,506755,chore,2014-01-15T23:29:45Z,https://www.pivotaltracker.com/story/show/63916864
2014-01-15T23:30:28Z,2014-01-15T20:22:42Z,accepted,,Rearrange jobs in CI to better parallelize with number of slaves/executors that we have.,,63918326,story,[],CI Job cleanup,1068489.0,[1068489],956238,1068489,chore,2014-01-15T23:30:29Z,https://www.pivotaltracker.com/story/show/63918326
2014-01-16T00:54:37Z,2013-12-26T23:28:27Z,accepted,,"Example output below (see last line). if binding configuration takes 1 sec then total duration goes up by a sec.

→ bosh deploy
Getting deployment properties from director...
Compiling deployment manifest...
Detecting changes in deployment...

Release
No changes

Releases
No changes

Compilation
No changes

Update
No changes

Resource pools
No changes

Networks
No changes

Jobs
dummy3
  update
    removed serial: true
dummy6
  update
    removed serial: true

Properties
dummy_with_properties
  changed echo_value:
    - whatever3
    + whatever4

Please review all changes carefully
Deploying `manifest.yml' to `ec-dk-bosh' (type 'yes' to continue): yes

Director task 209

Preparing deployment
  binding deployment (00:00:00)
  binding releases (00:00:00)
  binding existing deployment (00:00:00)
  binding resource pools (00:00:00)
  binding stemcells (00:00:00)
  binding templates (00:00:00)
  binding properties (00:00:00)
  binding unallocated VMs (00:00:00)
  binding instance networks (00:00:00)
Done                    9/9 00:00:00

Preparing package compilation

Preparing DNS
  binding DNS (00:00:00)
Done                    1/1 00:00:00

Preparing configuration
  binding configuration (00:00:00)
Done                    1/1 00:00:00

  Started updating job dummy2: dummy2/0 (canary)
  Started updating job dummy3: dummy3/0 (canary)
  Started updating job dummy1: dummy1/0 (canary)
  Started updating job dummy5: dummy5/0 (canary)
  Started updating job dummy4: dummy4/0 (canary)
  Started updating job dummy7: dummy7/0 (canary)
  Started updating job dummy6: dummy6/0 (canary)
  Started updating job dummy8: dummy8/0 (canary)
     Done updating job dummy2: dummy2/0 (canary)
     Done updating job dummy6: dummy6/0 (canary)
     Done updating job dummy7: dummy7/0 (canary)
     Done updating job dummy1: dummy1/0 (canary)
     Done updating job dummy5: dummy5/0 (canary)
     Done updating job dummy3: dummy3/0 (canary)
     Done updating job dummy4: dummy4/0 (canary)
     Done updating job dummy8: dummy8/0 (canary)

Task 209 done
Started		2013-12-23 21:11:20 UTC
Finished	2013-12-23 21:11:20 UTC
Duration	00:00:00

Deployed `manifest.yml' to `ec-dk-bosh'
",,62996410,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]",bosh cli should report correct total duration,81882.0,[81882],956238,81882,bug,2014-01-16T00:54:35Z,https://www.pivotaltracker.com/story/show/62996410
2014-01-16T17:45:42Z,2013-11-08T18:38:08Z,accepted,,"output at: http://172.16.68.6:8080/job/promote_artifacts/161/console

we failed a promote artifacts due to a rogue push to master, on retry it failed because we had already push the 'pre' reversion to rubygems.

Pushing gem to https://rubygems.org...
Repushing of gem versions is not allowed.
Please use `gem yank` to remove bad gem releases.
Failed to promote: bosh_common-1.5.0.pre.1278.gem
",,60427770,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",promote artifacts should push to github before it pushes to rubygems.com (rubygems prevents pushing same gem version),1406536.0,[1406536],956238,637633,chore,2014-01-16T17:45:42Z,https://www.pivotaltracker.com/story/show/60427770
2014-01-16T22:35:16Z,2013-11-13T20:21:28Z,accepted,,"Specifying an incorrect release & release-job combination should result in a release-job not found error.
Sepcifying multiple releases errors out.
Specifying the release at the deployment-job level AND the release-job level errors out.
Specifying the release at the deployment-job level applies to all the release-jobs underneath it (to be backwards compatible).

From:

Job:
- name: ccng
  release: cf
- template: 
  - nats
  - agent

To:

Job:
- name: ccng
- templates: 
  - name: nats
    release: cf
  - name: agent
    release: cf

Examples:

release: cf
Job:
- name: ccng
- templates: 
  - name: nats
  - name: agent

# Invalid: errors out.
Job:
- name: ccng
  release: cf
- templates: 
  - nats
  - name: agent

# Errors out.
Job:
- name: ccng
  release: cf
- template: 
  ...
- templates: 
  ...

# Deprecation: use new ""templates"" and hash syntax.
Job:
- name: ccng
  release: cf
- template: 
  - nats
  - agent

# Error, use of hash syntax with plural ""templates"" key.
Job:
- name: ccng
  release: cf
- templates: 
  - nats
  - agent

Job:
- name: ccng
  release: cf
- templates: 
  - name: nats
  - name: agent

Job:
- name: ccng
- templates: 
  - name: nats
    release: cf
  - name: agent
    release: cf

Job:
- name: ccng
  release: cf
- templates: 
  - name: nats
  - name: agent
    release: bar

Ignore the deprecations - those are specified in another story.",2.0,60722770,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Deployment Op can specify the release a job template comes from,1068489.0,[1068489],956238,14062,feature,2014-01-16T22:35:16Z,https://www.pivotaltracker.com/story/show/60722770
2014-01-16T23:12:36Z,2013-12-09T22:18:26Z,accepted,,"Sent from the community pair to merge the following PR:
https://github.com/cloudfoundry/bosh/pull/466",,62192362,story,[],micro deploy --update where only disk_cid exists doesn't work on AWS,81882.0,[81882],956238,1080034,bug,2014-01-16T23:12:36Z,https://www.pivotaltracker.com/story/show/62192362
2014-01-16T23:25:48Z,2013-10-28T18:48:59Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/447

I was unable to `bosh aws create` in the AWS `eu-west-1` region. It turned out that the `ec2_endpoint` needed to be set. Using this commit I was able to deploy microbosh in the `eu-west-1` region.

Filed by tader",1.0,59694234,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #447: Fix `bosh aws create` for multiple regions by setting ec2_endpoint.,1068489.0,[1068489],956238,1134058,feature,2014-05-21T21:55:09Z,https://www.pivotaltracker.com/story/show/59694234
2014-01-17T22:09:32Z,2014-01-07T04:37:01Z,accepted,,"Current code (below) does not guarantee that multiple processes read 0 records, save uuid and end up with multiple uuid.

(bosh/bosh-director/lib/bosh/director/config.rb:62)
def retrieve_uuid
        directors = Bosh::Director::Models::DirectorAttribute.all
        director = directors.first

        if directors.size > 1
          @logger.warn(""More than one UUID stored in director table, using #{director.uuid}"")
        end

        unless director
          director = Bosh::Director::Models::DirectorAttribute.new
          director.uuid = gen_uuid
          director.save
          @logger.info(""Generated director UUID #{director.uuid}"")
        end

        director.uuid
      end",,63353700,story,[],director should properly lock director_attributes table before creating new director guid,1183746.0,[1183746],956238,81882,chore,2014-01-17T22:09:33Z,https://www.pivotaltracker.com/story/show/63353700
2014-01-17T22:37:41Z,2014-01-15T21:58:53Z,accepted,,We may be able to get some massive BATs speed increases if tests can be parallelized. ,,63925962,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Investigate parallelizing BATs,81882.0,[81882],956238,1068489,chore,2014-01-18T02:04:36Z,https://www.pivotaltracker.com/story/show/63925962
2014-01-17T22:37:46Z,2014-01-16T22:51:53Z,accepted,,,,64012688,story,[],Clean up volumes at start of BAT run along with instances for openstack,81882.0,[81882],956238,1068489,chore,2014-01-17T22:38:02Z,https://www.pivotaltracker.com/story/show/64012688
2014-01-17T23:20:34Z,2013-11-13T20:21:28Z,accepted,,"resource_pool:
- templates:
  - name: foo
    release: cf
  - <<&:shared_templates

Document in the public docs, and email to the bosh-users mailing list.",1.0,60722778,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}, {'name': '☢☢☢ blocked ☢☢☢', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034570, 'updated_at': '2014-01-08T18:38:05Z'}]",Document how to use yaml lookbacks to include a shared set of release jobs in a deploy job,756869.0,[756869],956238,14062,feature,2014-01-17T23:20:33Z,https://www.pivotaltracker.com/story/show/60722778
2014-01-20T17:25:30Z,2013-12-20T16:37:38Z,accepted,,"We are experiencing isses within customers (of PCF) who want to reference networks, resource pools, clusters, etc. within folders. Storing the 'path' of the object in vCenter terms is problematic. Storing the object ID means that it doesn't matter what the logical organization is.

We would also like to know, for each of these objects, if BOSH supports 1, or an array:

Cluster, Resource Pool, Network, Datacenter

Document both what bosh expects in the deploy manifest, and what it stores internally.  Document this in a single page in cf-docs.",2.0,62847488,story,"[{'name': 'documentation', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-05T18:20:33Z', 'id': 7646972, 'updated_at': '2014-02-05T18:20:33Z'}, {'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Document how vSphere objects are stored and referenced.,756869.0,[756869],956238,548715,feature,2014-03-13T15:54:26Z,https://www.pivotaltracker.com/story/show/62847488
2014-01-20T18:56:59Z,2014-01-20T18:41:02Z,accepted,,"It was just the resource pool increase. No code change.
See task 12746 on a1.

We needed to cancel it from a separate terminal.",,64172272,story,[],Investigate deployment to A1 hanging,14062.0,[14062],956238,81882,chore,2014-01-20T18:56:59Z,https://www.pivotaltracker.com/story/show/64172272
2014-01-20T20:54:59Z,2013-12-19T18:46:00Z,accepted,,"Only bosh_cli changes.

current output:

```
  Started updating job loggregator_z2: loggregator_z2/0 (canary)                                                            <-------------------------------- stage's task (start)
     Done updating job loggregator_z2: loggregator_z2/0 (canary)                                                              <-------------------------------- stage's task (end)
```

expected output:

```
  Started updating job loggregator_z2                                                                                             <-------------------------------- stage (start)
  Started updating job loggregator_z2: loggregator_z2/0 (canary)
     Done updating job loggregator_z2: loggregator_z2/0 (canary) (00:01:05)
  Started updating job loggregator_z2: loggregator_z2/1                                                            <-------------------------------- stage's task (start)
     Done updating job loggregator_z2: loggregator_z2/1 (00:00:35)                                          <-------------------------------- stage's task (end)
     Done updating job loggregator_z2 (00:01:40)                                                                           <-------------------------------- stage (end)
```",1.0,62794030,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]",user should be able to see how long it took to update a job and a specific instance,756869.0,[756869],956238,81882,feature,2014-01-20T20:54:59Z,https://www.pivotaltracker.com/story/show/62794030
2014-01-20T21:34:04Z,2013-11-13T20:21:28Z,accepted,,"To accept:

- Tammer targets a director of version <TBD: insert build number when CI >= 1779 goes green>
- Tammer uploads release dummy-release.tgz (attached to story)
- Tammer uploads release dummy2-release.tgz (attached to story)
- in the release part of the deployment manifest, tammer specifies this:

```
releases:
- name: dummy
  version: latest
- name: dummy2
  version: latest
```

- In the jobs part of the deployment manifest, tammer specifies this:

```
jobs:
- name: dummyjob
  instances: 1
  resource_pool: default
  templates:
  - name: dummy_with_package
    release: dummy
  - name: template2
    release: dummy2
  networks:
  - name: default
    # try removing this
    default: [dns, gateway]
```

When Tammer deploys, he should see error message:

Unable to deploy: package name collision in job definitions.
",2.0,60722772,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",The director refuses to deploy if two jobs or packages on the same VM share the same name,81882.0,[81882],956238,14062,feature,2014-01-20T21:34:04Z,https://www.pivotaltracker.com/story/show/60722772
2014-01-20T21:34:50Z,2014-01-15T18:31:53Z,accepted,,"On any 404 from the director, the bosh_cli should print the following:

The <NAME> bosh director doesn't understand the following API call: <URL>
The bosh deployment may need to be upgraded.",1.0,63908918,story,[],"When the bosh_cli receives a 404 from the director, give upgrade message to user",1068489.0,[1068489],956238,14062,feature,2014-01-20T21:34:51Z,https://www.pivotaltracker.com/story/show/63908918
2014-01-22T22:46:31Z,2014-01-22T19:53:21Z,accepted,,deployment artifacts are currently in the incorrect repository and the directory structure leaves much to be desired.,,64339478,story,[],Move and cleanup PM acceptance environment,1406536.0,"[1406536, 1338772]",956238,1406536,chore,2014-01-22T22:46:31Z,https://www.pivotaltracker.com/story/show/64339478
2014-01-22T22:58:55Z,2014-01-09T22:05:21Z,accepted,,"This was an oversight on our part, we update our scripts to require 1.2 without first requesting an update to the Jumpbox.

For acceptance, a runtime pair should make sure our deploy checklist works from A1's jumpbox.",1.0,63574208,story,[],Jumpbox should have go 1.2 installed for spiff & runtime's deploy,756869.0,[756869],956238,5637,feature,2014-01-22T22:58:55Z,https://www.pivotaltracker.com/story/show/63574208
2014-01-23T17:46:14Z,2014-01-21T21:49:29Z,accepted,,https://github.com/cloudfoundry/bosh/commit/d05783fabd92aa3bdbc9bb8b898131204486a853,,64269314,story,[],Merge bats typo fix by Kai into develop.,1068489.0,[1068489],956238,756869,bug,2014-01-23T17:46:15Z,https://www.pivotaltracker.com/story/show/64269314
2014-01-24T18:45:46Z,2013-12-23T18:46:45Z,accepted,,"We've had several people join CF since the current jumpboxes were created. Can we please recreate them to include the new keys?

Pair with @mattr to get this done and educate him on the process.",1.0,62924220,story,[],Rebuild jumpboxes to get new user keys,1068489.0,[1068489],956238,285989,feature,2014-01-24T18:45:47Z,https://www.pivotaltracker.com/story/show/62924220
2014-01-24T18:46:04Z,2013-08-13T16:43:18Z,accepted,,"Snapshotting syslog_aggregator on A1 failed due to an abnormally long snapshotting time.  After 47 minutes the 98 GiB snapshot timed out, and the deploy failed.  The snapshot quietly finished on AWS (snap-c51a0c91).


Gave following error when failing:

syslog_aggregator/0 (canary) |oo                 | 0/1 00:47:01  ETA: --:--:--
                                                                                
  syslog_aggregator/0 (canary) (00:47:02)

                                                                                
                             |ooooooooooooooooooo| 1/1 00:47:02  ETA: --:--:--
                                                                                
Done                          1/1 00:47:02

Error 100: Timed out waiting for snap-c51a0c91 to be completed, took 2817.966622625s

Task 3936 error",,55096726,story,"[{'name': 'aws-snapshots', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:44Z', 'id': 7034900, 'updated_at': '2014-01-08T18:20:03Z'}]",Snapshots should be more graceful with long running tasks,1406536.0,"[1406536, 756869]",956238,1068489,bug,2014-01-24T18:46:04Z,https://www.pivotaltracker.com/story/show/55096726
2014-01-24T20:03:34Z,2014-01-24T18:41:52Z,accepted,,,,64482450,story,[],Manually remove bosh releases 1836 and 1840,1406536.0,[1406536],956238,14062,chore,2014-01-24T20:03:35Z,https://www.pivotaltracker.com/story/show/64482450
2014-01-27T17:25:30Z,2013-12-10T22:01:47Z,accepted,2014-01-31T20:00:00Z,,,62270122,story,[],Jan 31st Release,,[],956238,1161568,release,2014-01-27T17:25:30Z,https://www.pivotaltracker.com/story/show/62270122
2014-01-27T17:25:31Z,2013-12-10T22:04:52Z,accepted,2014-01-17T20:00:00Z,,,62270368,story,[],Cut a Build for Jan 31st release,,[],956238,1161568,release,2014-01-27T17:25:32Z,https://www.pivotaltracker.com/story/show/62270368
2014-01-28T18:06:34Z,2014-01-28T16:15:19Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/81

1.1.2 got yanked from rubygems.org: http://rubygems.org/gems/net-ssh-gateway/versions

Filed by jtuchscherer",,64669124,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh-lite #81: Upgrading the net-ssh-gateway version in the Gemfile lock,46031.0,[46031],956238,1134058,chore,2014-01-28T18:06:33Z,https://www.pivotaltracker.com/story/show/64669124
2014-01-29T02:01:05Z,2014-01-23T22:47:50Z,accepted,,"CentOS Stemcell 1798 on AWS consumes 100%CPU. At syslog appears multiple times: ""Jan 23 17:44:52 ip-10-238-141-155 rsyslogd: imklog: error reading kernel log - shutting down: Bad file descriptor"". So I disabled ""imklog"" module and rsyslogd cpu consumption dropped off.",,64426276,story,[],rsyslog on CentOs stemcell consumes 100% CPU,1406536.0,[1406536],956238,1015645,bug,2014-01-29T02:01:05Z,https://www.pivotaltracker.com/story/show/64426276
2014-01-29T21:54:57Z,2014-01-29T18:59:10Z,accepted,,,,64764502,story,[],Fix BATs manifest templates to work with 0000 builds,506755.0,[506755],956238,506755,chore,2014-01-29T21:54:58Z,https://www.pivotaltracker.com/story/show/64764502
2014-01-30T01:58:44Z,2014-01-29T18:28:14Z,accepted,,,,64761788,story,[],Move manual openstack bats from hetzner to pistoncloud.,756869.0,[756869],956238,756869,chore,2014-01-30T01:58:44Z,https://www.pivotaltracker.com/story/show/64761788
2014-01-30T18:09:59Z,2013-11-13T20:21:28Z,accepted,,"acceptance criteria will be to deploy two jobs from different releases with package and job names that do not conflict.

To accept:
- Tammer examines <deployment-repo>/deployments/60722774_collocated_deployment/collocated_deployment.yml to verify that a single job contains templates from 
different releases

```yaml
jobs:
- name: etcd_and_redis
instances: 1
resource_pool: default
templates:
- name: etcd
release: etcd
- name: redis
release: redis
```

- Tammer sets deployment to <deployment-repo>/deployments/60722774_collocated_deployment/collocated_deployment.yml by doing:
`bosh deployment 60722774_collocated_deployment/collocated_deployment.yml`
- `bosh deploy` <- this command should succeed
- `redis-cli -h 172.16.68.17` <- mess around with the deployed redis
- `curl -v -L http://172.16.68.17:4001/v2/keys/mykey -XPUT -d value=""this story is awesome"" #put key into etcd `
- `curl -v -L http://172.16.68.17:4001/v2/keys/mykey #get key from etcd`",4.0,60722774,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Deploying co-located jobs from separate releases succeeds,1338772.0,[1338772],956238,14062,feature,2014-01-30T18:09:58Z,https://www.pivotaltracker.com/story/show/60722774
2014-01-30T23:27:08Z,2014-01-16T18:13:01Z,accepted,,"When we fail to deploy due to a name collision it could be helpful to tell the user what job causes the name collision and optionally which releases have name collisions in the case of packages. 

To accept, see story #60722772
The steps are identical, but the error message should match that in this story",1.0,63988466,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Add descriptive error message when refusing deploy of collocated jobs with name collision ,1338772.0,[1338772],956238,1406536,feature,2014-01-30T23:27:09Z,https://www.pivotaltracker.com/story/show/63988466
2014-02-03T17:39:49Z,2014-01-24T17:57:07Z,accepted,,"To accept:
- Greg does a full BOSH deploy with BOSH release 29 (bosh-1836.tgz), this should fail due to the mysql regression
- Greg deploys full BOSH 32 (bosh-1868.tgz), and the director starts up correctly",,64478642,story,[],#63353700 caused a regression breaking mysql by using a primary key ,1068489.0,[1068489],956238,1183746,bug,2014-02-03T17:39:49Z,https://www.pivotaltracker.com/story/show/64478642
2014-02-05T21:33:55Z,2014-01-30T18:52:08Z,accepted,,"(JZ in MS' voice) It might be worth checking the following scenario with two releases R1 and R2, each containing two jobs called foo and bar

### first deploy
```yaml
jobs:
 - name: chaos
   templates:
    - name: foo
      release: R1
    - name: bar
       release: R2
```

### second deploy

```yaml
jobs:
- name: chaos
  templates:
  - name: foo
    release: R2
  - name: bar
    release: R1
```

In side the releases, jobs are versioned as:
R1:
- name: foo
  version: 1.1-dev
- name: bar
  version: 3.1-dev

R2:
- name: foo
  version: 1.1-dev
- name: bar
  version: 3.1-dev

in other words, the jobs that got ""swapped"" happened to have the same version, but they should have entirely different content. We should verify that the correct jobs (processes) get started. And we cannot trust the ""get_state"" response from the agent because this potential bug is more about whether the agent has a false positive when deciding whether there was a cache hit before downloading a package or job template.",0.0,64847782,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Investigate collocated deployment edge cases  ,1406536.0,[1406536],956238,1406536,feature,2014-02-05T21:33:55Z,https://www.pivotaltracker.com/story/show/64847782
2014-02-06T00:21:01Z,2013-12-20T19:53:32Z,accepted,,,,62859876,story,[],Next Prod Deploy,1406536.0,"[1406536, 506755]",956238,1068489,chore,2014-02-06T00:21:01Z,https://www.pivotaltracker.com/story/show/62859876
2014-02-06T00:37:43Z,2013-12-02T23:45:19Z,accepted,,"Acceptance criteria:

demo",2.0,61777744,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Update microbosh stemcell in production,1406536.0,[1406536],956238,14062,feature,2014-02-06T00:37:44Z,https://www.pivotaltracker.com/story/show/61777744
2014-02-06T00:37:46Z,2013-10-14T22:06:23Z,accepted,,"Acceptance criteria:

demo",2.0,58847334,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",Update bosh stemcell in production,1406536.0,[1406536],956238,14062,feature,2014-02-06T00:37:46Z,https://www.pivotaltracker.com/story/show/58847334
2014-02-06T17:48:58Z,2014-02-03T18:09:52Z,accepted,,"CI master is out of disk space. It looks like it's being consumed in both /tmp and /var/lib/jenkins/jobs

This chore is to cover the investigation of what's not clearing up after itself in /tmp and also to see if we can delete some old jobs.",,65031572,story,[],Figure out why CI master is running out of space,1068489.0,"[1068489, 1183746]",956238,506755,chore,2014-02-06T17:48:57Z,https://www.pivotaltracker.com/story/show/65031572
2014-02-06T17:54:22Z,2013-12-04T00:15:03Z,accepted,,"For context, Jalapeño micro doesn't have its corresponding states checked into bosh-deployments.yml. This prevents us from updating the micro for acceptance.
At some point we should blow the jalapeno micro away and re-build it properly. And REMEMBER TO CHECK STUFF IN!!!",,61863184,story,[],Clean up acceptance environment micro BOSH,1406536.0,[1406536],956238,1338772,chore,2014-02-06T17:54:22Z,https://www.pivotaltracker.com/story/show/61863184
2014-02-06T19:56:21Z,2014-02-06T00:54:28Z,accepted,,See if we can keep it all in the VPC,,65250954,story,[],Investigate dropping vip from production microBOSH manifest,1406536.0,[1406536],956238,1406536,chore,2014-02-06T19:56:20Z,https://www.pivotaltracker.com/story/show/65250954
2014-02-06T23:18:23Z,2014-01-24T20:19:59Z,accepted,,,,64490236,story,[],Run integration tests using postgres AND mysql in Jenkins,756869.0,[756869],956238,1068489,chore,2014-02-06T23:18:22Z,https://www.pivotaltracker.com/story/show/64490236
2014-02-07T22:16:03Z,2013-11-13T22:37:38Z,accepted,,,,60733376,story,"[{'name': 'centos aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:12Z', 'id': 7034892, 'updated_at': '2013-11-14T21:07:12Z'}]",CentOS on AWS Finished,,[],956238,14062,release,2014-02-07T22:16:02Z,https://www.pivotaltracker.com/story/show/60733376
2014-02-07T23:30:11Z,2014-01-29T20:20:54Z,accepted,,Time box to 1 pair for 1 day.,1.0,64772116,story,[],Spike to determine difficulty of adding multiple network support to microbosh and deployerI,668075.0,[668075],956238,14062,feature,2014-02-07T23:30:10Z,https://www.pivotaltracker.com/story/show/64772116
2014-02-10T22:34:32Z,2014-02-10T17:54:34Z,accepted,,See #65494688,1.0,65490028,story,[],yaml check,786819.0,"[786819, 506755]",956238,1210852,feature,2014-02-10T22:34:32Z,https://www.pivotaltracker.com/story/show/65490028
2014-02-11T01:01:21Z,2014-02-05T18:29:09Z,accepted,,"Multiple AWS BATs runs running on same AWS account can end up with IP address collisions.

Currently, Aws::BatManifestGenerator just shells out to `bosh aws generate bat`, which always sets up IP ranges the same.",,65217978,story,[],Allow passing reserved IP ranges for AWS BATs runs,1068489.0,[1068489],956238,1068489,chore,2014-02-11T01:01:21Z,https://www.pivotaltracker.com/story/show/65217978
2014-02-11T01:33:35Z,2014-02-04T19:24:24Z,accepted,,,1.0,65135200,story,[],Rebuild jumpbox to add Tammer,756869.0,"[756869, 1123776]",956238,5637,feature,2014-02-11T01:33:36Z,https://www.pivotaltracker.com/story/show/65135200
2014-02-12T17:47:16Z,2013-08-08T17:59:45Z,accepted,,"Extracted from: #54635262

To accept, Gregg runs (Talk to Foley or Jesse for stuff in the angle brackets)
1. bosh target <the target>
2. bosh status
Step two should show that snapshots are enabled
",1.0,54857620,story,"[{'name': 'aws-snapshots', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:44Z', 'id': 7034900, 'updated_at': '2014-01-08T18:20:03Z'}, {'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]","Turn on snapshotting in Tabasco, A1 & prod",756869.0,"[756869, 1338772]",956238,1338772,feature,2014-02-12T17:47:17Z,https://www.pivotaltracker.com/story/show/54857620
2014-02-12T17:59:16Z,2014-01-16T17:48:08Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/488#issuecomment-32482858

This is from the LAMB team to better support JMX metrics. ",1.0,63986626,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Merge Pull Request: TSDB plugin sends deployment name as a tag,1338772.0,[1338772],956238,827921,feature,2014-02-12T17:59:17Z,https://www.pivotaltracker.com/story/show/63986626
2014-02-13T18:16:15Z,2014-01-08T18:20:44Z,accepted,,,,63477950,story,"[{'name': 'aws-snapshots', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T21:07:44Z', 'id': 7034900, 'updated_at': '2014-01-08T18:20:03Z'}]",Snapshots being taken on AWS in production,,[],956238,14062,release,2014-02-13T18:16:15Z,https://www.pivotaltracker.com/story/show/63477950
2014-02-13T20:05:49Z,2014-02-04T18:09:41Z,accepted,,,1.0,65126256,story,[],Allow `bosh upload stemcell --skip-if-exists`,668075.0,"[668075, 756869]",956238,668075,feature,2014-02-13T20:05:49Z,https://www.pivotaltracker.com/story/show/65126256
2014-02-13T23:29:36Z,2014-01-03T18:13:34Z,accepted,,"Print deprecation warning in red during manifest parsing.

# Deprecation: Please use ""templates"" when specifying multiple templates for a job.  ""template"" for multiple templates will soon be unsupported.
Job: 
- name: ccng
  release: cf
- template: 
  - nats
  - agent
",2.0,63216320,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]","Deprecate use of ""template"" in deploy manifest when multiple templates are specified",1183746.0,"[1183746, 756869]",956238,14062,feature,2014-02-13T23:29:37Z,https://www.pivotaltracker.com/story/show/63216320
2014-02-14T23:45:35Z,2013-10-31T17:25:42Z,accepted,,`bosh aws backup route53 filename` should dump all current route53 data to filename in some machine and human readable way.,2.0,59926938,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",Ability to backup route53,81882.0,[81882],956238,1210852,feature,2014-02-14T23:45:34Z,https://www.pivotaltracker.com/story/show/59926938
2014-02-14T23:45:39Z,2013-12-16T23:56:39Z,accepted,,And commit the file to a git repo,4.0,62600628,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",Provide a release with a job that can be configured to run periodically to backup route53 zones,81882.0,[81882],956238,1210852,feature,2014-02-14T23:45:37Z,https://www.pivotaltracker.com/story/show/62600628
2014-02-19T22:52:07Z,2014-01-21T23:40:51Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/501

```
curl -k -u admin:admin https://192.168.50.4:25555/stemcells | jazor
[
  {
    ""name"": ""bosh-stemcell"",
    ""version"": ""993"",
    ""cid"": ""stemcell-6e6b9689-8b03-42cd-a6de-7784e3c421ec"",
    ""deployments"": [
      ""#<Bosh::Director::Models::Deployment:0x00000004696c30>""
    ]
  },
  {
    ""name"": ""bosh-warden-boshlite-ubuntu"",
    ""version"": ""24"",
    ""cid"": ""stemcell-6936d497-b8cd-4e12-af0a-5f2151834a1a"",
    ""deployments"": [

    ]
  }
]
```

Filed by drnic",0.0,64276970,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #501: /stemcells returns non-JSON value for deployments,,[],956238,1134058,feature,2014-02-19T22:51:52Z,https://www.pivotaltracker.com/story/show/64276970
2014-02-20T18:44:00Z,2014-02-07T19:28:42Z,accepted,,"The switch from serial to parallel job execution needs to be well communicated to the user community 6 months in advance of the change of default behavior.

Begin communication now.",,65383442,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]",Complete Communication to Community 3 Months in Advance of Default Behavior Change,1210852.0,[1210852],956238,1210852,chore,2014-02-20T18:43:42Z,https://www.pivotaltracker.com/story/show/65383442
2014-02-20T19:00:57Z,2013-12-23T19:55:24Z,accepted,,"cluster key should be full path to cluster, including datacenter and any folders.

it appears that currently, bosh makes placement decisions across multiple clusters without a user being able to express a preference or requirement. this mapping of vsphere clusters to resource pools feature is required to associate a job with a particular vsphere cluster.

this capability will enable cloud foundry deployments to have a group of DEAs (a job with instances) associated with a vsphere cluster and behave similarly to an EC2 AZ and be marked with a particular zone in the dea config file.

Under the `datacenters` key in the resource pool:

``` yaml
cloud_properties:
- datacenters:
  name: ""foo datacenter""
  clusters:
  - name: ""cluster 1""
```

Specifying more than one cluster is an error: ""BOSH does not currently support multiple clusters per datacenter.""   See https://www.pivotaltracker.com/story/show/64199482 for that feature.

Specifying more than one datacenter is an error: ""BOSH does not currently support multiple datacenters per deployment.""   See https://www.pivotaltracker.com/story/show/64244970 for that feature.

",4.0,62927424,story,"[{'name': 'ds-clusters', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:25Z', 'id': 7518040, 'updated_at': '2014-08-05T08:12:01Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]","as a bosh deployment operator, i can associate vsphere clusters with bosh resource pool cloud properties",756869.0,"[756869, 1115834]",956238,494053,feature,2014-08-05T08:12:01Z,https://www.pivotaltracker.com/story/show/62927424
2014-02-20T20:03:17Z,2013-12-20T23:50:45Z,accepted,,"current:

Creating bound missing VMs
default/0, default/1, default/2,... |                        | 0/4 00:00:18  ETA: --:--:--

new style:

  Started creating bound missing VMs: default/0
  Started creating bound missing VMs: default/1
  Started creating bound missing VMs: default/2
  Started creating bound missing VMs: default/3
     Done creating bound missing VMs: default/1
",1.0,62871282,story,"[{'name': 'parallel jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-12T17:51:47Z', 'id': 7243752, 'updated_at': '2013-12-12T17:51:47Z'}]","convert ""creating bound vms"", ""binding instance VMs"", and ""preparing dns"" to use new style cli event log",1338772.0,[1338772],956238,81882,feature,2014-02-20T20:03:01Z,https://www.pivotaltracker.com/story/show/62871282
2014-02-20T20:05:05Z,2014-02-19T18:24:31Z,accepted,,"accept crit:
a list of gaps that currently exist with the doc set.  That list will become stories for future documentation effort",1.0,66079798,story,"[{'name': 'documentation', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-05T18:20:33Z', 'id': 7646972, 'updated_at': '2014-02-05T18:20:33Z'}, {'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Identify gaps in bosh documentation,1406536.0,[1406536],956238,1210852,feature,2014-02-20T20:04:48Z,https://www.pivotaltracker.com/story/show/66079798
2014-02-20T22:10:53Z,2014-02-04T17:42:21Z,accepted,,See #64847782 but the tl;dr version is that packages on the VM are not wiped during a collocated deploy so if two releases which contain jobs with the same name and version it won't be clear which job (by which I mean process) will run,,65124050,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Files for collocated jobs can be stale,506755.0,"[506755, 1406536]",956238,1406536,bug,2014-02-20T22:10:35Z,https://www.pivotaltracker.com/story/show/65124050
2014-02-24T23:07:32Z,2014-02-19T06:43:42Z,accepted,,"Please read the docs.cloudfoundry.org doc set pertaining to these two sections:

http://docs.cloudfoundry.com/docs/running/bosh/
http://docs.cloudfoundry.com/docs/running/bosh/setup/index.html

",,66035746,story,"[{'name': 'documentation', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-05T18:20:33Z', 'id': 7646972, 'updated_at': '2014-02-05T18:20:33Z'}, {'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Bosh Documentation Familiarization,1202076.0,[1202076],956238,1210852,chore,2014-02-24T23:07:33Z,https://www.pivotaltracker.com/story/show/66035746
2014-02-25T01:36:17Z,2014-02-12T23:41:39Z,accepted,,See #58848106 For context on original adding of mirror,,65683910,story,[],Remove mirror from stemcell building VM,1406536.0,[1406536],956238,1068489,chore,2014-02-25T01:36:19Z,https://www.pivotaltracker.com/story/show/65683910
2014-02-25T19:04:40Z,2014-02-03T14:38:37Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/514

When I'm trying to connect to our atmos blobstore I get the following error:
```
Bosh::Blobstore::BlobstoreError: Failed to create object, underlying error: 
#<Atmos::Exceptions::NotImplementedException: 
This library does not support Atmos version 2.1.5.0.  
Supported versions: [""1.2.7"", ""1.3.4"", ""1.4.0"", ""1.4.1"", ""1.4.2"", ""2.0.1""].  
To silence this, pass :unsupported => true when instantiating.
```

Since the blobstore-client already uses the latest version of [ruby-atmos-pure](the https://rubygems.org/gems/ruby-atmos-pure) gem the only option was to use the `:unsupported => true`.

Filed by rkoster",1.0,65007460,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #514: Allow atmos unsupported versions,756869.0,[756869],956238,1134058,feature,2014-02-25T19:04:10Z,https://www.pivotaltracker.com/story/show/65007460
2014-02-26T17:49:16Z,2014-01-27T22:23:10Z,accepted,,The AWS CPI never throws a Bosh::Clouds::VMNotFound error when #delete_vm from the AWS CPI fails in the case the IaaS has no record of the vm,,64617300,story,[],Resurrector fails to recreate vm on AWS if vm was deleted ,81882.0,[81882],956238,1406536,bug,2014-02-26T17:48:44Z,https://www.pivotaltracker.com/story/show/64617300
2014-02-26T18:59:49Z,2014-02-13T19:19:58Z,accepted,,aid discovering of uuid; instead of cutting pieces out of bosh status.,1.0,65749890,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should be able to get uuid of currently targeted director via bosh status --uuid,1068489.0,[1068489],956238,81882,feature,2014-02-26T18:59:17Z,https://www.pivotaltracker.com/story/show/65749890
2014-02-27T01:44:31Z,2014-02-26T03:25:48Z,accepted,,"http://172.16.68.6:8080/job/bat_micro_openstack_centos_manual/219/

Is an example of a run that failed due to this.  Another run on that Jenkins slave had spun up, and was running the registry.  So when the second build came along, it tried to spawn a registry, immediately was able to attach to the port, and then deleted the config file before the process that was trying to read it could do so.

Only worry about openstack",,66479054,story,[],Allow passing registry endpoint for BATs microbosh_deployment_manifest to avoid overlapping runs failing due to race condition,1123776.0,[1123776],956238,1068489,chore,2014-02-27T01:44:33Z,https://www.pivotaltracker.com/story/show/66479054
2014-02-27T07:51:08Z,2014-02-11T20:05:16Z,accepted,,"The change was additional release in releases section of deployment manifest:

```
releases:
  - name: cf
    version: latest
  - name: decker
    version: latest
```

Director task 118

Error 100: undefined method `use_template_named' for nil:NilClass
Task 118 error

```
I, [2014-02-11T22:17:44.501310 #5557] [task:128]  INFO -- : Creating deployment plan
I, [2014-02-11T22:17:44.501743 #5557] [task:128]  INFO -- : Deployment plan options: {}

E, [2014-02-11T22:17:44.942110 #5557] [task:128] ERROR -- : undefined method `use_template_named' for nil:NilClass
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/job_spec_parser.rb:76:in `block in parse_template'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/job_spec_parser.rb:75:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/job_spec_parser.rb:75:in `parse_template'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/job_spec_parser.rb:24:in `parse'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/job.rb:74:in `parse'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/planner.rb:231:in `block in parse_jobs'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/planner.rb:218:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/planner.rb:218:in `parse_jobs'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/deployment_plan/planner.rb:52:in `parse'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/jobs/update_deployment.rb:31:in `initialize'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/job_runner.rb:83:in `new'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/job_runner.rb:83:in `perform_job'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.1836.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1836.0/bin/bosh-director-worker:76:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `<main>'
D, [2014-02-11T22:17:44.942956 #5557] [task:128] DEBUG -- : Acquired connection: 23484040
D, [2014-02-11T22:17:44.943410 #5557] [task:128] DEBUG -- : (0.000214s) BEGIN
D, [2014-02-11T22:17:44.945521 #5557] [task:128] DEBUG -- : (0.000560s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2014-02-11 22:17:44.942447+0000', ""description"" = 'create deployment', ""result"" = 'undefined method `use_template_named'' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/128', ""user_id"" = 2, ""checkpoint_time"" = '2014-02-11 22:17:43.878727+0000', ""type"" = 'update_deployment' WHERE (""id"" = 128)
D, [2014-02-11T22:17:44.947582 #5557] [task:128] DEBUG -- : (0.001851s) COMMIT
D, [2014-02-11T22:17:44.947699 #5557] [task:128] DEBUG -- : Released connection: 23484040
I, [2014-02-11T22:17:44.947857 #5557] [0xef7888]  INFO -- : Task took 0.447284714 seconds to process.
```

In this case when the deployment manifest has more than one release specified, there's no obvious way for the director to guess which release a template ""implicitly"" comes from. The director should give a clearer indication what went wrong so the user can correct it.

To accept:
- Greg targets his AWS acceptance environment
`bosh target GREGS_BOSH_DIRECTOR_ADDR`
- Greg checks the attached manifest to ensure that
  - It contains two releases
  - the jobs refers to a template without specifying a release
- Greg sets deployment to the attached manifest
`bosh deployment 65585778_manifest.yml`
- Greg deploys
`bosh deploy`

The last step should produce an error containing the following text:
```
Cannot tell what release job `dummy_job' is supposed to use, please explicitly specify one
```
instead of the previous
```
undefined method `use_template_named' during bosh deploy with several releases
```",,65585778,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Error undefined method `use_template_named' during bosh deploy with several releases,1338772.0,[1338772],956238,553935,bug,2014-02-27T07:50:36Z,https://www.pivotaltracker.com/story/show/65585778
2014-02-27T07:51:21Z,2013-11-13T20:21:29Z,accepted,,,,60722780,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}]",Co-located Jobs from Composed Releases,,[],956238,14062,release,2014-02-27T07:50:47Z,https://www.pivotaltracker.com/story/show/60722780
2014-02-27T18:03:13Z,2014-01-17T17:52:19Z,accepted,,"This means that the resurrector often fails to bring back an entire CF deployment when it's turned on in bosh-lite.  This is preventing us from releasing an ""Micro Cloud Foundry"" bosh-lite box that comes with cf preinstalled.

see line note:
https://github.com/cloudfoundry/bosh/commit/aeafcf3234353e7fc3bea82e03b924739d860bd7#diff-1ed5a7ab1854c727a6b31801a98b62a4R34

how to reproduce:
1. on a working cf deployment, disable resurrector
2. make at least 2 vms disapear, like vagrant-reload a bosh-lite box
3. run bosh cck, press Ctrl-c after vm scan completes
4. now we have at least 2 open problems in bosh db, confirm this using psql
SELECT * FROM ""deployment_problems"" WHERE ""state"" = 'open'; 
5. enable resurrector
6. $ watch bosh tasks recent --no-filter
you will see new tasks keep being created and error ""Resolution for problem # (missing_vm) is not provided"" in endless mode
",,64062728,story,[],Resurrector can't solve two problem at once,756869.0,[756869],956238,46031,bug,2014-02-27T18:02:37Z,https://www.pivotaltracker.com/story/show/64062728
2014-02-27T18:14:28Z,2014-02-20T19:50:55Z,accepted,,"Ideas for copy:

 - My goal is to get a deployment of software that I care about.
 - BOSH provides stable deployments built on ephemeral resources. 
 - This is the guiding principle and the main attraction of BOSH.
 - The beauty of a BOSH deployment is that when there is an infrastructure failure, I don’t notice.


Include a page or section describing why BOSH is different than other deployment and lifecycle management alternatives, eg Chef, Puppet, Salt, Ansible, JuJu
Acceptance Criteria
 - New page describing what BOSH is
 - Page describing why BOSH is different than other tools...and why that's great",1.0,66177870,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]","Add content to ""What BOSH is""  ",1068489.0,[1068489],956238,1406536,feature,2014-02-27T18:13:54Z,https://www.pivotaltracker.com/story/show/66177870
2014-02-27T19:33:22Z,2014-02-18T16:53:40Z,accepted,,"when i want to run a script, I use bosh cli and pass the name + index of a job and the path to the script that I want to run so that I can:
-run tests
-clean up things

errand job will return an exit code ",4.0,65986854,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Simple job as first errand,81882.0,[81882],956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/65986854
2014-02-27T19:36:34Z,2014-02-20T20:26:23Z,accepted,,"Gap:	Need an understandable structure for the group of docs right now called “BOSH”
Gap:	Need corrections or better explanations in several places

Understanding BOSH and Blobstore — sketchy and stale, rework.

One-liners and very short topics—move into to an overview or intro together:

NATS
Workers — need better explanation
Health Monitor — need better explanation
including: 
plugin system
Resurrector (plugin)
when VM is gone or Agent unresponsive, works automatically
BOSH CCK (part of CLI)
replace “problem manager” with “Health Monitor” —> scrub story
Stemcell — wrong
Agent — should video be there? product question
MicroBOSH — need better explanation … bootstraps full BOSH or anything deployable by BOSH
make clear that across the board, full BOSH is optional


Fix: Refactor existing 'BOSH components' content into coherent narratives for users (deployment operator) and contributors ",2.0,66181010,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]","Create ""How BOSH works"" page",1202076.0,"[1202076, 1068489]",956238,1406536,feature,2014-02-27T19:35:58Z,https://www.pivotaltracker.com/story/show/66181010
2014-02-27T21:40:07Z,2014-02-18T22:06:26Z,accepted,,"when i run a script and use bosh cli to pass the name + index of a job and the path to the script that I want to run , I want the resulting behavior to include  an exit code and return the output

Note that story #65986854 included all the above *except* addition of returning output.",1.0,66014920,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Bosh Errand returns output upon completion,81882.0,[81882],956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66014920
2014-02-28T18:37:59Z,2014-02-24T19:53:18Z,accepted,,"Goal is to remove ability to use space & only use slash.

eg - not good = decker 0     good=decker/0

Output is a list of commands that need to remove the space option.  in story #65681144, tasks will be added for the work to resolve the 'space' vs 'slash' issue .  ",1.0,66362140,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Audit CLI commands to determine which take spaces vs slashes,81882.0,[81882],956238,1210852,feature,2014-02-28T18:37:22Z,https://www.pivotaltracker.com/story/show/66362140
2014-02-28T19:52:30Z,2014-01-24T08:08:14Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/509

Adds support for starting AWS spot instances rather than AWS on demand instances.

Triggered by the presence of `spot_bid_price` in the `resource_pools > cloud_properties` section of your deployment manifest

Filed by mrdavidlaing",2.0,64444950,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #509: Add aws spot support,1123776.0,[1123776],956238,1134058,feature,2014-07-09T01:03:29Z,https://www.pivotaltracker.com/story/show/64444950
2014-03-03T17:49:54Z,2014-02-24T23:05:19Z,accepted,,"Should be the last listing in the list under the BOSH section (as opposed to the Deploying CF section)
 of docs.df.org

Draft & review - send review request to cf-bosh-eng DL ",1.0,66379402,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Build a BOSH Glossary,1068489.0,"[1068489, 1202076]",956238,1210852,feature,2014-03-05T17:43:53Z,https://www.pivotaltracker.com/story/show/66379402
2014-03-03T17:51:59Z,2014-02-20T20:22:57Z,accepted,,"Gap:	Need to make clear that:
Beauty of BOSH is that the manifest has a few infrastructure-specific places but otherwise is the same for all deployments.
(spiff just merges manifest)

Fix:
Create deployment manifest topic and explain that a deployment is the state of a collection of VMs:
 - what software is on them
 - what resources they use
 - how these are orchestrated

Rework topics below to fit the new structure and correct flaws noted:

http://docs.cloudfoundry.org/deploying/openstack/
Gap—see across the board
refactor:
1-6 yes, and some bullets
some of this should go to topic about deploying to all CPIs

http://docs.cloudfoundry.org/deploying/vsphere/
	pull out stuff for general level verify your deployment
	fix obsolete terminology
	here is too late for this

http://docs.cloudfoundry.org/deploying/vsphere/deploying_micro_bosh.html
	refactor—fix terminology
	move generic paragraph up to CPI page

  
http://docs.cloudfoundry.org/deploying/vsphere/bosh-example-manifest.html
see product questions doc about blessing spiff
explain colocating jobs on 1 VM
possible advanced topic: colocating from different releases
manifest should be a stub

http://docs.cloudfoundry.org/deploying/vsphere/cloud-foundry-example-manifest.html
manifest should be a stub

http://docs.cloudfoundry.com/docs/running/deploying-cf/vsphere/deploying_micro_bosh.html
refactor and move generic paragraph up to CPI page—fix terminology




Acceptance Criteria:
 - Page(s) on manifest creation, splitting a manifest into IaaS specific and IaaS agnostic stubs and manifest merging strategies",2.0,66180752,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Create page describing deployment manifest creation,1202076.0,[1202076],956238,1406536,feature,2014-03-05T17:42:06Z,https://www.pivotaltracker.com/story/show/66180752
2014-03-03T17:55:06Z,2014-02-20T19:55:53Z,accepted,,"Gap: Convey that while BOSH is complex under the hood, interactions with BOSH accomplish powerful things through simple, clear patterns.

Fix:

Incorporate something like the following in  “How to work with BOSH, in simple terms” topic, which could also be called “Working with BOSH: Overview.”

How to work with BOSH, in simple terms.

Before deploying, I obtain a stemcell and obtain or create a release and a manifest.
Stemcells are obtained from the Pivotal Network. 

My interaction with BOSH follows a four-step pattern:

Upload stemcell.
Upload release.
Set deployment with a manifest.
Deploy.
minimal example:
deploy BOSH with MicroBOSH


I take actions and the BOSH Director supports them.

The BOSH Director: 

Communicates with the BOSH Agents that run on one or more VMs.
Interacts with the CPI to manage the lifecycle of the VMs. 


Acceptance Criteria
 - Create page outlining the essential simplicity of a user's (deployment operator) interaction with BOSH",1.0,66178436,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Add 'Simple introduction to BOSH' page,1202076.0,[1202076],956238,1406536,feature,2014-03-05T17:41:19Z,https://www.pivotaltracker.com/story/show/66178436
2014-03-04T23:46:40Z,2014-01-23T22:52:31Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/508

We added configuration so that BOSH can use S3-compatible blobstores like Riak CS, Swift, etc.

In the manifest, you can now specify `host`, `port`, and `use_ssl` in `properties.blobstore`. We then use those properties to configure the S3 client.

Filed by cf-blobstore-eng",1.0,64426614,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #508: Added support for S3-compatible blobstores,1068489.0,"[1068489, 1183746, 81882]",956238,1134058,feature,2014-03-04T23:45:54Z,https://www.pivotaltracker.com/story/show/64426614
2014-03-05T18:22:38Z,2014-02-18T17:41:00Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/526



Filed by dingyin",4.0,65990852,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #526: update vCloud CPI gem and add vCloud infrastructure support to stemcell builder,1338772.0,[1338772],956238,1134058,feature,2014-07-31T05:43:25Z,https://www.pivotaltracker.com/story/show/65990852
2014-03-06T20:26:34Z,2014-03-04T00:50:23Z,accepted,,,1.0,66825798,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Create Deploying MicroBOSH Topic,1202076.0,[1202076],956238,1202076,feature,2014-03-06T20:25:41Z,https://www.pivotaltracker.com/story/show/66825798
2014-03-07T22:07:25Z,2013-12-23T19:55:57Z,accepted,,"Currently we don't have enough slaves to run everything in parallel.  This is slowing down our CI runs considerably.  Either AWS or vSphere could work, and it may be worth going ahead and automating the slave creation process a bit.",,62927456,story,[],Create new Jenkins slave(s),756869.0,"[756869, 553935]",956238,1068489,chore,2014-03-07T22:07:25Z,https://www.pivotaltracker.com/story/show/62927456
2014-03-08T00:24:52Z,2014-02-07T23:13:16Z,accepted,,"When deploying microbosh, set up 2 networks so that Cloud Foundry is isolated from VSphere.  

The deployer will need to know the distinction between MicroBOSH's ""service address"" that's reachable from the app network and the director's other interfaces for outbound communication with vCenter.

Rod Day-Reynolds & Amit Gupta spiked on this topic.  Outcome of that spike including implementation detail is here:   https://docs.google.com/a/pivotallabs.com/document/d/1UBm7AwX6NA8zdZR0bKtEayWjZrf1KxK-BxoeSBJu70w/edit#

To accept:
vCenter networks:
- create a standard network on the host without any physical adapter, call it ""Lonely""
- the ESXi host should already have a standard network connected to its physical adapter, called ""VM Network"", which is connected to the management network.

- Reider downloads a vSphere stemcell:
`bosh download public stemcell bosh-stemcell-2131-vsphere-esxi-ubuntu.tgz`
- Install the latest version of the micro plugin (the plugin for the bosh CLI that enables deployment of a microbosh):
`gem install bosh_cli_plugin_micro`
- Reider uses the following microbosh manifest to deploy microbosh: 

```
name: micro_multi_nets

logging:
  level: debug

network:
  ip:      172.16.45.4
  netmask: 255.255.255.0
  gateway: 172.16.45.2
  dns:     [""10.80.130.1""]
  cloud_properties:
    name: 'VM Network'

deployment_network:
  ip:       101.0.0.3
  netmask:  255.255.255.0
  gateway:  101.0.0.1
  dns:      [""10.80.130.1""]
  cloud_properties:
    name: 'Lonely'

resources:
  persistent_disk: 5384
  cloud_properties:
    ram: 2048
    disk: 5000
    cpu: 1

cloud:
  plugin: vsphere
  properties:
    agent:
      ntp: [""10.80.130.1""]
    vcenters:
      - host: 172.16.45.132
        user: root
        password: vmware
        datacenters:
          - name: arbitrarily
            vm_folder:       elephantitis
            template_folder: staplegun
            disk_path:       bidoof
            datastore_pattern:            dupapis
            persistent_datastore_pattern: dupapis
            allow_mixed_datastores: true
            clusters:
              - elemayo:
                  resource_pool: beeefffeff

apply_spec:
  properties:
    ntp: [""10.80.130.1""]
    dns:
      recursor: ""10.80.130.1""

```

He saves that to a file, then runs the following command: 
`bosh micro deployment <file_name>`
This changes the state of the micro plugin. Now deploy commands will use this manifest. 
- Now actually deploy microbosh:
`bosh micro deploy bosh-stemcell-2131-vsphere-esxi-ubuntu.tgz`
- Now deploy something via microbosh:
1. target the new microbosh:
`bosh target x.x.x.x` (this was listed as the ""network -> ip"" property in the microbosh manifest above
2. Upload the stem cell (`bosh upload stemcell /path/to/bosh-stemcell-2131-vsphere-esxi-ubuntu.tgz`
3. Upload a release next: `bosh upload release iota.tgz` (attached to the story - this is a dummy release that does nothing but create a VM that does nothing)
4.  change the director_uuid in iota_manifest.yml (attached to the story).
5. `bosh deployment /path/to/iota_manifest.yml`
6. `bosh deploy`

- Next, go into that VM and verify that it can not talk to vcenter, but that it can talk to the microbosh director. 
",2.0,65398698,story,"[{'name': 'multi-networks', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-07T23:13:15Z', 'id': 7670532, 'updated_at': '2014-02-07T23:13:15Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",MicroBosh can be deployed on two networks,81882.0,"[81882, 1338772, 1123776]",956238,1210852,feature,2014-04-16T21:53:59Z,https://www.pivotaltracker.com/story/show/65398698
2014-03-08T00:24:53Z,2014-02-11T23:20:01Z,accepted,,Director changes ,2.0,65600186,story,"[{'name': 'multi-networks', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-07T23:13:15Z', 'id': 7670532, 'updated_at': '2014-02-07T23:13:15Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]","MicroBOSH can choose which IP to tell agents to listen to for NATS, blobstore & DNS",1123776.0,"[1123776, 81882]",956238,1210852,feature,2014-03-08T00:24:53Z,https://www.pivotaltracker.com/story/show/65600186
2014-03-08T00:24:57Z,2014-02-07T23:22:13Z,accepted,,Microbosh can be deployed on multiple networks and CF is securely isolated from VSphere,,65399028,story,"[{'name': 'multi-networks', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-07T23:13:15Z', 'id': 7670532, 'updated_at': '2014-02-07T23:13:15Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Microbosh can be deployed on multiple networks for VSphere,1210852.0,[1210852],956238,1210852,release,2014-03-08T00:24:56Z,https://www.pivotaltracker.com/story/show/65399028
2014-03-10T16:47:20Z,2014-03-04T00:55:55Z,accepted,,"This fits into our new BOSH docs nav flow, between 'Basic BOSH Workflow' and 'Deploying BOSH.'",1.0,66826078,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Create Setting Up your IaaS for BOSH topic,1202076.0,[1202076],956238,1202076,feature,2014-03-10T16:46:18Z,https://www.pivotaltracker.com/story/show/66826078
2014-03-10T16:47:22Z,2014-03-04T00:59:05Z,accepted,,Basic example of deploying multi-VM BOSH.,1.0,66826246,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Create Deploying Distributed Software with BOSH topic,1202076.0,[1202076],956238,1202076,feature,2014-03-10T16:46:18Z,https://www.pivotaltracker.com/story/show/66826246
2014-03-11T01:01:56Z,2014-03-11T00:03:00Z,accepted,,,,67270090,story,[],fixed bosh-artifacts not showing all latest releases,81882.0,[81882],956238,81882,chore,2014-03-11T01:01:57Z,https://www.pivotaltracker.com/story/show/67270090
2014-03-11T17:21:40Z,2014-03-05T22:15:56Z,accepted,,"IaaS-specific material about creating manifests needs to be in its own topic. Should incorporate existing example manifests (right now in the old Deploying Cloud Foundry with BOSH docs). New topic will be a logical follow-up to Creating a BOSH Deployment Manifest, in the BOSH docs. ",1.0,66984438,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Create 'Creating a MicroBOSH Deployment Manifest' topic,1202076.0,[1202076],956238,1202076,feature,2014-03-11T17:20:35Z,https://www.pivotaltracker.com/story/show/66984438
2014-03-12T16:20:48Z,2014-02-19T22:59:20Z,accepted,,"If a manifest has the following:
```
clusters:
  - cluster_name
```

instead of the usual
```
clusters:
  - cluster_name:
      resource_pool: resource_pool_name
```

the micro deploy fails with the following message: `create stemcell failed: Field entity is not optional:`

In select_active_host_mobs() the obj key is a string like it is in the tests. The code that sets this uses the symbol :obj.

To accept:
- Greg installs the latest CLI (version <TBD>):
`gem install bosh_cli_plugin_micro`
- Greg examines the attached yaml and make sure that property `cloud.properties.vcenters[0].datacenters[0].clusters` contains one string value instead of a hash
- Greg sets his micro deployment with the attached yaml (TBA)
`bosh micro deployment 66104896_micro_manifest.yml`
- Greg downloads a vSphere stemcell
- Greg tries to deploy a new micro
`bosh micro deploy <the vsphere stemcell>`

In the last step, the previous version of CPI would have blown up. The new version should successfully deploy the micro.",,66104896,story,"[{'name': 'issue', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-28T01:39:09Z', 'id': 7574372, 'updated_at': '2014-01-28T01:39:09Z'}]",Cannot deploy micro using string only cluster name in manifest.,1338772.0,[1338772],956238,756869,bug,2014-03-12T16:19:41Z,https://www.pivotaltracker.com/story/show/66104896
2014-03-13T19:04:41Z,2014-03-10T16:26:06Z,accepted,,"Add vagrant and the vagrant plugins required for stemcell building to jenkins slave building scripts.

Rebuild slave2-aws, slave3-aws and slave4-aws.",,67231050,story,[],Add vagrant and plugins to jenkins slaves,1123776.0,[1123776],956238,756869,chore,2014-03-13T19:04:41Z,https://www.pivotaltracker.com/story/show/67231050
2014-03-13T23:27:15Z,2014-02-24T22:59:26Z,accepted,,"How to tell whether a job or an errand?  add errand=true as property of job in manifest (as opposed to having 'errand' section of manifest)
- if <parameter> is true, then instances=0

possible follow on stories:
what happens during `bosh vms`?
what during CCK'ing?
what happens during resurrection?
what happens during `bosh stop` / `bosh start`",4.0,66378954,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",`bosh deploy` should not create vms for errand jobs before `bosh run errand ...` is called,81882.0,[81882],956238,81882,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66378954
2014-03-13T23:27:18Z,2014-02-24T23:30:12Z,accepted,,do not validate that resource pool has enough VMs,2.0,66380830,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",`bosh run errand ...` command should place appropriate job and dependent packages on empty vm,81882.0,[81882],956238,81882,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66380830
2014-03-13T23:37:11Z,2014-02-12T23:50:21Z,accepted,,,,65684338,story,[],Extract operating system stages from stemcell building into artifact to use in rest of building process,1123776.0,"[1123776, 1068489]",956238,1068489,chore,2014-03-13T23:37:12Z,https://www.pivotaltracker.com/story/show/65684338
2014-03-17T17:15:12Z,2014-03-11T17:44:11Z,accepted,,Note:  This should not be done before EOD Wed 3/12,,67326804,story,[],Publish 1471.2 stemcell to public stemcell list,756869.0,"[756869, 553935]",956238,1123776,chore,2014-03-17T17:15:12Z,https://www.pivotaltracker.com/story/show/67326804
2014-03-18T00:40:03Z,2014-03-11T21:11:05Z,accepted,,"this happened in ci already twice over last few weeks.

method: /Users/pivotal/workspace/bosh/bosh_cli_plugin_micro/lib/bosh/deployer/registry.rb#start

```
bosh -v -n -P 10 --config '/tmp/bosh_config20140311-25183-1mdu2sg' micro deployment microbosh
WARNING! Your target has been changed to `https://54.208.15.101:25555'!
Deployment set to '/tmp/ci-artifacts/aws/manual/centos/ruby/deployments/microbosh/micro_bosh.yml'
I, [2014-03-11T12:41:58.911869 #25183]  INFO -- : Running micro deploy
bosh -v -n -P 10 --config '/tmp/bosh_config20140311-25183-1mdu2sg' micro deploy /tmp/ci-artifacts/aws/manual/centos/ruby/deployments/light-bosh-stemcell-2173-aws-xen-centos.tgz
/var/lib/jenkins/jobs/bat_micro_aws_centos/workspace/bosh-registry/lib/bosh/registry/yaml_helper.rb:8:in `load_yaml_file': Cannot find file `/tmp/d20140311-25567-o8ceg0/bosh_registry_yml20140311-25567-mgbca9' (Bosh::Registry::ConfigError)
	from /var/lib/jenkins/jobs/bat_micro_aws_centos/workspace/bosh-registry/lib/bosh/registry/runner.rb:8:in `initialize'
	from /var/lib/jenkins/jobs/bat_micro_aws_centos/workspace/bosh-registry/bin/bosh-registry:21:in `new'
	from /var/lib/jenkins/jobs/bat_micro_aws_centos/workspace/bosh-registry/bin/bosh-registry:21:in `<top (required)>'
	from /tmp/bat_micro_aws_centos/ruby/1.9.1/bin/bosh-registry:23:in `load'
	from /tmp/bat_micro_aws_centos/ruby/1.9.1/bin/bosh-registry:23:in `<main>'
Unable to connect to Bosh agent. Check logs for more details.
```",,67348056,story,[],set up google doc to track how frequently race condition happens in which registry config file is deleted before registry comes up,1068489.0,[1068489],956238,81882,chore,2014-03-18T00:40:03Z,https://www.pivotaltracker.com/story/show/67348056
2014-03-18T17:06:00Z,2014-03-11T00:09:46Z,accepted,,"Jeff is not with us anymore and we have a link to his dummy release repo.

https://github.com/abic/dummy-boshrelease

We should fork it and link to that.

Link is in this topic:

http://docs-cloudfoundry-staging.cfapps.io/bosh/deploy-microbosh.html
",1.0,67270444,story,[],fork dummy release repo and link to that instead of Jeff's,756869.0,[756869],956238,1202076,feature,2014-07-31T05:40:38Z,https://www.pivotaltracker.com/story/show/67270444
2014-03-18T20:46:07Z,2014-03-12T21:44:19Z,accepted,,,1.0,67434244,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",landing page for bosh repo,1202076.0,[1202076],956238,1210852,feature,2014-03-18T20:44:43Z,https://www.pivotaltracker.com/story/show/67434244
2014-03-19T18:50:42Z,2014-01-21T01:14:07Z,accepted,,,,64199550,story,"[{'name': 'ds-clusters', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:25Z', 'id': 7518040, 'updated_at': '2014-08-05T08:12:01Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Full vSphere Cluster support finished,,[],956238,14062,release,2014-08-05T08:12:01Z,https://www.pivotaltracker.com/story/show/64199550
2014-03-20T18:55:25Z,2014-02-19T22:30:42Z,accepted,,"all stemcells should be updated to either include libyaml >= 0.1.5, or not ship with libyaml at all so that release creators can supply their own.

vulnerability details:
http://www.cvedetails.com/cve/CVE-2013-6393/",,66102110,story,[],upgrade version of libyaml in stemcells to fix vulnerability CVE-2013-6393,1406536.0,[1406536],956238,1017737,bug,2014-03-20T18:55:28Z,https://www.pivotaltracker.com/story/show/66102110
2014-03-20T18:59:07Z,2014-03-05T01:44:46Z,accepted,,"BOSH release 32 running on dijon works normally, when we update to BOSH 38 we started having issue talking to S3 and were getting frequent timeout. Rolling back solved the issue. To triple check, we rolled to 38 again and had the issue.

Caleb was assisting and has context.",,66913564,story,[],BOSH release 38 looks like it has an issue talking to S3,756869.0,"[756869, 81882]",956238,637633,bug,2014-03-20T18:59:08Z,https://www.pivotaltracker.com/story/show/66913564
2014-03-20T22:31:16Z,2014-03-20T22:12:58Z,accepted,,"currently BATs are running with ruby agent to deploy micro, then using go agent stemcell for everything else",,67961822,story,[],Run BATs using go agent for microbosh as well as bats VMs,1123776.0,"[1123776, 1068489]",956238,1123776,chore,2014-03-20T22:31:16Z,https://www.pivotaltracker.com/story/show/67961822
2014-03-24T16:27:41Z,2014-03-15T01:04:30Z,accepted,,,,67595202,story,[],Rebuild prod jumpboxes with new users,756869.0,"[756869, 1266614]",956238,756869,chore,2014-03-24T16:27:41Z,https://www.pivotaltracker.com/story/show/67595202
2014-03-24T22:15:41Z,2014-02-11T23:50:22Z,accepted,,,,65601508,story,[],Next Prod Deploy,756869.0,"[756869, 1266614]",956238,568617,chore,2014-03-24T22:15:42Z,https://www.pivotaltracker.com/story/show/65601508
2014-03-24T22:20:15Z,2014-03-18T21:38:30Z,accepted,,,,67796062,story,[],Add errand job to dummy release.,1068489.0,[1068489],956238,756869,chore,2014-03-24T22:20:15Z,https://www.pivotaltracker.com/story/show/67796062
2014-03-25T16:29:46Z,2014-03-24T22:15:23Z,accepted,,,,68150166,story,[],Deploy aws tools to autodeploy,756869.0,[756869],956238,756869,chore,2014-03-25T16:29:47Z,https://www.pivotaltracker.com/story/show/68150166
2014-03-25T20:47:50Z,2013-06-10T17:16:11Z,accepted,,"determine if we can remove all of rbenv/rvm, update ci scripts as needed, update slaves and master to latest stable ruby

everything in CI -- master and slave servers",,51423535,story,[],Update to Ruby 1.9.3-p545 (latest stable),756869.0,"[756869, 1068489]",956238,5637,chore,2014-03-25T20:47:51Z,https://www.pivotaltracker.com/story/show/51423535
2014-03-25T21:19:42Z,2014-03-25T16:29:12Z,accepted,,,,68203554,story,[],deploy aws tools route 53 backup to a1,756869.0,[756869],956238,756869,chore,2014-03-25T21:19:42Z,https://www.pivotaltracker.com/story/show/68203554
2014-03-25T22:52:21Z,2014-03-07T19:56:31Z,accepted,,https://github.com/cloudfoundry/bosh/pull/407,0.0,67129974,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #407: Remove unuseful max_errors from the universe,756869.0,[756869],956238,1210852,feature,2014-03-25T22:52:22Z,https://www.pivotaltracker.com/story/show/67129974
2014-03-25T22:52:21Z,2014-03-07T19:58:52Z,accepted,,https://github.com/cloudfoundry/bosh/pull/411,0.0,67130142,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #411:[director] Fix tracking preparing package compilation stage,756869.0,[756869],956238,1210852,feature,2014-03-28T00:10:28Z,https://www.pivotaltracker.com/story/show/67130142
2014-03-25T22:52:22Z,2014-03-07T20:01:40Z,accepted,,https://github.com/cloudfoundry/bosh/pull/418,0.0,67130366,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #418: Consistent layout for bosh ip & dns,756869.0,[756869],956238,1210852,feature,2014-03-28T00:15:46Z,https://www.pivotaltracker.com/story/show/67130366
2014-03-26T23:42:04Z,2014-03-08T01:29:39Z,accepted,,,1.0,67148918,story,"[{'name': 'oss-docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-19T18:03:02Z', 'id': 7763174, 'updated_at': '2014-02-19T18:03:02Z'}]",Replace BOSH CLI with steps to run `bosh help --all`,1202076.0,[1202076],956238,1210852,feature,2014-03-26T23:42:05Z,https://www.pivotaltracker.com/story/show/67148918
2014-03-26T23:45:58Z,2014-03-11T17:04:51Z,accepted,,"All sections should be rendered with new style like shown below (with couple of optimizations):

```
○ → bo task 226

Director task 226
  Started preparing deployment
  Started preparing deployment: Binding deployment
     Done preparing deployment: Binding deployment (00:00:00)
  Started preparing deployment: Binding releases
     Done preparing deployment: Binding releases (00:00:00)
  Started preparing deployment: Binding existing deployment
     Done preparing deployment: Binding existing deployment (00:00:00)
  Started preparing deployment: Binding resource pools
     Done preparing deployment: Binding resource pools (00:00:00)
  Started preparing deployment: Binding stemcells
     Done preparing deployment: Binding stemcells (00:00:00)
  Started preparing deployment: Binding templates
     Done preparing deployment: Binding templates (00:00:00)
  Started preparing deployment: Binding properties
     Done preparing deployment: Binding properties (00:00:00)
  Started preparing deployment: Binding unallocated VMs
     Done preparing deployment: Binding unallocated VMs (00:00:00)
  Started preparing deployment: Binding instance networks
     Done preparing deployment: Binding instance networks (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation
  Started preparing package compilation: Finding packages to compile
     Done preparing package compilation: Finding packages to compile (00:00:00)
     Done preparing package compilation (00:00:00)

  Started updating job dummy1_as_errand_slow
  Started updating job dummy1_as_errand_slow: dummy1_as_errand_slow/0 (canary)
     Done updating job dummy1_as_errand_slow: dummy1_as_errand_slow/0 (canary) (00:00:44)
     Done updating job dummy1_as_errand_slow (00:00:44)

  Started running errand
  Started running errand: dummy1_as_errand_slow/0
     Done running errand: dummy1_as_errand_slow/0 (00:06:46)
     Done running errand (00:06:46)

  Started deleting instances dummy1_as_errand_slow
  Started deleting instances dummy1_as_errand_slow: vm-2da3174d-0e44-42ac-b50c-9d2af19050ee
     Done deleting instances dummy1_as_errand_slow: vm-2da3174d-0e44-42ac-b50c-9d2af19050ee (00:00:07)
     Done deleting instances dummy1_as_errand_slow (00:00:07)


Task 226 done

Started		2014-03-11 04:26:51 UTC
Finished	2014-03-11 04:34:28 UTC
Duration	00:07:37
```

Do smart collapsing of sections that only have 1 task:

```
  Started preparing package compilation
  Started preparing package compilation: Finding packages to compile
     Done preparing package compilation: Finding packages to compile (00:00:00)
     Done preparing package compilation (00:00:00)
```

becomes 

```
  Started preparing package compilation: Finding packages to compile. Done (00:00:00)
```

Tasks that were started on the previous line and become done should also be collapsed:

```
  Started preparing deployment
  Started preparing deployment: Binding deployment
     Done preparing deployment: Binding deployment (00:00:00)
  Started preparing deployment: Binding releases
     Done preparing deployment: Binding releases (00:00:00)
  Started preparing deployment: Binding existing deployment
     Done preparing deployment: Binding existing deployment (00:00:00)
  Started preparing deployment: Binding resource pools
     Done preparing deployment: Binding resource pools (00:00:00)
  Started preparing deployment: Binding stemcells
     Done preparing deployment: Binding stemcells (00:00:00)
  Started preparing deployment: Binding templates
     Done preparing deployment: Binding templates (00:00:00)
  Started preparing deployment: Binding properties
     Done preparing deployment: Binding properties (00:00:00)
  Started preparing deployment: Binding unallocated VMs
     Done preparing deployment: Binding unallocated VMs (00:00:00)
  Started preparing deployment: Binding instance networks
     Done preparing deployment: Binding instance networks (00:00:00)
     Done preparing deployment (00:00:00)
```

becomes

```
  Started preparing deployment
  Started preparing deployment: Binding deployment. Done (00:00:00)
  Started preparing deployment: Binding releases. Done (00:00:00)
  Started preparing deployment: Binding existing deployment. Done (00:00:00)
  Started preparing deployment: Binding resource pools. Done (00:00:00)
  Started preparing deployment: Binding stemcells. Done (00:00:00)
  Started preparing deployment: Binding templates. Done (00:00:00)
  Started preparing deployment: Binding properties. Done (00:00:00)
  Started preparing deployment: Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment: Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)
```",1.0,67322632,story,[],bosh cli should only use single formatting (new style) for event log rendering,81882.0,"[81882, 756869]",956238,81882,feature,2014-03-26T23:45:58Z,https://www.pivotaltracker.com/story/show/67322632
2014-03-26T23:47:13Z,2014-03-05T18:15:56Z,accepted,,#66964014,2.0,66963854,story,[],Stemcell Investigation,1068489.0,"[1068489, 1123776]",956238,1210852,feature,2014-03-26T23:47:13Z,https://www.pivotaltracker.com/story/show/66963854
2014-03-27T17:58:58Z,2014-03-27T17:16:42Z,accepted,,http://172.16.68.6:8080/job/bat_micro_openstack_centos_dynamic/327/,,68378044,story,[],Investigate the failure of BATS test that check for agent task id on failed deployment,756869.0,[756869],956238,756869,chore,2014-03-27T17:58:58Z,https://www.pivotaltracker.com/story/show/68378044
2014-03-27T23:31:31Z,2014-02-25T00:37:47Z,accepted,,"- it should error out on bosh deploy if errand is not associated with resource pool that is big enough

- if you have any errands, you must have jobs+1 in each resource_pool",1.0,66385224,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}, {'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}]",throw error when running `bosh deploy` without enough resources,756869.0,"[756869, 1123776]",956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66385224
2014-03-27T23:56:15Z,2014-02-24T23:00:53Z,accepted,,"- it should error out if resource pool is not big enough and does not have a vm that can be used for an errand

should have CLI feedback to user (like bosh deploy event rendering)",2.0,66379048,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",`bosh run errand ...` should create missing vm for errand job,81882.0,[81882],956238,81882,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66379048
2014-03-27T23:57:30Z,2014-03-27T23:57:28Z,accepted,,,,68411114,story,[],Fix jalapeno CI,756869.0,[756869],956238,756869,chore,2014-03-27T23:57:29Z,https://www.pivotaltracker.com/story/show/68411114
2014-03-28T18:15:33Z,2014-03-27T16:39:43Z,accepted,,,,68374674,story,[],Copy bosh private.yml as part of ci run,553935.0,[553935],956238,756869,chore,2014-03-28T18:17:09Z,https://www.pivotaltracker.com/story/show/68374674
2014-03-28T21:24:54Z,2014-03-28T18:34:17Z,accepted,,,,68463930,story,[],Investigate extracting stemcell failure,553935.0,[553935],956238,553935,chore,2014-03-28T21:24:54Z,https://www.pivotaltracker.com/story/show/68463930
2014-03-29T01:09:26Z,2014-03-28T16:50:28Z,accepted,,,,68455432,story,[],remind people to use released gems instead of pre versions,1210852.0,[1210852],956238,81882,chore,2014-03-29T01:09:26Z,https://www.pivotaltracker.com/story/show/68455432
2014-04-01T17:53:22Z,2014-03-31T17:44:35Z,accepted,,"```
bundle exec rake stemcell:build_os_image[ubuntu,/home/ubuntu/base_os_image.tgz]
```

```
  1) Ubuntu OS image installed by base_apt Package ""libyaml-dev"" should be installed
     Failure/Error: it { should be_installed }
       dpkg-query -f '${Status}' -W libyaml-dev | grep '^install ok installed$'
       expected Package ""libyaml-dev"" to be installed
     # ./spec/os_image/ubuntu_spec.rb:119:in `block (5 levels) in <top (required)>'

  2) Ubuntu OS image installed by system_kernel Package ""linux-image-virtual-lts-backport-oneiric"" should be installed
     Failure/Error: it { should be_installed }
       dpkg-query -f '${Status}' -W linux-image-virtual-lts-backport-oneiric | grep '^install ok installed$'
       expected Package ""linux-image-virtual-lts-backport-oneiric"" to be installed
     # ./spec/os_image/ubuntu_spec.rb:151:in `block (5 levels) in <top (required)>'

  3) Ubuntu OS image installed by system_kernel Package ""linux-headers-virtual-lts-backport-oneiric"" should be installed
     Failure/Error: it { should be_installed }
       dpkg-query -f '${Status}' -W linux-headers-virtual-lts-backport-oneiric | grep '^install ok installed$'
       expected Package ""linux-headers-virtual-lts-backport-oneiric"" to be installed
     # ./spec/os_image/ubuntu_spec.rb:151:in `block (5 levels) in <top (required)>'
```",,68567976,story,[],Fix OS image specs,553935.0,[553935],956238,553935,chore,2014-04-01T17:53:23Z,https://www.pivotaltracker.com/story/show/68567976
2014-04-03T17:23:44Z,2014-04-01T17:27:00Z,accepted,,https://docs.google.com/a/pivotallabs.com/document/d/1q7z7enof65kP3ee-x-zwIDQDrlBVjc-CSdPU3gFGTJg/edit,,68651814,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Create Google Doc to capture 'All Things BOSH Errand',1210852.0,[1210852],956238,1210852,chore,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/68651814
2014-04-03T18:31:10Z,2014-02-24T23:02:29Z,accepted,,"replace it with an empty vm in the resource pool (i.e. refill pool at end of errand run)

should have CLI feedback to user (like bosh deploy event rendering)",1.0,66379188,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]","after `bosh run errand ...` completes, bosh should delete errand vm",1123776.0,"[1123776, 553935]",956238,81882,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66379188
2014-04-03T18:50:45Z,2014-03-25T19:58:46Z,accepted,,"bat_micro_vsphere_centos build #685

```
.I, [2014-03-24T22:03:00.997003 #13798]  INFO -- : Requirement bat-2
I, [2014-03-24T22:03:01.035279 #13798]  INFO -- : release already uploaded
I, [2014-03-24T22:03:01.035430 #13798]  INFO -- : Satisfied requirement bat-2
I, [2014-03-24T22:03:01.035508 #13798]  INFO -- : Requirement bosh-vsphere-esxi-centos-2274
I, [2014-03-24T22:03:01.046904 #13798]  INFO -- : Stemcell already uploaded
I, [2014-03-24T22:03:01.047037 #13798]  INFO -- : Satisfied requirement bosh-vsphere-esxi-centos-2274
I, [2014-03-24T22:03:01.057098 #13798]  INFO -- : Requirement #<Bat::Deployment:0x00000001c3ad60>
I, [2014-03-24T22:03:01.066440 #13798]  INFO -- : deployment not deployed
I, [2014-03-24T22:03:01.072748 #13798]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /tmp/bosh_config20140324-13798-1pnxt8f --user admin --password admin deployment /tmp/d20140324-13798-tjnxzk/deployment 2>&1
I, [2014-03-24T22:03:03.962027 #13798]  INFO -- : Deployment set to `/tmp/d20140324-13798-tjnxzk/deployment'

I, [2014-03-24T22:03:03.962523 #13798]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /tmp/bosh_config20140324-13798-1pnxt8f --user admin --password admin deploy 2>&1
I, [2014-03-24T22:06:23.365428 #13798]  INFO -- : Getting deployment properties from director...
Unable to get properties list from director, trying without it...
Compiling deployment manifest...

Director task 20
Deprecation: Please use `templates' when specifying multiple templates for a job. `template' for multiple templates will soon be unsupported.

  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages > batarang/1. Done (00:01:11)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started creating bound missing vms > common/0. Done (00:01:06)

  Started binding instance vms > colocated/0. Done (00:00:04)

  Started preparing configuration > Binding configuration. Done (00:00:00)

  Started updating job colocated > colocated/0 (canary). Done (00:00:45)

Task 20 done

Started		1970-01-01 00:00:00 UTC
Finished	2014-03-25 05:07:25 UTC
Duration	387701:07:25

Deployed `deployment' to `microbosh-vsphere-jenkins'

I, [2014-03-24T22:06:23.366221 #13798]  INFO -- : Satisfied requirement #<Bat::Deployment:0x00000001c3ad60>
I, [2014-03-24T22:06:23.367570 #13798]  INFO -- : Requirement no_tasks_processing
I, [2014-03-24T22:06:23.367751 #13798]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /tmp/bosh_config20140324-13798-1pnxt8f --user admin --password admin tasks 2>&1
I, [2014-03-24T22:06:26.441807 #13798]  INFO -- : No running tasks
```",,68224052,story,[],"""template"" deprecation message from director should have a timestamp",553935.0,[553935],956238,81882,bug,2014-04-03T18:50:45Z,https://www.pivotaltracker.com/story/show/68224052
2014-04-03T18:52:00Z,2014-03-10T18:29:15Z,accepted,,"1 pair - 1 day 

deliverable:   14.04 stemcell that can be used to deploy bosh + follow on stories",1.0,67243630,story,[],Spike on Ubuntu 14.04 beta,1123776.0,"[1123776, 553935]",956238,1210852,feature,2014-04-05T06:45:18Z,https://www.pivotaltracker.com/story/show/67243630
2014-04-04T18:15:24Z,2014-02-10T20:31:58Z,accepted,,"Show all information from each of theses commands every time. Kill `--all`.

Make sure help message and global flags are both in output",1.0,65505988,story,"[{'name': 'bosh syntax inconsistency', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-10T23:04:43Z', 'id': 7686248, 'updated_at': '2014-02-10T23:04:43Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","consolidate bosh --help, bosh help and bosh help --all",1068489.0,[1068489],956238,81882,feature,2014-04-04T18:15:25Z,https://www.pivotaltracker.com/story/show/65505988
2014-04-07T04:12:23Z,2014-03-12T21:31:24Z,accepted,,"```
[vcap@bm-0077e30b-7aea-4c53-b9d4-e6753323b96a 1]$ cat event
# Logfile created on 2014-03-12 14:23:24 -0400 by logger.rb/31641
{""time"":1394648612,""stage"":""Update stemcell"",""tags"":[],""total"":5,""task"":""Extracting stemcell archive"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1394648871,""stage"":""Update stemcell"",""tags"":[],""total"":5,""task"":""Extracting stemcell archive"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1394648871,""stage"":""Update stemcell"",""tags"":[],""total"":5,""task"":""Verifying stemcell manifest"",""index"":2,""state"":""started"",""progress"":0}
{""time"":1394648871,""stage"":""Update stemcell"",""tags"":[],""total"":5,""task"":""Verifying stemcell manifest"",""index"":2,""state"":""finished"",""progress"":100}
```

```
Extracting stemcell archive  |                   | 0/5 00:02:41  ETA: --:--:--
Extracting stemcell archive  |                   | 0/5 00:02:42  ETA: --:--:--
Extracting stemcell archive  |                   | 0/5 00:02:49  ETA: --:--:--                                                                         
Extracting stemcell archive  |                   | 0/5 00:02:51  ETA: --:--:--
Perform request get, https://172.16.69.40:25555/tasks/1, {""Authorization""=>""Basic...""}, nil
REST API call exception: execution expired #<-------------------------------------
```

It broke connection after tracking for ~3min. It actually did succeed after ~4mins.",,67433350,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",bosh cli should retry performing http request if it received timeout::errror,81882.0,[81882],956238,81882,chore,2014-04-07T04:12:23Z,https://www.pivotaltracker.com/story/show/67433350
2014-04-07T22:13:37Z,2014-03-07T20:03:48Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/421

When this PR is complete, #62353024 can be closed.  See #62353024 for use case & accept crit.",1.0,67130524,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #421: Add command to list current locks,553935.0,"[553935, 1123776, 81882]",956238,1210852,feature,2014-04-07T22:13:38Z,https://www.pivotaltracker.com/story/show/67130524
2014-04-07T23:50:42Z,2014-03-07T19:52:46Z,accepted,,"As a BOSH team member, I should complete the refactoring that Caleb requested below and then complete PR https://github.com/cloudfoundry/bosh/pull/406

",1.0,67129724,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #406: [director] Track failed job updates in event log,553935.0,"[553935, 1123776]",956238,1210852,feature,2014-04-07T23:50:43Z,https://www.pivotaltracker.com/story/show/67129724
2014-04-08T22:40:12Z,2014-03-26T22:47:25Z,accepted,,"As a BOSH team member, I should complete the refactoring that Caleb requested below and then complete PR https://github.com/cloudfoundry/bosh/pull/419",1.0,68325962,story,[],cloudfoundry/bosh #419: [bosh-cli] Create release defaults dev name to final name,1123776.0,"[1123776, 553935]",956238,1210852,feature,2014-04-08T22:40:12Z,https://www.pivotaltracker.com/story/show/68325962
2014-04-09T18:26:02Z,2014-03-31T18:49:05Z,accepted,,,,68574858,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Investigate options for canceling a bosh errand,756869.0,[756869],956238,1068489,chore,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/68574858
2014-04-09T18:44:31Z,2014-04-08T16:27:22Z,accepted,,#69106274,1.0,69106298,story,[],stemcell upgrade,1123776.0,[1123776],956238,1210852,feature,2014-04-09T18:44:32Z,https://www.pivotaltracker.com/story/show/69106298
2014-04-09T19:23:40Z,2014-03-27T21:47:34Z,accepted,,,,68404116,story,[],Find & prep vsphere env for SAN testing,553935.0,"[553935, 1123776, 1158132]",956238,1210852,chore,2014-04-09T19:23:40Z,https://www.pivotaltracker.com/story/show/68404116
2014-04-10T17:23:01Z,2014-04-08T14:53:07Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/556

To not lose the blobstores (and db on solo installations), these
jobs should use a persistent disk.

People will use these examples as a starting point and once they update for example a stemcell, they will suddently lose all the blobs
if they haven't configured a persistent disk for these jobs. So we
should provide a sane example.

Filed by duritong",1.0,69094898,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #556: blobstores and solo installations should have a persistent disk.,553935.0,[553935],956238,1134058,feature,2014-04-10T17:23:01Z,https://www.pivotaltracker.com/story/show/69094898
2014-04-11T00:50:20Z,2014-03-26T21:07:12Z,accepted,,,,68317490,story,[],Update stemcell building documentation to reflect new build and build_os_image tasks,1123776.0,[1123776],956238,81882,chore,2014-04-11T00:50:20Z,https://www.pivotaltracker.com/story/show/68317490
2014-04-14T15:48:40Z,2014-04-10T18:51:02Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/560

vcloud does not support any additional features as compared to vsphere. this spec is breaking the vcloud BAT runs.

Filed by molteanu",1.0,69290650,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #560: Disable network_reconfiguration bat test for vcloud,1123776.0,[1123776],956238,1134058,feature,2014-04-14T15:48:34Z,https://www.pivotaltracker.com/story/show/69290650
2014-04-14T15:48:47Z,2014-04-10T18:57:25Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/562

This change passes the vcloud control parameters from the deploy manifest into the director.

Filed by dingyin",1.0,69291240,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #562: Let bosh director take the vcloud control parameters.,1123776.0,[1123776],956238,1134058,feature,2014-04-14T15:48:41Z,https://www.pivotaltracker.com/story/show/69291240
2014-04-14T15:48:50Z,2014-04-10T18:54:01Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/561

Currently vcloud BAT tests leave the independent disks behind in vCD. This change fixes the cleanup code to explicitly remove independent disks attached to VMs.

Filed by molteanu",1.0,69290916,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #561: vcloud BATs need to remove independent disks explicitly,1123776.0,[1123776],956238,1134058,feature,2014-04-14T15:48:42Z,https://www.pivotaltracker.com/story/show/69290916
2014-04-14T19:44:51Z,2014-02-25T01:14:15Z,accepted,,"should bosh deploy be blocked until finishing an errand (or reverse)?

isolate spin up & tear down of VMs in deployment log.
-confirm what happens when an errand is running and a `bosh deploy` is performed",2.0,66386686,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Running an errand blocks `bosh deploy`,553935.0,[553935],956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66386686
2014-04-15T04:41:57Z,2014-03-29T00:56:39Z,accepted,,"See docs and discussion here:

https://docs.google.com/a/pivotallabs.com/document/d/17gNedc62PKRJEqF5Bo7QVyuSPv50S3RD4fYfo1dV9Hg/edit#",1.0,68483180,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}]","Investigate the networks block in BOSH manifest, and network block in MicroBOSH manifest",553935.0,[553935],956238,1202076,feature,2014-07-31T05:40:38Z,https://www.pivotaltracker.com/story/show/68483180
2014-04-16T18:03:16Z,2014-03-29T05:02:24Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/549

Here is little fix to make code of `stemcell/stage_collection.rb` more understandable. All tests within `stemcell/spec` are passing.


Filed by allomov",1.0,68485390,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #549: Small refactor within stemcell/stage_collection.rb methods.,1123776.0,[1123776],956238,1134058,feature,2014-04-16T18:03:17Z,https://www.pivotaltracker.com/story/show/68485390
2014-04-17T20:41:40Z,2014-04-08T00:06:28Z,accepted,,covered for go agent in bosh agent backlog #67488650,1.0,69048744,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",When running errand agent should use TMPDIR env variable for ruby agent,1123776.0,[1123776],956238,81882,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/69048744
2014-04-17T20:58:02Z,2014-04-02T09:14:38Z,accepted,,Running `bosh micro deploy` used to give incremental feedback while deploying. Recently a change was made and it now waits until the end and outputs the entire activity log at once.,,68700958,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh micro deploy` does not give incremental feedback anymore,1123776.0,[1123776],956238,1218342,bug,2014-04-17T20:58:05Z,https://www.pivotaltracker.com/story/show/68700958
2014-04-17T22:41:16Z,2014-04-15T16:36:41Z,accepted,,,,69554908,story,[],Investigate merging in warden-cpi branch,1068489.0,[1068489],956238,1068489,chore,2014-04-17T22:41:26Z,https://www.pivotaltracker.com/story/show/69554908
2014-04-18T16:06:31Z,2014-03-31T18:52:13Z,accepted,,,1.0,68575172,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Clean up errand VM in error cases,1123776.0,"[1123776, 1068489]",956238,1068489,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/68575172
2014-04-18T17:06:35Z,2014-04-16T23:05:02Z,accepted,,"See 
- #69667298
- #69667348",,69667464,story,[],CF deployment maintenance,1406536.0,[1406536],956238,553935,chore,2014-04-18T17:06:40Z,https://www.pivotaltracker.com/story/show/69667464
2014-04-18T18:55:25Z,2014-04-18T17:43:46Z,accepted,,"We don't really want to have to specify a base os image as a SHA, S3 let's us version an object in a bucket so we can use a human readable name",,69779934,story,[],Investigate bucket object versioning in S3,553935.0,[553935],956238,553935,chore,2014-04-18T18:55:26Z,https://www.pivotaltracker.com/story/show/69779934
2014-04-18T22:03:00Z,2014-04-18T19:30:34Z,accepted,,,,69786448,story,[],Clean up old stemcells before vsphere BATs,1068489.0,[1068489],956238,1068489,chore,2014-04-18T22:03:00Z,https://www.pivotaltracker.com/story/show/69786448
2014-04-22T17:16:00Z,2014-03-17T17:16:24Z,accepted,,Since upload 1471_2 they always show up at the top because they are interpreted as a different variety.,1.0,67675768,story,[],Patch versions of public stemcells should be sorted correctly in public stemcells list.,553935.0,"[553935, 1406536]",956238,756869,feature,2014-04-22T17:16:00Z,https://www.pivotaltracker.com/story/show/67675768
2014-04-22T17:17:48Z,2014-03-31T23:34:43Z,accepted,,"Hitting the base_url for our bosh-jenkins-artifacts bucket returns enough results that things are starting to fall off the list.  We saw this same behavior on bosh_artifacts.  Very soon now, public stemcells may very well no longer return all of the most recent set of stemcells created.",,68597004,story,[],`bosh public stemcells` should properly fetch multiple pages of results,553935.0,"[553935, 1406536]",956238,81882,bug,2014-04-22T17:17:48Z,https://www.pivotaltracker.com/story/show/68597004
2014-04-22T17:34:06Z,2014-04-18T19:06:35Z,accepted,,,1.0,69785084,story,[],Use S3 Bucket Versioning for stemcell base os image,553935.0,"[553935, 1406536]",956238,553935,feature,2014-04-22T17:34:04Z,https://www.pivotaltracker.com/story/show/69785084
2014-04-23T17:54:58Z,2014-04-07T19:07:23Z,accepted,,fill in the details like #62015812,4.0,69022850,story,[],move Ubuntu to newer version than 10 LTS (target 14.04) for AWS with Go Agent only,553935.0,"[553935, 1165386, 1406536]",956238,1210852,feature,2014-04-23T18:17:34Z,https://www.pivotaltracker.com/story/show/69022850
2014-04-23T18:24:48Z,2014-04-17T00:14:49Z,accepted,,"We ran `bosh micro deploy` and expected to see each task begin and end:
```
Will deploy due to stemcell changes
  Started prepare for update
  Started prepare for update > Delete stemcell. Done

  Started deploy micro bosh
  Started deploy micro bosh > Unpacking stemcell. Done
  Started deploy micro bosh > Uploading stemcell. Done
  Started deploy micro bosh > Creating VM from ami-970e17fe light
```
 Instead, we saw

```
Will deploy due to stemcell changes
  Started prepare for update
  Started prepare for update > Delete stemcell
  Started deploy micro bosh > Creating VM from ami-970e17fe light
```",,69670520,story,[],`bosh micro deploy` does not display all tasks in the output,1123776.0,[1123776],956238,1123776,bug,2014-04-23T18:24:48Z,https://www.pivotaltracker.com/story/show/69670520
2014-04-23T19:32:02Z,2014-04-21T22:26:42Z,accepted,,"When using the Ruby agent the message is improperly encoded, so the CLI doesn't realize the errand has finished with output too large, instead it tries to keep polling.",1.0,69881258,story,[],"Run errand should fail gracefully with output > 1 MB, using the Ruby agent",1123776.0,"[1123776, 553935]",956238,1123776,feature,2014-04-23T19:32:02Z,https://www.pivotaltracker.com/story/show/69881258
2014-04-23T22:15:05Z,2014-04-21T17:52:41Z,accepted,,"```
bosh run errand dummy_errand
Director task 116
Started preparing deployment
Started preparing deployment > Binding deployment. Done (00:00:00)
Started preparing deployment > Binding releases. Done (00:00:00)
Started preparing deployment > Binding existing deployment^C
Do you want to cancel task 116? [yN] (^C again to detach): yes
Cancelling task 116...
. Failed: Timed out sending `get_state' to ed55a5eb-7a68-4a4f-aea3-35ff53feeb96 after 45 seconds (00:02:15)
Error 100: undefined method `vm' for nil:NilClass
Task 116 error
Errand `dummy_errand' did not complete
For a more detailed error report, run: bosh task 116 --debug

```",1.0,69858970,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Cancelling task during bind existing deployment should not  try to delete non-existent  vm ,1123776.0,"[1123776, 553935]",956238,1123776,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/69858970
2014-04-23T22:43:41Z,2014-03-19T18:54:53Z,accepted,,"As a BOSH user, I want to know about Errands so that I can begin to use this feature.

please write up a brief but comprehensive introduction to Bosh Errands including an example manifest snippet for inclusion on the doc.cf.org and the bosh-user/bosh-dev github communities.",0.0,67866244,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}, {'name': 'documentation', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-05T18:20:33Z', 'id': 7646972, 'updated_at': '2014-02-05T18:20:33Z'}]",Bosh Errands Feature Description and Manifest Example,553935.0,"[553935, 1406536]",956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/67866244
2014-04-24T17:12:18Z,2014-04-01T17:28:25Z,accepted,,"PM puts a chore at the end of the epic, to distill the document, make sure it contains enough information, and hand it off to docs. This way, docs gets necessary new feature information quickly.",,68651964,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Compile BOSH Errand Google Doc into Acceptable State for Handoff to Doc Team,,[],956238,1210852,chore,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/68651964
2014-04-24T20:58:53Z,2014-03-21T18:20:33Z,accepted,,"as a bosh operator on VCHS, I should be able to access VCHS ubuntu & centos stemcells via same methodology as opeators on AWS,openstack.vsphere so that I can easily administer CF on VCHS

1.  When stemcells pass CI, they should go into the S3 blobstore that contains all BOSH teams' stemcells
2.  Stemcells should be available via https://s3.amazonaws.com/bosh-jenkins-artifacts and http://bosh_artifacts.cfapps.io/

Required for 1.2",1.0,68014462,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Add VCHS Stemcells to current  pipeline,1068489.0,[1068489],956238,1210852,feature,2014-07-31T05:43:25Z,https://www.pivotaltracker.com/story/show/68014462
2014-04-24T21:52:19Z,2014-03-05T00:06:04Z,accepted,,"as a BOSH user, I want to know about co-located jobs so that I can begin to use this feature.

please write up a brief but comprehensive introduction to co-location including an example manifest snippet for inclusion on the doc.cf.org and the bosh-user/bosh-dev github communities.",1.0,66908722,story,"[{'name': 'colocation', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-13T20:12:18Z', 'id': 7025430, 'updated_at': '2013-11-13T20:12:18Z'}, {'name': 'documentation', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-05T18:20:33Z', 'id': 7646972, 'updated_at': '2014-02-05T18:20:33Z'}]",Co-Located Job Feature Description and Manifest Example,1123776.0,"[1123776, 553935]",956238,1210852,feature,2014-04-24T23:02:53Z,https://www.pivotaltracker.com/story/show/66908722
2014-04-25T19:00:00Z,2014-02-25T01:17:30Z,accepted,,"task checkpoint after refilling resource pool, after job spec is applied and after errand. ",2.0,66386810,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Cancel a bosh errand,1123776.0,"[1123776, 1068489]",956238,1210852,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/66386810
2014-04-25T21:05:41Z,2014-03-19T01:03:07Z,accepted,,recently we missed open-vm-tools.,1.0,67807638,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",BATS should test network reconfiguration (e.g. change static ip),1123776.0,"[1123776, 553935]",956238,81882,feature,2014-04-25T21:05:42Z,https://www.pivotaltracker.com/story/show/67807638
2014-04-28T23:08:58Z,2014-03-07T19:22:53Z,accepted,,Looks like at least vSphere CPI lifecycle tests aren't deleting VMs after running.  This leads to runaway CPU usage on vSphere eventually.,,67127424,story,[],Clean up VMs after vsphere CPI lifecycle tests,553935.0,"[553935, 1061299]",956238,1068489,chore,2014-04-28T23:07:44Z,https://www.pivotaltracker.com/story/show/67127424
2014-04-29T03:35:44Z,2014-02-07T22:19:03Z,accepted,,Use environment variables. ,,65395500,story,[],Remove jalapeno hardcoding in vsphere lifecycle.,553935.0,[553935],956238,756869,chore,2014-04-29T03:35:21Z,https://www.pivotaltracker.com/story/show/65395500
2014-04-29T18:28:09Z,2013-10-29T18:01:15Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/448

This change adds support for boot from volume to the OpenStack CPI.  To use, add the following property to the Micro BOSH or BOSH deployment manifest:

    cloud:
      plugin: openstack
      properties:
        openstack:
          boot_from_volume: true

Changes:
* Add a reference to the fog Volume API which supports created volumes with an imageRef
* Create a boot volume before creating the VM with the disk size specified by the flavor
* Add a block device mapping to the VM on creation to mount the new volume at /dev/vda
* Skip setting the ""personality"" property on VM creation since file injection is not supported with BFV
* Added test stubs for the Fog::Volume API to fix failures
* Added new tests for creating a boot volume and creating a VM using boot from volume

Filed by amhuber",0.0,59773368,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'sanity-checked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-20T23:53:57Z', 'id': 7517282, 'updated_at': '2014-01-20T23:53:57Z'}]",cloudfoundry/bosh #448: Boot from volume,1123776.0,[1123776],956238,1134058,feature,2014-05-21T21:55:09Z,https://www.pivotaltracker.com/story/show/59773368
2014-04-29T20:24:36Z,2014-03-18T23:57:25Z,accepted,,,,67804426,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",___Bosh Errands Ready for May Release___,,[],956238,1210852,release,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/67804426
2014-04-29T22:24:56Z,2014-04-03T15:11:18Z,accepted,,"When a job is shut down, our drain script is first called with 'job_shutdown'. We have work to do, which we kick off asynchronously, and we then echo '-10' to STDOUT.

10 seconds later, the drain script is called again, as expected. This time it is called with 'job_check_status'. If our work is still not done, we echo '-10' again to ask for more time. However, as soon as the drain script exits, the job is killed and the VM goes away. Expected behaviour is that the drain script would be called again 10 seconds later, but the VM doesn't live long enough to see this happen.

A drain script with the following contents demonstrates the issue:

```
#!/bin/bash

echo ""Script called with $@"" >> /tmp/drain.log
echo -10
exit 0
```

Of course, you'll want to tail the logs as the job is being shut down to see this happen.",,68808888,story,[],Drain script status checks don't respect requests for more time ,1165386.0,"[1165386, 1406536]",956238,1139720,bug,2014-04-29T22:24:57Z,https://www.pivotaltracker.com/story/show/68808888
2014-04-30T18:26:50Z,2014-04-25T19:19:46Z,accepted,,"while working on another charter, we noticed that powerdns was failing to start (SIGSEGV - signal 11)",0.0,70191210,story,"[{'name': 'charter', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-14T18:01:33Z', 'id': 8206016, 'updated_at': '2014-04-14T18:01:33Z'}, {'name': 'et', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-18T17:12:53Z', 'id': 7982484, 'updated_at': '2014-03-18T17:12:53Z'}]",explore powerdns failure to start on trusty stemcell to learn if we need to update the version of powerdns we are using,1165386.0,"[1165386, 1406536]",956238,1165386,feature,2014-04-30T18:26:48Z,https://www.pivotaltracker.com/story/show/70191210
2014-04-30T21:57:46Z,2014-04-21T17:45:22Z,accepted,,When a drain script returns a negative number the InstanceDeleter checks to see if the task was canceled before calling it again. During an errand we should prevent any cancellation within the cleanup.,1.0,69858338,story,"[{'name': 'bosh errand mvp', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:15:47Z', 'id': 8466466, 'updated_at': '2014-05-16T21:15:47Z'}]",Errands do not clean up VMs when cancelled and using drain script ,1123776.0,"[1123776, 553935]",956238,1123776,feature,2014-07-09T00:58:21Z,https://www.pivotaltracker.com/story/show/69858338
2014-05-01T21:04:29Z,2013-12-11T17:51:30Z,accepted,,see for ref #62181872,4.0,62328814,story,"[{'name': 'folder-paths', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-09T18:16:32Z', 'id': 7435310, 'updated_at': '2014-07-31T05:06:43Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",vsphere cpi should allow to specify datacenter inside a folder,1123776.0,"[1123776, 553935]",956238,506755,feature,2014-07-31T05:06:43Z,https://www.pivotaltracker.com/story/show/62328814
2014-05-01T21:47:59Z,2014-04-08T12:21:30Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/555

We had a tenant created by our openstack provider called ""0001"". It currently ends up in the yaml without quotes. When the yaml is loaded, the 0001 is interpreted to be an integer and resolves to 1. This breaks our setup.

Filed by omarreiss",,69080790,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #555: Make sure openstack tenant is defined as string,,[],956238,1134058,chore,2014-05-01T21:48:09Z,https://www.pivotaltracker.com/story/show/69080790
2014-05-02T17:46:34Z,2014-05-01T21:35:44Z,accepted,,"Doc has been shared out with BOSH team and will be completed and circulated with London Svcs, PHD",0.0,70545170,story,[],Capture Drain & Persistent Disk Proposal in Doc,553935.0,"[553935, 1165386]",956238,1210852,feature,2014-05-02T17:46:34Z,https://www.pivotaltracker.com/story/show/70545170
2014-05-02T19:00:00Z,2014-04-24T03:47:09Z,accepted,,,1.0,70063638,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",make sure that trusty includes default root certificates,553935.0,"[553935, 1061299]",956238,81882,feature,2014-05-05T22:20:58Z,https://www.pivotaltracker.com/story/show/70063638
2014-05-02T19:00:00Z,2014-04-29T17:00:05Z,accepted,,"Warden doesn't start on Trusty stemcells because quota format is not supported in kernel:

{""timestamp"":1398782373.3103306,""message"":""Exited with status 2 (50.914s): [[\""/var/vcap/data/packages/warden/43.1/warden/src/closefds/closefds\"", \""/var/vcap/data/packages/warden/43.1/warden/src/closefds/closefds\""], \""/var/vcap/data/pac
kages/warden/43.1/warden/root/linux/setup.sh\""]"",""log_level"":""warn"",""source"":""Warden::Container::Linux"",""data"":{""stdout"":"" * Unloading AppArmor profiles\n   ...done.\n"",""stderr"":""quotaon: using //aquota.group on /dev/disk/by-uuid/b6d42f62
-0d02-438f-aaed-14dc475caab1 [/]: No such process\nquotaon: Quota format not supported in kernel.\nquotaon: using //aquota.user on /dev/disk/by-uuid/b6d42f62-0d02-438f-aaed-14dc475caab1 [/]: No such process\nquotaon: Quota format not supported in kernel.\n""},""thread_id"":17901680,""fiber_id"":23277240,""process_id"":2935,""file"":""/var/vcap/data/packages/warden/43.1/warden/lib/warden/container/spawn.rb"",""lineno"":135,""method"":""set_deferred_success""}",1.0,70376624,story,[],Trusty stemcell should support quota format ,553935.0,"[553935, 1061299]",956238,1015645,feature,2014-05-05T22:40:30Z,https://www.pivotaltracker.com/story/show/70376624
2014-05-02T19:00:00Z,2014-04-24T20:58:10Z,accepted,,"Was reported by @alex that warden containers use cgroup option for swap memory and it was not honored on trusty.

Kernel should set:
swapaccount=1
",1.0,70123498,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",Trusty should allow to set limit for swap memory,553935.0,"[553935, 1061299]",956238,1123776,feature,2014-05-05T22:43:29Z,https://www.pivotaltracker.com/story/show/70123498
2014-05-06T17:41:01Z,2014-01-23T22:51:16Z,accepted,,"- By default it should set the UTC timezone (like the Ubuntu stemcell), and not the timezone of the instance where the stemcell has been generated.
- CentOS stemcell doesn't set any locale - The Ubuntu stemcell sets the locale to 'en_US.UTF-8'. The CentOS stemcell doesn't set any locale.",,64426488,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}]",CentOS stemcell should set the UTC timezone or a locale,553935.0,"[553935, 1061299]",956238,1015645,bug,2014-05-06T17:39:48Z,https://www.pivotaltracker.com/story/show/64426488
2014-05-06T19:06:02Z,2014-02-20T13:30:41Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/529

Default 1 second sleep time on resource creation in the Openstack CPI could cause load/performance issues with some OpenStack installations. Making this configurable seems like a better option that enforcing a new default.

Change will keep the existing default if option is not set.

https://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-dev/Q8_oZd9shJ8

Filed by matjohn2",1.0,66141966,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #529: Expose the Openstack CPI wait_resource poll interval as a config item,1123776.0,"[1123776, 553935]",956238,1134058,feature,2014-05-06T19:06:02Z,https://www.pivotaltracker.com/story/show/66141966
2014-05-06T19:11:40Z,2014-03-12T05:42:44Z,accepted,,"As a user of BOSH, I should be able to stay relatively current with ruby releases so that I can stay within a supported/stable set of releases

For the outcome of this story, please articulate what is required to support ruby 2.1 with BOSH.  This output should be a story or set of stories ",2.0,67369232,story,[],Investigate Support Ruby 2.1.1 ,514635.0,"[514635, 1406536]",956238,1210852,feature,2014-05-07T05:13:09Z,https://www.pivotaltracker.com/story/show/67369232
2014-05-07T18:13:46Z,2014-04-22T18:59:33Z,accepted,,"Verify where we are checking sha1 and fingerprints of packages, jobs, and their compiled versions and whether we are checking the correct things in each case.",1.0,69952006,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",investigate usages of sha1 and fingerprints,1406536.0,[1406536],956238,1210852,feature,2014-05-07T18:13:47Z,https://www.pivotaltracker.com/story/show/69952006
2014-05-07T23:43:00Z,2014-04-23T18:27:04Z,accepted,,"good:
 Started deploy micro bosh > Creating VM from ami-11b0ab78 light. Done (00:00:35)

bad:
 Started deploy micro bosh > Creating VM from ami-11b0ab78 light. Done
",,70031890,story,[],Deploy Micro lost 'time taken per task' somewhere between cli_2334 and cli_2448,553935.0,"[553935, 514635]",956238,1210852,bug,2014-05-07T23:41:47Z,https://www.pivotaltracker.com/story/show/70031890
2014-05-09T00:08:03Z,2014-04-23T10:29:09Z,accepted,,In Bluefin we couldn't see the IP address of the PIVOTAL_CLOUDBEES Jenkins VM because vmware tools were not running.,,69996098,story,[],Notify stemcell team why vmware tools are not running,1068489.0,[1068489],956238,1300498,chore,2014-05-09T00:08:03Z,https://www.pivotaltracker.com/story/show/69996098
2014-05-09T12:00:00Z,2014-04-25T01:46:17Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/571



Filed by jbayer",1.0,70136990,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #571: add aws dyanmic network subnet with specs,553935.0,[553935],956238,1134058,feature,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/70136990
2014-05-09T16:02:40Z,2014-01-23T23:18:50Z,accepted,,,,64428218,story,"[{'name': 'warden-cpi', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-23T23:16:56Z', 'id': 7547258, 'updated_at': '2014-01-23T23:16:56Z'}]",bosh-lite is merged in.,,[],956238,14062,release,2014-05-09T16:02:40Z,https://www.pivotaltracker.com/story/show/64428218
2014-05-09T17:00:39Z,2014-04-22T18:54:39Z,accepted,,,2.0,69951554,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",investigate versions & how they are either affected by switch to fingerprints,1406536.0,[1406536],956238,1210852,feature,2014-05-09T16:59:26Z,https://www.pivotaltracker.com/story/show/69951554
2014-05-09T19:00:00Z,2014-04-30T23:06:06Z,accepted,,"We have a medium sized deployment with enough number of IP addresses (both dynamic and static). The first deployment always hits the “foo/0 asked for a dynamic IP but there were no more available” exception, but the second deployment always succeeds.

Talk to Jesse for context",,70483918,story,[],Investigate Dynamic IP allocation problem,553935.0,"[553935, 1068489]",956238,1338772,bug,2014-05-13T21:37:15Z,https://www.pivotaltracker.com/story/show/70483918
2014-05-09T22:17:36Z,2014-05-05T19:09:15Z,accepted,,see #70702898,1.0,70703028,story,[],Investigate Trusty Stemcell on AWS,514635.0,[514635],956238,1210852,feature,2014-05-09T22:16:23Z,https://www.pivotaltracker.com/story/show/70703028
2014-05-14T18:23:43Z,2014-04-09T21:38:41Z,accepted,,"We have a constraint in Ops Manager based on a BOSH that seems to have been solved in vSphere 5.0 patch 1.

See here: 

http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=2003484

Can we remove this constraint?",1.0,69222212,story,[],Power of 2 constraint unnecessary?,1406536.0,[1406536],956238,548715,feature,2014-05-14T19:19:58Z,https://www.pivotaltracker.com/story/show/69222212
2014-05-15T20:51:02Z,2014-05-12T19:18:06Z,accepted,,see #71173640,1.0,71173704,story,[],Investigation: Stemcell check,1123776.0,[1123776],956238,1210852,feature,2014-05-15T20:51:03Z,https://www.pivotaltracker.com/story/show/71173704
2014-05-16T19:19:57Z,2014-02-12T18:06:36Z,accepted,,"This is related to #65070172

We are seeing an error when uploading  a release where the SHA1 and version do not agree.

There should be only one unique identifier for Jobs (and packages), rather than two - because these can get out of sync.

This is especially important because the release creation process does not produce deterministic checksums.

See https://github.com/pivotal-cf/runtime-checklists/issues/13#issuecomment-34896371",2.0,65657038,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Replace job versions,1123776.0,[1123776],956238,1054467,feature,2014-05-16T19:18:45Z,https://www.pivotaltracker.com/story/show/65657038
2014-05-16T19:21:01Z,2014-02-04T01:06:01Z,accepted,,"Packages have SHA1's and versions.  Why?  Seems like packages should just have checksums which uniquely identify them, and no versions.",4.0,65070172,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Replace package versions,786819.0,"[786819, 1123776]",956238,14062,feature,2014-05-28T16:54:28Z,https://www.pivotaltracker.com/story/show/65070172
2014-05-16T19:34:05Z,2014-05-13T18:01:16Z,accepted,,see #71255776,1.0,71255882,story,[],BOSH Stemcell Check Part 2,514635.0,[514635],956238,1210852,feature,2014-05-16T19:34:06Z,https://www.pivotaltracker.com/story/show/71255882
2014-05-19T16:54:17Z,2014-02-18T18:11:01Z,accepted,,Bundler fills the screen with warnings because of duplicate gem declarations,,65993822,story,[],Clean up bosh Gemfile,1123776.0,[1123776],956238,1123776,chore,2014-05-19T16:54:18Z,https://www.pivotaltracker.com/story/show/65993822
2014-05-19T20:58:20Z,2014-05-12T06:46:43Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/577

Creation of jobs based on AWS spot instances often fail with errors like the following:
```
D, [2014-05-10T08:16:45.392793 #3757] [create_vm(d8c85ece-3d74-43df-9c6d-2da7dd03f9aa, ...)] DEBUG -- : Checking state of spot instance requests...
I, [2014-05-10T08:16:45.481592 #3757] [create_vm(d8c85ece-3d74-43df-9c6d-2da7dd03f9aa, ...)]  INFO -- : [AWS Core 400 0.088475 0 retries] describe_spot_instance_requests(:spot_instance_request_ids=>[""sir-37a31a3b""]) AWS::EC2::Errors::InvalidSpotInstanceRequestID::NotFound The spot instance request ID 'sir-37a31a3b' does not exist
E, [2014-05-10T08:17:45.482022 #3757] [create_vm(d8c85ece-3d74-43df-9c6d-2da7dd03f9aa, ...)] ERROR -- : The spot instance request ID 'sir-37a31a3b' does not exist (AWS::EC2::Errors::InvalidSpotInstanceRequestID::NotFound)
```

This is caused by a delay in AWS updating its state - the spot request has been created, but isn't yet available for querying via `describe_spot_instance_requests`

Retrying the `describe_spot_instance_requests` resolves the error; which is what this PR implements

Filed by mrdavidlaing",1.0,71109702,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #577: Retry checking spot instance request state on AWS::EC2::Errors::InvalidSpotInstanceRequestID::NotFound,514635.0,"[514635, 1406536]",956238,1134058,feature,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/71109702
2014-05-19T21:00:32Z,2014-03-05T22:45:43Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/539

These blobstore properties are added for main blobstore and now also compiled_package_cache.

This PR was recreated from https://github.com/cloudfoundry/bosh/pull/495 as that PR was originally submitted against cloudfoundry/master instead of develop branch.

Filed by grenzr


To Accept:

1. Install [aws cli](http://docs.aws.amazon.com/cli/latest/index.html) use same credentials as director for configuration
1. Deploy BOSH with s3 blobstore:
```
  blobstore:
    provider: s3
    bucket_name: <bucket_name>
    access_key_id: <access_key>
    secret_access_key: <access_secret_key>
    s3_multipart_threshold: 1048576
```
1. Upload stemcell to director, at the same time watch aws multipart uploads: http://docs.aws.amazon.com/cli/latest/reference/s3api/list-multipart-uploads.html
1. Uploads should be split up by 1MB size.",1.0,66986818,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'triaged', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T15:41:31Z', 'id': 7533156, 'updated_at': '2014-01-22T15:41:31Z'}]",cloudfoundry/bosh #539: Adding S3 blobstore options: ssl_verify_peer + s3_multipart_threshold,553935.0,"[553935, 1406536]",956238,1134058,feature,2014-05-19T21:00:33Z,https://www.pivotaltracker.com/story/show/66986818
2014-05-29T03:09:31Z,2014-05-05T23:52:35Z,accepted,,"including
- patches of patches
- cloud ops requirements/use case

this doc exists :  Proposal: Final Release Production Patching With BOSH
https://docs.google.com/a/pivotallabs.com/document/d/1DrZxDrTVyVt7hMeivgc6aujIRILVdQaZ1mzKBZFMATI/edit",,70726110,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",capture use cases into a document,1123776.0,[1123776],956238,1210852,chore,2014-05-29T03:09:32Z,https://www.pivotaltracker.com/story/show/70726110
2014-05-29T05:56:52Z,2014-05-08T20:03:23Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/105

Setting up a vagrant box with bosh-lite braught me to the latest-bosh-stemcell-warden. Changing the README.md and the provision_cf script might help people to get started without researching issues. 

Filed by lwieske",1.0,70973990,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'waiting-on-github-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T18:34:21Z', 'id': 7524758, 'updated_at': '2014-01-21T18:34:21Z'}]",cloudfoundry/bosh-lite #105: Changed README.md an scripts/provision_cf to reflect new warden stemcell...,1123776.0,[1123776],956238,1134058,feature,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/70973990
2014-05-29T17:31:28Z,2014-05-27T17:08:31Z,accepted,,,,72105026,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Add Ruby 2.1 to all CI boxes,786819.0,"[786819, 1054467]",956238,1123776,chore,2014-05-29T17:31:30Z,https://www.pivotaltracker.com/story/show/72105026
2014-05-29T17:53:17Z,2014-05-29T16:36:08Z,accepted,,,,72273202,story,[],symlink /tmp/ci-artifacts to /mnt/ci-artifacts on aws ci slaves,1054467.0,[1054467],956238,81882,chore,2014-07-02T16:52:25Z,https://www.pivotaltracker.com/story/show/72273202
2014-05-29T22:18:36Z,2014-04-28T23:01:55Z,accepted,,"```
be bosh create release --force --final
WARNING: use of blobstore_secret is deprecated
Syncing blobs...
Total: 0, 0B

You have some blobs that need to be uploaded:
new	goloang/go1.2.1.linux-amd64.tar.gz	53.5M

When ready please run `bosh upload blobs'
Proceeding with dirty blobs as '--force' is given
Are you sure you want to generate final version? yes

Building FINAL release
-----------------------------------

Building packages
-----------------
Building golang...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
No matching build found for `golang' package.
Please consider creating a dev release first.
The fingerprint is `b22c4efaa412257cd4aeaf1f04850edcf4281383'.
```",1.0,70326302,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",a release developer should be able to create a final release without having first created a dev release,1406536.0,[1406536],956238,1054467,feature,2014-07-31T05:41:13Z,https://www.pivotaltracker.com/story/show/70326302
2014-05-30T16:48:38Z,2014-05-30T00:21:53Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/113

Signed-off-by: Matthew Boedicker <mboedicker@pivotallabs.com>

Filed by gajwani",1.0,72312812,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh-lite #113: Support deploying to a VPC on AWS.,,[],956238,1134058,feature,2014-06-02T16:03:39Z,https://www.pivotaltracker.com/story/show/72312812
2014-06-02T15:55:40Z,2014-05-22T16:24:53Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/112

* Updated .ruby-version for version managers
* Updated url's for ubuntu iso's (old-releases now)
* Converged on one ruby in the chef recipes

[#71367862]

Signed-off-by: Michael Fraenkel <fraenkel@us.ibm.com>

Filed by sykesm",1.0,71858994,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh-lite #112: Update ruby to 1.9.3-p547,,[],956238,1134058,feature,2014-06-02T15:55:40Z,https://www.pivotaltracker.com/story/show/71858994
2014-06-02T23:21:56Z,2014-06-02T18:47:45Z,accepted,,"breaking out compilation of this list from #72376106

After the documentation is done, we can move on to #72376106",2.0,72466652,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Document scenarios where release versions are passed between CLI and Director,1266616.0,"[1266616, 320647]",956238,1210852,feature,2014-06-03T00:59:32Z,https://www.pivotaltracker.com/story/show/72466652
2014-06-05T18:20:57Z,2014-06-05T17:01:17Z,accepted,,create matrix of what version of open_ssl is in which stemcell for all stemcells,,72716056,story,[],investigate stemcells,1266616.0,[1266616],956238,1210852,chore,2014-06-05T18:20:58Z,https://www.pivotaltracker.com/story/show/72716056
2014-06-05T21:08:23Z,2014-05-12T21:29:49Z,accepted,,"On my setup in Germany, I'm sometimes running into ""execution expired"" when deleting a VM, and I traced it back to this line in the vSphere CPI:
```
      http_client.connect_timeout = 4
```

Admittedly, it should be able to *connect* in 4 seconds, but I'm not sure it would hurt us to raise to 30 seconds?

The stacktrace is this:
```
D, [2014-05-12T21:17:52.592920 #32699] [0x1adee30] DEBUG -- : Worker thread raised exception: execution expired - /var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:775:in `initialize'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:775:in `new'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:775:in `create_socket'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:732:in `block in connect'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:731:in `connect'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:594:in `query'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient/session.rb:161:in `query'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:1060:in `do_get_block'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:869:in `block in do_request'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:956:in `protect_keep_alive_disconnected'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:868:in `do_request'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:756:in `request'
/var/vcap/packages/director/gem_home/gems/httpclient-2.2.4/lib/httpclient.rb:666:in `post'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:29:in `invoke_method'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:13:in `invoke_method'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:45:in `block (3 levels) in finalize'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/client.rb:209:in `find_by_inventory_path'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/resources/folder.rb:29:in `find_folder'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/resources/folder.rb:19:in `find_or_create_folder'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/resources/folder.rb:13:in `initialize'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/resources/datacenter.rb:19:in `new'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/resources/datacenter.rb:19:in `vm_folder'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/cloud.rb:588:in `block in get_vm_by_cid'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/cloud.rb:587:in `each_value'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/cloud.rb:587:in `get_vm_by_cid'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/cloud.rb:192:in `block in delete_vm'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2366.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.2366.0/lib/cloud/vsphere/cloud.rb:189:in `delete_vm'

```",1.0,71185662,story,[],Should we increase the timeout for connecting to vSphere?,1123776.0,[1123776],956238,1408486,feature,2014-06-05T21:08:23Z,https://www.pivotaltracker.com/story/show/71185662
2014-06-05T21:08:46Z,2014-06-02T22:57:17Z,accepted,,,0.0,72486664,story,[],investigate whether `bosh create release` with no files changing creates different fingerprint,1266616.0,"[1266616, 320647]",956238,1210852,feature,2014-06-05T21:08:47Z,https://www.pivotaltracker.com/story/show/72486664
2014-06-05T21:56:35Z,2014-06-02T17:16:32Z,accepted,,"as a release developer, I don't want hidden files to be included in fingerprint/tgz file so that for tgzs that are the same, the same fingerprint is generated.  This results in pre-compilation happening every time.",1.0,72458736,story,[],investigate hidden files (.name) cause uniqueness in fingerprints,514635.0,[514635],956238,1210852,feature,2014-06-05T21:56:36Z,https://www.pivotaltracker.com/story/show/72458736
2014-06-05T22:12:40Z,2013-12-06T22:34:40Z,accepted,,"as a release developer, i should be able to define a final release version using
```
 `bosh create release --final --version= x`
 `bosh create release --final --version= x.x`
 `bosh create release --final --version= x.x.x`
 `bosh create release --final [default increments]`
```

also
- Attempting to specify a version when creating a dev release should return an error
- we need to enforce a restriction on versions so that we can compare them numerically",2.0,62085266,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",BOSH releases should have a user-defined final version,1266616.0,"[1266616, 553935]",956238,14062,feature,2014-06-09T23:33:26Z,https://www.pivotaltracker.com/story/show/62085266
2014-06-09T12:00:00Z,2014-06-06T07:33:05Z,accepted,,"https://github.com/cloudfoundry/bosh/issues/587

When deploying with a ubuntu trusty stemcell, we get 

   mkswap: warning: /dev/sdb1 is misaligned

in the log in /var/vcap/bosh/log. Both the ephemeral (sdb) and persistent (sdc) disks are misaligned. Go agent and ruby agent use the same partitioning logic, and give the same result. 

Is this significant?

fdisk -l output can be found below. 

Regards
Jon Kåre Hellan, UNINETT AS, Trondheim, Norway

fdisk -l says:

Disk /dev/sdb: 21.0 GB, 20971520000 bytes
255 heads, 63 sectors/track, 2549 cylinders, total 40960000 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x00000000

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               1     7020404     3510202   82  Linux swap / Solaris
Partition 1 does not start on physical sector boundary.
/dev/sdb2         7020405    40949684    16964640   83  Linux
Partition 2 does not start on physical sector boundary.

and for the persistent disk sdc:

Disk /dev/sdc: 17.2 GB, 17179869184 bytes
255 heads, 63 sectors/track, 2088 cylinders, total 33554432 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x00000000

   Device Boot      Start         End      Blocks   Id  System
/dev/sdc1               1    33543719    16771859+  83  Linux
Partition 1 does not start on physical sector boundary.


Filed by jhellan",,72758866,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",cloudfoundry/bosh #587: misaligned partitions on trusty ephemeral and persistent disks,553935.0,[553935],956238,1134058,bug,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/72758866
2014-06-09T17:16:04Z,2014-04-22T08:06:05Z,accepted,,"boshartifacts.cloudfoundry.org was cnamed to bosh-artifacts.cfapps.io .  in order for the run.pivotal.io router to support that, we need to move the bosh-artifacts app / routes to the cfcommunity org that owns the cloudfoundry.org domain.",0.0,69904516,story,[],move the bosh-artifacts app / routes to the cfcommunity org,553935.0,"[553935, 514635]",956238,1210852,feature,2014-06-09T17:16:04Z,https://www.pivotaltracker.com/story/show/69904516
2014-06-10T17:42:57Z,2014-04-30T16:27:34Z,accepted,,"powerdns is gpl.  need to check with the people who we need to check with.  This story comes from the charter conducted here #70191210

acceptance:
powerdns works on trusty with mysql in *full bosh*",1.0,70452708,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",upgrade powerdns to 3.3.1,1123776.0,[1123776],956238,1210852,feature,2014-06-10T17:42:58Z,https://www.pivotaltracker.com/story/show/70452708
2014-06-10T18:14:27Z,2014-02-27T20:37:22Z,accepted,,"Use case:

CF v159 loggregator worked
CF v160 loggergator had an issuse
CF v161 is git revert of the loggregator bump that moved to v160.

The problem is that because the fingerprints are the same for v159 and v161, bosh thinks it doesn't have to do anything and won't update the loggregator job to v161, it'll stay at v160. 

The work around is to change whitespace or something that'll force new fingerprints to make bosh behave as expected.

",,66621702,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",BOSH should allow uploading release with same pkg/job fingerprints but different SHA1's,1266616.0,[1266616],956238,637633,bug,2014-06-10T18:14:27Z,https://www.pivotaltracker.com/story/show/66621702
2014-06-10T20:09:27Z,2014-06-02T16:59:20Z,accepted,,"AS any bosh user, I should know about all the changes happening with release versioning so that I can effectively make the appropriate decisions that impact my release strategy/release naming strategy",1.0,72457474,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Release Versioning changes need to be well documented,81882.0,[81882],956238,1210852,feature,2014-06-10T20:09:27Z,https://www.pivotaltracker.com/story/show/72457474
2014-06-10T20:59:54Z,2014-05-12T21:58:47Z,accepted,,"in the case discovered, mysql & riak both have the same package with the same version.  The director does not recognize them as unique packages and the second release will fail to upload release with a SHA1 mismatch",,71187928,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",non-collocated jobs from different releases can collide on packages where package/version is the exact same,1266616.0,[1266616],956238,1210852,bug,2014-06-10T20:59:54Z,https://www.pivotaltracker.com/story/show/71187928
2014-06-10T21:13:57Z,2013-11-19T21:50:00Z,accepted,,"Fingerprints (what's actually in the tgz) can be identical, with different checksums due to a variety of reasons.  When we determine whether the package is already uploaded, we check both checksum and fingerprint, but then only confirm validity on checksum.

This causes valid releases to fail to upload.",1.0,61077500,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Bosh release upload should check fingerprints rather than checksums,1266616.0,[1266616],956238,1068489,feature,2014-06-10T21:13:58Z,https://www.pivotaltracker.com/story/show/61077500
2014-06-10T21:26:14Z,2013-10-29T18:32:14Z,accepted,,"See the attached logs for the error. The release specifies that the package mysqlclient should have sha 'd76d1d18321d5ccb5e9fbb21ffe4c1cb47866e36', but for some reason the packages table has sha '518ba2b619bc4656aa0b8cfd944f11bfe78047fe'

The release was too large to attach, so we put it in the shared Dropbox:
https://www.dropbox.com/sh/j4s2y5so2exosap/shvl1U0Dtj",,59776002,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Bosh should not save incorrect sha for a package,1266616.0,"[1266616, 81882]",956238,285989,bug,2014-06-10T21:26:15Z,https://www.pivotaltracker.com/story/show/59776002
2014-06-10T23:37:32Z,2014-04-08T23:24:20Z,accepted,,"This appears to be a problem with the transmission of the compiled_packages TGZ from the director to the bosh_cli.

The director has artifacts of the correct size:
```
-rw-r--r-- 1 vcap vcap 1446062738 2014-04-08 23:20 compiled_packages_1396999167.4664404.tar.gz
```

but the CLI consistently downloads a smaller file:
```
-rw-r--r--   1 pivotal  staff  1077469184 Apr  8 16:21 cf-165-bosh-vsphere-esxi-ubuntu-2200.tgz
```
",,69144598,story,"[{'name': 'compiled-packages', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-09T18:12:51Z', 'id': 7435282, 'updated_at': '2014-07-31T04:56:46Z'}]",`bosh export compiled_packages` should not truncate .tgz,1406536.0,[1406536],956238,1161568,bug,2014-07-31T04:56:46Z,https://www.pivotaltracker.com/story/show/69144598
2014-06-11T00:20:27Z,2014-06-05T17:21:37Z,accepted,,"ReleaseVersion.valid checks for both uniqueness and validity, but the error thrown when it's used in UpdateRelease.process_release is always ReleaseAlreadyExists. This is confusing if you manage to upload a release with a bad version.

One way to repro would be to create release with a custom manifest and then upload it.

While we're fixing this it might make sense to use Bosh::Common::Version::ReleaseVersion.parse to validate, instead of relying on the simple regex of allowed characters that Model::ReleaseVersion.valid uses.",,72717508,story,[],Director should error with ReleaseVersionInvalid instead of ReleaseAlreadyExists if the version is invalid on upload release,553935.0,[553935],956238,1266616,bug,2014-06-11T00:20:28Z,https://www.pivotaltracker.com/story/show/72717508
2014-06-11T17:36:23Z,2014-06-09T23:50:07Z,accepted,,as a new bosh user I need the net-ssh-gateway gem bumped otherwise BOSH will not be operable,1.0,72929204,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Bump net-ssh-gateway gem in bosh cli,1266616.0,[1266616],956238,1210852,feature,2014-06-11T17:36:23Z,https://www.pivotaltracker.com/story/show/72929204
2014-06-11T17:54:48Z,2014-05-06T00:27:53Z,accepted,,"Can one of our existing tools (e.g. Code Climate, Travis) publish this for us? Needs to be publically-available.",,70727746,story,[],Add code coverage to CI,1123776.0,[1123776],956238,514635,chore,2014-06-11T17:54:48Z,https://www.pivotaltracker.com/story/show/70727746
2014-06-11T19:00:23Z,2014-05-30T22:14:19Z,accepted,,"After pushing #62085266 we found that existing dev releases cannot be redeployed. 

The cause is that the director pre-parses the requested dev version to the new format, looks for it in the database, can't find it, and returns an error:

```
bosh -v -n -P 10 --config '/tmp/d20140530-24630-qo88i8/bosh_config20140530-24630-1yilby6' deploy
Getting deployment properties from director...
Compiling deployment manifest...

Director task 242
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Failed: Release version `dummy/0+dev.22' doesn't exist (00:00:00)

Error 30006: Release version `dummy/0+dev.22' doesn't exist
```

The proposed solution is to have ReleaseManager.find_version look for the old version if a new version is passed in and doesn't exist in the database. This will require reverse parsing from ""+dev.#"" to ""#-dev"".",,72374480,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]","A bosh user should be able to redeploy an existing dev release (new cli, new director, old data)",1266616.0,[1266616],956238,1266616,bug,2014-06-11T19:00:24Z,https://www.pivotaltracker.com/story/show/72374480
2014-06-11T19:23:50Z,2014-05-30T22:48:15Z,accepted,,"The new CLI should be able to handle the output of the old Director.
The old Director should be able to handle the input from the new CLI.

1) Given the new CLI and the old Director, where a previously deployed release (with the old CLI) is being re-released (with the new CLI), an error should not occur during deployment.
2) Given the new CLI and the old Director, where a new release is being released (with an old-format release version), an error should not occur.
3) Given the new CLI and the old Director, where a previously deployed release (with the old CLI) is being re-released (with the new CLI), an error should not occur during deployment.

The CLI may have to check the version of the Director it's talking to and reverse-parse new-format versions to old-format versions (if talking to an old Director) in order for this to work.

This case is more likely than #72375740, because the CLI (gem) is easier to upgrade than the Director (stemcell)",4.0,72376106,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]","As a bosh operator, I should be able to use the new CLI with the old Director (pre/post version changes)",1266616.0,"[1266616, 553935, 320647]",956238,1266616,feature,2014-06-11T19:23:51Z,https://www.pivotaltracker.com/story/show/72376106
2014-06-11T20:35:09Z,2014-02-13T17:50:47Z,accepted,,See #65512070,,65742514,story,[],Deployment Cleanup,1068489.0,[1068489],956238,1068489,chore,2014-06-11T20:35:09Z,https://www.pivotaltracker.com/story/show/65742514
2014-06-11T20:40:56Z,2014-05-19T16:30:26Z,accepted,,"automate so that deps are bumped every time the bosh-lite ci builds

",2.0,71591118,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",bump dependencies in bosh-lite CI,514635.0,[514635],956238,1210852,feature,2014-06-11T21:35:39Z,https://www.pivotaltracker.com/story/show/71591118
2014-06-11T21:14:47Z,2014-05-30T22:37:48Z,accepted,,"The old CLI should be able to handle the output of the new Director.
The new Director should be able to handle the input from the old CLI.

1) Given the old CLI and the new Director, where a previously deployed release (with the old Director) is being re-released (with the new Director), an error should not occur during deployment.
2) Given the old CLI and the new Director, where a new release is being released, an error should not occur.
3) Given the old CLI and the new Director, where a previously deployed release (with the new Director) is being re-released (with the new Director), an error should not occur during deployment.",2.0,72375740,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]","As a bosh operator, I should be able to use the old CLI with the new Director (pre/post version changes)",553935.0,"[553935, 320647]",956238,1266616,feature,2014-06-11T21:14:48Z,https://www.pivotaltracker.com/story/show/72375740
2014-06-11T21:31:29Z,2014-06-10T00:19:40Z,accepted,,"as a bosh user, i should have access to information about changes to release versioning written by the team who worked on the changes so that I can have better information than just release notes ",1.0,72930238,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",BOSH Release Versioning Blog Post,1266616.0,[1266616],956238,1210852,feature,2014-06-11T21:32:07Z,https://www.pivotaltracker.com/story/show/72930238
2014-06-11T21:51:28Z,2014-06-10T18:37:16Z,accepted,,"bosh micro plugin requires cloud/provider, which was moved to the director from bosh_cpi. Remove the require.
The broken gems need to be yanked for bosh and bosh micro.

```
bosh create release
Failed to load plugin /Users/pivotal/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2597.0/lib/bosh/cli/commands/micro.rb: cannot load such file -- cloud/provider
```",,72991988,story,[],bosh micro should not require cloud/provider (which it doesn't have access to),1266616.0,"[1266616, 81882]",956238,1266616,bug,2014-06-11T21:51:28Z,https://www.pivotaltracker.com/story/show/72991988
2014-06-11T21:51:45Z,2014-05-10T00:17:16Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/576

This change adds support for boot from volume to the OpenStack CPI. To use, add the following property to the Micro BOSH or BOSH deployment manifest:

    cloud:
      plugin: openstack
      properties:
        openstack:
          boot_from_volume: true

Changes:

• Add a reference to the fog Volume API which supports creating volumes with an imageRef
• Create a boot volume before creating the VM with the disk size specified by the flavor
• Add a block device mapping to the VM on creation to mount the new volume at /dev/vda
• Skip setting the ""personality"" property on VM creation since file injection is not supported with BFV
• Added test stubs for the Fog::Volume API to fix failures
• Added new tests for creating a boot volume and creating a VM using boot from volume


Filed by amhuber",2.0,71066352,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #576: Adding boot from volume changes.in,514635.0,[514635],956238,1134058,feature,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/71066352
2014-06-11T22:10:51Z,2014-05-30T19:35:58Z,accepted,,blocked by #72354178,,72364708,story,[],upgrade rspec to latest (3.0.0),786819.0,"[786819, 320647]",956238,1054467,chore,2014-06-11T22:10:52Z,https://www.pivotaltracker.com/story/show/72364708
2014-06-11T22:10:54Z,2014-05-30T17:09:49Z,accepted,,,,72354178,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",specinfra is incompatible with rspec 3.0.0.rc1 (which is ~> 3.0.0.beta1),786819.0,"[786819, 320647]",956238,1054467,chore,2014-06-11T22:10:55Z,https://www.pivotaltracker.com/story/show/72354178
2014-06-12T00:36:24Z,2014-06-12T00:27:03Z,accepted,,sprout-wrap should install this for us,,73101402,story,[],Remove unneeded development dependencies,1123776.0,[1123776],956238,1123776,chore,2014-06-12T00:36:24Z,https://www.pivotaltracker.com/story/show/73101402
2014-06-12T16:02:45Z,2014-06-11T19:20:50Z,accepted,,"It seems there is an issue where VMs in a default VPC are unable to access the internet unless an elastic IP has been associated with the instance. Investigate what AWS configuration options, if any, are required to allow outbound internet connections from VMs deployed into the default VPC without an associated elastic IP.

Acceptance Criteria:
- Stories to address issues in BOSH or Documentation for users deploying to AWS if no issues found",1.0,73080910,story,[],Investigate outbound networking behavior for AWS accounts with a default VPC ,1266616.0,"[1266616, 1123776]",956238,1210852,feature,2014-06-12T20:32:45Z,https://www.pivotaltracker.com/story/show/73080910
2014-06-12T23:42:45Z,2014-05-05T16:57:24Z,accepted,,Not an exhaustive list of gems—others may need to be upgraded too,4.0,70689810,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Upgrade gems in preparation for Ruby 2.1.1 support,786819.0,"[786819, 320647, 553935]",956238,514635,feature,2014-06-12T23:42:46Z,https://www.pivotaltracker.com/story/show/70689810
2014-06-12T23:43:45Z,2014-06-02T18:44:09Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/114

...#72378510].

Signed-off-by: Nikhil Gajwani <ngajwani@pivotallabs.com>

Filed by gajwani",1.0,72466378,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",cloudfoundry/bosh-lite #114: Adds IPTable entries for port forwarding inside the BOSH-lite [finishes ...,786819.0,[786819],956238,1134058,feature,2014-06-12T23:43:45Z,https://www.pivotaltracker.com/story/show/72466378
2014-06-13T15:58:30Z,2014-06-03T22:10:07Z,accepted,,,,72570686,story,[],Latest AWS stemcells should be public,1123776.0,[1123776],956238,668075,chore,2014-06-13T15:58:31Z,https://www.pivotaltracker.com/story/show/72570686
2014-06-13T22:37:12Z,2014-06-13T18:25:04Z,accepted,,@thansmann requested latest openstack stemcell so we decided to produce 2605 manually,,73223980,story,[],Build openstack trusty go_agent with 2605,553935.0,[553935],956238,553935,chore,2014-06-13T22:37:13Z,https://www.pivotaltracker.com/story/show/73223980
2014-06-14T15:08:10Z,2014-06-12T16:29:46Z,accepted,,https://travis-ci.org/cloudfoundry/bosh/jobs/27377397,,73144340,story,[],Travis fails to install go vet,81882.0,"[81882, 553935]",956238,1123776,chore,2014-06-14T15:08:11Z,https://www.pivotaltracker.com/story/show/73144340
2014-06-19T00:19:58Z,2014-04-01T17:46:32Z,accepted,,,,68653578,story,[],Create Jenkins jobs for OS image build,1123776.0,[1123776],956238,553935,chore,2014-06-19T00:19:58Z,https://www.pivotaltracker.com/story/show/68653578
2014-06-19T00:45:17Z,2014-06-19T00:06:33Z,accepted,,"Travis builds are flakey, particularly the integration builds with ruby agent on 2.1.1. Remove those builds",,73513652,story,[],Fixing travis,514635.0,[514635],956238,1123776,chore,2014-06-19T00:45:17Z,https://www.pivotaltracker.com/story/show/73513652
2014-06-19T18:07:22Z,2014-06-05T20:14:18Z,accepted,,See #72732618,2.0,72732806,story,[],Appendicitis,1123776.0,[1123776],956238,1406536,feature,2014-06-21T00:36:48Z,https://www.pivotaltracker.com/story/show/72732806
2014-06-19T18:11:29Z,2014-06-19T17:13:21Z,accepted,,,,73561078,story,[],switch auto-deploy aws build to use trusty stemcell,1123776.0,[1123776],956238,1123776,chore,2014-06-19T18:11:29Z,https://www.pivotaltracker.com/story/show/73561078
2014-06-19T19:27:34Z,2014-06-19T16:47:56Z,accepted,,,,73558386,story,[],Fix stat collector tests flakiness in go agent,553935.0,[553935],956238,553935,chore,2014-06-19T19:27:35Z,https://www.pivotaltracker.com/story/show/73558386
2014-06-19T21:27:01Z,2014-06-17T01:12:05Z,accepted,,"as a deployment operator, I should be able to view exact CPU utilitaztion when I execute `bosh vms --vitals` so that I can troubleshoot issues

See snippet in activity below",,73357316,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Go Agent Incorrectly reports User CPU Utilitaztion,553935.0,"[553935, 1115834]",956238,1210852,bug,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/73357316
2014-06-19T21:27:08Z,2014-05-06T23:25:46Z,accepted,,"- release job has <%= spec.networks('blah').ip %> in a template
- deployment manifest puts that job on a vm with following network
```
networks:
- name: default
  type: dynamic
  dns:
  - 64.151.120.61
  cloud_properties:
    security_groups:
    - default
    net_id: 261460c4-8ad1-4412-b810-5a7eb8adff79
```
- ip address that gets interpolated should match correct ip address",4.0,70811750,story,[],release job that determines its own ip address from a job template should get a correct ip address when deployment puts that job on a 'dynamic' type network,81882.0,[81882],956238,81882,feature,2014-06-19T21:27:08Z,https://www.pivotaltracker.com/story/show/70811750
2014-06-19T22:10:53Z,2014-05-29T00:41:28Z,accepted,,"as a deployment operator, i should be able to have bosh retry blob downloads wen i encounter things like poor connections, etc. so that the deployment is streamlined as efficiently as possible",2.0,72233564,story,"[{'name': 'parity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623540, 'updated_at': '2014-06-06T18:53:38Z'}]",blobstore download retry,81882.0,[81882],956238,1210852,feature,2014-06-19T22:10:54Z,https://www.pivotaltracker.com/story/show/72233564
2014-06-23T17:26:17Z,2014-06-04T00:21:43Z,accepted,2014-06-20T19:00:00Z,,,72576940,story,[],BOSH freeze date for july release,,[],956238,1210852,release,2014-06-23T17:26:17Z,https://www.pivotaltracker.com/story/show/72576940
2014-06-23T17:27:37Z,2014-06-19T17:05:27Z,accepted,,Sometimes ruby 2.1.1 tests are hanging. Looks like some processes ignore kill signals.,,73560270,story,[],Investigate ruby 2.1.1 builds hanging in CI,553935.0,[553935],956238,553935,chore,2014-06-23T17:27:50Z,https://www.pivotaltracker.com/story/show/73560270
2014-06-23T18:07:42Z,2014-06-19T16:23:42Z,accepted,,,,73556684,story,"[{'name': 'juju', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-18T16:28:00Z', 'id': 8747846, 'updated_at': '2014-06-18T16:28:00Z'}]",Break out erb evaluation helpers into a separate gem with a small CLI so it can be reused by the python based CF charm,5637.0,[5637],956238,1406536,chore,2014-06-23T18:07:42Z,https://www.pivotaltracker.com/story/show/73556684
2014-06-23T22:14:53Z,2014-06-12T18:09:34Z,accepted,,,,73151580,story,[],Investigate freshness of debootstrap kernel packages,553935.0,"[553935, 786819]",956238,786819,chore,2014-06-23T22:14:53Z,https://www.pivotaltracker.com/story/show/73151580
2014-06-24T01:36:20Z,2013-12-12T18:10:01Z,accepted,,"vSphere has this notion of datastore clusters, which can present multiple datastores as one object. It is where things are moving in VMware's customer base, but conflicts a little with how we use regexes to find multiple datastores.

We should investigate this feature, what it would mean in terms of code changes, etc.

one pair - target one day/sync at end of day

output:
stories to complete the areas probed in tasks",2.0,62405674,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",Investigate compatability of Datastore Clusters,553935.0,"[553935, 320647]",956238,548715,feature,2014-06-24T01:36:21Z,https://www.pivotaltracker.com/story/show/62405674
2014-06-24T02:19:49Z,2014-05-05T22:20:41Z,accepted,,"We want to keep director and other components running 1.9.3, only CLI 2.1.1.

This is verify that the BOSH CLI is supported in Ruby 2.1.1

A potential solution is:
*  Have a script to install ruby 2.1.1 for Travis
* Run test with a rake argument 2.1.1,
* Change BoshRunner to shell out to a ruby management tool (rvm) to use the correct version of ruby",4.0,70721334,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Run BOSH CLI with Ruby 2.1.1,553935.0,"[553935, 320647]",956238,514635,feature,2014-06-24T02:19:50Z,https://www.pivotaltracker.com/story/show/70721334
2014-06-25T06:22:35Z,2014-05-12T16:53:33Z,accepted,,See https://github.com/cloudfoundry/bosh-checklists/issues/5,,71158468,story,[],On-call checklist for Adam,1123776.0,[1123776],956238,1068489,chore,2014-06-25T06:22:25Z,https://www.pivotaltracker.com/story/show/71158468
2014-06-26T22:25:46Z,2014-06-23T17:04:36Z,accepted,,"as part of ruby upgrade, need to determine problem with looping in thread pool even though threads have completed work

Compile package keeps looping because thread pool is still working but thread task already finished",,73729518,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Fix thread pool issue,553935.0,"[553935, 786819, 81882]",956238,553935,bug,2014-06-26T22:25:47Z,https://www.pivotaltracker.com/story/show/73729518
2014-06-27T15:56:11Z,2014-06-26T23:32:39Z,accepted,,,,74010728,story,[],Speed up integration tests,1123776.0,"[1123776, 514635]",956238,1123776,chore,2014-06-27T15:56:10Z,https://www.pivotaltracker.com/story/show/74010728
2014-06-27T17:51:36Z,2014-06-26T23:20:01Z,accepted,,"> The attached errand show some awesome behavior - it deletes its vm on the way out. The 'reduced' file saves you cruft.
>
> Tony && Jesse",,74010076,story,[],Investigate why Tony's errand did not refill the resource pool,1123776.0,[1123776],956238,1123776,chore,2014-06-27T17:51:36Z,https://www.pivotaltracker.com/story/show/74010076
2014-06-27T19:00:00Z,2014-06-05T21:12:08Z,accepted,,"as a release developer, i should be able to use an exclusion pattern to exclude specifically defined files from fingerprints so that releases that are the same have the same fingerprints",2.0,72738194,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",add support for an exclusion pattern in package contents,514635.0,[514635],956238,1210852,feature,2014-07-31T05:41:01Z,https://www.pivotaltracker.com/story/show/72738194
2014-06-30T23:40:13Z,2014-06-30T23:13:45Z,accepted,,The latest nats requires new thin which conflicts with other gems. This breaks installing director as a gem. We decided to update NATS in separate story.,,74184720,story,[],Lock down nats dependency,553935.0,[553935],956238,553935,chore,2014-06-30T23:40:14Z,https://www.pivotaltracker.com/story/show/74184720
2014-07-01T02:25:04Z,2014-05-27T19:33:47Z,accepted,,Run the tests and fix anything that's broken. Note 1.9.3 should still be supported.,1.0,72120786,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Upgrade bosh-dev to Ruby 2.x,514635.0,[514635],956238,1123776,feature,2014-07-01T02:25:05Z,https://www.pivotaltracker.com/story/show/72120786
2014-07-02T00:26:46Z,2014-07-01T17:22:44Z,accepted,,,,74242904,story,[],Add onboarding documentation,1123776.0,[1123776],956238,1123776,chore,2014-07-02T00:27:45Z,https://www.pivotaltracker.com/story/show/74242904
2014-07-02T18:39:00Z,2014-07-02T00:24:16Z,accepted,,,,74271702,story,[],Install go on other CI slaves so that integration builds can be parallelizable,553935.0,[553935],956238,553935,chore,2014-07-02T18:39:14Z,https://www.pivotaltracker.com/story/show/74271702
2014-07-02T20:36:45Z,2014-06-24T09:45:32Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/594

[@calebamiles asks](https://github.com/cloudfoundry/bosh/pull/585#issuecomment-45359345):
> Additionally, since the spot instance creation fundamentally changes how a VM is created, would it be possible for you to add an additional test (in a separate PR) to the AWS lifecycle tests to reflect the additional functionality?

This PR adds:
*  simple AWS Spot lifecycle integration test

 

Filed by mrdavidlaing",0.0,73783876,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #594: Aws spot lifecycle tests,553935.0,"[553935, 320647]",956238,1134058,feature,2014-07-09T01:00:57Z,https://www.pivotaltracker.com/story/show/73783876
2014-07-02T20:49:45Z,2014-05-27T19:37:17Z,accepted,,Run the tests and fix anything that's broken. Note 1.9.3 should still be supported.,0.0,72121112,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]","Upgrade bosh_common, bosh-core to Ruby 2.x",553935.0,"[553935, 320647, 1123776]",956238,1123776,feature,2014-07-02T20:49:46Z,https://www.pivotaltracker.com/story/show/72121112
2014-07-02T20:49:46Z,2014-05-27T19:36:43Z,accepted,,"Run the tests including lifecycle tests and fix anything that's broken. Lifecycle tests should be updated to use ruby 2.1.1 Make sure 1.9.3 still works
",1.0,72121062,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Upgrade bosh cpis to Ruby 2.x,553935.0,"[553935, 1123776]",956238,1123776,feature,2014-07-02T20:49:47Z,https://www.pivotaltracker.com/story/show/72121062
2014-07-02T20:54:31Z,2014-06-12T17:56:25Z,accepted,,"as a stemcell producer on BOSH, i should be able to utilize the CI build for vSphere trusty go agent stemcell testing so that I know that these stemcells pass testing and are production worthy.

Create build for vSphere trusty and verify that the latest kernel fixes are in the code


insert into build flow",1.0,73150748,story,"[{'name': 'os', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-16T18:34:48Z', 'id': 8730020, 'updated_at': '2014-06-16T18:34:48Z'}]",Create build for vSphere trusty go agent only and put it in CI,553935.0,"[553935, 786819]",956238,1210852,feature,2014-07-02T20:54:32Z,https://www.pivotaltracker.com/story/show/73150748
2014-07-02T21:32:38Z,2013-12-05T22:12:02Z,accepted,,"we're currently on:

DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=10.04
DISTRIB_CODENAME=lucid
DISTRIB_DESCRIPTION=""Ubuntu 10.04.4 LTS""
",2.0,62015812,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",move Ubuntu to newer version than 10 LTS (target 14.04) for vsphere with Go Agent only,320647.0,"[320647, 553935]",956238,637633,feature,2014-07-02T21:32:39Z,https://www.pivotaltracker.com/story/show/62015812
2014-07-03T01:16:00Z,2014-07-03T01:16:41Z,accepted,,Update the README to use Markdown format correctly and reflect the current design.,,74352632,story,[],Update BOSH Monitor README,1266616.0,[1266616],956238,1266616,chore,2014-07-03T01:18:37Z,https://www.pivotaltracker.com/story/show/74352632
2014-07-03T19:00:00Z,2014-06-25T23:25:14Z,accepted,,"During a bosh deploy, if we try to do `bosh status` we sometimes get an error: ""execution expiredExited""
The status command times out after 3 seconds, during the deployment the director seems to take longer than that to respond.

Details of our environment:

Microbosh 1.2605
 - 4 CPUs
 - 3 GB RAM
CLI 1.2611

The deployment that is running is spinning up 4 VMs, the error happened during the ""updating jobs"" phase.
",1.0,73933116,story,[],Investigation: cli times out doing bosh status (sometimes),1266616.0,"[1266616, 81882]",956238,707557,feature,2014-07-07T15:22:42Z,https://www.pivotaltracker.com/story/show/73933116
2014-07-03T19:00:00Z,2014-07-01T03:48:24Z,accepted,,"as a bosh team engineer, i should be able to review the Resurrector Documentation that Docs wrote so that I can validate its accuracy and depth before it goes to production to be consumed by the OSS",1.0,74197114,story,[],Critique Bosh Resurrector Doc,1266616.0,[1266616],956238,1210852,feature,2014-07-07T23:28:37Z,https://www.pivotaltracker.com/story/show/74197114
2014-07-07T22:50:31Z,2014-07-03T21:10:44Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/606

...building openstack stemcell.

Filed by dengwa",,74417376,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]","cloudfoundry/bosh #606: When running openstack stemcell build test, skip the vsphere steps when ...",1123776.0,[1123776],956238,1134058,chore,2014-07-07T22:50:31Z,https://www.pivotaltracker.com/story/show/74417376
2014-07-08T21:45:27Z,2014-06-19T21:28:49Z,accepted,,"Reider:

> User tried to upgrade from 1.1 to 1.2. The upgrades failed. 
> 
> The error in the installation log during a DEA deploy was:
> 
> Error 450002: Timed out sending `get_state' to 440a3fe0-bcdb-484a-a1a3-1566f87752f2 after 45 seconds
> 
> Which means the VM is unreachable. Next, we looked at the BOSH task logs for the first task that started seeing this error. We saw an error that a certain file was not found...
> 
> ERROR -- : Error updating instance: #<RuntimeError: File [QCLABUCS01_VMAX3420_04] vm-fad46552-3789-498b-8276-35f1286c7d12 was not found>
> 
> After comparing this file's location (QCLABUCS01_VMAX3420_04) we saw that datastore location in our installation settings (QCLABUCS01_VMAX3420_01) was different. There was the problem.
> 
> So we think there was a datastore migrations happening underneath the install at the IaaS layer. This was either triggered by something in vSphere or were done manually. Hard to say.
> 
> The Elastic Runtime VMs were shutdown and deleted. We ran a BOSH cck and this recreated all of the VMs. This got Cloud Foundry into a healthy state and all apps back up.

Not sure how vSphere got into this state, so this might be difficult to reproduce. But it would be good to know how we can avoid checking the wrong datastore when deleting a VM's metadata (or was this the best information we could access and not our fault?). Currently using #get_primary_datastore which validates that all (1) ephemeral disks are on the same datastore, yet in this case that information was incorrect.",1.0,73583378,story,[],Investigate why BOSH tried to delete VM env.iso from wrong datastore,553935.0,"[553935, 1406536]",956238,1123776,feature,2014-07-08T21:47:00Z,https://www.pivotaltracker.com/story/show/73583378
2014-07-09T18:11:25Z,2014-03-26T22:44:19Z,accepted,,"When bosh fails to associate instance to a created vm that was given a static ip, subsequent bosh deploys might improperly reassign static ip to other vms which will cause bosh to have 2 vms with same static ip.

Related code: 

- ResourcePoolUpdater#create_missing_vm creates vm but does not save relationship between idle_vm.bound_instance.model and idle_vm.vm

- Assembler#bind_existing_vm/bind_idle_vm: bind_idle_vm puts static ip reservation back into network pool even though there is an actual vm with that static ip.",,68325788,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked on ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T22:46:11Z', 'id': 7537412, 'updated_at': '2014-01-22T22:46:11Z'}]",Improper static IP association,81882.0,"[81882, 1266616]",956238,81882,bug,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68325788
2014-07-10T23:19:15Z,2014-07-09T23:58:34Z,accepted,,Since we switched to ruby 2.1.2 the gem home is not in 1.9.1 and should be taken from current environment.,,74743486,story,[],Remove hardcoded gems dir 1.9.1 from auto-deploy tasks,553935.0,"[553935, 320647]",956238,553935,chore,2014-07-10T23:19:15Z,https://www.pivotaltracker.com/story/show/74743486
2014-07-10T23:38:55Z,2014-07-02T18:22:52Z,accepted,,"Build openstack lucid stemcells with go agent instead of ruby agent, because we are killing the ruby agent",1.0,74324534,story,[],Move Openstack stemcells to go agent,553935.0,"[553935, 1123776]",956238,553935,feature,2014-07-10T23:38:56Z,https://www.pivotaltracker.com/story/show/74324534
2014-07-10T23:41:08Z,2014-06-12T17:53:47Z,accepted,,"as a bosh user on vsphere, i should be able to use go agent stemcells so that I can take advantage of go agent

Basically, the BOSH team needs to shut down ruby stemcell builds so that it has the build bandwidth to incorporate new vsphere/openstack go trusty stemcells",1.0,73150600,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Turn off vSphere ruby agent stemcell build,1123776.0,[1123776],956238,1210852,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/73150600
2014-07-10T23:41:31Z,2014-05-22T08:20:54Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/585

The current spot creation process is pretty dumb when it comes to AWS errors - for the majority of the error cases it retries for 20 minutes before throwing an error.

This PR implements a set of changes that enable faster failing:

1.  Only wait `total_spot_instance_request_wait_time()` == 5 minutes in total.
2.  Fail immediately if `spot_request_status[:state] == 'failed'`  (spot requests never recover from this state)
3.  Fail immediately if `spot_request_status[:status][:code] == 'price-too-low'` (it's highly unlikely that spot requests will recover from this state)



Filed by mrdavidlaing",1.0,71827594,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #585: Fail fast(er) when AWS spot creation errors detected,553935.0,"[553935, 320647]",956238,1134058,feature,2014-07-10T23:41:31Z,https://www.pivotaltracker.com/story/show/71827594
2014-07-14T19:07:08Z,2014-07-08T00:49:12Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/612

rather than env.director

Filed by liuhewei",0.0,74571214,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #612: Fix a BAT dns value error: the nameserver should come from env.dns_host,1266616.0,"[1266616, 1406536]",956238,1134058,feature,2014-07-14T19:07:28Z,https://www.pivotaltracker.com/story/show/74571214
2014-07-14T21:29:43Z,2014-06-28T00:44:49Z,accepted,,,,74083988,story,[],Make integration tests use go_agent by default,553935.0,"[553935, 81882]",956238,1266616,chore,2014-07-14T21:31:51Z,https://www.pivotaltracker.com/story/show/74083988
2014-07-14T23:32:04Z,2014-06-16T19:12:49Z,accepted,,See #73332816,1.0,73332884,story,[],Ballistic trauma,1123776.0,"[1123776, 553935]",956238,1406536,feature,2014-07-14T23:32:04Z,https://www.pivotaltracker.com/story/show/73332884
2014-07-16T01:40:05Z,2014-07-14T21:36:01Z,accepted,,"http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent/323/consoleFull

```
Failures:

  1) cancel task spawns a job and then successfully cancel it
     Failure/Error: expect(task_event).to include('error')
       expected {""time"" => 1404842424, ""stage"" => ""Updating job"", ""tags"" => [""foobar""], ""total"" => 3, ""task"" => ""foobar/2"", ""index"" => 3, ""state"" => ""finished"", ""progress"" => 100} to include ""error""
       Diff:
       @@ -1,2 +1,9 @@
       -[""error""]
       +""index"" => 3,
       +""progress"" => 100,
       +""stage"" => ""Updating job"",
       +""state"" => ""finished"",
       +""tags"" => [""foobar""],
       +""task"" => ""foobar/2"",
       +""time"" => 1404842424,
       +""total"" => 3,
     # ./spec/integration/cancel_task_spec.rb:15:in `block (2 levels) in <top (required)>'
```",,74992676,story,[],Fix flaky cancel task spec,1123776.0,"[1123776, 553935]",956238,1123776,chore,2014-07-16T01:40:05Z,https://www.pivotaltracker.com/story/show/74992676
2014-07-16T04:01:41Z,2014-07-03T18:17:28Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/605

The following micro_bosh.yml snippet previously failed unnecessarily:

    network:
     type: manual
     ip: 10.15.212.70
     cloud_properties:
       subnet: subnet-0420752c

Now, the network.ip will be used as the client_services_ip.

The following is output in the logs:

    using configured ip=10.15.212.70

Filed by drnic",1.0,74405440,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #605: [microbosh] fall back to configured network.ip [fixes #558],1266616.0,"[1266616, 1406536]",956238,1134058,feature,2014-07-16T04:01:42Z,https://www.pivotaltracker.com/story/show/74405440
2014-07-16T17:40:03Z,2014-07-07T16:25:15Z,accepted,,Talk to @dk for context,,74531054,story,[],Remove bosh cdrom,1123776.0,[1123776],956238,1123776,chore,2014-07-16T17:40:03Z,https://www.pivotaltracker.com/story/show/74531054
2014-07-16T18:58:30Z,2014-07-03T22:30:12Z,accepted,,"We had an existing deployment on a vSphere environment and we were upgrading our deployment when we ran into problems.

Running `bosh vms` shows that cloud controller worker was unknown/unknown and unresponsive agent.

Relevant output from SSHing to the CC Worker VM and looking at /var/vcap/bosh/log/current:
```
2014-07-03_22:15:45.35338 #[6810] INFO: Starting agent 1.2611.0...
2014-07-03_22:15:45.35343 #[6810] INFO: Configuring agent...
2014-07-03_22:15:45.35446 #[6810] INFO: Configuring instance
2014-07-03_22:15:45.35599 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:45.85890 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:46.36079 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:46.86362 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:47.36648 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:47.87304 #[6810] INFO: udevadm: #<Bosh::Exec::Result:0x0000000243a920>
2014-07-03_22:15:47.87448 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)
2014-07-03_22:15:48.37721 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)
2014-07-03_22:15:48.88023 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)
2014-07-03_22:15:49.38309 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)
2014-07-03_22:15:49.88485 #[6810] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)
2014-07-03_22:15:50.38779 #[6810] INFO: failed to load infrastructure settings: No bosh cdrom env: #<Errno::ENOMEDIUM: No medium found - /dev/sr0>
2014-07-03_22:15:50.38874 /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/settings.rb:52:in `rescue in load_from_cache': could neither load infrastructure settings nor cached settings from: /var/vcap/bosh/settings.json (Bosh::Agent::LoadSettingsError)
2014-07-03_22:15:50.38886       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/settings.rb:47:in `load_from_cache'
2014-07-03_22:15:50.38888       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/settings.rb:29:in `rescue in load'
2014-07-03_22:15:50.38890       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/settings.rb:25:in `load'
2014-07-03_22:15:50.38892       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/settings.rb:14:in `load'
2014-07-03_22:15:50.38894       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/bootstrap.rb:69:in `load_settings'
2014-07-03_22:15:50.38896       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/bootstrap.rb:35:in `configure'
2014-07-03_22:15:50.38898       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/runner.rb:17:in `start'
2014-07-03_22:15:50.38900       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/lib/bosh_agent/runner.rb:5:in `run'
2014-07-03_22:15:50.38904       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2611.0/bin/bosh_agent:102:in `<top (required)>'
2014-07-03_22:15:50.38906       from /var/vcap/bosh/bin/bosh_agent:23:in `load'
2014-07-03_22:15:50.38908       from /var/vcap/bosh/bin/bosh_agent:23:in `<main>'
2014-07-03_22:15:50.73184 #[6824] INFO: Starting agent 1.2611.0...
2014-07-03_22:15:50.73189 #[6824] INFO: Configuring agent...
```

Full output from `bosh task 24 --debug` is attached in story #74421758.",,74421790,story,[],Agent on vSphere became unresponsive when upgrading deployment,553935.0,[553935],956238,465545,bug,2014-07-16T18:58:30Z,https://www.pivotaltracker.com/story/show/74421790
2014-07-16T22:30:12Z,2014-06-05T21:55:18Z,accepted,,"Currently, resource pool sizes are defined in the manifest. Unused vms show up as unknown/unknown in the vms list.

As a deployment operator, I should be able to delegate this sizing to BOSH, to create new vms as required.

Acceptance Use Cases:
- Errand with dynamically sized resource pool does not add ""unknown/unknown"" vms
- Errand with statically sized resource pool does add ""unknown/unknown"" vms
- Job(s) with dynamically sized resource pool can have any number of instances
- Job(s) with statically sized (N) resource pool can have up to N number of instances",2.0,72740830,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]",Support dynamically sized resource pools,1266616.0,"[1266616, 81882, 320647]",956238,1210852,feature,2014-07-31T05:46:59Z,https://www.pivotaltracker.com/story/show/72740830
2014-07-16T22:39:01Z,2014-06-30T22:06:38Z,accepted,,"The current bosh lite box produced a warning about versions with the latest CLI:

You are using CLI > 1.2579.0 with a director that doesn't support the new version format you are using. Upgrade your director to match the version of your CLI or downgrade your CLI to 1.2579.0 to avoid versioning mismatch issues.

Can we get a new box out?",0.0,74181228,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Bosh-lite Director-CLI Compatability,320647.0,"[320647, 553935]",956238,1210852,feature,2014-07-16T22:39:01Z,https://www.pivotaltracker.com/story/show/74181228
2014-07-16T22:39:50Z,2014-06-18T23:53:16Z,accepted,,stack trace unavailable ,,73513206,story,[],bosh cck should mount disks on VMs that are expected to have disks mounted without raising reattach_disk unknown resolution for ... error,553935.0,"[553935, 81882]",956238,1123776,bug,2014-07-16T22:39:50Z,https://www.pivotaltracker.com/story/show/73513206
2014-07-17T20:59:38Z,2014-07-02T18:09:14Z,accepted,,,0.0,74323440,story,[],Turn off Openstack ruby agent stemcell builds,1123776.0,[1123776],956238,553935,feature,2014-07-31T05:41:01Z,https://www.pivotaltracker.com/story/show/74323440
2014-07-17T21:42:43Z,2014-07-08T21:12:37Z,accepted,,"Currently this is not working. get_current_env fails to get env.iso once the VM was migrated. 

",2.0,74642918,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",Vsphere cpi should be able to attach_disk/detach_disk/configure_networks a disk after VM was migrated ,553935.0,"[553935, 320647]",956238,1406536,feature,2014-07-17T21:42:44Z,https://www.pivotaltracker.com/story/show/74642918
2014-07-18T21:23:47Z,2014-06-13T19:52:19Z,accepted,,"when upgrading tabasco and a1 from lucid to trusty, we say network connectivity problems between the job VMs as they came up.

Initial investigation revealed that clearing the arp cache on the VMs ""fixed"" the connectivity problems. CF remained in a bad state for a while, though.",,73230550,story,[],"network problems, lucid->trusty upgrade",1406536.0,"[1406536, 81882]",956238,1165386,bug,2014-12-01T20:14:44Z,https://www.pivotaltracker.com/story/show/73230550
2014-07-21T17:56:15Z,2014-07-16T17:44:56Z,accepted,,,,75131708,story,[],reduce redis log noise in debug task logs,81882.0,[81882],956238,1123776,chore,2014-07-21T17:56:15Z,https://www.pivotaltracker.com/story/show/75131708
2014-07-21T22:58:12Z,2014-07-21T17:01:02Z,accepted,,"Currently, it is using small TMPDIR on root partition and director runs out of space when uploading release",,75372912,story,[],Fix bosh-lite on AWS TMPDIR,553935.0,"[553935, 1115834]",956238,553935,chore,2014-07-21T22:58:12Z,https://www.pivotaltracker.com/story/show/75372912
2014-07-23T18:19:36Z,2014-07-03T22:15:46Z,accepted,,this should fix reported `bosh status` times out problem in #73933116.,4.0,74421246,story,[],switch to using nginx upload module for upload file handling in BOSH director,81882.0,[81882],956238,81882,feature,2014-07-23T18:19:36Z,https://www.pivotaltracker.com/story/show/74421246
2014-07-23T18:30:22Z,2014-07-09T18:44:32Z,accepted,,"something like this inside nginx.conf:

```
log_format timed_combined '$remote_addr - $remote_user [$time_local]  '
    '""$request"" $status $body_bytes_sent '
    '""$http_referer"" ""$http_user_agent"" '
    '$request_time $upstream_response_time $pipe';
```

this would be useful for debugging to see when director slows down.",0.0,74721092,story,[],director and blobstore nginx should log response times ,81882.0,[81882],956238,1266616,feature,2014-07-23T18:30:23Z,https://www.pivotaltracker.com/story/show/74721092
2014-07-23T18:44:29Z,2014-07-15T16:37:49Z,accepted,,,1.0,75047396,story,[],investigate slow than expected 'binding existing deployment' step during bosh deploy for >20 vms,553935.0,"[553935, 81882]",956238,553935,feature,2014-07-23T18:44:29Z,https://www.pivotaltracker.com/story/show/75047396
2014-07-23T18:49:46Z,2014-07-09T18:46:39Z,accepted,,consider removing timeout in the command and making sure that director client is configured.,1.0,74721280,story,[],increase bosh status timeout,1123776.0,[1123776],956238,1266616,feature,2014-07-23T18:49:47Z,https://www.pivotaltracker.com/story/show/74721280
2014-07-24T01:02:37Z,2014-07-23T18:00:22Z,accepted,,"create stemcell failed: Could not acquire HTTP NFC lease, message is: 'A specified parameter was not correct.",,75534820,story,[],Investigate vsphere micro bosh deployment failure HTTP NFC lease,553935.0,"[553935, 1386874]",956238,553935,chore,2014-07-24T01:02:38Z,https://www.pivotaltracker.com/story/show/75534820
2014-07-24T17:33:21Z,2014-07-21T20:53:56Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/30481918
ruby-1.9.3	COVERAGE=true BOSH_MAX_THREADS=1 TASKS=""travis:install_go spec:unit ci:publish_coverage_report""

```
** Execute spec:unit:go_agent
go_agent/bin/test
 Formatting packages...
 Installing ginkgo...
 Testing packages...
[1405966448] Agent Suite - 46/46 specs •••••••••••••••••••••••••••••••••••••••••••••• SUCCESS! 31.841234ms PASS
[1405966448] Action Suite - 190/190 specs •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• SUCCESS! 38.190327ms PASS
[1405966448] Alert Suite - 6/6 specs •••••• SUCCESS! 1.479216ms PASS
Failed to compile applier:
go build testmain: signal: killed
Ginkgo ran in 1m57.062866096s
Test Suite Failed
 Vetting packages for potential issues...
 Checking with golint...
 Running build script to confirm everything compiles...
SUITE FAILURE
rake aborted!
Command failed with status (1): [go_agent/bin/test...]
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/file_utils.rb:55:in `block in create_shell_runner'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/file_utils.rb:45:in `call'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/file_utils.rb:45:in `sh'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/file_utils_ext.rb:37:in `sh'
/home/travis/build/cloudfoundry/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:80:in `block (3 levels) in <top (required)>'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:240:in `call'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:240:in `block in execute'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:235:in `each'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:235:in `execute'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:179:in `block in invoke_with_call_chain'
/home/travis/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:201:in `block in invoke_prerequisites'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:199:in `each'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:199:in `invoke_prerequisites'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:178:in `block in invoke_with_call_chain'
/home/travis/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/monitor.rb:211:in `mon_synchronize'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:172:in `invoke_with_call_chain'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/task.rb:165:in `invoke'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:150:in `invoke_task'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:106:in `block (2 levels) in top_level'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:106:in `each'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:106:in `block in top_level'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:115:in `run_with_threads'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:100:in `top_level'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:78:in `block in run'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:176:in `standard_exception_handling'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/lib/rake/application.rb:75:in `run'
/home/travis/.rvm/gems/ruby-1.9.3-p545/gems/rake-10.3.2/bin/rake:33:in `<top (required)>'
/home/travis/.rvm/gems/ruby-1.9.3-p545/bin/rake:23:in `load'
/home/travis/.rvm/gems/ruby-1.9.3-p545/bin/rake:23:in `<main>'
Tasks: TOP => spec:unit => spec:unit:go_agent
The command ""bundle exec rake --trace $TASKS"" exited with 1.
Done. Your build exited with 1.
```",,75393576,story,[],"Investigate Travis go_agent test failure ""go build testmain: signal: killed""",81882.0,[81882],956238,81882,chore,2014-07-24T17:33:21Z,https://www.pivotaltracker.com/story/show/75393576
2014-07-24T17:34:05Z,2014-07-21T20:48:06Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/30489908
ruby-2.1.2	TASKS=""travis:install_go spec:integration:go_agent""

```
I, [2014-07-21T19:53:52.719179 #1356]  INFO -- : Truncating postgres database b20952f553fa
          tablename           
------------------------------
 rendered_templates_archives
 schema_migrations
 vms
 releases
 instances
 compiled_packages
 templates
 packages_release_versions
 release_versions_templates
 packages
 deployments
 stemcells
 release_versions
 deployments_stemcells
 deployments_release_versions
 users
 deployment_properties
 tasks
 deployment_problems
 log_bundles
 director_attributes
 domains
 records
 persistent_disks
 dns_schema
 snapshots
(26 rows)
I, [2014-07-21T19:53:55.426626 #1356]  INFO -- : Reset took 2.708763575 seconds
I, [2014-07-21T19:53:55.501595 #1356]  INFO -- : Running ... bosh -n target http://localhost:61502
I, [2014-07-21T19:53:58.094395 #1356]  INFO -- : Command took 2.592533442 seconds
I, [2014-07-21T19:53:58.094699 #1356]  INFO -- : Running ... bosh -n login admin admin
I, [2014-07-21T19:54:01.052567 #1356]  INFO -- : Command took 2.957749556 seconds
I, [2014-07-21T19:54:01.052814 #1356]  INFO -- : Running ... bosh -n reset release
I, [2014-07-21T19:54:04.262691 #1356]  INFO -- : Command took 3.209606047 seconds
I, [2014-07-21T19:54:04.262914 #1356]  INFO -- : Running ... bosh -n create release --force
I, [2014-07-21T19:54:06.325345 #1364]  INFO -- : Command took 19.865550841 seconds
I, [2014-07-21T19:54:07.211546 #1356]  INFO -- : Command took 2.948526874 seconds
I, [2014-07-21T19:54:07.211730 #1356]  INFO -- : Running ... bosh -n upload release
I, [2014-07-21T19:54:07.326140 #1364]  INFO -- : Running ... bosh -n vms --details
I, [2014-07-21T19:54:12.532516 #1356]  INFO -- : Command took 5.320674833 seconds
I, [2014-07-21T19:54:12.532763 #1356]  INFO -- : Running ... bosh -n upload stemcell /home/travis/build/cloudfoundry/bosh/spec/assets/valid_stemcell.tgz
I, [2014-07-21T19:54:17.694353 #1356]  INFO -- : Command took 5.161486114 seconds
I, [2014-07-21T19:54:17.774468 #1356]  INFO -- : Running ... bosh -n deployment /tmp/simple20140721-1356-w5k1u1
I, [2014-07-21T19:54:20.354491 #1356]  INFO -- : Command took 2.579804329 seconds
I, [2014-07-21T19:54:20.354686 #1356]  INFO -- : Running ... bosh -n deploy
I, [2014-07-21T19:54:28.193980 #1364]  INFO -- : Command took 20.867649933 seconds
I, [2014-07-21T19:54:29.194807 #1364]  INFO -- : Running ... bosh -n vms --details
I, [2014-07-21T19:54:45.893679 #1356]  INFO -- : Command took 25.53887679 seconds
I, [2014-07-21T19:54:45.894258 #1356]  INFO -- : Running ... bosh -n backup backup.tgz
I, [2014-07-21T19:54:49.052437 #1364]  INFO -- : Command took 19.857434252 seconds
I, [2014-07-21T19:54:49.053304 #1364]  INFO -- : Running ... bosh -n vms --details
I, [2014-07-21T19:54:50.837568 #1356]  INFO -- : Command took 4.943179596 seconds
F
Failures:
  1) director_scheduler manual backup backs up task logs, database and blobs
     Failure/Error: expect(backup_file.file_names).to match_array(%w(task_logs.tgz director_db.sql blobs.tgz))
       expected collection contained:  [""blobs.tgz"", ""director_db.sql"", ""task_logs.tgz""]
       actual collection contained:    ["""", ""task_logs.tgz""]
       the missing elements were:      [""blobs.tgz"", ""director_db.sql""]
       the extra elements were:        [""""]
     # ./spec/integration/director_scheduler_spec.rb:76:in `block (3 levels) in <top (required)>'
```",,75392998,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","Investigate Travis integration test failure ""manual backup""",81882.0,"[81882, 553935]",956238,81882,chore,2014-07-31T05:41:30Z,https://www.pivotaltracker.com/story/show/75392998
2014-07-24T19:06:05Z,2014-07-21T20:49:52Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/30489908
ruby-2.1.2	TASKS=""travis:install_go spec:integration:go_agent""

```
  1) cli: compiled_packages allows user to export compiled packages after a deploy
     Failure/Error: expect(result).to be_success
       expected `#<Bosh::Exec::Result:0x000000041cd9d8 @command=""tar -Oxzf '/tmp/d20140721-1361-1pfwyp3/bosh-release-0.1-dev-ubuntu-stemcell-1.tgz' compiled_packages.MF"", @output=""---\nrelease_name: bosh-release\nrelease_version: 0+dev.1\nrelease_commit_hash: '74148036'\ncompiled_packages:\n- package_name: bar\n  package_fingerprint: f1267e1d4e06b60c91ef648fb9242e33ddcffa73\n  compiled_package_sha1: b6a5874b566a822ef213a00151a91a8cee3a97a5\n  stemcell_sha1: shawone\n  blobstore_id: f89fd396-03f5-4ed5-668d-02089a4377cb\n- package_name: foo\n  package_fingerprint: 0ee95716c58cf7aab3ef7301ff907118552c2dda\n  compiled_package_sha1: 92fcee688d42d1686ad81c3b22bd23fde9172fef\n  stemcell_sha1: shawone\n  blobstore_id: f78bd918-e4e2-43d6-4d7d-d17237512e6d\n"", @exit_status=2, @not_found=false>.success?` to return true, got false
     # ./spec/integration/cli_compiled_packages_spec.rb:15:in `block (3 levels) in <top (required)>'
     # ./spec/integration/cli_compiled_packages_spec.rb:9:in `block (2 levels) in <top (required)>'
```",,75393228,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","Investigate Travis integration test failure ""export compiled packages""",1266616.0,[1266616],956238,81882,chore,2014-07-31T05:41:30Z,https://www.pivotaltracker.com/story/show/75393228
2014-07-24T19:53:08Z,2014-04-08T00:06:38Z,accepted,,,0.0,69048760,story,"[{'name': 'drain', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623538, 'updated_at': '2014-06-06T18:53:38Z'}]","When running drain script, agent should use TMPDIR env variable",,[],956238,81882,feature,2014-07-24T19:53:09Z,https://www.pivotaltracker.com/story/show/69048760
2014-07-24T19:55:18Z,2014-07-08T00:14:37Z,accepted,,,2.0,74570058,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]","Vsphere delete_vm method should clean up env.iso and env.json based on CDROM location, not disks",1406536.0,"[1406536, 553935, 1115834]",956238,1406536,feature,2014-07-24T19:55:18Z,https://www.pivotaltracker.com/story/show/74570058
2014-07-24T20:43:09Z,2014-07-07T17:41:03Z,accepted,,,,74539272,story,[],Add bosh-lite CI to build monitor,81882.0,[81882],956238,1210852,chore,2014-07-24T20:43:08Z,https://www.pivotaltracker.com/story/show/74539272
2014-07-25T00:21:09Z,2014-07-15T00:06:22Z,accepted,,"It seems that the compilation step always fails. After much exploration with Caleb and Karl, we found that the heartbeat messages form the containers being spun up were not reaching the Director. This is most likely because of something related to networking on the VM, or related to the IP of NATS, or the port on which NATS services are accessed.

When the BOSH-lite is supposed to be compiling packages, the `bosh vms` command returns:
```
root@9cedd31e-060e-44db-ac2c-0d996891264a:/tmp/cf-release# bosh vms
Deployment `cf-warden'

Director task 19

Task 19 done

+-----------------+--------------------+---------------+-----+
| Job/index       | State              | Resource Pool | IPs |
+-----------------+--------------------+---------------+-----+
| unknown/unknown | unresponsive agent |               |     |
| unknown/unknown | unresponsive agent |               |     |
| unknown/unknown | unresponsive agent |               |     |
| unknown/unknown | unresponsive agent |               |     |
| unknown/unknown | unresponsive agent |               |     |
| unknown/unknown | unresponsive agent |               |     |
+-----------------+--------------------+---------------+-----+

VMs total: 6
```",,75001684,story,[],BOSH-lite AWS AMI is unable to compile packages.,553935.0,"[553935, 1386874]",956238,1194390,bug,2014-07-25T00:21:10Z,https://www.pivotaltracker.com/story/show/75001684
2014-07-25T17:24:43Z,2014-06-23T19:12:51Z,accepted,,"as a bosh user on AWS, i should be able to use go agent stemcells so that I can take advantage of go agent

Basically, the BOSH team needs to shut down ruby stemcell builds so that it has the build bandwidth to incorporate new vsphere/openstack go trusty stemcells",1.0,73740974,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Turn off AWS ruby agent stemcell build,553935.0,"[553935, 81882]",956238,1210852,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/73740974
2014-07-25T17:25:48Z,2014-06-27T02:44:05Z,accepted,,"A trusty stemcell with the patched kernel is needed to avoid further delays in shipping trusty and the go-agent.

https://github.com/dotcloud/docker/issues/2960
https://gist.github.com/karlkfi/e543880dc4a13a2f4616 <--- from Alex",1.0,74015624,story,[],As a bosh release author I expect to have a trusty kernel that does not panic when destroying containers.,553935.0,"[553935, 81882]",956238,1266616,feature,2014-07-25T17:25:48Z,https://www.pivotaltracker.com/story/show/74015624
2014-07-25T19:24:16Z,2014-07-21T20:57:39Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/30481921

This build seems completely hosed. Postgres is returning 512 with many test failures and nats connection failures.

```
E, [2014-07-21T18:13:15.899220 #30629] ERROR -- : Failed to connect to nats: #<Errno::ECONNREFUSED: Connection refused - connect(2)> host=localhost port=61001
```

```
1) cli: package compilation uses compile package cache for previously compiled packages
     Failure/Error: Unable to find matching line from backtrace
     Errno::ECONNREFUSED:
       Connection refused - connect(2)
     # ./bosh-dev/lib/bosh/dev/sandbox/socket_connector.rb:19:in `initialize'
     # ./bosh-dev/lib/bosh/dev/sandbox/socket_connector.rb:19:in `new'
     # ./bosh-dev/lib/bosh/dev/sandbox/socket_connector.rb:19:in `block in try_to_connect'
     # ./bosh-dev/lib/bosh/dev/sandbox/socket_connector.rb:19:in `try_to_connect'
     # ./bosh-dev/lib/bosh/dev/sandbox/main.rb:164:in `start'
     # ./spec/support/integration_example_group.rb:125:in `start_sandbox'
     # ./spec/support/integration_example_group.rb:206:in `block in with_reset_sandbox_before_each'
  2) cli: package compilation compiles explicit requirements and dependencies recursively, but only applies explicit requirements to jobs
     Failure/Error: Unable to find matching line from backtrace
     RuntimeError:
       Failed: 'psql -U postgres 633b4113f270 -c ""select tablename from pg_tables where schemaname='public';""' from /home/travis/build/cloudfoundry/bosh, with exit status 512
```",,75393882,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","Investigate Travis test failures Postgress ""exit status 512""",1393546.0,"[1393546, 81882]",956238,81882,chore,2014-07-31T05:41:30Z,https://www.pivotaltracker.com/story/show/75393882
2014-07-25T19:34:24Z,2014-04-28T17:25:02Z,accepted,,see #62015812 for details,0.0,70291668,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",move Ubuntu to newer version than 10 LTS (target 14.04) for openstack with Go Agent only,553935.0,"[553935, 81882]",956238,1210852,feature,2014-07-25T19:34:25Z,https://www.pivotaltracker.com/story/show/70291668
2014-07-25T19:34:33Z,2014-04-11T21:57:36Z,accepted,,"as a bosh developer, i want to add ubuntu 14.04 stemcells to the CI build pipeline so that they are available to the public.  ",,69380048,story,"[{'name': 'trusty', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-24T20:59:24Z', 'id': 8285324, 'updated_at': '2014-04-24T20:59:24Z'}]",14.04 stemcell builds in pipeline,,[],956238,1210852,release,2014-07-25T19:34:34Z,https://www.pivotaltracker.com/story/show/69380048
2014-07-25T21:14:36Z,2014-07-18T17:07:02Z,accepted,,If creating vm fails for some reason before BOSH register that vm in database the vm becomes untracked. Bosh should either rollback and delete vm or register before creating it. The user facing issue is that on second deploy after the failed deploy BOSH fails to allocate IP for new instance since it was already taken by created instance.,1.0,75278320,story,[],Bosh creating instance should be transactional,81882.0,[81882],956238,553935,feature,2014-07-25T21:14:35Z,https://www.pivotaltracker.com/story/show/75278320
2014-07-25T21:16:00Z,2014-07-16T00:06:47Z,accepted,,"```
Failures:

  1) director_scheduler manual backup backs up task logs, database and blobs
     Failure/Error: expect(backup_file.file_names).to match_array(%w(task_logs.tgz director_db.sql blobs.tgz))
     Archive::Tar::Minitar::UnexpectedEOF:
       Archive::Tar::Minitar::UnexpectedEOF
     # ./spec/support/tar_file_inspector.rb:24:in `entries'
     # ./spec/support/tar_file_inspector.rb:10:in `file_names'
     # ./spec/integration/director_scheduler_spec.rb:76:in `block (3 levels) in <top (required)>'
```

The tgz created by the director is fine, but when the client downloads it, it cannot be extracted. There are 10 extra bytes (the same 2 bytes (0D0A) inserted about every 8K) in the file, making it unreadable by gzip or tar.",,75080876,story,[],Fix integration test in Ruby 2.1 on developer workstations,81882.0,"[81882, 553935]",956238,1123776,chore,2014-07-25T21:15:56Z,https://www.pivotaltracker.com/story/show/75080876
2014-07-25T22:45:57Z,2014-07-23T17:29:21Z,accepted,,"We clean up /etc/resolv.con in the end of stemcell building. Agent should be the one responsible for managing /etc/resolv.conf
",,75532412,story,[],Firstboot should not clean up /etc/resolv.conf,553935.0,"[553935, 1386874]",956238,553935,bug,2014-07-25T22:45:57Z,https://www.pivotaltracker.com/story/show/75532412
2014-07-28T20:59:54Z,2014-06-05T21:56:08Z,accepted,,"Currently, packaging script output is being saved in memory and passed around. This can crash go-agent if the output is sufficiently large (>2mb). go-agent should not crash during package compilation, regardless of how much is printed to STDOUT/STDERR (unless the disk size is exceeded...)

- Agent should cleans up old packaging log files before executing the new packaging script
- Agent should pipe STDOUT/STDERR of packaging script to log files in /var/vcap/sys/log/blah
- Agent should continue to include the last 100 lines of the STDOUT/STDERR in the go ExecError, but not the entire output (for reverse compatibility)

Acceptance criteria:
- Agent should not crash with large packaging output
- Agent should (still) return last 100 lines of output on packaging failure

This feature is part of a set including #73744892 & #73742758",2.0,72740894,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Agent redirects packaging script stdout/stderr to a file and returns last 100 lines,553935.0,"[553935, 1386874]",956238,1266616,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/72740894
2014-07-29T19:09:00Z,2014-07-28T21:44:38Z,accepted,,runtime team noticed that nodejs_buildpack blob keeps on failing to download and error claims that nodejs_buildpack is corrupted because checksum did not match. they are using 1.9.3. they do seem it pass every so often. this probably relates to the size of the buildpack blobs.,0.0,75853518,story,[],"investigate why downloading big packages from cf-release causes ""blob is corrupted error""",553935.0,"[553935, 1386874]",956238,81882,feature,2014-07-29T19:07:52Z,https://www.pivotaltracker.com/story/show/75853518
2014-07-29T19:19:48Z,2014-07-28T16:43:13Z,accepted,,"     Failure/Error: ssh(public_ip, 'vcap', ""echo 'foobar' > #{SAVE_FILE}"", @our_ssh_options)
     Errno::ETIMEDOUT:
       Operation timed out - connect(2) for ""172.16.99.10"" port 22

       Error 100: Could not delete object, 401/<html>
       <head><title>401 Authorization Required</title></head>
       <body bgcolor=""white"">
       <center><h1>401 Authorization Required</h1></center>
       <hr><center>nginx</center>
       </body>
       </html>",,75825120,story,[],Investigate why BATS are failing on latest develop,553935.0,"[553935, 320647]",956238,553935,chore,2014-07-29T19:19:49Z,https://www.pivotaltracker.com/story/show/75825120
2014-07-29T19:20:03Z,2014-07-14T21:57:13Z,accepted,,It seems as if the raring sources are no longer available. We should update so that stemcell building vm provisioning does not fail.,,74994436,story,[],Upgrade stemcell building VM from raring,553935.0,"[553935, 1115834, 1393546]",956238,1123776,chore,2014-07-29T19:20:03Z,https://www.pivotaltracker.com/story/show/74994436
2014-07-30T00:27:42Z,2014-07-03T22:43:29Z,accepted,,"output:

Running bosh upload stemcell /Users/pivotal/stemcells/bosh-stemcell-2611-vsphere-esxi-ubuntu-lucid-go_agent.tgz --skip-if-exists

Verifying stemcell...
File exists and readable                                     OK
Verifying tarball...
Read tarball                                                 OK
Manifest exists                                              OK
Stemcell image file                                          OK
Stemcell properties                                          OK

Stemcell info
-------------
Name:    bosh-vsphere-esxi-ubuntu-lucid-go_agent
Version: 2611

Checking if stemcell already exists...
No

Uploading stemcell...


bosh-stemcell:   0% |                                          | ETA:  --:--:--
bosh-stemcell:   1% |                        |   3.9MB  35.3MB/s ETA:  00:00:10
bosh-stemcell:   2% |                        |   7.8MB  36.6MB/s ETA:  00:00:10
bosh-stemcell:   3% |                        |  11.6MB  37.8MB/s ETA:  00:00:09
bosh-stemcell:   4% |                        |  15.5MB  38.2MB/s ETA:  00:00:09
bosh-stemcell:   5% |o                       |  19.4MB  38.5MB/s ETA:  00:00:09
bosh-stemcell:   6% |o                       |  23.2MB  38.7MB/s ETA:  00:00:09
bosh-stemcell:   7% |o                       |  27.1MB  38.6MB/s ETA:  00:00:09
bosh-stemcell:   8% |o                       |  31.0MB  38.8MB/s ETA:  00:00:09
bosh-stemcell:   9% |oo                      |  34.8MB  38.8MB/s ETA:  00:00:09
bosh-stemcell:  10% |oo                      |  38.7MB  39.0MB/s ETA:  00:00:08
bosh-stemcell:  11% |oo                      |  42.6MB  39.1MB/s ETA:  00:00:08
bosh-stemcell:  12% |oo                      |  46.5MB  39.2MB/s ETA:  00:00:08
bosh-stemcell:  13% |ooo                     |  50.3MB  38.7MB/s ETA:  00:00:08
bosh-stemcell:  14% |ooo                     |  54.2MB  38.8MB/s ETA:  00:00:08
bosh-stemcell:  15% |ooo                     |  58.1MB  38.9MB/s ETA:  00:00:08
bosh-stemcell:  16% |ooo                     |  61.9MB  39.0MB/s ETA:  00:00:08
bosh-stemcell:  17% |oooo                    |  65.8MB  39.1MB/s ETA:  00:00:08
bosh-stemcell:  18% |oooo                    |  69.7MB  39.1MB/s ETA:  00:00:08
bosh-stemcell:  19% |oooo                    |  73.6MB  39.2MB/s ETA:  00:00:08
bosh-stemcell:  20% |oooo                    |  77.4MB  39.3MB/s ETA:  00:00:07
bosh-stemcell:  21% |ooooo                   |  81.3MB  39.3MB/s ETA:  00:00:07
bosh-stemcell:  22% |ooooo                   |  85.2MB  39.3MB/s ETA:  00:00:07
bosh-stemcell:  23% |ooooo                   |  89.0MB  39.3MB/s ETA:  00:00:07
bosh-stemcell:  24% |ooooo                   |  92.9MB  39.4MB/s ETA:  00:00:07
bosh-stemcell:  25% |oooooo                  |  96.8MB  39.4MB/s ETA:  00:00:07
bosh-stemcell:  26% |oooooo                  | 100.7MB  39.5MB/s ETA:  00:00:07
bosh-stemcell:  27% |oooooo                  | 104.5MB  39.5MB/s ETA:  00:00:07
bosh-stemcell:  28% |oooooo                  | 108.4MB  39.5MB/s ETA:  00:00:07
bosh-stemcell:  29% |oooooo                  | 112.3MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  30% |ooooooo                 | 116.1MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  31% |ooooooo                 | 120.0MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  32% |ooooooo                 | 123.9MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  33% |ooooooo                 | 127.7MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  34% |oooooooo                | 131.6MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  35% |oooooooo                | 135.5MB  39.5MB/s ETA:  00:00:06
bosh-stemcell:  36% |oooooooo                | 139.4MB  39.6MB/s ETA:  00:00:06
bosh-stemcell:  37% |oooooooo                | 143.2MB  39.6MB/s ETA:  00:00:06
bosh-stemcell:  38% |ooooooooo               | 147.1MB  39.6MB/s ETA:  00:00:06
bosh-stemcell:  39% |ooooooooo               | 151.0MB  39.6MB/s ETA:  00:00:05
bosh-stemcell:  40% |ooooooooo               | 154.8MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  41% |ooooooooo               | 158.7MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  42% |oooooooooo              | 162.6MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  43% |oooooooooo              | 166.5MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  44% |oooooooooo              | 170.3MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  45% |oooooooooo              | 174.2MB  39.6MB/s ETA:  00:00:05
bosh-stemcell:  46% |ooooooooooo             | 178.1MB  39.6MB/s ETA:  00:00:05
bosh-stemcell:  47% |ooooooooooo             | 181.9MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  48% |ooooooooooo             | 185.8MB  39.7MB/s ETA:  00:00:05
bosh-stemcell:  49% |ooooooooooo             | 189.7MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  50% |oooooooooooo            | 193.5MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  51% |oooooooooooo            | 197.4MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  52% |oooooooooooo            | 201.3MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  53% |oooooooooooo            | 205.2MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  54% |oooooooooooo            | 209.0MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  55% |ooooooooooooo           | 212.9MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  56% |ooooooooooooo           | 216.8MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  57% |ooooooooooooo           | 220.6MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  58% |ooooooooooooo           | 224.5MB  39.7MB/s ETA:  00:00:04
bosh-stemcell:  59% |oooooooooooooo          | 228.4MB  39.7MB/s ETA:  00:00:03
bosh-stemcell:  60% |oooooooooooooo          | 232.2MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  61% |oooooooooooooo          | 236.1MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  62% |oooooooooooooo          | 240.0MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  63% |ooooooooooooooo         | 243.9MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  64% |ooooooooooooooo         | 247.7MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  65% |ooooooooooooooo         | 251.6MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  66% |ooooooooooooooo         | 255.5MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  67% |oooooooooooooooo        | 259.3MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  68% |oooooooooooooooo        | 263.2MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  69% |oooooooooooooooo        | 267.1MB  39.8MB/s ETA:  00:00:03
bosh-stemcell:  70% |oooooooooooooooo        | 271.0MB  39.8MB/s ETA:  00:00:02
bosh-stemcell:  71% |ooooooooooooooooo       | 274.8MB  39.8MB/s ETA:  00:00:02
bosh-stemcell:  72% |ooooooooooooooooo       | 278.7MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  73% |ooooooooooooooooo       | 282.6MB  39.8MB/s ETA:  00:00:02
bosh-stemcell:  74% |ooooooooooooooooo       | 286.4MB  39.8MB/s ETA:  00:00:02
bosh-stemcell:  75% |oooooooooooooooooo      | 290.3MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  76% |oooooooooooooooooo      | 294.2MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  77% |oooooooooooooooooo      | 298.0MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  78% |oooooooooooooooooo      | 301.9MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  79% |oooooooooooooooooo      | 305.8MB  39.9MB/s ETA:  00:00:02
bosh-stemcell:  80% |ooooooooooooooooooo     | 309.7MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  81% |ooooooooooooooooooo     | 313.5MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  82% |ooooooooooooooooooo     | 317.4MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  83% |ooooooooooooooooooo     | 321.3MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  84% |oooooooooooooooooooo    | 325.1MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  85% |oooooooooooooooooooo    | 329.0MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  86% |oooooooooooooooooooo    | 332.9MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  87% |oooooooooooooooooooo    | 336.8MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  88% |ooooooooooooooooooooo   | 340.6MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  89% |ooooooooooooooooooooo   | 344.5MB  39.9MB/s ETA:  00:00:01
bosh-stemcell:  90% |ooooooooooooooooooooo   | 348.4MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  91% |ooooooooooooooooooooo   | 352.2MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  92% |oooooooooooooooooooooo  | 356.1MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  93% |oooooooooooooooooooooo  | 360.0MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  94% |oooooooooooooooooooooo  | 363.9MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  95% |oooooooooooooooooooooo  | 367.7MB  39.9MB/s ETA:  00:00:00
bosh-stemcell:  96% |ooooooooooooooooooooooo | 371.6MB  39.9MB/s ETA:  00:00:00
bosh-stemcell: 100% |oooooooooooooooooooooooo| 387.1MB  41.5MB/s Time: 00:00:09
Director task 22
Error 100: execution expired

Task 22 error

For a more detailed error report, run: bosh task 22 --debug
Try no. 4 failed.
Exited with 1.",,74422340,story,[],stemcell upload fails with execution expired,,[],956238,1123778,bug,2014-07-30T00:26:33Z,https://www.pivotaltracker.com/story/show/74422340
2014-07-30T17:00:03Z,2014-07-28T23:37:27Z,accepted,,"https://github.com/cloudfoundry/bosh_artifacts/pull/3

AWS's S3 ETags are usually MD5 checksums. This allows us to verify our downloads.


Filed by chou",1.0,75860224,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh_artifacts #3: Change file names to S3 etags,,[],956238,1134058,feature,2014-07-30T17:00:03Z,https://www.pivotaltracker.com/story/show/75860224
2014-07-30T19:24:13Z,2014-07-25T19:14:20Z,accepted,,,,75736710,story,[],Fix golint type name issues,1123776.0,[1123776],956238,1123776,chore,2014-07-30T19:24:08Z,https://www.pivotaltracker.com/story/show/75736710
2014-07-30T23:35:41Z,2014-07-30T23:28:19Z,accepted,,"Seems to be a flakey unit test. Failed while running rake spec locally.

```
Failures:

  1) Bosh::Director::Lock should not let two clients to acquire the same lock at the same time
     Failure/Error: Unable to find matching line from backtrace
       (Double ""redis"").set(""foo"", anything)
           expected: 1 time with arguments: (""foo"", anything)
           received: 0 times
     # ./bosh/bosh-director/spec/unit/lock_spec.rb:65:in `block (2 levels) in <module:Director>'
```

```
Failed examples:

rspec ./spec/unit/lock_spec.rb:44 # Bosh::Director::Lock should not let two clients to acquire the same lock at the same time
/Users/pivotal/workspace/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:69:in `block (6 levels) in <top (required)>'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `call'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
```

Unit test succeeded when run again.",,76021152,story,[],Investigate director lock unit test failure,1266616.0,[1266616],956238,1266616,chore,2014-08-01T17:48:44Z,https://www.pivotaltracker.com/story/show/76021152
2014-07-31T18:49:21Z,2014-07-28T21:14:57Z,accepted,,"bosh status should only show director status

previously, if the user was in a release dir, the latest dev & final versions were shown.

In the future, since multiple release names are allowed, the existing functionality doesn't make sense. If this is needed in the future, it should be a new command.",1.0,75851322,story,[],bosh status should NOT show release information,1266616.0,"[1266616, 1393546]",956238,81882,feature,2014-07-31T18:48:07Z,https://www.pivotaltracker.com/story/show/75851322
2014-07-31T19:32:50Z,2014-07-27T06:32:47Z,accepted,,"bosh_cli_plugin_micro depends on bosh_vcloud_cpi, which now depends on httpclient 2.4.0; while bosh_cli depends on httpclient 2.2.4, which causes a dependency conflict.

```
$ gem install bosh_cli_plugin_micro
ERROR:  While executing gem ... (Gem::DependencyError)
    Unable to resolve dependencies: bosh_vsphere_cpi requires httpclient (~> 2.2.4); bosh_vcloud_cpi requires httpclient (~> 2.4.0); bosh_openstack_cpi requires httpclient (= 2.2.4); agent_client requires httpclient (= 2.2.4); bosh_aws_cpi requires httpclient (= 2.2.4); bosh_cli requires httpclient (= 2.2.4); blobstore_client requires httpclient (= 2.2.4)
```

Somehow this is not a problem in Ruby 2.1.2, only in 1.9.3 (did not test any other versions)",,75767498,story,[],httpclient dependency conflict means you can't use bosh_cli_plugin_micro in Ruby 1.9.3,1386874.0,"[1386874, 1123776, 553935]",956238,1123776,bug,2014-07-31T19:31:36Z,https://www.pivotaltracker.com/story/show/75767498
2014-07-31T21:05:20Z,2014-06-10T17:50:53Z,accepted,,"If your new release version (dev or final) is the same fingerprint as an old release version bosh currently ""creates"" the old existing version. This is confusing UX. 

Creating a release should _always_ create a new release even if the new fingerprint is the same as a previous version

The message ""This version is no different from version X"" should be removed, and the conditional that stopped release creation in that case should be removed.

For reverse compatibility reasons, the index.yml file still needs to be maintained and updated for new releases. So the release fingerprinting still needs to happen.

Acceptance:
`create release` => 0.dev.1
`create release` (again, without changes) => 0.dev.2
 -- and --
`create release --final` => 1
`create release --final` (again, without changes) => 2",4.0,72987828,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",bosh create release should not downgrade to an old version,1266616.0,[1266616],956238,1266616,feature,2014-07-31T21:04:06Z,https://www.pivotaltracker.com/story/show/72987828
2014-07-31T21:30:37Z,2014-07-22T16:48:41Z,accepted,,"Log: https://www.pivotaltracker.com/file_attachments/34043026/download?inline=true

Blocks #74823266 & #72987828

Acceptance:
(do not do dev release)
`create release --final` => 1
`git add .`
`git commit -m 'final release 1'`
`git clean -fdx` (delete non-ignored files, like package tarballs)
`create release --final` (again, without changes) => 2",,75451362,story,[],create release fails after downloading package from blobstore,1266616.0,[1266616],956238,1266616,bug,2014-07-31T21:29:31Z,https://www.pivotaltracker.com/story/show/75451362
2014-07-31T21:31:46Z,2014-07-10T23:43:29Z,accepted,,"When creating a final release (without a dev release first):

First time:
```
Building FINAL release
----------------------

Building packages
-----------------
Building bar...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version f1267e1d4e06b60c91ef648fb9242e33ddcffa73
Uploading final version f1267e1d4e06b60c91ef648fb9242e33ddcffa73...
This package has already been uploaded
```

Second time:
```
Building FINAL release
----------------------

Building packages
-----------------
Building bar...
  Final version:   No blobstore id
  Dev version:     NOT FOUND
  Generating...
  Generated version f1267e1d4e06b60c91ef648fb9242e33ddcffa73
Uploading final version f1267e1d4e06b60c91ef648fb9242e33ddcffa73...
This package has already been uploaded
```

The output for the 2nd time is slightly different because the index.yml gets created without a blobstore_id.

Acceptance:
 - ensure config/final.yml includes blobstore for final release packages
`create release --final` => 1",,74823266,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",New Jobs/Packages fail to upload unless the tarballs were created by a previous dev release,1266616.0,[1266616],956238,1266616,bug,2014-07-31T21:30:32Z,https://www.pivotaltracker.com/story/show/74823266
2014-07-31T22:28:10Z,2014-07-24T17:11:23Z,accepted,,"Reported in mailing list:

```
Hi all,

I think I may have found the source of my woes deploying to vCHS. The full Bosh VM's all seem to come up. During the canary stage the Director fails. Looking at the error, it seems it can't locate a Gemfile. Any ideas would help.

Here is my environment.

Bosh Release
| bosh | 92*      | 43d2e856+

Stemcell
bosh-vcloud-esxi-ubuntu-lucid | 2641*   | urn:vcloud:catalogitem:6a4aa707-2228-4ae3-9f3f-5c9f35ea1e26

Error
root@7329cc12-1c8e-4b7a-8f91-109a4cb4221a:/var/vcap/sys/log/director# cat drain.stderr.log
/var/vcap/packages/ruby/lib/ruby/gems/1.9.1/gems/bundler-1.5.2/lib/bundler/definition.rb:23:in `build': /var/vcap/data/packages/data/compile/director/bosh/bosh-director/Gemfile not found (Bundler::GemfileNotFound)
                from /var/vcap/packages/ruby/lib/ruby/gems/1.9.1/gems/bundler-1.5.2/lib/bundler.rb:152:in `definition'
                from /var/vcap/packages/ruby/lib/ruby/gems/1.9.1/gems/bundler-1.5.2/lib/bundler.rb:115:in `setup'
                from /var/vcap/packages/ruby/lib/ruby/gems/1.9.1/gems/bundler-1.5.2/lib/bundler/setup.rb:17:in `<top (required)>'
                from /var/vcap/packages/ruby/lib/ruby/site_ruby/1.9.1/rubygems/core_ext/kernel_require.rb:135:in `require'
                from /var/vcap/packages/ruby/lib/ruby/site_ruby/1.9.1/rubygems/core_ext/kernel_require.rb:135:in `rescue in require'
                from /var/vcap/packages/ruby/lib/ruby/site_ruby/1.9.1/rubygems/core_ext/kernel_require.rb:144:in `require'
                from /var/vcap/packages/director/bin/bosh-director-drain-workers:14:in `<main>'


Yml is attached.

Thanks,
Zach
```

The problem is that drain script in director job does not set BUNDLE_GEMFILE and GEM_HOME correctly.",,75621994,story,[],Fix worker drain script after Ruby upgrade,553935.0,"[553935, 1115834]",956238,553935,bug,2014-07-31T22:26:57Z,https://www.pivotaltracker.com/story/show/75621994
2014-08-01T02:18:14Z,2014-06-20T17:58:10Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/591

To support non-Ruby clients who need to render BOSH job templates (e.g. the JuJu pipeline).

Pivotal Tracker story: https://www.pivotaltracker.com/story/show/73556684

Filed by hiremaga",1.0,73635400,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #591: Extract bosh-template gem from bosh_common,1386874.0,[1386874],956238,5637,feature,2014-08-01T02:18:14Z,https://www.pivotaltracker.com/story/show/73635400
2014-08-01T23:31:22Z,2014-07-31T05:13:12Z,accepted,,"https://github.com/cloudfoundry/bosh-lite/pull/145

tested on Windows 7. Basically same as unix script, but with Windows syntax.

Filed by dkoper",1.0,76030996,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh-lite #145: added Windows version of add-route script,,[],956238,1134058,feature,2014-08-01T23:31:22Z,https://www.pivotaltracker.com/story/show/76030996
2014-08-02T00:02:53Z,2014-07-26T01:01:57Z,accepted,,"currently timeout is 30 seconds. it should be 1min.

```
include VimSdk
client = cpi.instance_variable_get(""@delegate"").client
vm  = client.find_by_inventory_path(['BOSH_DC', 'vm', 'SYSTEM_MICRO_VSPHERE_VMs/vm-folder-ID', 'vm-ID'])
vm.shutdown_guest
c.get_property(vm, Vim::VirtualMachine, 'runtime.powerState')
```",1.0,75753316,story,[],increase timeout for shutting vm via vsphere agent,553935.0,"[553935, 1386874]",956238,81882,feature,2014-08-02T00:03:01Z,https://www.pivotaltracker.com/story/show/75753316
2014-08-04T21:56:08Z,2014-05-29T20:48:11Z,accepted,,,,72296090,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Convert go_agent to use godeps,1123776.0,[1123776],956238,46031,chore,2014-08-04T21:57:55Z,https://www.pivotaltracker.com/story/show/72296090
2014-08-05T21:26:39Z,2014-07-29T21:34:18Z,accepted,,"currently bosh-monitor sleeps right before exiting

`sleep(0.1) until !EM.reactor_running? # EM.stop is not blocking with EM 1.0` (https://github.com/cloudfoundry/bosh/blob/master/bosh-monitor/lib/bosh/monitor/runner.rb#L41) 

this should not happen because it might loop here for a long time.",,75930128,story,[],bosh-monitor should be more pro-active at exiting when it's stopped,1123776.0,[1123776],956238,81882,bug,2014-08-05T21:25:14Z,https://www.pivotaltracker.com/story/show/75930128
2014-08-05T21:27:23Z,2014-07-24T17:24:49Z,accepted,,"This is because the `prepare` call triggers downloading of required blobs from the blobstore before the VM is stopped.

Proposed solution: Skip or ignore the prepare call if the VM is going to be recreated.

This would allow the user to deploy with --recreate. So the recreated VMs would come up with the new blobstore credentials.",,75622904,story,[],Recreate fails after blobstore credentials change,81882.0,"[81882, 1386874]",956238,1266616,bug,2014-08-05T21:26:53Z,https://www.pivotaltracker.com/story/show/75622904
2014-08-05T21:41:49Z,2014-07-25T17:23:17Z,accepted,,,,75727724,story,[],Remove ruby agent from integration tests,553935.0,"[553935, 1386874]",956238,1123776,chore,2014-08-05T21:41:49Z,https://www.pivotaltracker.com/story/show/75727724
2014-08-05T22:09:59Z,2014-06-27T16:37:31Z,accepted,,"See bug #73515178 - 

Investigate various ways that disks can be removed from a vm and what the repercussions of each are to bosh on vsphere
- human deletes vm - does it delete disk?
- Does DRS move disks into diff datastores?
- human removes disk
- does bosh move disk during migration
- other scenarios",1.0,74054086,story,"[{'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",Investigate persistent disk disappearance on vSphere,553935.0,"[553935, 1386874]",956238,1210852,feature,2014-08-05T22:08:31Z,https://www.pivotaltracker.com/story/show/74054086
2014-08-05T23:39:54Z,2014-08-05T21:57:23Z,accepted,,"The googlecode download path is being flakey on Travis. So we're copying the tarball to bosh's S3.

New URL: https://s3.amazonaws.com/bosh-dependencies/go1.2.linux-amd64.tar.gz

Once we update to v1.2.2+ we should move to using the golang.org path so we're not paying for the bandwidth.",,76401424,story,[],Move golang 1.2 blob to S3,1266616.0,[1266616],956238,1266616,chore,2014-08-05T23:39:54Z,https://www.pivotaltracker.com/story/show/76401424
2014-08-06T17:54:57Z,2014-07-28T21:55:51Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/31063013

```
** Invoke spec:unit:go_agent (first_time)
** Execute spec:unit:go_agent
go_agent/bin/test
 Formatting packages...
 Installing ginkgo...
 Testing packages...
/home/travis/build.sh: line 236:  9653 Killed                  bundle exec rake --trace travis:install_go $TASKS
```",,75854450,story,[],Investigate why travis is killing go agent unit tests,1115834.0,"[1115834, 1266616]",956238,1123776,chore,2014-08-06T17:54:57Z,https://www.pivotaltracker.com/story/show/75854450
2014-08-06T17:55:06Z,2014-08-06T16:31:30Z,accepted,,"```
[1407282995] Syslog Suite - 6/6 specs ••••
------------------------------
• Failure [0.105 seconds]
Server [It] logs parsing error to error log if parsing syslog message fails 
/home/travis/gopath/src/github.com/cloudfoundry/bosh-agent/syslog/server_test.go:184
  Expected
      <string>: 
  to contain substring
      <string>: Failed to parse syslog message
  /home/travis/gopath/src/github.com/cloudfoundry/bosh-agent/syslog/server_test.go:177
------------------------------
```

https://travis-ci.org/cloudfoundry/bosh-agent/jobs/31764046",,76457522,story,[],"Investigate travis bosh-agent failure ""Failed to parse syslog message""",1266616.0,[1266616],956238,1266616,chore,2014-08-06T17:55:06Z,https://www.pivotaltracker.com/story/show/76457522
2014-08-06T18:38:22Z,2014-08-06T17:12:19Z,accepted,,"```
Failures:

  1) Ubuntu 14.04 stemcell installed by image_install_grub File ""/boot/grub/grub.conf"" should contain ""title Ubuntu 14.04 LTS""
     Failure/Error: it { should contain 'title Ubuntu 14.04 LTS' }
       expected File ""/boot/grub/grub.conf"" to contain ""title Ubuntu 14.04 LTS""
     # ./spec/stemcells/ubuntu_trusty_spec.rb:9:in `block (4 levels) in <top (required)>'
     # ./spec/support/stemcell_image.rb:8:in `block (3 levels) in <top (required)>'
     # ./lib/bosh/stemcell/disk_image.rb:35:in `while_mounted'
     # ./spec/support/stemcell_image.rb:6:in `block (2 levels) in <top (required)>'
```

http://172.16.68.6:8080/job/stemcell_vsphere_ubuntu_trusty_go_agent/21/console",,76460612,story,[],Investigate grub.conf setup failure during stemcell building,1266616.0,[1266616],956238,1266616,chore,2014-08-08T00:35:31Z,https://www.pivotaltracker.com/story/show/76460612
2014-08-07T00:33:21Z,2014-08-06T17:03:04Z,accepted,,"```
[1407344373] Settings Suite - 24/24 specs •••••••••••••••••••
------------------------------
• Failure [0.006 seconds]
Networks DefaultIP [It] with two networks 
/home/travis/gopath/src/github.com/cloudfoundry/bosh-agent/settings/settings_test.go:81
  Expected
      <string>: aa.aa.aa.aa
  to equal
      <string>: xx.xx.xx.xx
  /home/travis/gopath/src/github.com/cloudfoundry/bosh-agent/settings/settings_test.go:80
------------------------------
```",,76460006,story,[],Investigate Travis settings_test failure,1266616.0,[1266616],956238,1266616,chore,2014-08-07T00:33:21Z,https://www.pivotaltracker.com/story/show/76460006
2014-08-07T17:30:39Z,2014-08-06T17:54:22Z,accepted,,"In order to reduce the number of threads Travis uses, our tests need to be tolerant of reduced resource usage.

When running bosh-agent test with GOMAXPROCS=2 or =3 we were able to see several consistent test errors in `bosh-agent/syslog/server_test.go`. The test is launching a syslog server and then sending messages to it over an ephemeral port. Somehow this is failing when constraining the number of threads.

This came from investigation around #76457522 & #75854450",,76464216,story,[],Syslog Server tests should be tolerant of low thread environment,1266616.0,[1266616],956238,1266616,chore,2014-08-07T17:30:40Z,https://www.pivotaltracker.com/story/show/76464216
2014-08-07T19:19:23Z,2014-08-05T20:44:09Z,accepted,,"The CI build is failing

```
Failures:

  1) network configuration resolving DNS entries resolves instance names from deployed VM
     Failure/Error: ssh(public_ip, 'vcap', cmd, ssh_options).should include(public_ip)
       expected "";; connection timed out; no servers could be reached\n;; connection timed out; no servers could be reached\n"" to include ""172.16.69.10""
       Diff:
       @@ -1,2 +1,3 @@
       -172.16.69.10
       +;; connection timed out; no servers could be reached
       +;; connection timed out; no servers could be reached
     # ./spec/system/network_configuration_spec.rb:43:in `block (3 levels) in <top (required)>'
```

```
Deployed `deployment' to `microbosh-vsphere-jenkins'

I, [2014-08-04T17:38:52.569828 #12202]  INFO -- : Satisfied requirement #<Bat::Deployment:0x000000030106a0>
I, [2014-08-04T17:38:52.570994 #12202]  INFO -- : Requirement no_tasks_processing
I, [2014-08-04T17:38:52.571130 #12202]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20140804-12202-18fyo5m --user admin --password admin tasks 2>&1
I, [2014-08-04T17:38:55.951541 #12202]  INFO -- : No running tasks

I, [2014-08-04T17:38:55.951815 #12202]  INFO -- : Satisfied requirement no_tasks_processing
at depth 0 - 18: self signed certificate
.I, [2014-08-04T17:38:59.663101 #12202]  INFO -- : Requirement no_tasks_processing
I, [2014-08-04T17:38:59.663197 #12202]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20140804-12202-18fyo5m --user admin --password admin tasks 2>&1
I, [2014-08-04T17:39:03.724213 #12202]  INFO -- : No running tasks

I, [2014-08-04T17:39:03.724342 #12202]  INFO -- : Satisfied requirement no_tasks_processing
.I, [2014-08-04T17:39:03.737136 #12202]  INFO -- : Requirement no_tasks_processing
I, [2014-08-04T17:39:03.737230 #12202]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20140804-12202-18fyo5m --user admin --password admin tasks 2>&1
I, [2014-08-04T17:39:06.945430 #12202]  INFO -- : No running tasks

I, [2014-08-04T17:39:06.945535 #12202]  INFO -- : Satisfied requirement no_tasks_processing
I, [2014-08-04T17:39:06.972042 #12202]  INFO -- : --> ssh: vcap@172.16.69.10 ""cat /etc/resolv.conf""
I, [2014-08-04T17:39:06.972179 #12202]  INFO -- : --> ssh options: {:password=>""c1oudc0w"", :user_known_hosts_file=>[""/dev/null""]}
I, [2014-08-04T17:39:14.224299 #12202]  INFO -- : --> ssh output: """"
I, [2014-08-04T17:39:14.224444 #12202]  INFO -- : Contents of resolv.conf ''
I, [2014-08-04T17:39:14.224509 #12202]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20140804-12202-18fyo5m --user admin --password admin logs batlight 0 --agent --dir /tmp 2>&1
I, [2014-08-04T17:39:22.992524 #12202]  INFO -- : 
Director task 4
  Started fetching logs for batlight/0 > Finding and packing log files. Done (00:00:01)

Task 4 done
```

http://bosh-jenkins.cf-app.com:8080/job/bat_micro_vsphere_ubuntu_trusty_go_agent/30/consoleText",,76394918,story,[],stemcell's firstboot.sh script should not empty resolv.conf,1266616.0,[1266616],956238,1266616,bug,2014-08-07T19:19:40Z,https://www.pivotaltracker.com/story/show/76394918
2014-08-07T20:51:17Z,2014-07-29T22:09:54Z,accepted,,"See for example: https://github.com/cloudfoundry/cf-release/blob/master/jobs/dea_next/monit

bosh monitor runs an http server. it should be checked against /varz endpoint either with correct creds or just with status = 401. 

From monit doc (https://mmonit.com/monit/documentation/#http)

```
STATUS option can be used to explicitly test the HTTP status code returned by the HTTP server. If not used, the http protocol test will fail if the status code returned is greater than or equal to 400. You can override this behaviour by using the status qualifier.

For example to test that a page does not exist (404 should be returned in this case):

  if failed
     port 80
     protocol http
     request ""/non/existent.php""
     status = 404
  then alert
```",1.0,75932906,story,[],bosh-monitor should be restarted automatically if it fails to respond to / after several requests,320647.0,[320647],956238,81882,feature,2014-08-07T20:51:35Z,https://www.pivotaltracker.com/story/show/75932906
2014-08-07T21:31:02Z,2014-07-24T22:22:14Z,accepted,,"We've found this on this happening on the services team when we added several new dependencies to a package. 

It seems that dependencies are stored in the `dependeny_key` column of the `compiled_packages` table, and this column has a limit of 255 characters. If the package has enough dependencies to overflow the character limit, the database (or maybe the ORM?) will truncate the unique key defined by package_id-stemcell_id-dependency_key. This attempt to compile and deploy actually works fine. But during the *next* deploy, the same truncation happens and the database raises a DuplicateKey error.

I've included the debug logs (debug.txt) and the output of our deploy (output.txt).",,75658922,story,[],Bosh can't compile a package if it has too many dependencies,553935.0,"[553935, 1115834]",956238,795489,bug,2014-08-07T21:30:08Z,https://www.pivotaltracker.com/story/show/75658922
2014-08-07T21:56:10Z,2014-06-13T22:31:29Z,accepted,,"Numbers with over 7 digits in deployment manifest cause json unmarshalling error.

Happened when persistent disk size was 1024000.

Started updating job gocd_server > gocd_server/0 (canary). Failed: Action Failed get_task: Task 8d074d3b-0912-4c19-77be-89c4cfec018b result: Extracting method arguments from payload: Unmarshalling action argument: json: cannot unmarshal number 1.024e+06 into Go value of type int (00:00:00)
  Started updating job gocd_agent
  Started updating job gocd_agent > gocd_agent/0 (canary). Failed: `gocd_agent/0' is not running after update (00:13:40)

Error 450001: Action Failed get_task: Task 8d074d3b-0912-4c19-77be-89c4cfec018b result: Extracting method arguments from payload: Unmarshalling action argument: json: cannot unmarshal number 1.024e+06 into Go value of type int
",,73239278,story,"[{'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",Cannot create persistent disk with 7 digits (>1TB) on AWS - get json:cannot unmarshal number error,553935.0,"[553935, 1386874]",956238,756869,bug,2014-08-07T21:55:08Z,https://www.pivotaltracker.com/story/show/73239278
2014-08-07T22:03:10Z,2014-07-30T23:13:37Z,accepted,,,1.0,76020490,story,[],vsphere should power off VM if it is still running after graceful shut down (configure_networks cpi method in vsphere),553935.0,"[553935, 1386874]",956238,553935,feature,2014-08-07T22:01:36Z,https://www.pivotaltracker.com/story/show/76020490
2014-08-07T22:05:36Z,2014-08-04T21:58:05Z,accepted,,,,76319440,story,"[{'name': 'blocked on ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T22:46:11Z', 'id': 7537412, 'updated_at': '2014-01-22T22:46:11Z'}]",Add test-integration to agent bin/test,320647.0,[320647],956238,1123776,chore,2014-08-07T22:05:36Z,https://www.pivotaltracker.com/story/show/76319440
2014-08-07T22:05:38Z,2014-08-01T17:52:12Z,accepted,,"Currently only in `bin/test`, which travis doesn't run (only runs unit tests)",,76144168,story,"[{'name': 'blocked on ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-22T22:46:11Z', 'id': 7537412, 'updated_at': '2014-01-22T22:46:11Z'}]",run vet and lint in bin/test-unit,320647.0,[320647],956238,1123776,chore,2014-08-07T22:05:38Z,https://www.pivotaltracker.com/story/show/76144168
2014-08-08T16:53:34Z,2014-08-04T17:42:24Z,accepted,,,1.0,76296616,story,[],[shanghai] update stemcells,320647.0,[320647],956238,81882,feature,2014-08-08T16:51:59Z,https://www.pivotaltracker.com/story/show/76296616
2014-08-08T17:34:44Z,2014-07-29T19:09:06Z,accepted,,use suggested methods from https://www.pivotaltracker.com/story/show/75853518,1.0,75918476,story,[],"user should see less ""blob is corrupted error"" errors when downloading blobs",553935.0,"[553935, 1386874, 1115834]",956238,81882,feature,2014-08-08T17:33:26Z,https://www.pivotaltracker.com/story/show/75918476
2014-08-08T18:45:46Z,2014-07-03T22:40:55Z,accepted,,"log output:

Running bosh upload release /Users/pivotal/releases/dummy-0.1-dev.tgz --skip-if-exists

Verifying release...
File exists and readable                                     OK
Extract tarball                                              
OK
Manifest exists                                              OK
Release name/version                                         OK
Read package 'dummy_package' (1 of 1)                        OK
Package 'dummy_package' checksum                             OK
Package dependencies                                         OK
Checking jobs format                                         OK
Read job 'dummy' (1 of 2), version 0.1-dev                   OK
Job 'dummy' checksum                                         OK
Extract job 'dummy'                                          OK
Read job 'dummy' manifest                                    OK
Check template 'dummy_ctl' for 'dummy'                       OK
Monit file for 'dummy'                                       OK
Read job 'dummy_with_package' (2 of 2), version 0.1-dev      OK
Job 'dummy_with_package' checksum                            OK
Extract job 'dummy_with_package'                             OK
Read job 'dummy_with_package' manifest                       OK
Check template 'dummy_with_package_ctl' for 'dummy_with_package' OK
Job 'dummy_with_package' needs 'dummy_package' package       OK
Monit file for 'dummy_with_package'                          OK

Release info
------------
Name:    dummy
Version: 0.1-dev

Packages
  - dummy_package (0.1-dev)

Jobs
  - dummy (0.1-dev)
  - dummy_with_package (0.1-dev)

Checking if can repack release for faster upload...
dummy_package (0.1-dev)       SKIP
dummy (0.1-dev)               UPLOAD
dummy_with_package (0.1-dev)  UPLOAD
Release repacked (new size is 2.1K)

Uploading release

release-repac:   0% |                                          | ETA:  --:--:--
release-repac:  97% |ooooooooooooooooooooooo |   2.0KB   7.0MB/s ETA:  00:00:00release-repac: 100% |oooooooooooooooooooooooo|   2.1KB   5.5MB/s Time: 00:00:00
Director task 12
  Started extracting release > Extracting release. Done (00:00:00)

  Started verifying manifest > Verifying manifest. Done (00:00:00)

Error 30000: Release `dummy/0+dev.1' already exists

Task 12 error

For a more detailed error report, run: bosh task 12 --debug
Try no. 4 failed.
Exited with 1.",,74422238,story,"[{'name': 'idempotent-upload', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-30T01:49:44Z', 'id': 9070428, 'updated_at': '2014-07-30T01:49:44Z'}]",bosh upload release --skip-if-exists doesn't skip upload with releases using pre-semver naming convention,320647.0,[320647],956238,1123778,bug,2014-08-08T18:45:47Z,https://www.pivotaltracker.com/story/show/74422238
2014-08-09T01:28:28Z,2014-06-10T18:59:43Z,accepted,,"Acceptance:
```
bosh create release --final --name cf-release-fork --version 173.1.0
```

Expected Outcome:
Created release version should be cf-release-fork-173.1.0

When --name is provided:
- The new release should use the provided name
- The new release should default to 0+dev.1, if --final is not used
- The new release version should be 1, if --final is used without --version
- The new release version should be the specified version, if --version is used (note: --version currently requires --final)

In order to support reverse compatibility, but also move towards release named directories:
- new named releases will go in subdirs (release/<release-name>/index.yml) and will NOT go in the primary index (release/index.yml)
- new releases of the original name continue to go in the parent release dir (release/index.yml)
- All releases should optionally support release named dirs so that we can migrate to this in the future.

Renaming future releases by changing the dev_name/final_name in the config should also be supported.

acceptance criteria:
no other release names are affected by the numbering scheme.  (if a fork is made and increments, our numbering will remain continuous)",2.0,72994716,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",Add a command to change a bosh release name,1266616.0,[1266616],956238,1266616,feature,2014-08-09T01:28:28Z,https://www.pivotaltracker.com/story/show/72994716
2014-08-09T18:34:03Z,2014-05-19T17:55:14Z,accepted,,,,71599548,story,"[{'name': 'release_versioning', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-24T23:44:36Z', 'id': 8031116, 'updated_at': '2014-04-30T23:20:35Z'}]",release versioning/naming,,[],956238,1210852,release,2014-08-09T18:34:03Z,https://www.pivotaltracker.com/story/show/71599548
2014-08-11T17:52:55Z,2014-08-07T22:14:01Z,accepted,,"DB=postgres TASKS=""spec:integration""

Integration tests are frequently timing out after 50 minutes on Travis.
Ex: https://travis-ci.org/cloudfoundry/bosh/jobs/31955223

```
I'm sorry but your test run exceeded 50.0 minutes. 
```

We either need to speed up the tests or split them up into separate stages. Speeding up is preferable, because splitting increases the number of permutations in the test matrix.",,76564098,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Integration tests should not time out on Travis,1266616.0,[1266616],956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/76564098
2014-08-11T18:05:12Z,2014-08-08T01:06:55Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh/jobs/31966958

```
Failures:
  1) cli: vms should return vms in a deployment
     Failure/Error: deploy_simple(manifest_hash: manifest_hash)
     RuntimeError:
       ERROR: bosh -n -c /tmp/d20140808-24979-1lkp8ze/bosh_config.yml deploy failed with output:
       Getting deployment properties from director...
       Unable to get properties list from director, trying without it...
       Compiling deployment manifest...
       
       Director task 18
         Started preparing deployment
         Started preparing deployment > Binding deployment. Done (00:00:00)
         Started preparing deployment > Binding releases. Done (00:00:00)
         Started preparing deployment > Binding existing deployment. Done (00:00:00)
         Started preparing deployment > Binding resource pools. Done (00:00:00)
         Started preparing deployment > Binding stemcells. Done (00:00:00)
         Started preparing deployment > Binding templates. Done (00:00:00)
         Started preparing deployment > Binding properties. Done (00:00:00)
         Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
         Started preparing deployment > Binding instance networks. Done (00:00:00)
            Done preparing deployment (00:00:00)
       
         Started preparing package compilation > Finding packages to compile. Done (00:00:00)
       
         Started compiling packages
         Started compiling packages > foo/0ee95716c58cf7aab3ef7301ff907118552c2dda. Done (00:00:02)
         Started compiling packages > bar/f1267e1d4e06b60c91ef648fb9242e33ddcffa73. Done (00:00:05)
            Done compiling packages (00:00:07)
       
         Started preparing dns > Binding DNS. Done (00:00:00)
       
         Started creating bound missing vms
         Started creating bound missing vms > a/0
         Started creating bound missing vms > a/1
         Started creating bound missing vms > a/2
          Failed creating bound missing vms > a/1: Timed out pinging to ce98cf6b-664c-47da-8961-b1014a3b07ed after 600 seconds (00:10:00)
          Failed creating bound missing vms > a/0: Timed out pinging to e13198f3-be84-45cf-8f5b-812b189bf5c1 after 600 seconds (00:10:00)
          Failed creating bound missing vms > a/2: Timed out pinging to e18daa44-6b66-486d-ac2d-d748fd5fde7e after 600 seconds (00:10:00)
          Failed creating bound missing vms (00:10:00)
       
       Error 450002: Timed out pinging to ce98cf6b-664c-47da-8961-b1014a3b07ed after 600 seconds
       
       Task 18 error
       
       For a more detailed error report, run: bosh task 18 --debug
```",,76573148,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]","Investigate ""Timed out pinging [agent-id]""",553935.0,"[553935, 1115834]",956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/76573148
2014-08-11T19:26:02Z,2014-08-08T18:28:35Z,accepted,,"Travis fails to install golint because it hasn't been downloaded. It appears we have go get and go install in different files and the get isn't being run by whatever path Travis is using (bin/test-unit).

Move the go get golint to right before install it in bin/golint.

```
Checking with golint...
can't load package: package github.com/golang/lint/golint: cannot find package ""github.com/golang/lint/golint"" in any of:
	/home/travis/build/cloudfoundry/bosh/tmp/go/src/pkg/github.com/golang/lint/golint (from $GOROOT)
	/home/travis/build/cloudfoundry/bosh/go/src/github.com/golang/lint/golint (from $GOPATH)
	/home/travis/build/cloudfoundry/bosh/go/src/github.com/cloudfoundry/bosh-agent/Godeps/_workspace/src/github.com/golang/lint/golint
go/src/github.com/cloudfoundry/bosh-agent/bin/env: line 16: exec: golint: not found
```

https://travis-ci.org/cloudfoundry/bosh/jobs/32036340",,76620970,story,[],go get golint before go install in Travis test,1266616.0,[1266616],956238,1266616,chore,2014-08-13T16:03:42Z,https://www.pivotaltracker.com/story/show/76620970
2014-08-12T17:02:08Z,2014-08-08T17:20:43Z,accepted,,https://github.com/cloudfoundry/bosh/pull/629,0.0,76615246,story,[],merge #629 PR to make go agent running on vcloud think it's on vsphere,553935.0,"[553935, 1115834]",956238,81882,feature,2014-08-12T17:02:15Z,https://www.pivotaltracker.com/story/show/76615246
2014-08-12T17:28:41Z,2014-04-17T00:11:17Z,accepted,,"""Invalid Subject Identifier (sid), already taken""",,69670182,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent fails to subscribe during integration tests because sub id is taken,1266616.0,[1266616],956238,81882,bug,2014-08-12T17:28:48Z,https://www.pivotaltracker.com/story/show/69670182
2014-08-12T17:29:09Z,2013-12-06T01:39:05Z,accepted,,talk to exploratory tester dave L if you have questions,,62026388,story,[],bosh vms should not add % sign after load average values,1266616.0,[1266616],956238,494053,bug,2014-08-12T17:29:15Z,https://www.pivotaltracker.com/story/show/62026388
2014-08-12T17:36:44Z,2014-08-05T17:23:30Z,accepted,,stemcell-bat-test-ubuntu-trusty build should update http://bosh-jenkins-gems-warden.s3.amazonaws.com/stemcells/latest-bosh-stemcell-warden.tgz (used by bosh-lite/scripts/provision_cf) with trusty stemcell,1.0,76377448,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Make sure latest bosh-lite stemcell pointer gets updated when new warden stemcell is pushed,1266616.0,[1266616],956238,81882,feature,2014-08-12T17:36:50Z,https://www.pivotaltracker.com/story/show/76377448
2014-08-12T21:04:28Z,2014-08-12T20:42:01Z,accepted,,Currently we are using snowflake asset which is Centos 6.4. That stemcell does not have fixes for CD-rom lock which we are testing in lifecycle tests. ,,76824666,story,[],Update vsphere_cpi_lifecycle stemcell to ubuntu trusty,553935.0,"[553935, 1393546]",956238,553935,chore,2014-08-12T21:04:29Z,https://www.pivotaltracker.com/story/show/76824666
2014-08-14T19:29:31Z,2014-08-06T23:13:08Z,accepted,,"original & tail files should not be here

```
vcap@8041375f-c765-4666-86e4-7b77551f2d0c:~$ tail /etc/resolvconf/resolv.conf.d/*
==> /etc/resolvconf/resolv.conf.d/base <==

==> /etc/resolvconf/resolv.conf.d/head <==
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN

==> /etc/resolvconf/resolv.conf.d/original <==
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 172.16.0.23
search compute-1.internal

==> /etc/resolvconf/resolv.conf.d/tail <==
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 172.16.0.23
search compute-1.internal
```

reported in https://github.com/cloudfoundry/bosh/issues/627",2.0,76488842,story,[],properly configure /etc/resolvconf/resolv.conf.d/ in ubuntu trusty while building a stemcell,553935.0,"[553935, 1115834]",956238,81882,feature,2014-08-14T19:29:32Z,https://www.pivotaltracker.com/story/show/76488842
2014-08-14T21:02:30Z,2014-08-11T18:05:02Z,accepted,,,1.0,76726258,story,[],[karachi] update stemcells,1123776.0,[1123776],956238,81882,feature,2014-08-14T21:02:31Z,https://www.pivotaltracker.com/story/show/76726258
2014-08-14T21:16:28Z,2014-07-31T06:02:26Z,accepted,,each controller inherits from the sinatra base controller. when route is determines sinatra appears to run all befores which cause auth to happen multiple times.,2.0,76032364,story,"[{'name': 'perf', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:41:09Z', 'id': 8096474, 'updated_at': '2014-08-05T08:12:30Z'}]",bosh-director should not perform authentication multiple times for each request,320647.0,[320647],956238,81882,feature,2014-08-14T21:16:29Z,https://www.pivotaltracker.com/story/show/76032364
2014-08-14T22:06:37Z,2014-08-14T20:58:05Z,accepted,,,,76984816,story,[],Investigate why bosh micro deploy does not create VM in a folder ,553935.0,"[553935, 1393546]",956238,553935,chore,2014-08-14T22:14:36Z,https://www.pivotaltracker.com/story/show/76984816
2014-08-15T00:38:00Z,2014-08-12T16:31:26Z,accepted,,"seems that in ResourcePoolUpdater#reserve_networks it tries to reserve a network on a vm even if there is an instance already bound to it

```
(""deployments_stemcells"".""deployment_id"" = 1))
D, [2014-08-09T04:56:33.748710 #21120] [0x1a08984] DEBUG -- : Lock renewal thread exiting
D, [2014-08-09T04:56:33.748646 #21120] [task:17] DEBUG -- : Deleting lock: lock:deployment:cf-db074d50fe5bd7530a30
D, [2014-08-09T04:56:33.749866 #21120] [task:17] DEBUG -- : Deleted lock: lock:deployment:cf-db074d50fe5bd7530a30
E, [2014-08-09T04:56:33.750198 #21120] [task:17] ERROR -- : 'loggregator/0' asked for a dynamic IP but there were no more available
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:146:in `block in reserve_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:173:in `block in each_vm_with_index'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:166:in `block in each_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:166:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:166:in `each_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:172:in `each_vm_with_index'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/resource_pool_updater.rb:137:in `reserve_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/deployment_plan/resource_pools.rb:43:in `block in refill'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/deployment_plan/resource_pools.rb:42:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/deployment_plan/resource_pools.rb:42:in `refill'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/deployment_plan/updater.rb:46:in `update'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/jobs/update_deployment.rb:55:in `update'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/jobs/update_deployment.rb:81:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/jobs/update_deployment.rb:74:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_runner.rb:104:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/bin/bosh-director-worker:78:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,76803812,story,[],bosh deploy should not raise 'asked for a dynamic IP but there were no more available' when trying to refill resource pool,1266616.0,[1266616],956238,81882,bug,2014-11-24T17:51:33Z,https://www.pivotaltracker.com/story/show/76803812
2014-08-15T00:54:47Z,2014-06-16T00:40:16Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/588

Fix for https://github.com/cloudfoundry/bosh/issues/557

Filed by dims",1.0,73271108,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #588: Get bosh_cli to work with Director on https with port 443,320647.0,[320647],956238,1134058,feature,2014-08-15T00:54:59Z,https://www.pivotaltracker.com/story/show/73271108
2014-08-18T02:55:48Z,2014-07-31T06:09:24Z,accepted,,"bosh vms command takes a long time to run. deliver a document/comment that breaks down full bosh vms time into detailed smaller chunks. test in multiple envs:
- 4-5vm vsphere
- 20vm aws
- a1's cf deployment
- when several agents are not responsive

see tasks for few ideas on what to measure...",2.0,76032510,story,"[{'name': 'perf', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:41:09Z', 'id': 8096474, 'updated_at': '2014-08-05T08:12:30Z'}]",profile 'bosh vms' command in several environments,1123776.0,"[1123776, 320647]",956238,81882,feature,2014-08-18T02:58:29Z,https://www.pivotaltracker.com/story/show/76032510
2014-08-18T18:08:30Z,2014-08-12T16:03:50Z,accepted,,"Gems occasionally fail to push, but the output isn't specific enough to say why.

```
rake aborted!
Command failed with status (1): [gem push tmp/gems-2685/bosh-stemcell-1.268...]
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/rake-10.3.2/lib/rake/file_utils.rb:54:in `block in create_shell_runner'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/rake-10.3.2/lib/rake/file_utils.rb:45:in `call'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/rake-10.3.2/lib/rake/file_utils.rb:45:in `sh'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/rake-10.3.2/lib/rake/file_utils_ext.rb:37:in `sh'
/mnt/jenkins/workspace/promote_artifacts/bosh/bosh-dev/lib/bosh/dev/gem_artifact.rb:15:in `block in promote'
/home/jenkins/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.6.2/lib/bundler.rb:235:in `block in with_clean_env'
/home/jenkins/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.6.2/lib/bundler.rb:222:in `with_original_env'
/home/jenkins/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.6.2/lib/bundler.rb:228:in `with_clean_env'
/mnt/jenkins/workspace/promote_artifacts/bosh/bosh-dev/lib/bosh/dev/gem_artifact.rb:15:in `promote'
/mnt/jenkins/workspace/promote_artifacts/bosh/bosh-dev/lib/bosh/dev/build.rb:94:in `block in promote_artifacts'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/peach-0.5.1/lib/peach.rb:22:in `block (2 levels) in peach'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/peach-0.5.1/lib/peach.rb:22:in `each'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/peach-0.5.1/lib/peach.rb:22:in `block in peach'
/mnt/ci-tmp/promote_artifacts/ruby/2.1.0/gems/peach-0.5.1/lib/peach.rb:13:in `block (2 levels) in _peach_run'
Tasks: TOP => ci:promote
```",,76801756,story,[],Retry pushing gems in promote artifacts,1266616.0,[1266616],956238,553935,chore,2014-08-18T18:08:31Z,https://www.pivotaltracker.com/story/show/76801756
2014-08-19T21:54:33Z,2014-08-09T02:40:22Z,accepted,,"Trigger on every commit to develop, no assets produced",,76642222,story,"[{'name': 'gocd', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-13T00:02:05Z', 'id': 9188506, 'updated_at': '2014-08-13T00:02:05Z'}]",create simple gocd pipeline for unit/integration tests,1266616.0,[1266616],956238,1123776,chore,2014-08-21T22:30:07Z,https://www.pivotaltracker.com/story/show/76642222
2014-08-19T23:55:43Z,2014-08-19T21:10:19Z,accepted,,"https://server.gocd.cf-app.com/go/tab/build/detail/bosh/38/tests/1/integration-2.1.2-mysql

```
Failures:

  1) deploy deletes extra vms when switching from fixed-size to dynamically-sized resource pools
     Failure/Error: expect(vms.map(&:job_name_index)).to match_array(job_name_index_list)
       expected collection contained:  [""foobar/0""]
       actual collection contained:    [""unknown/unknown""]
       the missing elements were:      [""foobar/0""]
       the extra elements were:        [""unknown/unknown""]
     # ./spec/integration/deploy_spec.rb:107:in `expect_running_vms'
     # ./spec/integration/deploy_spec.rb:102:in `block (2 levels) in <top (required)>'
```

Flakey test failure. 
NOTE: the 'mysql' tests are currently running 'postgres' due to #77228988",,77241316,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]","Investigate ""deletes extra vms"" integration test failure",553935.0,"[553935, 1386874]",956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/77241316
2014-08-20T01:02:00Z,2014-08-19T17:52:24Z,accepted,,"```
Failures:

  1) VSphereCloud::Cloud vsphere specific lifecycle when vm was migrated should exercise the vm lifecycle
     Failure/Error: @cpi.client.wait_for_task(task)
     RuntimeError:
       Unable to access the virtual machine configuration: Unable to access file [jalapeno-ds2]
     # ./lib/cloud/vsphere/client.rb:196:in `block in wait_for_task'
     # ./lib/cloud/vsphere/client.rb:165:in `loop'
     # ./lib/cloud/vsphere/client.rb:165:in `wait_for_task'
     # ./spec/integration/lifecycle_spec.rb:240:in `relocate_vm_to_second_datastore'
     # ./spec/integration/lifecycle_spec.rb:245:in `block (5 levels) in <top (required)>'
     # ./spec/integration/lifecycle_spec.rb:87:in `vm_lifecycle'
     # ./spec/integration/lifecycle_spec.rb:244:in `block (4 levels) in <top (required)>'
```
",,77221634,story,[],Investigate why vsphere lifecycle test fail with  'Unable to access [jalapeno-ds2]',553935.0,"[553935, 1386874]",956238,553935,chore,2014-08-20T01:02:00Z,https://www.pivotaltracker.com/story/show/77221634
2014-08-20T18:16:01Z,2014-08-19T21:38:54Z,accepted,,"This test fails on its first deploy with 3 instances of 1 job. It appears that instance wants propagated to the vms. 

If I had to guess I'd blame some kind of race condition.

https://server.gocd.cf-app.com/go/tab/build/detail/bosh/38/tests/1/integration-2.1.2-postgres

```
  1) deploy allows removing deployed jobs and adding new jobs at the same time
     Failure/Error: expect(vms.map(&:job_name_index)).to match_array(job_name_index_list)
       expected collection contained:  [""fake-name1/0"", ""fake-name1/1"", ""fake-name1/2""]
       actual collection contained:    [""unknown/unknown"", ""unknown/unknown"", ""unknown/unknown""]
       the missing elements were:      [""fake-name1/0"", ""fake-name1/1"", ""fake-name1/2""]
       the extra elements were:        [""unknown/unknown"", ""unknown/unknown"", ""unknown/unknown""]
     # ./spec/integration/deploy_spec.rb:107:in `expect_running_vms'
     # ./spec/integration/deploy_spec.rb:10:in `block (2 levels) in <top (required)>'
```",,77243812,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]","Investigate ""unknown/unknown"" deploy integration test failure",553935.0,"[553935, 1386874]",956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/77243812
2014-08-21T21:39:04Z,2014-08-13T20:51:28Z,accepted,,,1.0,76907564,story,[],[beijing] update stemcells,320647.0,[320647],956238,81882,feature,2014-08-21T21:39:06Z,https://www.pivotaltracker.com/story/show/76907564
2014-08-21T22:00:36Z,2014-08-19T19:08:08Z,accepted,,"The environment variable is currently being ignored.

bosh/bosh-dev/lib/bosh/dev/sandbox/main.rb:134 is using :name instead of :type
",,77228988,story,[],integration tests should use mysql when DB=mysql,553935.0,"[553935, 1386874]",956238,1266616,bug,2014-08-21T22:00:53Z,https://www.pivotaltracker.com/story/show/77228988
2014-08-21T22:07:22Z,2014-08-13T20:41:05Z,accepted,,"Flakey test failure: server.Start returns a different error than the one expected.

```
[1407947484] Syslog Suite - 6/6 specs 
------------------------------
• Failure [0.003 seconds]
Server [It] it calls back on a new syslog message 
/home/travis/build/cloudfoundry/bosh/go/src/github.com/cloudfoundry/bosh-agent/syslog/server_test.go:108
  Expected
      <*errors.errorString | 0xc2100403d0>: {
          s: ""use of closed network connection"",
      }
  to be assignable to the type: *net.OpError
  /home/travis/build/cloudfoundry/bosh/go/src/github.com/cloudfoundry/bosh-agent/syslog/server_test.go:101
```

It has errored several times on Travis. Recently we added a type-check test so that the actual error was printed instead of a type cast error.

Ex: https://travis-ci.org/cloudfoundry/bosh/jobs/32451184",,76906652,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]","Investigate agent syslog server ""use of closed network connection"" unit test failure",1123776.0,"[1123776, 1386874]",956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/76906652
2014-08-21T22:28:32Z,2014-06-27T17:38:39Z,accepted,,"currently cck only scans disk attachments on live vms. we should account for cases when instance does not have a vm associated with it and the disk is not in the cloud. 

provide 2 options:
- delete disk reference
- ignore problem

!!! Before deleting disk from the DB we need to do one of the following: !!! 
- remove disk from the agent json by detaching the disk
- modify director to not care if that disk is attached to the vm in subsequent actions (bosh deploy)
- delete vm

CPI interface should be augmented with has_disk? method. If CPI does not implement has_disk skip this check. Relevant class is problem_scanner.",4.0,74058560,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}, {'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}]",bosh cck should be able to provide options to fix missing persistent disk on vSphere,553935.0,"[553935, 1386874]",956238,1210852,feature,2014-08-21T22:28:32Z,https://www.pivotaltracker.com/story/show/74058560
2014-08-22T18:53:54Z,2014-08-18T21:59:34Z,accepted,,,1.0,77159332,story,[],[lagos & istanbul] update stemcells,553935.0,"[553935, 1386874]",956238,81882,feature,2014-08-22T18:53:54Z,https://www.pivotaltracker.com/story/show/77159332
2014-08-22T21:42:09Z,2014-08-13T18:32:20Z,accepted,,"Stemcell: light-bosh-stemcell-2685-aws-xen-ubuntu-trusty-go_agent.tgz
Infrastructure: AWS (m1.small)
staging-aws sha: 789227deccaf25bfd0caa52216f6011c5913f1a0
CLI: 1.2685.0

When we deploy, we find the process hangs on ""waiting for director"".

Looking on the box, there's a postgres startup log file with an error:

```
FATAL:  could not create shared memory segment: Invalid argument
DETAIL:  Failed system call was shmget(key=5432001, size=37978112, 03600).
HINT:  This error usually means that PostgreSQL's request for a shared memory segment exceeded your kernel's SHMMAX parameter.  You can either reduce the request size or reconfigure the kernel with larger SHMMAX.  To reduce the request size (currently 37978112 bytes), reduce PostgreSQL's shared_buffers parameter (currently 4096) and/or its max_connections parameter (currently 104).
        If the request size is already small, it's possib le that it is less than your kernel's SHMMIN parameter, in which case raising the request size or reconfiguring SHMMIN is called for.
        The PostgreSQL documentation contains more information about shared memory configuration.
```

Relevant PR:

https://github.com/cloudfoundry/bosh/pull/635
Postgres was unable to allocate enough memory for mapping. Added sysctl to change memory before postgres starts.
Signed-off-by: Jason Smith <jasmith@pivotallabs.com>
Filed by thecadams",1.0,76895510,story,[],"Investigate whether it is true that ""Postgres could not start on the Trusty stemcell""",553935.0,"[553935, 1386874]",956238,1092360,feature,2014-08-22T21:42:23Z,https://www.pivotaltracker.com/story/show/76895510
2014-08-22T23:18:38Z,2014-08-07T15:59:52Z,accepted,,Dashboard credentials are in lastpass,,76534736,story,[],Run openstack BATs on new openstack cluster,1123776.0,"[1123776, 1386874]",956238,1123776,chore,2014-08-22T23:18:38Z,https://www.pivotaltracker.com/story/show/76534736
2014-08-22T23:52:02Z,2014-07-31T17:10:37Z,accepted,,"Take advantage of vsphere's vm anti-affinity rules. User configures their resource pool:

```
resource_pools:
- name: nodes
  cloud_properties:
    datacenters:
    - name: dc
      clusters:
      - TEST_CLUSTER:
          drs_rules: # <------------------------------------------ new
          - name: <some-unique-rule-name>
            type: separate_vms

jobs:
- name: nodes
  resource_pool: nodes
  instances: 3
```

As vms are added/deleted, drs_rule should be updated. If there are 2 hosts in a cluster, expected placement would be 2 vms on one host and 1 vm on another.

There is an example of a deployment that uses affinity rules on crab (vblock; see deployments-vcenter)

Potential problems: when is it safe to delete a drs_rule group?",4.0,76070138,story,"[{'name': 'affinity1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:55:22Z', 'id': 9223340, 'updated_at': '2014-08-18T03:55:22Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",user should be able to configure single job so that its instances are ideally placed onto different hosts,553935.0,"[553935, 1115834]",956238,684067,feature,2015-01-13T18:25:15Z,https://www.pivotaltracker.com/story/show/76070138
2014-08-25T16:43:05Z,2014-08-22T18:53:31Z,accepted,,These permutations are now being tested on GoCD for each develop branch commit. So we don't need them in Travis anymore. Hopefully this will also cut down on the number of false negatives on Travis.,,77469216,story,[],Remove ruby 1.9.3 & mysql tests from Travis,1266616.0,[1266616],956238,1266616,chore,2014-08-28T17:17:11Z,https://www.pivotaltracker.com/story/show/77469216
2014-08-25T19:14:25Z,2014-08-22T19:13:44Z,accepted,,"```
------------------------------
• Failure [0.022 seconds]
concreteV1Service PopulateDynamicNetworks when there are dynamic networks when associated network cannot be found in settings [It] returns error
/Users/pivotal/workspace/bosh/go/src/github.com/cloudfoundry/bosh-agent/agent/applier/applyspec/concrete_v1_service_test.go:195

  Expected
      <*errors.errorString | 0xc2080d4670>: {
          s: ""Network fake-net3 is not found in settings"",
      }
  to equal
      <*errors.errorString | 0xc2080d4680>: {
          s: ""Network fake-net2 is not found in settings"",
      }

  /Users/pivotal/workspace/bosh/go/src/github.com/cloudfoundry/bosh-agent/agent/applier/applyspec/concrete_v1_service_test.go:193
------------------------------
```",,77470626,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Investigate flaky agent applyspec test,1123776.0,[1123776],956238,1123776,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/77470626
2014-08-26T18:20:00Z,2014-08-22T21:58:36Z,accepted,,,1.0,77482698,story,[],build 2682.1 stemcell with bosh-agent fix for blobstore upload,1123776.0,[1123776],956238,81882,feature,2014-08-26T18:20:00Z,https://www.pivotaltracker.com/story/show/77482698
2014-08-26T19:04:46Z,2014-06-13T17:42:22Z,accepted,,The aws bootstrap Jenkins build was using reports. We removed it because it didn't seem useful at all. Make sure nothing else is using these reports and remove the reporter gem and its usage in `ci_build.sh`.,,73220014,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Remove ci_reporter gem,553935.0,[553935],956238,1123776,chore,2014-08-26T19:04:47Z,https://www.pivotaltracker.com/story/show/73220014
2014-08-27T17:31:02Z,2014-08-26T22:10:57Z,accepted,,,1.0,77667248,story,[],investigate several vsphere cpi exceptions,1123776.0,"[1123776, 553935]",956238,81882,feature,2014-08-27T17:31:03Z,https://www.pivotaltracker.com/story/show/77667248
2014-08-28T23:56:35Z,2014-08-27T21:09:24Z,accepted,,"We don't set the deployment to new file so we never update our persistent disk in BATs. 

The only way it succeeds is that in most cases available value is changed.

```
       expected: value != {""/dev/vda1""=>{:blocks=>""9710768"", :used=>""1266704"", :available=>""7927732"", :percent=>""14%"", :mountpoint=>""/""}, ""/dev/loop0""=>{:blocks=>""122835"", :used=>""1550"", :available=>""117354"", :percent=>""2%"", :mountpoint=>""/tmp""}, ""/dev/vdb1""=>{:blocks=>""2031428"", :used=>""3088"", :available=>""1907100"", :percent=>""1%"", :mountpoint=>""/var/vcap/store""}}
            got: {""/dev/vda1""=>{:blocks=>""9710768"", :used=>""1266704"", :available=>""7927732"", :percent=>""14%"", :mountpoint=>""/""}, ""/dev/loop0""=>{:blocks=>""122835"", :used=>""1550"", :available=>""117354"", :percent=>""2%"", :mountpoint=>""/tmp""}, ""/dev/vdb1""=>{:blocks=>""2031428"", :used=>""3088"", :available=>""1907100"", :percent=>""1%"", :mountpoint=>""/var/vcap/store""}}
```",,77760340,story,[],BATs should really update persistent disk when testing disk migrations,553935.0,"[553935, 1123776]",956238,553935,bug,2014-08-28T23:56:54Z,https://www.pivotaltracker.com/story/show/77760340
2014-08-29T00:16:07Z,2014-08-21T18:08:33Z,accepted,,,1.0,77393146,story,[],improve bosh deploy changeset,1266616.0,[1266616],956238,81882,feature,2014-08-29T00:16:14Z,https://www.pivotaltracker.com/story/show/77393146
2014-08-29T00:19:46Z,2014-08-27T19:14:50Z,accepted,,"http://bosh-jenkins.cf-app.com:8080/job/bat_micro_vsphere_ubuntu_go_agent/196/console

```
  5) network configuration when using manual networking changes static IP address
     Failure/Error: @requirements.requirement(deployment, @spec) # 2.5 min on local vsphere
       expected command to exit with 0 but was 1. output was
       
       Processing deployment manifest
       ------------------------------
       Getting deployment properties from director...
       Unable to get properties list from director, trying without it...
       Compiling deployment manifest...
       Cannot get current deployment information from director, possibly a new deployment
       
       Deploying
       ---------
       Deployment name: `deployment'
       Director name: `microbosh-vsphere-jenkins'
       
       Director task 3
         Started preparing deployment
         Started preparing deployment > Binding deployment. Done (00:00:01)
         Started preparing deployment > Binding releases. Done (00:00:00)
         Started preparing deployment > Binding existing deployment. Done (00:00:00)
         Started preparing deployment > Binding resource pools. Done (00:00:00)
         Started preparing deployment > Binding stemcells. Done (00:00:00)
         Started preparing deployment > Binding templates. Done (00:00:00)
         Started preparing deployment > Binding properties. Done (00:00:00)
         Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
         Started preparing deployment > Binding instance networks. Done (00:00:00)
            Done preparing deployment (00:00:01)
       
         Started preparing package compilation > Finding packages to compile. Done (00:00:00)
       
         Started compiling packages > batlight/1/home/jenkins/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/openssl/buffering.rb:53:in `sysread': SSL_read:: sslv3 alert bad record mac (OpenSSL::SSL::SSLError)
       	from /home/jenkins/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/openssl/buffering.rb:53:in `fill_rbuff'
       	from /home/jenkins/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/openssl/buffering.rb:200:in `gets'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:352:in `gets'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:881:in `block in parse_header'
       	from /home/jenkins/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/timeout.rb:68:in `timeout'
       	from /home/jenkins/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/timeout.rb:99:in `timeout'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:877:in `parse_header'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:860:in `read_header'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:667:in `get_header'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:1141:in `do_get_header'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:1090:in `do_get_block'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:891:in `block in do_request'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:985:in `protect_keep_alive_disconnected'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:890:in `do_request'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:778:in `request'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:692:in `perform_http_request'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:665:in `block in try_to_perform_http_request'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:663:in `downto'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:663:in `try_to_perform_http_request'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:616:in `request'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:545:in `block (2 levels) in <class:Director>'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:504:in `get_task_output'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/director_task.rb:22:in `output'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/task_tracking/task_tracker.rb:58:in `poll'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/task_tracking/task_tracker.rb:45:in `track'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:571:in `request_and_track'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/client/director.rb:238:in `deploy'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/commands/deployment.rb:104:in `perform'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/command_handler.rb:57:in `run'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/runner.rb:56:in `run'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/lib/cli/runner.rb:16:in `run'
       	from /var/lib/jenkins/jobs/bat_micro_vsphere_ubuntu_go_agent/workspace/bosh/bosh_cli/bin/bosh:7:in `<top (required)>'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/bin/bosh:23:in `load'
       	from /mnt/ci-tmp/bat_micro_vsphere_ubuntu_go_agent/ruby/1.9.1/bin/bosh:23:in `<main>'
     # ./lib/bat/requirements.rb:99:in `require_deployment'
     # ./lib/bat/requirements.rb:33:in `requirement'
     # ./spec/system/network_configuration_spec.rb:13:in `block (2 levels) in <top (required)>'
```",0.0,77748992,story,[],bosh cli should retry on OpenSSL::SSL::SSLError,553935.0,"[553935, 1123776]",956238,553935,feature,2014-08-29T00:19:47Z,https://www.pivotaltracker.com/story/show/77748992
2014-08-29T00:21:26Z,2014-08-22T16:49:21Z,accepted,,Relevant code: https://github.com/cloudfoundry/bosh-agent/blob/master/davcli/client/client.go. check with webdav and nginx plugin.,,77459132,story,[],Make sure dav-blobstore cli performs proper http error checking when uploading blobs from the agent,1123776.0,[1123776],956238,81882,bug,2014-08-29T00:22:24Z,https://www.pivotaltracker.com/story/show/77459132
2014-08-29T22:44:40Z,2014-08-20T04:10:47Z,accepted,,It is always the same. Fails to compile packages. On next rerun bosh is not running after update. bosh vms reveals everything is running.,,77261982,story,[],Investigate why auto-deploy aws trusty is always failing on first run,1123776.0,"[1123776, 553935]",956238,553935,chore,2014-08-29T22:44:41Z,https://www.pivotaltracker.com/story/show/77261982
2014-08-29T23:56:23Z,2014-08-29T18:17:59Z,accepted,,,1.0,77894442,story,[],build 2682.2 stemcell,553935.0,"[553935, 1123776]",956238,553935,feature,2014-08-29T23:56:23Z,https://www.pivotaltracker.com/story/show/77894442
2014-08-30T00:00:42Z,2014-08-18T03:00:07Z,accepted,,see for example of using packer-bosh to install bosh & bosh-warden-cpi: https://github.com/cppforlife/packer-bosh/tree/master/dev,4.0,77088646,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",try switching bosh-lite to use packer-bosh for building boxes,1393546.0,"[1393546, 553935]",956238,81882,feature,2014-08-30T00:00:42Z,https://www.pivotaltracker.com/story/show/77088646
2014-08-30T00:57:36Z,2014-08-21T23:06:22Z,accepted,,,1.0,77415182,story,[],Investigate how many db connections powerdns takes up,1123776.0,[1123776],956238,81882,feature,2014-08-30T00:57:38Z,https://www.pivotaltracker.com/story/show/77415182
2014-08-30T03:17:01Z,2013-11-20T10:10:55Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/458

Adding excon_loggin_instrumentor as was discussed on the [mailing list](https://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-dev/ATV6byhj2B8).
It is used to log the excon requests and responses mad to the OpenStack api to the bosh debug log. 

Filed by rkoster",1.0,61108276,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #458: Added excon_logging_instrumentor to OpenStack cpi,1123776.0,"[1123776, 320647]",956238,1134058,feature,2014-08-30T03:17:01Z,https://www.pivotaltracker.com/story/show/61108276
2014-08-30T04:54:55Z,2014-08-20T23:14:25Z,accepted,,"cloud.create_disk(disk_size, vm_cid)
-->
cloud.create_disk(disk_size, vm_cid, cloud_properties)

This will not be reverse compatible with older CPIs.
All CPIs within the BOSH repo should be changed to support this new API.",1.0,77338354,story,"[{'name': 'disk-pools1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-04T19:40:34Z', 'id': 9111434, 'updated_at': '2014-08-18T03:50:54Z'}]",CPIs should take cloud_properties on create_disk,1266616.0,[1266616],956238,1266616,feature,2014-08-30T04:54:56Z,https://www.pivotaltracker.com/story/show/77338354
2014-08-30T04:55:25Z,2014-08-18T03:37:17Z,accepted,,"```
resource_pools:
- name: fast_machines
  cloud_properties:
    instance_type: m3.x2large

disk_pools:
- name: fast_disks
  disk_size: 3000
  cloud_properties:
    type: gp2 # http://docs.aws.amazon.com/cli/latest/reference/ec2/create-volume.html

jobs:
- name: mydb
  templates: [ mysql ]
  instances: 1
  resource_pool: fast_machines
  persistent_disk_pool: fast_disks
  networks: [ {...} ] 
  properties: {}
```",2.0,77089406,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'disk-pools1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-04T19:40:34Z', 'id': 9111434, 'updated_at': '2014-08-18T03:50:54Z'}]",user should be able to specify ssd volume type for aws persistent disk via disk pool's cloud properties,553935.0,"[553935, 540781]",956238,81882,feature,2014-08-30T04:55:25Z,https://www.pivotaltracker.com/story/show/77089406
2014-09-05T16:30:15Z,2014-09-04T23:03:12Z,accepted,,,,78237330,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Make bosh-lite BATs run inside the vms ,320647.0,[320647],956238,320647,chore,2014-09-05T16:30:15Z,https://www.pivotaltracker.com/story/show/78237330
2014-09-05T17:49:40Z,2014-08-28T16:24:34Z,accepted,,,1.0,77816644,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",shrink bosh-lite produced box sizes,320647.0,"[320647, 1393546]",956238,81882,feature,2014-09-05T17:49:40Z,https://www.pivotaltracker.com/story/show/77816644
2014-09-05T21:57:58Z,2014-08-26T18:15:38Z,accepted,,currently perms on tmp are very strict. 'other' should allow creating files and executing them. ,1.0,77647520,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",bosh-lite box should allow vagrant to upload files into /tmp,320647.0,[320647],956238,81882,feature,2014-09-05T21:57:58Z,https://www.pivotaltracker.com/story/show/77647520
2014-09-05T21:58:23Z,2014-08-19T23:12:57Z,accepted,,"latest Riak CS release fails to deploy using bosh-lite with vmware_fusion provider built from trusty stemcell.

Apparently the virtualbox provider in the latest bosh-lite uses a trusty stemcell that works. 

As vmware_fusion is the recommended provider, we should have a vm for it that uses an updated trusty stemcell",1.0,77250924,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",user should be able to get new fusion bosh-lite box,320647.0,[320647],956238,58676,feature,2014-09-05T21:58:23Z,https://www.pivotaltracker.com/story/show/77250924
2014-09-05T22:36:01Z,2014-08-29T18:04:59Z,accepted,,,1.0,77893512,story,[],[guangzhou] update stemcell,553935.0,"[553935, 1123776]",956238,81882,feature,2014-09-05T22:36:01Z,https://www.pivotaltracker.com/story/show/77893512
2014-09-09T01:51:50Z,2014-09-06T00:44:21Z,accepted,,https://github.com/cloudfoundry/bosh-agent/blob/master/README.md,,78311066,story,[],document intellij setup for bosh-agent,1266616.0,[1266616],956238,1266616,chore,2014-09-09T01:51:51Z,https://www.pivotaltracker.com/story/show/78311066
2014-09-09T17:04:22Z,2014-08-29T17:56:07Z,accepted,,,,77892888,story,[],make promotable artifacts accept only stemcells to promote,553935.0,"[553935, 1123776, 1266616]",956238,553935,chore,2014-09-09T17:04:22Z,https://www.pivotaltracker.com/story/show/77892888
2014-09-09T19:05:43Z,2014-09-03T18:08:01Z,accepted,,,1.0,78136662,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",run bats before publishing fusion bosh-lite image,509171.0,"[509171, 1123776]",956238,81882,feature,2014-09-09T19:05:50Z,https://www.pivotaltracker.com/story/show/78136662
2014-09-09T19:09:06Z,2014-09-02T16:46:50Z,accepted,,"- make sure that sections are marked with ids such that they can be overwritten
- include a readme section about overwriting sections (matt mentions on how to do that in his PR)
- push to **develop** branch and only after everything goes green it should be promoted to master

See https://github.com/cloudfoundry/bosh-lite/pull/174",1.0,78043372,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",simplify Vagrantfile configuration by including defaults in their boxes,320647.0,"[320647, 509171]",956238,81882,feature,2014-09-09T19:09:18Z,https://www.pivotaltracker.com/story/show/78043372
2014-09-10T19:08:10Z,2014-09-09T17:27:05Z,accepted,,,,78482382,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Move stemcell BATs script to repo,320647.0,[320647],956238,1123776,chore,2014-09-10T19:08:10Z,https://www.pivotaltracker.com/story/show/78482382
2014-09-11T20:39:01Z,2014-09-09T20:56:32Z,accepted,,,,78499680,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",bump warden-cpi-release to version 8 in ci,320647.0,[320647],956238,81882,chore,2014-09-11T20:39:01Z,https://www.pivotaltracker.com/story/show/78499680
2014-09-11T22:08:30Z,2014-08-25T16:35:19Z,accepted,,"Enable for unit tests on ruby 2.1.2
Update the code coverage gem to avoid a multi-threading bug: 
https://github.com/colszowka/simplecov/issues/305
timebox to 2 hours",,77555690,story,[],Re-enabled code coverage reporting on GoCD,553935.0,"[553935, 1429768]",956238,1266616,chore,2014-09-11T22:09:26Z,https://www.pivotaltracker.com/story/show/77555690
2014-09-12T23:31:25Z,2014-09-03T21:58:21Z,accepted,,,0.0,78155904,story,[],remove lucid stemcells from the pipeline,509171.0,"[509171, 1393546]",956238,81882,feature,2014-09-12T23:31:38Z,https://www.pivotaltracker.com/story/show/78155904
2014-09-16T17:54:48Z,2014-09-15T21:36:07Z,accepted,,If one of Openstack BATs start later than other it will clean up VMs of other build which will cause it to fail. You can fix it by creating separate accounts for each test.,,78866842,story,[],Create separate accounts for openstack BATs,1123776.0,[1123776],956238,553935,chore,2014-09-16T17:54:48Z,https://www.pivotaltracker.com/story/show/78866842
2014-09-17T23:13:23Z,2014-09-16T17:54:20Z,accepted,,"Using ci-ubuntu-trusty-dynamic and ci-ubuntu-trusty-manual tenants cause micro deploy to fail (jenkins tenant is ok). Micro deploy times out on ""Waiting for agent"" step. Agent logs show that agent fails to write anything to /var/vcap/data.

```
2014-09-16_17:12:15.06903 [linuxPlatform] 2014/09/16 17:12:15 DEBUG - Getting device size of `/dev/vdb'
2014-09-16_17:12:15.06920 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: sfdisk -s /dev/vdb
2014-09-16_17:12:15.07127 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: 10485760
2014-09-16_17:12:15.07129 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.07129 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.07130 [linuxPlatform] 2014/09/16 17:12:15 DEBUG - Calculating ephemeral disk partition sizes of `/dev/vdb' with total disk size 10737418
240B
2014-09-16_17:12:15.07180 [linuxPlatform] 2014/09/16 17:12:15 INFO - Partitioning ephemeral disk `/dev/vdb' with [[Type: swap, SizeInBytes: 2099290112] [Typ
e: linux, SizeInBytes: 8638128128]]
2014-09-16_17:12:15.07181 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: sfdisk -d /dev/vdb
2014-09-16_17:12:15.07384 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: # partition table of /dev/vdb
2014-09-16_17:12:15.07385 unit: sectors
2014-09-16_17:12:15.07386
2014-09-16_17:12:15.07387 /dev/vdb1 : start=        1, size=  4100543, Id=82
2014-09-16_17:12:15.07387 /dev/vdb2 : start=  4100544, size= 16870896, Id=83
2014-09-16_17:12:15.07388 /dev/vdb3 : start=        0, size=        0, Id= 0
2014-09-16_17:12:15.07388 /dev/vdb4 : start=        0, size=        0, Id= 0
2014-09-16_17:12:15.07389 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.07389 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.07390 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: sfdisk -s /dev/vdb1
2014-09-16_17:12:15.07592 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: 2050271
2014-09-16_17:12:15.07594 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.07595 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.07595 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: sfdisk -s /dev/vdb2
2014-09-16_17:12:15.07796 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: 8435448
2014-09-16_17:12:15.07798 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.07798 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.07800 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: sfdisk -s /dev/vdb
2014-09-16_17:12:15.08009 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: 10485760
2014-09-16_17:12:15.08010 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.08011 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.08012 [SfdiskPartitioner] 2014/09/16 17:12:15 INFO - /dev/vdb already partitioned as expected, skipping
2014-09-16_17:12:15.08012 [linuxPlatform] 2014/09/16 17:12:15 INFO - Formatting `/dev/vdb1' as swap
2014-09-16_17:12:15.08013 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: blkid -p /dev/vdb1
2014-09-16_17:12:15.08279 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: /dev/vdb1: UUID=""1aa0e05f-391e-4dba-99ef-c2da61aba251"" VERSION=""2"" TYPE=""swap"" US
AGE=""other"" PART_ENTRY_SCHEME=""dos"" PART_ENTRY_TYPE=""0x82"" PART_ENTRY_NUMBER=""1"" PART_ENTRY_OFFSET=""1"" PART_ENTRY_SIZE=""4100543"" PART_ENTRY_DISK=""253:16""
2014-09-16_17:12:15.08281 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.08282 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.08282 [linuxPlatform] 2014/09/16 17:12:15 INFO - Formatting `/dev/vdb2' as ext4
2014-09-16_17:12:15.08283 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: blkid -p /dev/vdb2
2014-09-16_17:12:15.08556 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: /dev/vdb2: UUID=""768d5aa9-f746-45a4-9e58-5c7e6b1e5f3c"" VERSION=""1.0"" TYPE=""ext4"" USAGE=""filesystem"" PART_ENTRY_SCHEME=""dos"" PART_ENTRY_TYPE=""0x83"" PART_ENTRY_NUMBER=""2"" PART_ENTRY_OFFSET=""4100544"" PART_ENTRY_SIZE=""16870896"" PART_ENTRY_DISK=""253:16""
2014-09-16_17:12:15.08558 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.08559 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.08559 [linuxPlatform] 2014/09/16 17:12:15 INFO - Mounting `/dev/vdb1' as swap
2014-09-16_17:12:15.08560 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Running command: swapon -s
2014-09-16_17:12:15.08728 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stdout: Filename                             Type            Size    Used    Priority
2014-09-16_17:12:15.08730 /dev/vdb1                               partition     2050264 0       -1
2014-09-16_17:12:15.09631 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Stderr:
2014-09-16_17:12:15.09634 [Cmd Runner] 2014/09/16 17:12:15 DEBUG - Successful: true (0)
2014-09-16_17:12:15.09634 [linuxPlatform] 2014/09/16 17:12:15 INFO - Mounting `/dev/vdb2' at `/var/vcap/data'
2014-09-16_17:12:15.09635 [File System] 2014/09/16 17:12:15 DEBUG - Reading file /proc/mounts
2014-09-16_17:12:15.09635 [File System] 2014/09/16 17:12:15 DEBUG - Read content
2014-09-16_17:12:15.09636 ********************
2014-09-16_17:12:15.09636 rootfs / rootfs rw 0 0
2014-09-16_17:12:15.09639 sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
2014-09-16_17:12:15.09640 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
2014-09-16_17:12:15.09640 udev /dev devtmpfs rw,relatime,size=1014396k,nr_inodes=253599,mode=755 0 0
2014-09-16_17:12:15.09641 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
2014-09-16_17:12:15.09641 tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=205012k,mode=755 0 0
2014-09-16_17:12:15.09642 /dev/disk/by-uuid/1ebf2d9e-10a0-48c8-8a23-ba2da241daaa / ext4 rw,relatime,data=ordered 0 0
2014-09-16_17:12:15.09642 none /sys/fs/cgroup tmpfs rw,relatime,size=4k,mode=755 0 0
2014-09-16_17:12:15.09643 none /sys/fs/fuse/connections fusectl rw,relatime 0 0
2014-09-16_17:12:15.09643 none /sys/kernel/debug debugfs rw,relatime 0 0
2014-09-16_17:12:15.09644 none /sys/kernel/security securityfs rw,relatime 0 0
2014-09-16_17:12:15.09644 none /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
2014-09-16_17:12:15.09646 none /run/shm tmpfs rw,nosuid,nodev,relatime 0 0
2014-09-16_17:12:15.09646 none /run/user tmpfs rw,nosuid,nodev,noexec,relatime,size=102400k,mode=755 0 0
2014-09-16_17:12:15.09647 none /sys/fs/pstore pstore rw,relatime 0 0
2014-09-16_17:12:15.09647 rpc_pipefs /run/rpc_pipefs rpc_pipefs rw,relatime 0 0
2014-09-16_17:12:15.09649 /dev/vdb2 /var/vcap/data ext4 rw,relatime,data=ordered 0 0
2014-09-16_17:12:15.09649
2014-09-16_17:12:15.09650 ********************
2014-09-16_17:12:15.09650 [File System] 2014/09/16 17:12:15 DEBUG - Making dir /var/vcap/data/sys/log with perm 488
2014-09-16_17:12:15.10227 [main] 2014/09/16 17:12:15 ERROR - App setup Running bootstrap: Setting up data dir: Making /var/vcap/data/sys/log dir: mkdir /var/vcap/data/sys: input/output error
2014-09-16_17:12:15.11394 [main] 2014/09/16 17:12:15 DEBUG - Starting agent
```  ",,78935926,story,[],Investigate why piston cloud tenants are broken,1123776.0,[1123776],956238,553935,chore,2014-09-17T23:17:21Z,https://www.pivotaltracker.com/story/show/78935926
2014-09-18T23:41:11Z,2014-09-18T23:11:16Z,accepted,,We are not sure why we need this pointer so we decided to remove it and see if anyone is using it,,79151466,story,[],Remove LightStemcellPointer from promote artifacts,553935.0,[553935],956238,553935,chore,2014-09-18T23:41:11Z,https://www.pivotaltracker.com/story/show/79151466
2014-09-19T00:43:54Z,2014-05-27T19:34:42Z,accepted,,"Run the tests and fix anything that's broken.

This is in bosh/release/packages/ruby/packaging",2.0,72120872,story,"[{'name': 'ruby-2.x', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-05T16:57:53Z', 'id': 8364038, 'updated_at': '2014-05-05T16:57:53Z'}]",Upgrade bosh release to Ruby 2.x,553935.0,"[553935, 1429768]",956238,1123776,feature,2014-09-19T00:43:54Z,https://www.pivotaltracker.com/story/show/72120872
2014-09-19T00:57:11Z,2014-03-28T01:13:58Z,accepted,,"The fog gem we are using is pretty old 0.14. It was reported to have 'unf' issues we see sometimes on CI. 

see #69221112",1.0,68414164,story,[],Update the fog gem,1393546.0,"[1393546, 1429768]",956238,756869,feature,2014-09-19T00:57:12Z,https://www.pivotaltracker.com/story/show/68414164
2014-09-19T00:58:21Z,2014-09-08T18:25:32Z,accepted,,,1.0,78402580,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",investigate which feature flags are needed to replace -I flag in bosh agent,320647.0,[320647],956238,81882,feature,2014-09-19T00:58:21Z,https://www.pivotaltracker.com/story/show/78402580
2014-09-19T16:31:57Z,2014-08-19T21:49:24Z,accepted,,We need some way to get GoCD task success/failure on our CI board.,,77245160,story,[],Create Checkman script for GoCD test status,553935.0,"[553935, 509171]",956238,1266616,chore,2014-09-19T16:31:57Z,https://www.pivotaltracker.com/story/show/77245160
2014-09-19T16:33:03Z,2014-09-03T21:55:30Z,accepted,,,1.0,78155742,story,[],[mumbai] update stemcell,1393546.0,"[1393546, 320647]",956238,81882,feature,2014-09-19T16:33:03Z,https://www.pivotaltracker.com/story/show/78155742
2014-09-19T16:33:17Z,2014-09-09T21:19:50Z,accepted,,The changes to the Vagrantfile broke provision_cf,,78502084,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Fix provision_cf on master,509171.0,[509171],956238,509171,bug,2014-09-19T16:33:17Z,https://www.pivotaltracker.com/story/show/78502084
2014-09-21T19:00:00Z,2014-08-08T21:58:56Z,accepted,,"currently #configure_networks vsphere CPI method looks up datacenter dynamically and uses its name instead of its full path for lookup.

```
- host: 172.16.76.3
      user: root
      password: vmware
      datacenters:
      - name: my-dc-folder/clams-dc
        vm_folder: pivotal_cf_vms_46b35412c2c3085065e9
        template_folder: pivotal_cf_templates_46b35412c2c3085065e9
        disk_path: pivotal_cf_disk_46b35412c2c3085065e9
        datastore_pattern: ""^(clams\\-ds)$""
        persistent_datastore_pattern: ""^(clams\\-ds)$""
        allow_mixed_datastores: true
        clusters:
        - clams-cl

network:
  ip: 172.16.76.13
  netmask: 255.255.254.0
  gateway: 172.16.76.1
  dns:
  - 10.80.130.1
  cloud_properties:
    name: my-networks-folder/VM Network
```

example console usage:
```
d = cpi.instance_variable_get(""@delegate"")
d.client.find_by_inventory_path(['my-dc-folder/clams-dc', 'network', 'my-networks-folder/VM Network'])
```

```
bosh task 10 --debug
D, [2014-08-08T20:23:21.681775 #15305] [task:10] DEBUG -- : Deleted lock: lock:deployment:cf-1260c2cc63b8c84036ec
E, [2014-08-08T20:23:21.682074 #15305] [task:10] ERROR -- : Can't find network: my-networks-folder/VM Network
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2671.0/lib/cloud/vsphere/cloud.rb:783:in `create_nic_config_spec'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2671.0/lib/cloud/vsphere/cloud.rb:346:in `block (2 levels) in configure_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2671.0/lib/cloud/vsphere/cloud.rb:343:in `each_value'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2671.0/lib/cloud/vsphere/cloud.rb:343:in `block in configure_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2671.0/lib/cloud/vsphere/cloud.rb:310:in `configure_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/instance_updater/network_updater.rb:28:in `update'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/instance_updater.rb:297:in `update_networks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/instance_updater.rb:70:in `block in update'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/instance_updater.rb:37:in `step'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/instance_updater.rb:70:in `update'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_updater.rb:74:in `block (2 levels) in update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_updater.rb:72:in `block in update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_updater.rb:71:in `update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2671.0/lib/bosh/director/job_updater.rb:65:in `block (2 levels) in update_canaries'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2671.0/lib/common/thread_pool.rb:63:in `block in create_thread'
```

Relevant code from #configure_networks:

```
datacenter_name = @cloud_searcher.get_property(datacenter, Vim::Datacenter, 'name')
...
network_mob = client.find_by_inventory_path([datacenter_name, 'network', v_network_name])
```",,76635896,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}, {'name': 'vsphere-dc-fold1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-22T07:26:40Z', 'id': 9497486, 'updated_at': '2014-09-22T07:27:26Z'}]",vsphere cpi user should be able to deploy to the datacenter nested in a folder such that vms' network settings can be reconfigured,553935.0,"[553935, 1393546]",956238,81882,bug,2014-09-22T18:12:22Z,https://www.pivotaltracker.com/story/show/76635896
2014-09-22T04:34:48Z,2014-08-22T20:56:30Z,accepted,,"this should resolve majority of 503 service unavailable -- other action in progress errors.

The agent should only retry for 5 minutes when it gets the specific 503 service unavailable error. The moment any other response is received it should use the default 20 second retry period.",2.0,77478708,story,[],agent should wait until monit can schedule unmonitor and stop commands for up to 5 minutes,1393546.0,"[1393546, 1429768, 1266616]",956238,81882,feature,2014-09-22T04:34:49Z,https://www.pivotaltracker.com/story/show/77478708
2014-09-22T05:50:19Z,2014-07-30T00:02:49Z,accepted,,on openstack some flavors might not have ephemeral disk attached. bosh-agent should partition and format remaining disk space on the root partition as mount it on /var/vcap/data. should happen during bootstrap after reading settings. should be configurable via agent.json to turn on this behavior. turn on this on in the openstack stemcell (see warden agent.json).,4.0,75939098,story,"[{'name': 'ephemeral-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-30T19:35:53Z', 'id': 9078798, 'updated_at': '2014-07-30T19:35:53Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",bosh-agent should use unoccupied space on the root partition as ephemeral disk if no other eph. disk is available,553935.0,"[553935, 1123776, 1266616]",956238,81882,feature,2014-09-22T05:50:20Z,https://www.pivotaltracker.com/story/show/75939098
2014-09-22T06:51:29Z,2014-09-17T20:15:05Z,accepted,,after talking with maria we have decided to remove drs_rule attribute cleaner such that deletion of vms is not slowed down by drs locking,1.0,79038492,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}, {'name': 'waiting-develop', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-17T19:01:27Z', 'id': 9472384, 'updated_at': '2014-09-17T19:01:27Z'}]",vsphere cpi should not try remove drs_rule attribute,553935.0,"[553935, 509171]",956238,81882,feature,2014-09-22T06:51:30Z,https://www.pivotaltracker.com/story/show/79038492
2014-09-22T21:37:09Z,2014-09-11T18:52:27Z,accepted,,"Provisioning the stemcell builder with vagrant rsyncs the entire .git folder for bosh which is silly and takes a long time.

Let's do something other than that. Discuss?

Also: Maria thought there might be another story about this already here. I searched and couldn't find it. If somebody sees this and knows if that is a thing that exists let's just delete this story and use that one. Yay!",,78665652,story,[],Do Something Other Than RSync .git To the Stemcell Builder,553935.0,"[553935, 320647]",956238,1429768,chore,2014-09-22T21:37:09Z,https://www.pivotaltracker.com/story/show/78665652
2014-09-24T00:20:44Z,2014-09-04T18:06:53Z,accepted,,,2.0,78216530,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",produce bosh lite centos 6.5 stemcell,509171.0,"[509171, 1393546, 320647]",956238,81882,feature,2014-09-24T00:20:45Z,https://www.pivotaltracker.com/story/show/78216530
2014-09-25T19:01:25Z,2014-09-23T16:22:27Z,accepted,,,,79375018,story,[],investigate nebula env,1393546.0,"[1393546, 1429768]",956238,81882,chore,2014-09-25T19:01:25Z,https://www.pivotaltracker.com/story/show/79375018
2014-09-25T21:36:03Z,2014-09-24T16:54:10Z,accepted,,,1.0,79456996,story,[],[moscow] update stemcell,320647.0,[320647],956238,81882,feature,2014-09-25T21:36:04Z,https://www.pivotaltracker.com/story/show/79456996
2014-09-25T23:15:08Z,2014-09-25T14:36:19Z,accepted,,"Begin: Mounting root file system ... Begin: Running /scripts/local-top ... done.
Gave up waiting for root device.  Common problems:
 - Boot args (cat /proc/cmdline)
   - Check rootdelay= (did the system wait long enough?)
   - Check root= (did the system wait for the right device?)
 - Missing modules (cat /proc/modules; ls /dev)
ALERT!  /dev/disk/by-uuid/d0e5bb64-3a39-45dc-a638-d77f5e8b2d8b does not exist.  Dropping to a shell!",,79525364,story,[],Investigate why instances with 2719.1trusty with new linux headers failing to boot on ec2,320647.0,"[320647, 553935]",956238,553935,chore,2014-09-25T23:15:08Z,https://www.pivotaltracker.com/story/show/79525364
2014-09-27T02:06:22Z,2014-09-26T23:51:50Z,accepted,,,1.0,79639054,story,[],openstack stemcell compat level should be 0.10,553935.0,"[553935, 320647]",956238,81882,feature,2014-09-27T02:06:22Z,https://www.pivotaltracker.com/story/show/79639054
2014-09-27T02:06:40Z,2014-09-12T22:54:17Z,accepted,,"Newer OpenStack stemcells seems to use QCOW v3 image format instead of what we were using (v2) until now. A user reported that these newer stemcells are not working anymore for him. I ran into the same issue doing some tests at DreamHost.

If we are using a newer qemu-utils version (due to trusty), when converting the images we can use the 'compact=0.10' parm to use qcow2 image format and be backwards compatible with people using older versions of qemu.

More details at the mailing list: https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/zLqbdQDY2y8/aob8FUn3h8AJ",1.0,78756146,story,[],Investigate downgrading OpenStack stemcell image format to QCOW v2,553935.0,"[553935, 320647]",956238,1015645,feature,2014-09-27T02:06:40Z,https://www.pivotaltracker.com/story/show/78756146
2014-09-29T22:00:26Z,2014-09-29T20:21:30Z,accepted,,we agreed with @dk to change to m1.medium,,79735474,story,[],bosh cli plugin aws should not use instance_type m1.small,553935.0,[553935],956238,553935,chore,2014-09-29T22:00:27Z,https://www.pivotaltracker.com/story/show/79735474
2014-09-30T03:56:41Z,2014-05-19T19:17:52Z,accepted,,"Vagrant now has the ability to [version boxes](http://docs.vagrantup.com/v2/boxes/versioning.html) when they are stored in the Vagrant Cloud. We should publish boxes there so that bosh-lite users can run a simple  vagrant box update to get the latest bosh-lite with an update BOSH director. For reference [Vagrant Cloud](https://vagrantcloud.com/) provides additional Vagrant services such as the ability to search for Vagrant boxes, receive notification updates, share running boxes with additional users, easily download the correct box for a provider.",2.0,71609160,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",bosh lite boxes should use vagrant versioning,1429768.0,"[1429768, 1393546]",956238,514635,feature,2014-09-30T03:56:41Z,https://www.pivotaltracker.com/story/show/71609160
2014-09-30T03:58:09Z,2014-09-03T21:50:12Z,accepted,,,1.0,78155442,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]","after all virtualbox, aws, and fusion boxes are created and pass bats bosh-lite should be promoted to master",320647.0,[320647],956238,81882,feature,2014-09-30T03:58:09Z,https://www.pivotaltracker.com/story/show/78155442
2014-09-30T03:59:28Z,2014-09-30T03:59:23Z,accepted,,,1.0,79757882,story,[],[cairo] update stemcell,553935.0,[553935],956238,81882,feature,2014-09-30T03:59:34Z,https://www.pivotaltracker.com/story/show/79757882
2014-09-30T18:01:13Z,2014-09-26T23:50:28Z,accepted,,,,79639032,story,[],investigate openstack on hetzner floating ip allocation issues,1123776.0,"[1123776, 1266616]",956238,81882,chore,2014-09-30T18:01:14Z,https://www.pivotaltracker.com/story/show/79639032
2014-09-30T18:02:47Z,2014-09-03T18:06:53Z,accepted,,A WIP commit for this is on origin/run_bats_on_aws.,1.0,78136588,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",run bats before publishing bosh-lite ami,509171.0,[509171],956238,81882,feature,2014-09-30T18:02:48Z,https://www.pivotaltracker.com/story/show/78136588
2014-09-30T18:05:37Z,2014-09-19T19:02:36Z,accepted,,add 'options loop max_loop=n' to a new file in /etc/modprobe.d/,1.0,79202902,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",configure bosh-lite to have more loopback devices for persistent disks,1429768.0,"[1429768, 1393546]",956238,81882,feature,2014-09-30T18:05:38Z,https://www.pivotaltracker.com/story/show/79202902
2014-09-30T18:10:56Z,2014-09-02T16:34:53Z,accepted,, through code,1.0,78042384,story,[],explicitly lock down os image version used to build stemcells,320647.0,"[320647, 553935]",956238,81882,feature,2014-09-30T18:10:56Z,https://www.pivotaltracker.com/story/show/78042384
2014-09-30T21:46:41Z,2014-08-28T16:24:03Z,accepted,,,1.0,77816610,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",support compiled package cache for bosh-lite boxes,320647.0,"[320647, 1393546, 509171]",956238,81882,feature,2014-09-30T21:46:43Z,https://www.pivotaltracker.com/story/show/77816610
2014-10-01T17:08:12Z,2014-10-01T16:53:36Z,accepted,,"I see centos 348 in public stemcells, but not 360 (which trusty is at).",,79884978,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",bosh-lite centos stemcell should be published with the build flow,1393546.0,"[1393546, 509171]",956238,1266616,chore,2014-10-01T21:47:11Z,https://www.pivotaltracker.com/story/show/79884978
2014-10-01T21:21:07Z,2014-09-25T21:30:42Z,accepted,,Investigate why we need to cast before pulling this in. Time box 20 mins. Make the change with your own commit since we do not have a CLA. https://github.com/cloudfoundry/gosigar/pull/12,0.0,79558248,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Fix some kind of cast error on linux host in gosigar,1393546.0,"[1393546, 1429768]",956238,81882,feature,2014-10-01T21:21:32Z,https://www.pivotaltracker.com/story/show/79558248
2014-10-01T21:38:49Z,2014-09-09T23:16:14Z,accepted,,"Hello,
It's always frustrating to type ""vim"" and get an error on bosh vms. Any chance we can symlink it to vim.tiny or vi, whichever one is more functional?",1.0,78510404,story,[],Symlink vim to vim.tiny on stemcell,1393546.0,"[1393546, 1429768]",956238,1092360,feature,2014-10-01T21:57:23Z,https://www.pivotaltracker.com/story/show/78510404
2014-10-03T00:17:24Z,2014-10-02T15:02:43Z,accepted,,"In monit logs health monitor was restarted and there were no errors in logs:

/var/vcap/monit/monit.log:
```
[UTC Sep 24 00:02:59] info     : 'director_scheduler' start: /var/vcap/jobs/director/bin/scheduler_ctl
[UTC Sep 24 00:03:00] info     : 'director_scheduler' start action done
[UTC Sep 24 00:03:00] info     : 'director_nginx' start: /var/vcap/jobs/director/bin/nginx_ctl
[UTC Sep 24 00:03:01] info     : 'director_nginx' start action done
[UTC Sep 24 00:03:01] info     : 'registry' start: /var/vcap/jobs/registry/bin/registry_ctl
[UTC Sep 24 00:03:02] info     : 'registry' start action done
[UTC Sep 24 00:03:02] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl
[UTC Sep 24 00:03:03] info     : 'health_monitor' start action done
[UTC Sep 24 00:03:03] info     : 'route53_backup' start: /var/vcap/jobs/route53_backup/bin/ctl
[UTC Sep 24 00:03:04] info     : 'route53_backup' start action done
[UTC Sep 24 00:03:04] info     : Awakened by User defined signal 1
[UTC Sep 24 00:03:04] info     : 'nats' start action done
[UTC Sep 24 00:03:04] info     : 'redis' start action done
[UTC Sep 24 00:03:05] info     : 'powerdns' start action done
[UTC Sep 24 00:03:05] info     : 'director' start action done
[UTC Sep 24 00:03:05] info     : 'nats' process is running with pid 1670
[UTC Sep 24 00:03:05] info     : 'redis' process is running with pid 1681
[UTC Sep 24 00:03:05] info     : 'powerdns' process is running with pid 1692
[UTC Sep 24 00:03:05] info     : 'director' process is running with pid 1704
[UTC Oct  2 14:20:32] error    : HTTP error: Server returned status 32767
[UTC Oct  2 14:20:32] error    : 'health_monitor' failed protocol test [HTTP] at INET[localhost:25923/healthz] via TCP
[UTC Oct  2 14:20:33] info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl
[UTC Oct  2 14:20:43] error    : 'health_monitor' process is not running
[UTC Oct  2 14:20:43] info     : 'health_monitor' trying to restart
[UTC Oct  2 14:20:43] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl
[UTC Oct  2 14:20:54] info     : 'health_monitor' process is running with pid 3185
[UTC Oct  2 14:20:54] info     : 'health_monitor' connection succeeded to INET[localhost:25923/healthz] via TCP
```

/var/vcap/sys/log/health_monitor/health_monitor.stdout.log is empty

/var/vcap/sys/log/health_monitor/health_monitor.stderr.log is full of:
```
SSL_connect SYSCALL returned=5 errno=0 state=unknown state
```
ls -lrt /var/vcap/sys/log/health_monitor/health_monitor.stderr.log

```
-rw-r--r-- 1 vcap vcap 5231 Oct  2 14:18 /var/vcap/sys/log/health_monitor/health_monitor.stderr.log
```

/var/vcap/sys/log/health_monitor/health_monitor.log:

```
I, [2014-10-02T14:18:05.174719 #1810]  INFO : Managing 7 deployments, 120 agents
I, [2014-10-02T14:18:05.174905 #1810]  INFO : Agent heartbeats received = 1471598
I, [2014-10-02T14:18:21.366701 #1810]  INFO : Analyzing agents...
I, [2014-10-02T14:18:21.369640 #1810]  INFO : Analyzed 120 agents, took 0.002769509 seconds
I, [2014-10-02T14:18:21.716425 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az1'
I, [2014-10-02T14:18:22.041088 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az2'
I, [2014-10-02T14:18:22.367821 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az3'
I, [2014-10-02T14:18:22.690814 #1810]  INFO : Found deployment `cf-cfapps-io2'
I, [2014-10-02T14:18:23.022656 #1810]  INFO : Found deployment `jumpbox-az1'
I, [2014-10-02T14:18:23.343918 #1810]  INFO : Found deployment `jumpbox-az2'
I, [2014-10-02T14:18:23.662124 #1810]  INFO : Found deployment `nat-box-moniter'
I, [2014-10-02T14:19:21.370858 #1810]  INFO : Analyzing agents...
I, [2014-10-02T14:19:21.373797 #1810]  INFO : Analyzed 120 agents, took 0.002766455 seconds
I, [2014-10-02T14:19:21.714817 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az1'
I, [2014-10-02T14:19:22.040023 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az2'
I, [2014-10-02T14:19:22.364328 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az3'
I, [2014-10-02T14:19:22.687551 #1810]  INFO : Found deployment `cf-cfapps-io2'
I, [2014-10-02T14:19:23.023482 #1810]  INFO : Found deployment `jumpbox-az1'
I, [2014-10-02T14:19:23.343813 #1810]  INFO : Found deployment `jumpbox-az2'
I, [2014-10-02T14:19:23.664781 #1810]  INFO : Found deployment `nat-box-moniter'
I, [2014-10-02T14:19:32.581153 #1810]  INFO : [ALERT] Alert @ 2014-10-02 14:19:32 UTC, severity 3: total mem amount of 2326020kB matches resource limit [total mem amount>2304008kB]
W, [2014-10-02T14:19:32.581296 #1810]  WARN : (Resurrector) event did not have deployment, job and index: Alert @ 2014-10-02 14:19:32 UTC, severity 3: total mem amount of 2326020kB matches resource limit [
total mem amount>2304008kB]
I, [2014-10-02T14:20:21.374597 #1810]  INFO : Analyzing agents...
I, [2014-10-02T14:20:21.377626 #1810]  INFO : Analyzed 120 agents, took 0.002860486 seconds
I, [2014-10-02T14:20:21.722566 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az1'
I, [2014-10-02T14:20:22.046925 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az2'
I, [2014-10-02T14:20:22.367313 #1810]  INFO : Found deployment `cf-cfapps-io-logsearch-az3'
I, [2014-10-02T14:20:22.689317 #1810]  INFO : Found deployment `cf-cfapps-io2'
I, [2014-10-02T14:20:23.027068 #1810]  INFO : Found deployment `jumpbox-az1'
I, [2014-10-02T14:20:23.371034 #1810]  INFO : Found deployment `jumpbox-az2'
I, [2014-10-02T14:20:23.691550 #1810]  INFO : Found deployment `nat-box-moniter'
I, [2014-10-02T14:20:43.708758 #3185]  INFO : HealthMonitor starting...
I, [2014-10-02T14:20:43.710263 #3185]  INFO : Logging delivery agent is running...
I, [2014-10-02T14:20:43.710613 #3185]  INFO : Resurrector is running...
I, [2014-10-02T14:20:43.710727 #3185]  INFO : DataDog plugin is running...
I, [2014-10-02T14:20:43.710841 #3185]  INFO : HTTP server is starting on port 25923...
I, [2014-10-02T14:20:43.762134 #3185]  INFO : BOSH HealthMonitor 1.2693.0 is running...
I, [2014-10-02T14:20:43.764309 #3185]  INFO : Connected to NATS at `nats://10.10.0.7:4222'
I, [2014-10-02T14:20:44.106632 #3185]  INFO : Found deployment `cf-cfapps-io-logsearch-az1'
I, [2014-10-02T14:20:44.425712 #3185]  INFO : Found deployment `cf-cfapps-io-logsearch-az2'
W, [2014-10-02T14:20:44.733918 #3185]  WARN : Received heartbeat from unmanaged agent: 5e6ce479-5ea7-4dfd-918c-26698420ddaa
W, [2014-10-02T14:20:44.754084 #3185]  WARN : Received heartbeat from unmanaged agent: 4d48a89a-87ab-443b-9e83-75e7736b55f8
I, [2014-10-02T14:20:44.757201 #3185]  INFO : Found deployment `cf-cfapps-io-logsearch-az3'
W, [2014-10-02T14:20:44.790222 #3185]  WARN : Received heartbeat from unmanaged agent: 967bd56f-c428-49bc-87b1-0c13b42c4c71
I, [2014-10-02T14:20:45.078369 #3185]  INFO : Found deployment `cf-cfapps-io2'
I, [2014-10-02T14:20:45.422270 #3185]  INFO : Found deployment `jumpbox-az1'
I, [2014-10-02T14:20:45.741696 #3185]  INFO : Found deployment `jumpbox-az2'
I, [2014-10-02T14:20:46.061803 #3185]  INFO : Found deployment `nat-box-moniter'
I, [2014-10-02T14:21:44.109722 #3185]  INFO : Found deployment `cf-cfapps-io-logsearch-az1'
```
",,79953838,story,[],Investigate why health monitor silently failed,1123776.0,[1123776],956238,553935,chore,2014-10-03T19:07:57Z,https://www.pivotaltracker.com/story/show/79953838
2014-10-03T00:56:19Z,2014-09-12T00:03:32Z,accepted,,,1.0,78688186,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",install bosh_cli on the bosh-lite box,1393546.0,"[1393546, 1429768, 553935]",956238,81882,feature,2014-10-03T00:56:20Z,https://www.pivotaltracker.com/story/show/78688186
2014-10-03T00:59:34Z,2014-06-26T23:18:04Z,accepted,,https://github.com/cloudfoundry/bosh-checklists/issues/6,,74010006,story,[],On-call checklist for Karl,1266616.0,[1266616],956238,1123776,chore,2014-10-03T00:59:35Z,https://www.pivotaltracker.com/story/show/74010006
2014-10-03T16:24:09Z,2014-10-02T17:04:39Z,accepted,,https://github.com/cloudfoundry/bosh/pull/668,0.0,79963332,story,[],pull in vcloud cpi version into 2690.4 branch,553935.0,"[553935, 1266616]",956238,81882,feature,2014-10-03T16:24:13Z,https://www.pivotaltracker.com/story/show/79963332
2014-10-03T16:27:47Z,2014-09-28T22:29:42Z,accepted,,"for bosh-lite, I get these headers back:

```
Content-Type: application/x-tar
Content-Encoding: gzip
```

for aws, I get:

```
Content-Type: application/x-gtar
```

(and no `Content-Encoding`).

The AWS form is correct; the content is truly a `.tar.gz`, not a `.tar` that happens to be `gzip` encoded for the client. This is because the artifact itself is expected to be a `.tar.gz`, as that's what `bosh upload stemcell` accepts.

The current format (`gzip`-encoded `tar`) breaks when clients see `Content-Encoding` and use it to decode the content as it's downloaded, leaving me with just a `tar`, which I can't upload.",,79663534,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",bosh-lite stemcells are uploaded strangely,1393546.0,"[1393546, 509171]",956238,381857,bug,2014-10-03T16:27:50Z,https://www.pivotaltracker.com/story/show/79663534
2014-10-03T16:28:07Z,2014-09-30T17:45:41Z,accepted,,"currently there are
boxes under BOSH account
move to cloudfoundry",0.0,79804710,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",Migrate Vagrant Cloud Bosh Lite Boxes From BOSH to cloudfoundry Account,553935.0,"[553935, 1429768]",956238,1429768,feature,2014-10-03T16:28:20Z,https://www.pivotaltracker.com/story/show/79804710
2014-10-03T21:42:02Z,2014-10-01T18:22:37Z,accepted,,,1.0,79892736,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",use cloudfront urls when publishing bosh-lite boxes to vagrant cloud,553935.0,[553935],956238,81882,feature,2014-10-03T21:42:05Z,https://www.pivotaltracker.com/story/show/79892736
2014-10-04T00:44:06Z,2014-09-08T18:19:58Z,accepted,,"- introduce cpi wide configuration to use config-drive (not turned on by default)
- change cpi to use config-drive option while creating vms

see http://docs.openstack.org/grizzly/openstack-compute/admin/content/config-drive.html",2.0,78402044,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'openstack-cd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-08T18:56:09Z', 'id': 9393792, 'updated_at': '2014-09-08T18:56:09Z'}]",openstack cpi should attach config drive that includes user data,1429768.0,"[1429768, 553935, 509171]",956238,81882,feature,2014-10-04T00:44:07Z,https://www.pivotaltracker.com/story/show/78402044
2014-10-06T03:12:29Z,2014-08-18T03:50:17Z,accepted,,"```
resource_pools:
- name: fast_machines
  cloud_properties:
    instance_type: m3.x2large

disk_pools:
- name: fast_disks
  disk_size: 3000
  cloud_properties: {}

jobs:
- name: mydb
  templates: [ mysql ]
  instances: 1
  resource_pool: fast_machines
  persistent_disk_pool: fast_disks
  networks: [ {...} ] 
  properties: {}
```",2.0,77089636,story,"[{'name': 'disk-pools1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-04T19:40:34Z', 'id': 9111434, 'updated_at': '2014-08-18T03:50:54Z'}]",user should be able to specify size of a persistent disk via disk pool,1266616.0,"[1266616, 540781, 509171]",956238,81882,feature,2014-10-06T03:12:39Z,https://www.pivotaltracker.com/story/show/77089636
2014-10-06T05:10:39Z,2014-08-15T00:56:51Z,accepted,,"With max_in_flight > 1 you can get a race condition when creating VM folder. It checks for folder presence and then creates it. We should ignore this exact error in this case.

```
E, [2014-08-15T00:14:21.066274 #2413] [task:3] ERROR -- : The name 'da04e205-3066-4e4b-9b23-e68a6cadc500' already exists.
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:39:in `invoke_method'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:13:in `invoke_method'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:45:in `block (3 levels) in finalize'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/resources/folder.rb:45:in `find_or_create_sub_folder'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/resources/folder.rb:22:in `find_or_create_folder'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/resources/folder.rb:13:in `initialize'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/resources/datacenter.rb:19:in `new'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/resources/datacenter.rb:19:in `vm_folder'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/vm_creator.rb:76:in `create'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/cloud.rb:188:in `block in create_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_vsphere_cpi-1.2690.0/lib/cloud/vsphere/cloud.rb:187:in `create_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/vm_creator.rb:41:in `create'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/resource_pool_updater.rb:51:in `create_missing_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/resource_pool_updater.rb:34:in `block (4 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/resource_pool_updater.rb:32:in `block (3 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:36:in `track'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/resource_pool_updater.rb:31:in `block (2 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_pool.rb:63:in `block in create_thread'
D, [2014-08-15T00:14:21.067025 #2413] [0x1790d00] DEBUG -- : Lock renewal thread exiting
D, [2014-08-15T00:14:21.067566 #2413] [task:3] DEBUG -- : (0.000633s) BEGIN
D, [2014-08-15T00:14:21.068959 #2413] [task:3] DEBUG -- : (0.000332s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2014-08-15 00:14:21.066567+0000', ""description"" = 'create deployment', ""result"" = 'The name ''da04e205-3066-4e4b-9b23-e68a6cadc500'' already exists.', ""output"" = '/var/vcap/store/director/tasks/3', ""user_id"" = NULL, ""checkpoint_time"" = '2014-08-15 00:13:51.247560+0000', ""type"" = 'update_deployment' WHERE (""id"" = 3)
D, [2014-08-15T00:14:21.070023 #2413] [task:3] DEBUG -- : (0.000897s) COMMIT
I, [2014-08-15T00:14:21.070330 #2413] [0x72806c]  INFO -- : Task took 1 minute 1.0977058830000033 seconds to process.
```

",1.0,76997004,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere cpi should continue creating vm if vsphere folder is already created,320647.0,"[320647, 1429768, 553935]",956238,553935,feature,2014-10-06T05:10:40Z,https://www.pivotaltracker.com/story/show/76997004
2014-10-06T22:21:16Z,2014-06-05T19:12:50Z,accepted,,"There're some customers that want to use the new AWS M3 instance types, because they're cheaper, and for better performance. 

Those instance types don't work with BOSH, because the agent is expecting an ephemeral disk, and by default AWS doesn't add it unless you specify it when creating the instance:

""For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI."" (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#Using_AddingDefaultLocalInstanceStorageToAMI)
",1.0,72728264,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",investigate how to use m3 instance types,509171.0,"[509171, 1429768, 514635]",956238,1015645,feature,2014-10-06T22:21:16Z,https://www.pivotaltracker.com/story/show/72728264
2014-10-08T23:45:36Z,2014-09-16T00:01:00Z,accepted,,"From a recent Runtime GoCD provisioning step:

```
<snip>
==> default: Waiting for instance to become ""ready""...
==> default: Waiting for SSH to become available...
==> default: Machine is booted and ready for use!
==> default: Running provisioner: shell...
    default: Running: inline script
==> default: The public IP for this instance is <?xml version=""1.0"" encoding=""iso-8859-1""?>
==> default: <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""
==> default:          ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
==> default: <html xmlns=""http://www.w3.org/1999/xhtml"" xml:lang=""en"" lang=""en"">
==> default:  <head>
==> default:  <title>404 - Not Found</title>
==> default:  </head>
==> default:  <body>
==> default:  <h1>404 - Not Found</h1>
==> default:  </body>
==> default: </html>
==> default: You can bosh target <?xml version=""1.0"" encoding=""iso-8859-1""?>
==> default: <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""
==> default:          ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
==> default: <html xmlns=""http://www.w3.org/1999/xhtml"" xml:lang=""en"" lang=""en"">
==> default:  <head>
==> default:  <title>404 - Not Found</title>
==> default:  </head>
==> default:  <body>
==> default:  <h1>404 - Not Found</h1>
==> default:  </body>
==> default: </html>, or run vagrant ssh and then bosh target 127.0.0.1
==> default: Running provisioner: shell...
    default: Running: inline script
==> default: Setting up port forwarding for the CF Cloud Controller...
```",,78875888,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",Error when provisioning bosh-lite on AWS,1393546.0,"[1393546, 1429768]",956238,1158132,bug,2014-10-14T17:32:01Z,https://www.pivotaltracker.com/story/show/78875888
2014-10-09T19:22:09Z,2014-09-19T13:01:03Z,accepted,,"`/var/log/syslog` is now owned by root, breaking `cf-jenkins-release` and I suspect all the other CF London Services products as syslog messages will never appear in /var/log/syslog and adjacent log files (e.g. /var/log/user.log, /var/log/messages etc.)",,79178026,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",bosh-lite stemcell and box should have /var/log owned by syslog:adm instead of root,1266616.0,"[1266616, 509171]",956238,81882,bug,2014-10-09T19:25:40Z,https://www.pivotaltracker.com/story/show/79178026
2014-10-10T16:20:00Z,2014-09-25T21:16:35Z,accepted,,"bin/provision-cf 
vagrant halt
bosh cck

extra space is being used on the root partition",1.0,79557284,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'bosh-lite1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-18T03:52:59Z', 'id': 9223338, 'updated_at': '2014-08-18T03:52:59Z'}]",fix increasing space usage after bosh-lite vm is restarted,1429768.0,"[1429768, 1123776]",956238,81882,feature,2014-10-10T16:20:06Z,https://www.pivotaltracker.com/story/show/79557284
2014-10-10T16:20:09Z,2014-10-09T16:46:16Z,accepted,,,1.0,80393280,story,[],Change Vcloud CPI to raise NotImplemented error for unsupported methods,514635.0,[514635],956238,514635,feature,2014-10-10T16:20:15Z,https://www.pivotaltracker.com/story/show/80393280
2014-10-10T23:54:58Z,2014-09-30T17:56:29Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/658
https://github.com/cloudfoundry/bosh/pull/657",1.0,79805910,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",review PRs from voelzmo,1393546.0,"[1393546, 509171]",956238,1393546,feature,2014-10-10T23:54:58Z,https://www.pivotaltracker.com/story/show/79805910
2014-10-10T23:55:40Z,2014-10-03T21:30:49Z,accepted,,"merge in https://github.com/cloudfoundry/bosh-agent/pull/2. do not need to modify bin/build since bin/env is loaded by bin/build. And bump go-agent in bosh.
",0.0,80054192,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",support ppc64 building in go agent,514635.0,"[514635, 320647]",956238,81882,feature,2014-10-10T23:55:40Z,https://www.pivotaltracker.com/story/show/80054192
2014-10-12T19:00:00Z,2014-10-01T17:26:47Z,accepted,,,2.0,79887754,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'openstack-cd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-08T18:56:09Z', 'id': 9393792, 'updated_at': '2014-09-08T18:56:09Z'}]",openstack cpi should support using config with non-config-drive-cdrom configuration,553935.0,"[553935, 1266616]",956238,81882,feature,2014-10-17T08:36:19Z,https://www.pivotaltracker.com/story/show/79887754
2014-10-12T19:00:00Z,2014-03-24T20:34:57Z,accepted,,"- change bosh-agent to try to find config-drive via blkid before looking at metadata service endpoint

see http://docs.openstack.org/grizzly/openstack-compute/admin/content/config-drive.html

""I've ran into a common issue at several customer's OpenStack environments. They don't support injecting user data files, and when you try to do this, the boot vm process always fail. Also, some of them have not enabled the OpenStack meta-data server. config-drive is mostly supported in all recent OpenStack environments (from Folsom). FYI, this overrides PR 416 (https://github.com/cloudfoundry/bosh/pull/416).""",2.0,68141956,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'openstack-cd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-08T18:56:09Z', 'id': 9393792, 'updated_at': '2014-09-08T18:56:09Z'}]",agent should try to find metadata via config-drive if it's available,553935.0,"[553935, 509171, 514635]",956238,1015645,feature,2014-10-17T08:37:02Z,https://www.pivotaltracker.com/story/show/68141956
2014-10-13T02:11:36Z,2013-07-09T20:53:04Z,accepted,,"Backend just found a a bug in the instance updater/agent where an agent that has it's persistent disk size changed in a deploy will leave the old persistent disk reference in the /var/vcap/bosh/settings.json file on the agent even though the disk it is referencing has been deleted.

Upon subsequent deploys the agent tries to find this non-existent disk when the `list_disk` message is sent and then times out the deploy.

This problem was caught in a developer instance but the same persistent disk size increase has been applied to production and it is likely that subsequent deploys are going to fail there too.

The workaround for this is to manually edit the settings.json file (removing the old disk) and restart the agent. Deploys succeed after this action is taken so this isn't an end-of-the-world-oh-dear-what-are-we-going-to-do kind of a deal.

(This did not happen in the subsequent production deploy)",,53110543,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",Changing the persistent disk size leaves the old disk reference in the agent's settings.json which causes later deploys to fail while trying to find the disk,320647.0,[320647],956238,1218342,bug,2014-10-13T02:11:43Z,https://www.pivotaltracker.com/story/show/53110543
2014-10-15T19:11:51Z,2014-10-13T16:31:09Z,accepted,,merge https://github.com/cloudfoundry/bosh/pull/673/files into 2690.4 to make 2690.5,1.0,80567156,story,[],build a 2690.5 vcloud stemcell ,514635.0,[514635],956238,81882,feature,2014-10-15T19:12:18Z,https://www.pivotaltracker.com/story/show/80567156
2014-10-15T23:16:05Z,2014-08-19T21:23:40Z,accepted,,"I've seen this several times in the past few weeks on both Travis and now GoCD: ""compiling packages"" times out after 10 minutes of pinging the agent during miscellaneous integration tests.

https://server.gocd.cf-app.com/go/tab/build/detail/bosh/38/tests/1/integration-2.1.2-postgres

```
2) Director deprecating the 'template' syntax when the manifest uses template with an array issues a deprecation warning
     Failure/Error: output = deploy_simple(manifest_hash: manifest_hash)
     RuntimeError:
       ERROR: bosh -n -c /tmp/d20140819-24183-ucz8ul/bosh_config.yml deploy failed with output:
       Getting deployment properties from director...
       Unable to get properties list from director, trying without it...
       Compiling deployment manifest...
       
       Director task 88
       Deprecation: Please use `templates' when specifying multiple templates for a job. `template' for multiple templates will soon be unsupported.
       
         Started preparing deployment
         Started preparing deployment > Binding deployment. Done (00:00:00)
         Started preparing deployment > Binding releases. Done (00:00:00)
         Started preparing deployment > Binding existing deployment. Done (00:00:00)
         Started preparing deployment > Binding resource pools. Done (00:00:00)
         Started preparing deployment > Binding stemcells. Done (00:00:00)
         Started preparing deployment > Binding templates. Done (00:00:00)
         Started preparing deployment > Binding properties. Done (00:00:00)
         Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
         Started preparing deployment > Binding instance networks. Done (00:00:00)
            Done preparing deployment (00:00:00)
       
         Started preparing package compilation > Finding packages to compile. Done (00:00:00)
       
         Started compiling packages
         Started compiling packages > foo/0ee95716c58cf7aab3ef7301ff907118552c2dda. Failed: Timed out pinging to b9dd4f4e-1888-4209-bff0-840b9dbbfe03 after 600 seconds (00:10:00)
       
       Error 450002: Timed out pinging to b9dd4f4e-1888-4209-bff0-840b9dbbfe03 after 600 seconds
       
       Task 88 error
       
       For a more detailed error report, run: bosh task 88 --debug
     # ./spec/support/bosh_runner.rb:40:in `run_in_current_dir'
     # ./spec/support/bosh_runner.rb:12:in `block in run'
     # ./spec/support/bosh_runner.rb:12:in `chdir'
     # ./spec/support/bosh_runner.rb:12:in `run'
     # ./spec/support/integration_example_group.rb:78:in `deploy'
     # ./spec/support/integration_example_group.rb:92:in `deploy_simple_manifest'
     # ./spec/support/integration_example_group.rb:85:in `deploy_simple'
     # ./spec/integration/deprecating_template_keyword_spec.rb:14:in `block (3 levels) in <top (required)>'

```",,77242338,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}, {'name': 'gocd', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-13T00:02:05Z', 'id': 9188506, 'updated_at': '2014-08-13T00:02:05Z'}, {'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",Investigate timed out pinging worker scenario,1266616.0,"[1266616, 1429768]",956238,1266616,bug,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/77242338
2014-10-17T07:46:17Z,2014-10-02T23:59:17Z,accepted,,"talk to @dk for more details

```
-1 = v4
-2 = v6

irb(main):026:0> pp bd::Models::ReleaseVersion.all[-2].packages.map {|p| p.name+""-""+p.version+""-""+p.id.to_s }; nil
[""acceptance-tests-bd84db952e62787eb783e593d441a7f01d494e92-94"",
 ""broker-registrar-5528ca7f79a00cfeeab70bb98c7914cd3ab72596-95"",
 ""bucket_seeder-0401223b24e7cf009c808f9223e981f950ad12a6-96"",
 ""cf-riak-cs-broker-00539ffc73682f57c292ab52d0dc8be0889b4c9a-97"",
 ""cli-4bb5c60db940b97d34d4a916497f9c6c558be14f-98"",
 ""common-8ecced6383310492b543d2a5a3041410c7b33622-99"",
 ""erlang-a098675a4bcf187e911ee315e12dce15278d98c0-100"",
 ""git-08664a6fda7c85f7ea35e636ff5028c6b764a295-101"",
 ""golang-aa5f90f06ada376085414bfc0c56c8cd67abba9c-102"",
 ""python-995c5f1e4acd34ac847b0c8bc4a0aa60b8484b13-103"",
 ""riak-feafe4e3c776bea8248d60d8473e1436fc6e6d70-104"",
 ""riak-cs-e364eb7a465df8bfb0555d3a4d1c079ca55516cf-105"",
 ""route-registrar-ec284934d88ea47f45f241d58d7f4f2fb5914f55-106"",
 ""ruby-cd4d8494a137e228e6358eb18eee7528e371a0a0-107"",
 ""stanchion-92d46913a9f3e9c63372890e1203b0cd3d96dd65-108"",
 ""syslog_aggregator-29e843000a1d4c089023a030bc789eef1273ec51-109""]
=> nil

irb(main):027:0> pp bd::Models::ReleaseVersion.all[-1].packages.map {|p| p.name+""-""+p.version+""-""+p.id.to_s }; nil
[""broker-registrar-5528ca7f79a00cfeeab70bb98c7914cd3ab72596-95"",
 ""common-8ecced6383310492b543d2a5a3041410c7b33622-99"",
 ""erlang-a098675a4bcf187e911ee315e12dce15278d98c0-100"",
 ""golang-aa5f90f06ada376085414bfc0c56c8cd67abba9c-102"",
 ""riak-feafe4e3c776bea8248d60d8473e1436fc6e6d70-104"",
 ""riak-cs-e364eb7a465df8bfb0555d3a4d1c079ca55516cf-105"",
 ""route-registrar-ec284934d88ea47f45f241d58d7f4f2fb5914f55-106"",
 ""stanchion-92d46913a9f3e9c63372890e1203b0cd3d96dd65-108"",
 ""syslog_aggregator-29e843000a1d4c089023a030bc789eef1273ec51-109"",
 ""acceptance-tests-1dc242ebce641a136e1b8c965363d480f8445b71-110"",
 ""cf-riak-cs-broker-a7486ce7a1dc252737b47ace16b0a3864d558690-111"",
 ""cli-a15b4f39d9ed0c461cc747848e0136772d626a05-112"",
 ""git-a36ac611b800c0fd5a16239d4814c5a9a8ecc984-113"",
 ""ruby-8a1491120e429e911427a9896e218957951d3343-114""]

irb(main):052:0> bd::Models::ReleaseVersion.all[-1].packages
=> [#<Bosh::Director::Models::Package @values={:id=>95, :name=>""broker-registrar"", :version=>""5528ca7f79a00cfeeab70bb98c7914cd3ab72596"", :blobstore_id=>""db9d9996-886a-4018-a60b-fc52a6335f3b"", :sha1=>""9a32ea77ede4720a29a1d5fa20582b535f54356a"", :dependency_set_json=>""[\""ruby\""]"", :release_id=>5, :fingerprint=>""5528ca7f79a00cfeeab70bb98c7914cd3ab72596""}>, #<Bosh::Director::Models::Package @values={:id=>99, :name=>""common"", :version=>""8ecced6383310492b543d2a5a3041410c7b33622"", :blobstore_id=>""94d2f514-9774-4b49-8d44-c16ea8b0a9e1"", :sha1=>""2c731b10f06b25b1ab771353843b51fea2d4b90e"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""8ecced6383310492b543d2a5a3041410c7b33622""}>, #<Bosh::Director::Models::Package @values={:id=>100, :name=>""erlang"", :version=>""a098675a4bcf187e911ee315e12dce15278d98c0"", :blobstore_id=>""9fba5639-8364-41e5-b690-6963c1cb00b9"", :sha1=>""90679b358ff44d396f39c164e122a3cdad910141"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""a098675a4bcf187e911ee315e12dce15278d98c0""}>, #<Bosh::Director::Models::Package @values={:id=>102, :name=>""golang"", :version=>""aa5f90f06ada376085414bfc0c56c8cd67abba9c"", :blobstore_id=>""64412c4c-2603-477d-9bcc-1b3a6f1dfe9b"", :sha1=>""8e0b39557d859666bb129cdd2822f72a7d592338"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""aa5f90f06ada376085414bfc0c56c8cd67abba9c""}>, #<Bosh::Director::Models::Package @values={:id=>104, :name=>""riak"", :version=>""feafe4e3c776bea8248d60d8473e1436fc6e6d70"", :blobstore_id=>""e62cb3c2-c66e-4dc9-a53a-160193373e3c"", :sha1=>""674a1783e1432b4e99f84f2855394b47549558f8"", :dependency_set_json=>""[\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""feafe4e3c776bea8248d60d8473e1436fc6e6d70""}>, #<Bosh::Director::Models::Package @values={:id=>105, :name=>""riak-cs"", :version=>""e364eb7a465df8bfb0555d3a4d1c079ca55516cf"", :blobstore_id=>""4acbb3ec-5ad1-48d3-b8ce-a84788069178"", :sha1=>""006abf29c413f067acc0bf3e34355c60ddc8b575"", :dependency_set_json=>""[\""riak\"",\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""e364eb7a465df8bfb0555d3a4d1c079ca55516cf""}>, #<Bosh::Director::Models::Package @values={:id=>106, :name=>""route-registrar"", :version=>""ec284934d88ea47f45f241d58d7f4f2fb5914f55"", :blobstore_id=>""d80a42d1-884d-4a41-883a-a853a136b57f"", :sha1=>""c7fe95ba860d36480251de222a8b6e66088f09c5"", :dependency_set_json=>""[\""golang\""]"", :release_id=>5, :fingerprint=>""ec284934d88ea47f45f241d58d7f4f2fb5914f55""}>, #<Bosh::Director::Models::Package @values={:id=>108, :name=>""stanchion"", :version=>""92d46913a9f3e9c63372890e1203b0cd3d96dd65"", :blobstore_id=>""d3cf9483-fe3c-43f6-9537-839c9610c583"", :sha1=>""e2578c8b34fb51d23bfd3a00cfd00010405bcee1"", :dependency_set_json=>""[\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""92d46913a9f3e9c63372890e1203b0cd3d96dd65""}>, #<Bosh::Director::Models::Package @values={:id=>109, :name=>""syslog_aggregator"", :version=>""29e843000a1d4c089023a030bc789eef1273ec51"", :blobstore_id=>""9177f9f7-9f6d-49e6-9fd2-420f4ac6caf5"", :sha1=>""e962ecfa145f575d1d2ec139c825a7dfd4952930"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""29e843000a1d4c089023a030bc789eef1273ec51""}>, #<Bosh::Director::Models::Package @values={:id=>110, :name=>""acceptance-tests"", :version=>""1dc242ebce641a136e1b8c965363d480f8445b71"", :blobstore_id=>""ebde52bb-c459-4249-a99f-9d2654ba9af8"", :sha1=>""14102b41df323b70bfc91d1df48298eeea599f48"", :dependency_set_json=>""[\""golang\"",\""cli\""]"", :release_id=>5, :fingerprint=>""1dc242ebce641a136e1b8c965363d480f8445b71""}>, #<Bosh::Director::Models::Package @values={:id=>111, :name=>""cf-riak-cs-broker"", :version=>""a7486ce7a1dc252737b47ace16b0a3864d558690"", :blobstore_id=>""8054783a-c4f3-4edb-bbd8-6154b64a008f"", :sha1=>""68421429e3e2d871ecb18dc1386de711bce49248"", :dependency_set_json=>""[\""git\"",\""ruby\""]"", :release_id=>5, :fingerprint=>""a7486ce7a1dc252737b47ace16b0a3864d558690""}>, #<Bosh::Director::Models::Package @values={:id=>112, :name=>""cli"", :version=>""a15b4f39d9ed0c461cc747848e0136772d626a05"", :blobstore_id=>""05c9df2b-876f-4a3c-95d4-c71a65caf3de"", :sha1=>""ed58ffec8082c87a8b259580d710f73a812d6343"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""a15b4f39d9ed0c461cc747848e0136772d626a05""}>, #<Bosh::Director::Models::Package @values={:id=>113, :name=>""git"", :version=>""a36ac611b800c0fd5a16239d4814c5a9a8ecc984"", :blobstore_id=>""a84cfc3c-88ae-42d0-8703-8ba78fb5e2c4"", :sha1=>""a714374b61c1b4e7812bc114850bd0a6835b6748"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""a36ac611b800c0fd5a16239d4814c5a9a8ecc984""}>, #<Bosh::Director::Models::Package @values={:id=>114, :name=>""ruby"", :version=>""8a1491120e429e911427a9896e218957951d3343"", :blobstore_id=>""127ef9eb-ba80-4ebd-9a54-569c2dba34c3"", :sha1=>""3b00b5fab0e25bf83d4ab20a570d4313b44d117c"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""8a1491120e429e911427a9896e218957951d3343""}>]
irb(main):053:0> bd::Models::ReleaseVersion.all[-2].packages
=> [#<Bosh::Director::Models::Package @values={:id=>94, :name=>""acceptance-tests"", :version=>""bd84db952e62787eb783e593d441a7f01d494e92"", :blobstore_id=>""0fd50235-b27c-4432-8063-a019b98c9497"", :sha1=>""6de6e2231072248071b68c8ef4242a5a4fb719e1"", :dependency_set_json=>""[\""golang\"",\""cli\""]"", :release_id=>5, :fingerprint=>""bd84db952e62787eb783e593d441a7f01d494e92""}>, #<Bosh::Director::Models::Package @values={:id=>95, :name=>""broker-registrar"", :version=>""5528ca7f79a00cfeeab70bb98c7914cd3ab72596"", :blobstore_id=>""db9d9996-886a-4018-a60b-fc52a6335f3b"", :sha1=>""9a32ea77ede4720a29a1d5fa20582b535f54356a"", :dependency_set_json=>""[\""ruby\""]"", :release_id=>5, :fingerprint=>""5528ca7f79a00cfeeab70bb98c7914cd3ab72596""}>, #<Bosh::Director::Models::Package @values={:id=>96, :name=>""bucket_seeder"", :version=>""0401223b24e7cf009c808f9223e981f950ad12a6"", :blobstore_id=>""ba5ced5c-deff-48b0-9890-99ca15db7c1f"", :sha1=>""720dd14536693666d592813e07a1cfedb4799943"", :dependency_set_json=>""[\""ruby\""]"", :release_id=>5, :fingerprint=>""0401223b24e7cf009c808f9223e981f950ad12a6""}>, #<Bosh::Director::Models::Package @values={:id=>97, :name=>""cf-riak-cs-broker"", :version=>""00539ffc73682f57c292ab52d0dc8be0889b4c9a"", :blobstore_id=>""9532fe81-a4b8-4095-bb8e-4f8fc657ca76"", :sha1=>""5ec77475f398142bf6e0ae7d3d8f5d4458bdbc37"", :dependency_set_json=>""[\""git\"",\""python\"",\""ruby\""]"", :release_id=>5, :fingerprint=>""00539ffc73682f57c292ab52d0dc8be0889b4c9a""}>, #<Bosh::Director::Models::Package @values={:id=>98, :name=>""cli"", :version=>""4bb5c60db940b97d34d4a916497f9c6c558be14f"", :blobstore_id=>""15f04e64-b90a-46b5-939f-ae5017112e1b"", :sha1=>""66b197a18ab400e2e79ddb75ce15495c2f4f8f66"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""4bb5c60db940b97d34d4a916497f9c6c558be14f""}>, #<Bosh::Director::Models::Package @values={:id=>99, :name=>""common"", :version=>""8ecced6383310492b543d2a5a3041410c7b33622"", :blobstore_id=>""94d2f514-9774-4b49-8d44-c16ea8b0a9e1"", :sha1=>""2c731b10f06b25b1ab771353843b51fea2d4b90e"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""8ecced6383310492b543d2a5a3041410c7b33622""}>, #<Bosh::Director::Models::Package @values={:id=>100, :name=>""erlang"", :version=>""a098675a4bcf187e911ee315e12dce15278d98c0"", :blobstore_id=>""9fba5639-8364-41e5-b690-6963c1cb00b9"", :sha1=>""90679b358ff44d396f39c164e122a3cdad910141"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""a098675a4bcf187e911ee315e12dce15278d98c0""}>, #<Bosh::Director::Models::Package @values={:id=>101, :name=>""git"", :version=>""08664a6fda7c85f7ea35e636ff5028c6b764a295"", :blobstore_id=>""fb06e1dd-f3a9-48fd-bd8f-7c9f9bc3af2b"", :sha1=>""5eacce5765ac07c0fdc9b07cc23d42de1503accc"", :dependency_set_json=>""[\""python\""]"", :release_id=>5, :fingerprint=>""08664a6fda7c85f7ea35e636ff5028c6b764a295""}>, #<Bosh::Director::Models::Package @values={:id=>102, :name=>""golang"", :version=>""aa5f90f06ada376085414bfc0c56c8cd67abba9c"", :blobstore_id=>""64412c4c-2603-477d-9bcc-1b3a6f1dfe9b"", :sha1=>""8e0b39557d859666bb129cdd2822f72a7d592338"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""aa5f90f06ada376085414bfc0c56c8cd67abba9c""}>, #<Bosh::Director::Models::Package @values={:id=>103, :name=>""python"", :version=>""995c5f1e4acd34ac847b0c8bc4a0aa60b8484b13"", :blobstore_id=>""46a22222-2820-4e23-9c2e-0ed8b0c0de42"", :sha1=>""86522d10357f9b1c906cb2896a6a1e908a3cbeaa"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""995c5f1e4acd34ac847b0c8bc4a0aa60b8484b13""}>, #<Bosh::Director::Models::Package @values={:id=>104, :name=>""riak"", :version=>""feafe4e3c776bea8248d60d8473e1436fc6e6d70"", :blobstore_id=>""e62cb3c2-c66e-4dc9-a53a-160193373e3c"", :sha1=>""674a1783e1432b4e99f84f2855394b47549558f8"", :dependency_set_json=>""[\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""feafe4e3c776bea8248d60d8473e1436fc6e6d70""}>, #<Bosh::Director::Models::Package @values={:id=>105, :name=>""riak-cs"", :version=>""e364eb7a465df8bfb0555d3a4d1c079ca55516cf"", :blobstore_id=>""4acbb3ec-5ad1-48d3-b8ce-a84788069178"", :sha1=>""006abf29c413f067acc0bf3e34355c60ddc8b575"", :dependency_set_json=>""[\""riak\"",\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""e364eb7a465df8bfb0555d3a4d1c079ca55516cf""}>, #<Bosh::Director::Models::Package @values={:id=>106, :name=>""route-registrar"", :version=>""ec284934d88ea47f45f241d58d7f4f2fb5914f55"", :blobstore_id=>""d80a42d1-884d-4a41-883a-a853a136b57f"", :sha1=>""c7fe95ba860d36480251de222a8b6e66088f09c5"", :dependency_set_json=>""[\""golang\""]"", :release_id=>5, :fingerprint=>""ec284934d88ea47f45f241d58d7f4f2fb5914f55""}>, #<Bosh::Director::Models::Package @values={:id=>107, :name=>""ruby"", :version=>""cd4d8494a137e228e6358eb18eee7528e371a0a0"", :blobstore_id=>""a38c5a6f-b69f-40e8-b232-efe98630db27"", :sha1=>""19f6ae73a654af3522e0bf956a29e9eb48684599"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""cd4d8494a137e228e6358eb18eee7528e371a0a0""}>, #<Bosh::Director::Models::Package @values={:id=>108, :name=>""stanchion"", :version=>""92d46913a9f3e9c63372890e1203b0cd3d96dd65"", :blobstore_id=>""d3cf9483-fe3c-43f6-9537-839c9610c583"", :sha1=>""e2578c8b34fb51d23bfd3a00cfd00010405bcee1"", :dependency_set_json=>""[\""erlang\"",\""git\"",\""python\""]"", :release_id=>5, :fingerprint=>""92d46913a9f3e9c63372890e1203b0cd3d96dd65""}>, #<Bosh::Director::Models::Package @values={:id=>109, :name=>""syslog_aggregator"", :version=>""29e843000a1d4c089023a030bc789eef1273ec51"", :blobstore_id=>""9177f9f7-9f6d-49e6-9fd2-420f4ac6caf5"", :sha1=>""e962ecfa145f575d1d2ec139c825a7dfd4952930"", :dependency_set_json=>""[]"", :release_id=>5, :fingerprint=>""29e843000a1d4c089023a030bc789eef1273ec51""}>]
```

Task log:

```
I, [2014-10-01T23:26:18.458121 #26493] [task:226]  INFO -- : Job templates `cf-riak-cs/riak-cs' need to run on stemcell `bosh-vsphere-esxi-ubuntu-lucid-go_agent/2611'
I, [2014-10-01T23:26:18.458204 #26493] [task:226]  INFO -- : Checking whether package `riak/feafe4e3c776bea8248d60d8473e1436fc6e6d70' needs to be compiled for stemcell `bosh-vsphere-esxi-ubuntu-lucid-go_agent/2611'
D, [2014-10-01T23:26:18.458544 #26493] [task:226] DEBUG -- : Deleting lock: lock:deployment:p-riak-cs-7a341f1607b3a0000256
D, [2014-10-01T23:26:18.458792 #26493] [0xf65e8c] DEBUG -- : Lock renewal thread exiting
D, [2014-10-01T23:26:18.459814 #26493] [task:226] DEBUG -- : Deleted lock: lock:deployment:p-riak-cs-7a341f1607b3a0000256
E, [2014-10-01T23:26:18.460211 #26493] [task:226] ERROR -- : key not found: ""python""
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:21:in `fetch'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:21:in `package_by_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:16:in `block in dependencies'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `block in each'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each_key'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:15:in `map'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:15:in `dependencies'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/models/release_version.rb:25:in `package_dependency_key'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/compile_task_generator.rb:30:in `generate!'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:202:in `block (4 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:201:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:201:in `block (3 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:200:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:200:in `block (2 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:187:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:187:in `block in prepare_tasks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/event_log.rb:36:in `track'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:186:in `prepare_tasks'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/package_compiler.rb:52:in `compile'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/jobs/update_deployment.rb:47:in `prepare'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/jobs/update_deployment.rb:76:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/jobs/update_deployment.rb:74:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/job_runner.rb:104:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/bin/bosh-director-worker:78:in `<top (required)>'
```",2.0,79993652,story,[],upload release should properly capture package's transitive dependencies and resolve them in the future,553935.0,"[553935, 1266616]",956238,81882,feature,2014-10-17T07:46:18Z,https://www.pivotaltracker.com/story/show/79993652
2014-10-17T12:00:00Z,2014-07-10T00:36:37Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/613

...o BOSH agent.

The changes are as follows:

In OpenStack CPI, network_configurator.rb is modified, where we changed the “network_type” validation logic, and parsed the configuration to obtain and record the multipl enetowkr ids (NICs) that are manually configured.

In Ruby Go_agent, Ubuntu_net_manager.go is modified, to check how many NICs are available based on information in  ""/sys/class/net/xxx/devices"", and change the configuration file under ""/etc/network/interfaces "" for the manually configured multiple networks to take effect.

Filed by dengwa",4.0,74744712,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #613: add openstack CPI multi manual networks support for ubuntu 14.04 using g...,1266616.0,[1266616],956238,1134058,feature,2014-10-24T01:04:25Z,https://www.pivotaltracker.com/story/show/74744712
2014-10-19T19:00:00Z,2014-09-03T19:16:27Z,accepted,,some info: https://groups.google.com/a/cloudfoundry.org/forum/#!searchin/bosh-dev/hvm/bosh-dev/p8r9TtlDIjg/oHg_3BFebdgJ,4.0,78142518,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",create & publish _light_ hvm stemcell for ubuntu trusty,1393546.0,"[1393546, 514635, 1266616]",956238,81882,feature,2014-10-20T16:44:02Z,https://www.pivotaltracker.com/story/show/78142518
2014-10-19T19:00:00Z,2014-10-15T17:25:50Z,accepted,,"Disallowing SSLv3 should be sufficient to protect our SSL conversations from the POODLE attack vector.

Note: This does not cover HTTPS conversations with remote blobstores using SSL.
The internal blobstore does not use SSL.
The internal NATS communication also does not use SSL.",4.0,80756396,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Disable SSLv3 in bosh & bosh-agent https servers,1429768.0,"[1429768, 1266616]",956238,81882,feature,2014-10-20T19:09:49Z,https://www.pivotaltracker.com/story/show/80756396
2014-10-19T19:00:00Z,2014-10-14T18:34:27Z,accepted,,"http://www.ubuntu.com/usn/usn-2380-1
http://www.ubuntu.com/usn/usn-2385-1",1.0,80674000,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]","[sao-paulo] [USN-2380-1, USN-2385-1] update ubuntu & centos stemcells",1266616.0,[1266616],956238,81882,feature,2014-10-20T19:14:05Z,https://www.pivotaltracker.com/story/show/80674000
2014-10-19T19:00:00Z,2014-08-13T00:02:45Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/625
- do not include automatic running of errand if there is only one
- rebase against new controllers",2.0,76839972,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",take in `bosh errands` pull request,1393546.0,"[1393546, 1429768, 514635]",956238,81882,feature,2014-10-20T19:18:46Z,https://www.pivotaltracker.com/story/show/76839972
2014-10-22T18:17:06Z,2014-10-16T17:05:58Z,accepted,,,1.0,80843398,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",produce 2690.6 stemcells,1266616.0,"[1266616, 1429768]",956238,81882,feature,2014-10-22T18:17:06Z,https://www.pivotaltracker.com/story/show/80843398
2014-10-22T23:18:37Z,2014-07-31T06:10:56Z,accepted,,"bosh ssh command takes a long time to run. deliver a document/comment that breaks down full bosh ssh time into detailed smaller chunks. test in multiple envs e.g 
- 4-5vm vsphere
- 20vm aws
- a1's cf deployment
- UseDNS setting in sshd",2.0,76032540,story,"[{'name': 'perf', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:41:09Z', 'id': 8096474, 'updated_at': '2014-08-05T08:12:30Z'}]",profile bosh ssh command in several environments,1429768.0,"[1429768, 1266616]",956238,81882,feature,2014-10-22T23:18:37Z,https://www.pivotaltracker.com/story/show/76032540
2014-10-23T19:02:01Z,2014-10-18T00:08:19Z,accepted,,,,80950836,story,[],add vcloud stemcell build to checkman,514635.0,[514635],956238,1266616,chore,2014-10-23T19:02:04Z,https://www.pivotaltracker.com/story/show/80950836
2014-10-25T01:58:49Z,2014-10-22T18:54:50Z,accepted,,,,81219724,story,[],Jenkins promote build should be idempotent,1266616.0,[1266616],956238,1266616,chore,2014-11-05T18:57:02Z,https://www.pivotaltracker.com/story/show/81219724
2014-10-29T00:12:16Z,2014-09-25T07:31:49Z,accepted,,https://github.com/cloudfoundry/bosh/pull/655,1.0,79503492,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in deployment events PR,1429768.0,"[1429768, 1266616]",956238,81882,feature,2014-10-29T00:12:48Z,https://www.pivotaltracker.com/story/show/79503492
2014-10-29T00:51:55Z,2014-05-13T17:26:15Z,accepted,,"Currently go-agent achieved parity with ruby agent in which is looks for 'Accepted publickey' and 'disconnected by user'. This is not enough since there are other sshd logs like 'Failed password', etc....

~~~Suggestion is to alert on any sshd line (based on source instead of based on log content)~~~

```
May 13 17:15:00 ip-10-110-53-207 CRON[1295]: pam_unix(cron:session): session closed for user root
May 13 17:15:00 ip-10-110-53-207 CRON[1294]: pam_unix(cron:session): session closed for user root
May 13 17:15:23 ip-10-110-53-207 sshd[1303]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.16.79.1  user=vcap
May 13 17:15:25 ip-10-110-53-207 sshd[1303]: Failed password for vcap from 172.16.79.1 port 63696 ssh2
May 13 17:15:28 ip-10-110-53-207 sshd[1303]: Accepted password for vcap from 172.16.79.1 port 63696 ssh2
May 13 17:15:28 ip-10-110-53-207 sshd[1303]: pam_unix(sshd:session): session opened for user vcap by (uid=0)
May 13 17:15:28 ip-10-110-53-207 sshd[1314]: Received disconnect from 172.16.79.1: 11: disconnected by user
May 13 17:15:28 ip-10-110-53-207 sshd[1303]: pam_unix(sshd:session): session closed for user vcap
May 13 17:17:01 localhost CRON[1382]: pam_unix(cron:session): session opened for user root by (uid=0)
May 13 17:17:01 localhost CRON[1382]: pam_unix(cron:session): session closed for user root
May 13 17:18:54 localhost sshd[1393]: Accepted password for vcap from 172.16.79.1 port 65037 ssh2
May 13 17:18:54 localhost sshd[1393]: pam_unix(sshd:session): session opened for user vcap by (uid=0)
May 13 17:19:27 localhost sshd[1404]: Received disconnect from 172.16.79.1: 11: disconnected by user
May 13 17:19:27 localhost sshd[1393]: pam_unix(sshd:session): session closed for user vcap
```",1.0,71252412,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",improve ssh alerting,1266616.0,[1266616],956238,81882,feature,2014-10-29T00:51:55Z,https://www.pivotaltracker.com/story/show/71252412
2014-10-31T02:17:09Z,2014-10-15T19:08:11Z,accepted,,,1.0,80767128,story,[],investigate seeding of /dev/(u)random on stemcells,1266616.0,[1266616],956238,81882,feature,2014-10-31T18:12:11Z,https://www.pivotaltracker.com/story/show/80767128
2014-11-04T18:46:28Z,2014-10-07T17:59:43Z,accepted,,https://github.com/cloudfoundry/bosh/pull/663,2.0,80231858,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Adding volume_type configuration to MicroBOSH,514635.0,[514635],956238,81882,feature,2014-11-04T18:46:28Z,https://www.pivotaltracker.com/story/show/80231858
2014-11-05T20:16:52Z,2014-10-30T01:32:37Z,accepted,,see for details https://gist.github.com/cppforlife/745ee834e039487b21a3,1.0,81736572,story,[],bosh registry packaging script should not try to contact rubygems when installing pg/mysql gems,1266616.0,[1266616],956238,81882,feature,2014-11-05T20:16:52Z,https://www.pivotaltracker.com/story/show/81736572
2014-11-05T21:10:51Z,2014-10-30T21:28:26Z,accepted,,,1.0,81804294,story,[],[Lahore] [USN-2395-1] update stemcells,1266616.0,[1266616],956238,81882,feature,2014-11-05T21:17:05Z,https://www.pivotaltracker.com/story/show/81804294
2014-11-05T23:23:20Z,2014-11-04T18:10:49Z,accepted,,https://github.com/cloudfoundry/bosh/issues/692,,82054334,story,[],fix immediate nats connect bug in the bosh-director-worker,1266616.0,[1266616],956238,81882,bug,2014-11-05T23:23:29Z,https://www.pivotaltracker.com/story/show/82054334
2014-11-06T23:03:46Z,2014-10-21T18:31:06Z,accepted,,pair with @dk,2.0,81130790,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in misc PRs,514635.0,[514635],956238,81882,feature,2014-11-06T23:03:46Z,https://www.pivotaltracker.com/story/show/81130790
2014-11-07T02:42:49Z,2014-10-13T18:36:55Z,accepted,,https://github.com/cloudfoundry/gosigar/issues/13,1.0,80578780,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Pull in swap sizes by sysinfo syscall (linux),514635.0,[514635],956238,81882,feature,2014-11-07T02:42:49Z,https://www.pivotaltracker.com/story/show/80578780
2014-11-11T22:31:21Z,2014-11-10T22:52:14Z,accepted,,"Only Ubuntu trusty based on the latest OS image

",0.0,82439056,story,[],create 2690.6 aws stemcell and place it into the bucket,320647.0,[320647],956238,81882,feature,2014-11-11T22:31:28Z,https://www.pivotaltracker.com/story/show/82439056
2014-11-12T17:24:27Z,2014-11-12T17:22:38Z,accepted,,,1.0,82580568,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}]",investigate running bats on vsan enabled environment,1429768.0,[1429768],956238,81882,feature,2014-11-12T17:26:11Z,https://www.pivotaltracker.com/story/show/82580568
2014-11-12T19:38:16Z,2014-11-10T18:26:58Z,accepted,,,,82415550,story,[],On board Edward,1495236.0,[1495236],956238,514635,chore,2014-11-12T19:38:17Z,https://www.pivotaltracker.com/story/show/82415550
2014-11-13T01:40:28Z,2014-11-05T21:23:44Z,accepted,,"http://www.ubuntu.com/usn/usn-2397-1/
https://www.ruby-lang.org/en/news/2014/10/27/rexml-dos-cve-2014-8080/",1.0,82162190,story,[],[shenzhen] [usn-2397-1] update ruby in bosh release,320647.0,[320647],956238,81882,feature,2014-11-13T01:41:07Z,https://www.pivotaltracker.com/story/show/82162190
2014-11-14T18:37:30Z,2014-10-28T18:32:01Z,accepted,,"Some code is using MonoLogger.new
Some code is using Logger.new
Some code is using Logging.logger

All logging should use Logging so that logs are correctly printed in trap blocks where locks are not allowed. 

Notes:
- Logging's formatter also by default includes the thread id (so no ThreadFormatter is required).
- Logging.logger() emulates Logger.new()
- Logging::Logger emulates Logger, but does not extend it.
- Logging::Logger.new('some-name') is the easiest way to get a logger that doesn't log anywhere (rather than logging to /dev/null). This returns a logger with no appenders.
- Logging::Logger.new takes a name as the first argument and stores the loggers in a global repository so you can easily retrieve them by name.",,81611698,story,[],Switch BOSH to using Logging consistently in director,1266616.0,"[1266616, 514635, 1495236]",956238,514635,chore,2014-11-14T18:37:28Z,https://www.pivotaltracker.com/story/show/81611698
2014-11-14T18:37:50Z,2014-10-28T18:36:12Z,accepted,,"Unless testing error cases or logging, tests should log to String.IO and then dump logs to STDERR if a test fails. 

We should be able to set this up in the spec_helpers somewhere.",,81612110,story,[],Tests should dump all logs to STDERR when they fail in the director,1266616.0,[1266616],956238,514635,chore,2014-11-14T18:37:49Z,https://www.pivotaltracker.com/story/show/81612110
2014-11-17T22:16:51Z,2014-11-17T18:34:53Z,accepted,,,,82871804,story,[],Get Stev onboarded,514635.0,"[514635, 553935, 1488968]",956238,1429768,chore,2014-11-17T22:17:52Z,https://www.pivotaltracker.com/story/show/82871804
2014-11-18T19:47:50Z,2014-11-03T22:35:34Z,accepted,,"```
  1) Bosh::Monitor::Plugins::Email processes queue asynchronously when running
     Failure/Error: expect(@plugin.queue_size(:alert)).to eq(0)

       expected: 0
            got: 20

       (compared using ==)
     # ./spec/unit/bosh/monitor/plugins/email_spec.rb:113:in `block (2 levels) in <top (required)>'
```

Suspect it might be related to recent event machine changes.",,81992472,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Fix flakey email_spec test,1266616.0,"[1266616, 1429768, 514635]",956238,1266616,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/81992472
2014-11-20T20:32:55Z,2014-11-20T17:25:27Z,accepted,,,,83136346,story,[],Get Xian onboarded,553935.0,[553935],956238,553935,chore,2014-11-20T20:34:51Z,https://www.pivotaltracker.com/story/show/83136346
2014-11-21T21:34:31Z,2014-11-13T19:19:55Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/693
https://github.com/cloudfoundry/bosh/pull/691
https://github.com/cloudfoundry/bosh/pull/688",1.0,82678498,story,[],pull in misc PRs,320647.0,[320647],956238,320647,feature,2014-11-21T21:34:32Z,https://www.pivotaltracker.com/story/show/82678498
2014-11-21T21:46:31Z,2014-11-14T21:32:32Z,accepted,,,1.0,82769162,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}]",produce centos 6.6 stemcell since 6.5 packages were archived,320647.0,"[320647, 1429768]",956238,81882,feature,2014-11-21T21:46:32Z,https://www.pivotaltracker.com/story/show/82769162
2014-11-23T20:00:00Z,2014-10-15T18:57:15Z,accepted,,cannot turn it on for centos since it causes problems on aws.,1.0,80765688,story,[],turn on kernel logging in ubuntu trusty stemcells,320647.0,"[320647, 1495236]",956238,81882,feature,2014-11-24T20:22:22Z,https://www.pivotaltracker.com/story/show/80765688
2014-12-01T19:04:16Z,2014-11-14T18:57:00Z,accepted,,It seems that the reason the AWS slaves periodically disconnect from jenkins is that they slowly get out of time sync with the jenkins master. Add the NTP cookbook to chef and reprovision the slaves to use the NTP service to stay in sync with master.,,82756926,story,[],run NTP service on AWS slaves,553935.0,[553935],956238,1429768,chore,2014-12-01T19:04:42Z,https://www.pivotaltracker.com/story/show/82756926
2014-12-01T23:36:04Z,2014-12-01T17:22:40Z,accepted,,,,83682978,story,[],Get Tyler onboarded,553935.0,"[553935, 344]",956238,553935,chore,2014-12-01T23:36:31Z,https://www.pivotaltracker.com/story/show/83682978
2014-12-01T23:38:26Z,2014-09-30T18:55:48Z,accepted,,"Our server died and was resurrected. The old instance was terminate at 8:01:

```
I, [2014-09-30T08:01:43.968835 #19289] [delete_vm(i-97e33d79)]  INFO -- : i-97e33d79 is now terminated, took 996.842834145s
```

When it tried to attach the persistent disk to the new instance at 8:05 AWS reported it being already attached:

```
I, [2014-09-30T08:05:20.597275 #19289] [attach_disk(i-1d3dacf3, vol-521f5a1e)]  INFO -- : [AWS EC2 400 0.431883 0 retries] attach_volume(:device=>""/dev/sdf"",:instance_id=>""i-1d3dacf3"",:volume_id=>""vol-521f5a1e"") AWS::EC2::Errors::VolumeInUse vol-521f5a1e is already attached to an instance

E, [2014-09-30T08:05:20.597788 #19289] [task:6913] ERROR -- : Error resolving problem `28': vol-521f5a1e is already attached to an instance
E, [2014-09-30T08:05:20.597881 #19289] [task:6913] ERROR -- : /var/vcap/packages/director/gem_home/ruby/1.9.1/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:375:in `return_or_raise'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `attach_volume'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/aws-sdk-1.44.0/lib/aws/ec2/volume.rb:119:in `attach_to'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_aws_cpi-1.2710.0/lib/cloud/aws/cloud.rb:572:in `block in attach_ebs_volume'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2710.0/lib/common/retryable.rb:28:in `call'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2710.0/lib/common/retryable.rb:28:in `block in retryer'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2710.0/lib/common/retryable.rb:26:in `loop'
```",2.0,79810782,story,"[{'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}, {'name': 'running-tests-for-a-week', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-20T22:06:34Z', 'id': 10052866, 'updated_at': '2014-11-20T22:06:34Z'}]",Investigate what happens when resurrector fails to attach persistent disk,553935.0,"[553935, 344]",956238,756869,feature,2015-04-01T18:13:36Z,https://www.pivotaltracker.com/story/show/79810782
2014-12-03T18:43:19Z,2014-09-30T23:19:08Z,accepted,,use vagrant ,,79834786,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",add bosh-agent integration tests to see that config drive is properly mounted/used/unmountedd,1488968.0,"[1488968, 344, 1429768]",956238,81882,chore,2014-12-03T18:43:20Z,https://www.pivotaltracker.com/story/show/79834786
2014-12-03T19:48:35Z,2014-10-03T18:05:37Z,accepted,,,1.0,80040384,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",investigate device path resolved timeout for finding disks on aws and openstack,553935.0,"[553935, 344, 1495236]",956238,81882,feature,2014-12-03T19:49:12Z,https://www.pivotaltracker.com/story/show/80040384
2014-12-04T20:26:40Z,2014-12-03T21:41:33Z,accepted,,"That was required by old ruby agent in lucid stemcells, we don't need this anymore
",,83887114,story,[],Remove system_rescan_scsi_bus stemcell stage,553935.0,"[553935, 1495236]",956238,553935,chore,2014-12-04T20:26:48Z,https://www.pivotaltracker.com/story/show/83887114
2014-12-07T20:00:00Z,2014-10-16T16:29:56Z,accepted,,https://github.com/cloudfoundry/bosh/pull/667,4.0,80840686,story,"[{'name': 'disk-pools1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-04T19:40:34Z', 'id': 9111434, 'updated_at': '2014-08-18T03:50:54Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Adding changes to OpenStack CPI for volume type,320647.0,"[320647, 1495236]",956238,81882,feature,2014-12-08T22:23:21Z,https://www.pivotaltracker.com/story/show/80840686
2014-12-08T03:24:38Z,2014-11-12T19:30:39Z,accepted,,,4.0,82593254,story,[],set up another vcloud environment for ci,1495236.0,"[1495236, 1429768, 553935]",956238,81882,feature,2014-12-08T03:24:39Z,https://www.pivotaltracker.com/story/show/82593254
2014-12-08T03:44:46Z,2014-12-03T17:50:57Z,accepted,,"When trying to recreate our full bosh from our micro, the arp cache was stale and the microbosh could not talk to the newly created bosh. Clearing the arp cache fixed the problem.

gc_thresh settings for microbosh bosh-aws-xen-ubuntu-trusty-go_agent 2778.

$ sysctl net.ipv4.neigh.default.gc_thresh1
net.ipv4.neigh.default.gc_thresh1 = 1024

$ sysctl net.ipv4.neigh.default.gc_thresh2
net.ipv4.neigh.default.gc_thresh2 = 2048

$ sysctl net.ipv4.neigh.default.gc_thresh3
net.ipv4.neigh.default.gc_thresh3 = 4096

Relevant cf-release commit: https://github.com/cloudfoundry/cf-release/commit/cd0bbd5ed6617df71667c531b2833298fedea263

See #73230550",,83863904,story,[],remove gc_thresh1 value from bosh director nats job' ctl script,553935.0,"[553935, 1495236]",956238,756869,bug,2014-12-08T03:44:55Z,https://www.pivotaltracker.com/story/show/83863904
2014-12-08T03:45:47Z,2014-10-24T21:09:55Z,accepted,,,1.0,81404532,story,"[{'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",turn off reverse dns resolution in ssh daemon on stemcells,1488968.0,"[1488968, 344]",956238,81882,feature,2014-12-08T03:45:48Z,https://www.pivotaltracker.com/story/show/81404532
2014-12-08T03:48:18Z,2014-12-03T21:49:13Z,accepted,,https://gist.github.com/jpalermo/d9e475a55acddd37232a,,83887864,story,[],director is leaking fds for log files,553935.0,"[553935, 1495236]",956238,707557,bug,2014-12-08T03:48:18Z,https://www.pivotaltracker.com/story/show/83887864
2014-12-08T05:12:44Z,2014-12-02T20:35:19Z,accepted,,add a job with persistent disk pool and try to deploy it second time with no changes. it migrates the data instead of noting that nothing has changed. confirmed with bosh-lite,,83792678,story,[],bosh deploy should not recreate/migrate persistent disks that are part of persistent disk pools every time it deploys,553935.0,"[553935, 1495236]",956238,81882,bug,2014-12-08T05:12:45Z,https://www.pivotaltracker.com/story/show/83792678
2014-12-11T00:35:32Z,2014-10-08T17:40:59Z,accepted,,,,80315268,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",PR 'health monitor should not get stuck communicating with data dog',1488968.0,"[1488968, 344]",956238,637633,bug,2014-12-11T00:35:33Z,https://www.pivotaltracker.com/story/show/80315268
2014-12-11T00:41:36Z,2014-12-01T23:51:25Z,accepted,,https://github.com/cloudfoundry/bosh/pull/697,1.0,83720606,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Enable ssl configuration for EC2 endpoints',1429768.0,[1429768],956238,81882,feature,2014-12-11T00:41:37Z,https://www.pivotaltracker.com/story/show/83720606
2014-12-11T00:42:26Z,2014-12-01T23:52:07Z,accepted,,https://github.com/cloudfoundry/bosh/pull/698,1.0,83720652,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Use ec2_endpoint from the manifest in bosh-registry',1429768.0,[1429768],956238,81882,feature,2014-12-11T00:42:27Z,https://www.pivotaltracker.com/story/show/83720652
2014-12-11T01:21:54Z,2014-12-08T17:48:32Z,accepted,,account credentials in lastpass,,84138570,story,[],Publish GoCD docker image to docker hub,1429768.0,[1429768],956238,1266616,chore,2014-12-11T01:21:54Z,https://www.pivotaltracker.com/story/show/84138570
2014-12-11T01:24:22Z,2014-10-08T22:24:59Z,accepted,,,1.0,80340124,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}]",investigate running lifecycle tests on vsan enabled environment,1429768.0,"[1429768, 344]",956238,81882,feature,2014-12-11T01:24:47Z,https://www.pivotaltracker.com/story/show/80340124
2014-12-11T02:01:09Z,2014-11-03T18:11:07Z,accepted,,,2.0,81966000,story,[],user should be able to configure errands to live on the long running vms - e.g. bosh run errand should not perform any iaas commands,1488968.0,"[1488968, 344]",956238,81882,feature,2014-12-11T02:01:18Z,https://www.pivotaltracker.com/story/show/81966000
2014-12-11T17:04:14Z,2014-11-20T18:16:28Z,accepted,,"Happens on a customer's OpenStack environment. Using Bosh stemcell openstack kvm ubuntu trusty go_agent 2745. Booting vm's on a m1.xlarge flavor (16384Mb, 20Gb root disk, 0 ephemeral disk) causes the bosh agent to fail:

2014-11-20_16:10:58.74531 [main] 2014/11/20 16:10:58 ERROR - App setup Running bootstrap: Setting up ephemeral disk: Creating ephemeral partitions on root device: Partitioning root device `/dev/vda': Partitioning disk `/dev/vda': Running command: 'parted -s /dev/vda unit B mkpart primary 12272918272 21474836479', stdout: 'Error: You requested a partition from 12272918016B to 21474835968B.
2014-11-20_16:10:58.74536 The closest location we can manage is 12272918528B to 21474835968B.

",,83141758,story,[],Bosh agent fails to create swap & ephemeral partitions,1266616.0,[1266616],956238,1015645,bug,2014-12-11T17:04:14Z,https://www.pivotaltracker.com/story/show/83141758
2014-12-12T04:44:39Z,2014-11-06T19:01:42Z,accepted,,"oops...

```
I, [2014-11-05T10:57:08.419824 #20450]  INFO -- : Skipping bosh-stemcell-latest-vsphere-esxi-ubuntu-trusty-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.420366 #20450]  INFO -- : Skipping bosh-stemcell-latest-openstack-kvm-ubuntu-trusty-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.447201 #20450]  INFO -- : Skipping bosh-stemcell-latest-vsphere-esxi-centos-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.533355 #20450]  INFO -- : Skipping light-bosh-stemcell-latest-aws-xen-hvm-ubuntu-trusty-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.542756 #20450]  INFO -- : Skipping light-bosh-stemcell-latest-aws-xen-hvm-centos-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.599882 #20450]  INFO -- : Skipping light-bosh-stemcell-latest-aws-xen-centos-go_agent.tgz artifact promotion
I, [2014-11-05T10:57:08.658572 #20450]  INFO -- : Skipping light-bosh-stemcell-latest-aws-xen-ubuntu-trusty-go_agent.tgz artifact promotion
```

The ""idempotency"" broke the uploading of the latest stemcells because they already exist.

Solution: always re-copy latest stemcells (promoted? should always return false if the destination_version is 'latest'). 
This should be a pretty easy fix.",,82233156,story,[],Latest stemcell artifacts are not being promoted,1488968.0,"[1488968, 344]",956238,1266616,bug,2014-12-12T04:44:41Z,https://www.pivotaltracker.com/story/show/82233156
2014-12-12T04:45:34Z,2014-12-01T23:50:44Z,accepted,,https://github.com/cloudfoundry/bosh/pull/695,1.0,83720564,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Default NTP configuration.',1429768.0,[1429768],956238,81882,feature,2014-12-12T04:45:35Z,https://www.pivotaltracker.com/story/show/83720564
2014-12-12T22:52:56Z,2014-12-08T18:03:38Z,accepted,,https://github.com/cloudfoundry/gosigar/issues/17,1.0,84139938,story,[],PR 'Darwin 10.10 uptime bug',344.0,[344],956238,81882,feature,2014-12-12T22:52:57Z,https://www.pivotaltracker.com/story/show/84139938
2014-12-13T00:37:16Z,2014-11-12T17:25:19Z,accepted,,,2.0,82580826,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}]",change vsphere cpi to be able to run on vsan environment,1429768.0,"[1429768, 344]",956238,81882,feature,2014-12-13T00:37:18Z,https://www.pivotaltracker.com/story/show/82580826
2014-12-15T22:58:43Z,2014-12-11T18:55:37Z,accepted,,vcloud gem should be locked down to specific version: https://github.com/cloudfoundry/bosh/blob/master/bosh_cli_plugin_micro/bosh_cli_plugin_micro.gemspec#L32. leave a comment to make sure that it stays this way.,,84406944,story,[],lock down vcloud gem to specific version (use =),344.0,[344],956238,81882,bug,2014-12-15T22:58:48Z,https://www.pivotaltracker.com/story/show/84406944
2014-12-15T23:21:48Z,2014-10-24T17:38:32Z,accepted,,https://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-users/25FlZSZioU4,1.0,81387812,story,[],provide a cli flag to hide bosh deploy diff,344.0,[344],956238,81882,feature,2014-12-15T23:21:49Z,https://www.pivotaltracker.com/story/show/81387812
2014-12-16T19:33:27Z,2014-12-13T00:00:11Z,accepted,,- use new bosh stemcell #2788,2.0,84506542,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}]",deploy cloudfoundry to a vsan env,553935.0,"[553935, 1495236]",956238,81882,feature,2014-12-16T19:33:48Z,https://www.pivotaltracker.com/story/show/84506542
2014-12-16T21:55:46Z,2014-12-08T20:41:41Z,accepted,,"https://github.com/cloudfoundry/bosh/issues/709

may be use id:

root@bm-edfe5a07-9dd3-484d-9965-2ad1e00cd1b4:/home/vcap# ls -la /dev/disk/by-id/virtio-00b6e180-2fe1-42cf-8
lrwxrwxrwx 1 root root 9 Dec  8 20:22 /dev/disk/by-id/virtio-00b6e180-2fe1-42cf-8 -> ../../vdb",2.0,84156058,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",Investigate a more reliable way to find disks on openstack,553935.0,"[553935, 1495236]",956238,81882,feature,2014-12-16T21:55:48Z,https://www.pivotaltracker.com/story/show/84156058
2014-12-16T22:03:52Z,2014-12-16T19:16:00Z,accepted,,sometimes downloading OS images times out Timeout::Error: Timeout::Error it took ~10 min. ,,84701966,story,[],Increase timeout for downloading OS image for stemcell building,553935.0,"[553935, 1495236]",956238,553935,chore,2014-12-16T22:04:56Z,https://www.pivotaltracker.com/story/show/84701966
2014-12-17T20:11:38Z,2014-12-17T00:37:53Z,accepted,,,,84728390,story,[],travis tests should only run unit tests (no integration),1488968.0,"[1488968, 1429768]",956238,81882,chore,2014-12-19T21:09:48Z,https://www.pivotaltracker.com/story/show/84728390
2014-12-18T23:15:11Z,2014-12-15T07:43:05Z,accepted,,https://github.com/cloudfoundry/bosh/pull/714,1.0,84559336,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Bosh monitor graphite',553935.0,"[553935, 1495236]",956238,81882,feature,2014-12-18T23:15:12Z,https://www.pivotaltracker.com/story/show/84559336
2014-12-19T00:03:22Z,2014-10-14T18:19:59Z,accepted,,ask @dk for the link,1.0,80672056,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",investigate & remove denied packages from the centos and ubuntu stemcells,344.0,[344],956238,81882,feature,2014-12-19T00:03:22Z,https://www.pivotaltracker.com/story/show/80672056
2014-12-19T00:06:51Z,2014-12-02T22:15:47Z,accepted,,"bosh init release
.. add a job ...
bosh create release --force
Missing blobstore configuration, please update your release # <--- this should not happen",2.0,83801718,story,[],user should be able to create dev release without blobstore configuration,1266616.0,[1266616],956238,81882,feature,2014-12-19T00:06:51Z,https://www.pivotaltracker.com/story/show/83801718
2014-12-19T21:38:07Z,2014-12-01T22:57:17Z,accepted,,"Console keeps seeing errors like this:

```
Director task 2142
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started binding instance vms > push-app-usage-service/0. Done (00:00:00)

  Started updating job push-app-usage-service > push-app-usage-service/0 (canary)
System call error while talking to director: Connection reset by peer - SSL_connect
```

We would like it if the CLI could capture this error and either retry or present more information to the user.",,83717118,story,[],"As a user of the BOSH CLI, I would like the CLI to capture an `SSL_connect` error and either retry connecting to the director or surface more information",1488968.0,"[1488968, 1429768]",956238,1194390,bug,2014-12-19T21:38:21Z,https://www.pivotaltracker.com/story/show/83717118
2014-12-19T22:05:11Z,2014-12-16T18:46:13Z,accepted,,"- m1.medium
- m3.medium
- m3.xlarge (reported to not find ephemeral disk)
- c3.xlarge",1.0,84699078,story,[],investigate differences between ephemeral storage for different instance types,1429768.0,"[1429768, 1348450]",956238,81882,feature,2014-12-19T22:05:20Z,https://www.pivotaltracker.com/story/show/84699078
2014-12-23T03:14:20Z,2014-11-03T22:34:05Z,accepted,,https://github.com/cloudfoundry/bosh/pull/684,2.0,81992382,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in 'download missing blobs in parallel' PR,1488968.0,"[1488968, 1495236]",956238,81882,feature,2014-12-23T03:14:20Z,https://www.pivotaltracker.com/story/show/81992382
2014-12-23T17:01:42Z,2014-12-19T01:13:14Z,accepted,,utils.sh in cf-release,1.0,84896364,story,[],investigate zombie processes from ctl script if logs are redirected to ctl log files,1429768.0,"[1429768, 1348450]",956238,81882,feature,2014-12-23T17:01:43Z,https://www.pivotaltracker.com/story/show/84896364
2014-12-23T17:02:12Z,2014-12-17T22:40:25Z,accepted,,https://github.com/cloudfoundry/bosh/pull/713 and https://github.com/cloudfoundry/bosh/pull/712,1.0,84810548,story,[],"PR 712/713 'propagate proxy settings to VM, docker container and test run if set' and ...",344.0,[344],956238,81882,feature,2014-12-23T17:02:13Z,https://www.pivotaltracker.com/story/show/84810548
2014-12-23T17:03:14Z,2014-12-17T22:39:49Z,accepted,,https://github.com/cloudfoundry/bosh/pull/715,1.0,84810510,story,[],PR 715 'Upgrade to latest yajl-ruby towards support for traveling-ruby',1429768.0,"[1429768, 1348450]",956238,81882,feature,2014-12-23T17:03:15Z,https://www.pivotaltracker.com/story/show/84810510
2014-12-23T19:03:47Z,2014-12-19T00:17:51Z,accepted,,make sure that we can partition remainder of root disk (see agent on how it does it; may be even run agent and see it partition it?),1.0,84893804,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",investigate how to request custom root disk size for aws instances without modifying stemcells,1348450.0,"[1348450, 1429768]",956238,81882,feature,2014-12-29T19:24:46Z,https://www.pivotaltracker.com/story/show/84893804
2014-12-23T19:44:52Z,2014-12-17T22:41:18Z,accepted,,https://github.com/cloudfoundry/bosh/pull/710,1.0,84810640,story,[],PR 710 'Fix issue with having empty `release_versions` array in release parameters',1429768.0,"[1429768, 1348450]",956238,81882,feature,2014-12-23T19:44:52Z,https://www.pivotaltracker.com/story/show/84810640
2014-12-23T22:25:04Z,2014-12-22T19:39:08Z,accepted,,we increased read timeout from 60s to 300s but it seems like it did not help. We still see timeout errors. We should retry downloading stemcell.,,85034464,story,[],retry on read timeout error when downloading stemcells in ci,1348450.0,[1348450],956238,553935,chore,2014-12-29T19:24:45Z,https://www.pivotaltracker.com/story/show/85034464
2014-12-23T23:49:22Z,2014-12-08T18:00:00Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/6,1.0,84139616,story,[],PR 'Start monit after bootstrapping',1488968.0,"[1488968, 1495236]",956238,81882,feature,2014-12-23T23:49:23Z,https://www.pivotaltracker.com/story/show/84139616
2014-12-24T00:33:27Z,2014-12-23T21:55:44Z,accepted,,,,85100456,story,[],Update jenkins ssh slave plugin ,553935.0,[553935],956238,553935,chore,2014-12-24T01:45:09Z,https://www.pivotaltracker.com/story/show/85100456
2014-12-24T01:44:45Z,2014-12-16T22:00:23Z,accepted,,see document shared by Maria,4.0,84717348,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",openstack cpi should use by-label and then by-id disk identification and finally by disk suffix,553935.0,"[553935, 1488968]",956238,81882,feature,2014-12-24T01:44:45Z,https://www.pivotaltracker.com/story/show/84717348
2014-12-24T19:59:29Z,2014-12-17T22:42:01Z,accepted,,https://github.com/cloudfoundry/bosh/pull/707,2.0,84810698,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 707 'Support Nova scheduling hints in OpenStack CPI',206015.0,[206015],956238,81882,feature,2014-12-24T19:59:29Z,https://www.pivotaltracker.com/story/show/84810698
2014-12-25T03:54:39Z,2014-12-16T19:35:12Z,accepted,,"in addition to creating it should continue on if cpi fails to create a folder if folder already exists (i.e. race between other cpi call)

these folders are not datastore folders but rather organization folders",1.0,84703942,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere cpi should create vms and templates folders if they are not found,553935.0,"[553935, 1495236]",956238,81882,feature,2014-12-25T03:54:40Z,https://www.pivotaltracker.com/story/show/84703942
2014-12-25T04:03:52Z,2014-12-04T20:02:07Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/7/files,1.0,83961504,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Resolve localhost to 127.0.0.1 without using DNS',1429768.0,[1429768],956238,81882,feature,2014-12-25T04:03:53Z,https://www.pivotaltracker.com/story/show/83961504
2014-12-29T19:18:16Z,2014-12-29T19:20:06Z,accepted,,We should use git to determine if there were changes,,85231060,story,[],Fix auto-deploy git commit failures,553935.0,"[553935, 206015]",956238,553935,chore,2014-12-29T19:20:20Z,https://www.pivotaltracker.com/story/show/85231060
2014-12-29T22:45:09Z,2014-12-29T19:19:09Z,accepted,,"This test failed before and we changed it to actually see the logs:

```
HTTPSHandler routing and auth when an incorrect username/password was provided [It] returns a 401 
/home/travis/build/cloudfoundry/bosh/go/src/github.com/cloudfoundry/bosh-agent/micro/https_handler_test.go:205
  Expected error:
      <*url.Error | 0xc2081934a0>: {
          Op: ""Post"",
          URL: ""https://user:wrong@127.0.0.1:6900/agent"",
          Err: {
              Op: ""dial"",
              Net: ""tcp"",
              Addr: {
                  IP: ""\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xff\xff\u007f\x00\x00\x01"",
                  Port: 6900,
                  Zone: """",
              },
              Err: 0x6f,
          },
      }
      Post https://user:wrong@127.0.0.1:6900/agent: dial tcp 127.0.0.1:6900: connection refused
  not to have occurred
```
/home/travis/build/cloudfoundry/bosh/go/src/github.com/cloudfoundry/bosh-agent/micro/https_handler_test.go:199

https://s3.amazonaws.com/archive.travis-ci.org/jobs/45330393/log.txt",,85231014,story,[],Investigate travis go-agent failure,553935.0,"[553935, 206015]",956238,553935,chore,2014-12-29T22:47:34Z,https://www.pivotaltracker.com/story/show/85231014
2014-12-30T00:40:40Z,2014-12-26T18:42:48Z,accepted,,,,85167460,story,[],Investigate why vcloud tests try to delete CF-opsmanager vApp,553935.0,"[553935, 206015]",956238,553935,chore,2014-12-30T00:42:46Z,https://www.pivotaltracker.com/story/show/85167460
2014-12-30T18:37:56Z,2014-12-30T16:01:19Z,accepted,,https://server.gocd.cf-app.com/go/tab/build/detail/bosh/526/tests/1/unit-2.1.2,,85272180,story,[],investigate gocd unit test failure,553935.0,[553935],956238,553935,chore,2014-12-30T18:40:20Z,https://www.pivotaltracker.com/story/show/85272180
2014-12-31T00:55:57Z,2014-12-09T00:17:34Z,accepted,,"in addition to creating it should continue on if cpi fails to create a folder if folder already exists (i.e. race between other cpi call)

these folders _are_ datastore folders",2.0,84172054,story,"[{'name': 'vsan1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-12T17:25:53Z', 'id': 9974616, 'updated_at': '2014-11-12T17:25:53Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere cpi should create disks folders in the datastore if they are not found,553935.0,"[553935, 1488968]",956238,1054467,feature,2014-12-31T00:55:57Z,https://www.pivotaltracker.com/story/show/84172054
2014-12-31T01:12:40Z,2014-12-19T00:13:50Z,accepted,,sometimes older generation instances cannot properly expose ephemeral storage if disk slot options are only requested via ami configuration. cpi should request ephemeral disk slots explicitly via vm launch arguments (i.e. api call).,2.0,84893348,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",aws cpi should state ephemeral disk in vm launch parameters explicitly to make sure that m3 instances have ephemeral disk,1348450.0,"[1348450, 1488968]",956238,81882,feature,2014-12-31T01:12:41Z,https://www.pivotaltracker.com/story/show/84893348
2014-12-31T01:13:20Z,2014-12-11T00:36:01Z,accepted,,,0.0,84348176,story,[],log an error when health monitor is stuck and it returns 500 for /healthz,1488968.0,"[1488968, 344, 1348450]",956238,81882,feature,2014-12-31T01:13:21Z,https://www.pivotaltracker.com/story/show/84348176
2015-01-05T18:37:07Z,2015-01-05T17:22:04Z,accepted,,,,85445328,story,[],Get Colin onboarded,553935.0,"[553935, 687691]",956238,553935,chore,2015-01-05T18:39:44Z,https://www.pivotaltracker.com/story/show/85445328
2015-01-06T01:44:52Z,2015-01-06T01:33:30Z,accepted,,"Some gocd tests fail because gocd fails to run git clean at the beginning of test run. See: https://server.gocd.cf-app.com/go/tab/build/detail/bosh/614/tests/1/integration-1.9.3-mysql

Toolsmiths suggested to run gocd script to remove symlinks at the end of the build.

They also said that this will be fixed in the next gocd release",,85486572,story,[],Remove all symlinks at the end of gocd tests,553935.0,"[553935, 687691]",956238,553935,chore,2015-01-06T01:47:30Z,https://www.pivotaltracker.com/story/show/85486572
2015-01-07T01:30:27Z,2015-01-06T21:27:17Z,accepted,,"Out stemcell building CI job fails sometimes with the error:

```
VagrantPlugins::AWS::Errors::InstanceReadyTimeout: The instance never became ""ready"" in AWS. The timeout currently
set waiting for the instance to become ready is 120 seconds.
Please verify that the machine properly boots. If you need more time
set the `instance_ready_timeout` configuration on the AWS provider
```

http://bosh-jenkins.cf-app.com:8080/job/stemcell_aws_centos_go_agent/584/console

Let's increase it to 10 mins.",,85556920,story,[],Increase instance ready timeout when spinning stemcell building VM,687691.0,"[687691, 553935]",956238,553935,chore,2015-01-07T01:33:10Z,https://www.pivotaltracker.com/story/show/85556920
2015-01-07T19:12:49Z,2015-01-07T19:12:38Z,accepted,,,,85641588,story,[],Openstack BATs should retry on SocketError,553935.0,"[553935, 687691]",956238,553935,chore,2015-01-07T19:12:49Z,https://www.pivotaltracker.com/story/show/85641588
2015-01-08T01:58:42Z,2014-10-16T16:55:25Z,accepted,,"This test failed for no apparent reason. No stack trace, just a job whose logs say it finished while the cli says it errored...

https://server.gocd.cf-app.com/go/tab/build/detail/bosh/281/tests/1/integration-1.9.3-mysql

```
Failure/Error: deploy_simple(manifest_hash: manifest_hash)
     RuntimeError:
       ERROR: bosh -n -c /tmp/d20141016-11540-gwkj47/bosh_config.yml deploy failed with output:
       
       Processing deployment manifest
       ------------------------------
       Getting deployment properties from director...
       Unable to get properties list from director, trying without it...
       Compiling deployment manifest...
       Cannot get current deployment information from director, possibly a new deployment
       
       Deploying
       ---------
       Deployment name: `simple20141016-11540-frempq'
       Director name: `Test Director'
       
       Director task 3
         Started preparing deployment
         Started preparing deployment > Binding deployment. Done (00:00:00)
         Started preparing deployment > Binding releases. Done (00:00:00)
         Started preparing deployment > Binding existing deployment. Done (00:00:00)
         Started preparing deployment > Binding resource pools. Done (00:00:00)
         Started preparing deployment > Binding stemcells. Done (00:00:00)
         Started preparing deployment > Binding templates. Done (00:00:00)
         Started preparing deployment > Binding properties. Done (00:00:00)
         Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
         Started preparing deployment > Binding instance networks. Done (00:00:00)
            Done preparing deployment (00:00:00)
       
         Started preparing package compilation > Finding packages to compile. Done (00:00:00)
       
         Started compiling packages
         Started compiling packages > foo/0ee95716c58cf7aab3ef7301ff907118552c2dda. Done (00:00:02)
         Started compiling packages > bar/f1267e1d4e06b60c91ef648fb9242e33ddcffa73. Done (00:00:02)
            Done compiling packages (00:00:04)
       
         Started preparing dns > Binding DNS. Done (00:00:00)
       
         Started creating bound missing vms
         Started creating bound missing vms > a/0
         Started creating bound missing vms > a/1
         Started creating bound missing vms > a/2
            Done creating bound missing vms > a/0 (00:00:01)
            Done creating bound missing vms > a/1 (00:00:01)
            Done creating bound missing vms > a/2 (00:00:00)
            Done creating bound missing vms (00:00:01)
       
         Started binding instance vms
         Started binding instance vms > foobar/0
         Started binding instance vms > foobar/1
         Started binding instance vms > foobar/2
            Done binding instance vms > foobar/1 (00:00:00)
            Done binding instance vms > foobar/2 (00:00:00)
            Done binding instance vms > foobar/0 (00:00:00)
            Done binding instance vms (00:00:00)
       
         Started preparing configuration > Binding configuration. Done (00:00:00)
       
         Started updating job foobar
         Started updating job foobar > foobar/0 (canary). Done (00:00:05)
         Started updating job foobar > foobar/1 (canary). Done (00:00:05)
         Started updating job foobar > foobar/2
       
       Task 3 error
```",,80842620,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}, {'name': 'gocd', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-13T00:02:05Z', 'id': 9188506, 'updated_at': '2014-08-13T00:02:05Z'}]",Integration Test Failure:  deploy job template re-evaluates job templates with new manifest job properties,553935.0,"[553935, 687691]",956238,1266616,bug,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/80842620
2015-01-08T23:53:55Z,2014-02-10T21:20:16Z,accepted,,"When the user is associated with tasks, running `bosh delete user` responds ""HTTP 500"". Looking through the logs, we see:

> E, [2014-02-10T15:28:58.867796 #4133] [0xa63880] ERROR -- : Sequel::DatabaseError - PG::Error: ERROR:  update or delete on table ""users"" violates foreign key constraint ""tasks_user_id_fkey"" on table ""tasks""
> DETAIL:  Key (id)=(1) is still referenced from table ""tasks"".

Requirement: after user is deleted from the system, task should still show user's name.",2.0,65509840,story,[],deleting user should be possible even if it is associated with anything in the system,1348450.0,[1348450],956238,756869,feature,2015-01-08T23:53:55Z,https://www.pivotaltracker.com/story/show/65509840
2015-01-09T01:58:16Z,2014-12-22T20:21:17Z,accepted,,"Confirm and change API call if we need to use correct API endpoint.

```
On OpenStack, We recently discovered a bug with detaching volumes from servers using BOSH.  In summary, the request to detach was sent, but the volume was never detached and as a result BOSH would eventually flag the deployment as a failure.

The cause is due to BOSH not using the attachment ID as stipulated in the Openstack API documents (v2/​{tenant_id}​/servers/​{server_id}​/os-volume_attachments/​{attachment_id}​), but is instead using the volumeId.

See https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb#L519
```",,85037548,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",Detach volume from server fails due to wrong endpoint being used,1348450.0,[1348450],956238,1210852,bug,2015-01-09T01:58:17Z,https://www.pivotaltracker.com/story/show/85037548
2015-01-09T02:02:04Z,2014-11-26T23:07:43Z,accepted,,This is about /var/vcap/sys/log read more about logrotate -- based on size? ,1.0,83519092,story,[],logrotate should be configured to run at least once an hour,344.0,"[344, 206015]",956238,81882,feature,2015-01-09T02:02:20Z,https://www.pivotaltracker.com/story/show/83519092
2015-01-13T19:39:54Z,2014-12-19T00:12:55Z,accepted,,"need to access metadata service to get full networking configuration
see http://www.vmwareadmins.com/openstack-cloud-init-contact-ping-169-254-169-254-establish-meta-data-connection-fix/ ?
1 day of struggle",1.0,84893322,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",investigate how agent can support accessing metadata service over http when default nic does not have ip address associated with it because it's not configured via dhcp,1348450.0,[1348450],956238,81882,feature,2015-01-30T22:40:35Z,https://www.pivotaltracker.com/story/show/84893322
2015-01-13T20:06:40Z,2014-09-25T07:05:14Z,accepted,,"use case: deas need large ephemeral space but want to use instance types that do not come with large enough instance storage.

expected that:
- type should default to standard (aws default)
- if volume creation fails (in cpi's #create_vm), created vm should be properly deleted
- ephemeral disk should be deleted upon instance termination (deleteOnTermination flag?)
- disk should be placed into instance's availability zone
- for vms that have implicit instance storage (via its instance type) and user specifies ephemeral_disk, implicit instance storage should not be used.

example (acceptance configuration):

```
resource_pools:
- name: deas
  stemcell: {name: bosh-aws-xen-go_agent, version: 2719}
  cloud_properties:
    availability_zone: us-east1
    instance_type: m1.medium
    ephemeral_disk:                              # always asks for separate ebs disk, which becomes /dev/sdb
       type: standard | gp2                          # existing valid disk types
       size: 819_200                               # in mb, consistent with all other disk sizes in mb
```

--------------------------
Alternatives were

```
resource_pools:
- name: deas
  stemcell: {name: bosh-aws-xen-go_agent, version: 2719}
  cloud_properties:
    availability_zone: us-east1
    instance_type: m1.medium
    root_disk:
      size: 819_000 # in mb, similar to root disk size in flavor configuration, the rest will be partitioned by agent as ephemeral disk if there is no ephemeral; cpi does not request ephemeral0 slot
```

test with:
- instance types that do not have instance storage
- (extra-)large instance types
- micro instance types

--------------------------
future enhancements (outside of the scope for this story):

```
resource_pools:
- name: deas
  stemcell: {name: bosh-aws-xen-go_agent, version: 2719}
  cloud_properties:
    availability_zone: us-east1
    instance_type: m1.medium
    ephemeral_disk:
       backing: [instance, ebs]              # <------------------ future: backing's to try in that order?
       type: standard | gp2
       size: 1024
```",4.0,79502486,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",user should be able to request ebs backed ephemeral storage for any aws instance type,553935.0,"[553935, 687691, 1488968]",956238,81882,feature,2015-01-13T20:06:41Z,https://www.pivotaltracker.com/story/show/79502486
2015-01-13T20:07:01Z,2015-01-09T02:08:11Z,accepted,,54 network is setup much better and we do not loose connections to slave1-aws and slave4-aws,,85753944,story,[],Swicth slave2-aws and slave3-aws to be on 54 network,553935.0,[553935],956238,553935,chore,2015-01-13T20:07:29Z,https://www.pivotaltracker.com/story/show/85753944
2015-01-13T20:08:15Z,2014-10-08T22:55:10Z,accepted,,"diego team saw that /var/log directory gets filled up with application logs really quickly. 

",2.0,80342190,story,[],reconfigure syslog such that /var/log/ directory is logrotated and does not fill up root partition,344.0,"[344, 1488968, 206015]",956238,81882,feature,2015-01-13T20:08:15Z,https://www.pivotaltracker.com/story/show/80342190
2015-01-14T00:39:31Z,2015-01-12T17:19:22Z,accepted,,,,85919194,story,[],Get Mehran onboarded,553935.0,[553935],956238,553935,chore,2015-01-14T00:39:58Z,https://www.pivotaltracker.com/story/show/85919194
2015-01-14T02:05:53Z,2015-01-09T22:07:06Z,accepted,,what's with this file: https://github.com/cloudfoundry/bosh/blob/master/CONTRIBUTING.md. feel free to create docs/ folder in the root.,,85818544,story,[],Verify BATS Readme is up to date ,553935.0,[553935],956238,344,chore,2015-01-14T02:06:21Z,https://www.pivotaltracker.com/story/show/85818544
2015-01-14T22:39:02Z,2015-01-14T00:59:30Z,accepted,,Nebula keeps failing with getaddrinfo,,86073700,story,[],Switch OpenStack dynamic BATs to run on mirantis,553935.0,[553935],956238,553935,chore,2015-01-14T22:39:34Z,https://www.pivotaltracker.com/story/show/86073700
2015-01-16T19:00:17Z,2015-01-12T17:28:55Z,accepted,,,,85920414,story,[],Get Corey onboarded,553935.0,[553935],956238,553935,chore,2015-01-16T19:00:57Z,https://www.pivotaltracker.com/story/show/85920414
2015-01-17T01:25:09Z,2015-01-14T18:54:02Z,accepted,,https://github.com/cloudfoundry/bosh/pull/726,0.0,86138616,story,[],PR 'More explanation about OS image files',553935.0,"[553935, 1550486]",956238,81882,feature,2015-01-17T01:25:10Z,https://www.pivotaltracker.com/story/show/86138616
2015-01-17T01:25:29Z,2014-11-21T23:59:27Z,accepted,,"bosh unit tests failing `./spec/integration/promotion_spec.rb` on Travis

```
RuntimeError:
       Failed executing 'git commit -m 'initial commit''
       STDOUT: '', 
       STDERR: '
       *** Please tell me who you are.
       
       Run
       
         git config --global user.email ""you@example.com""
         git config --global user.name ""Your Name""
       
       to set your account's default identity.
       Omit --global to set the identity only in this repository.
       
       fatal: empty ident name (for <travis@localhost.localdomain>) not allowed
```

Easiest fix would be to use run the two config commands without --global after creating the local git repo in promotion_spec.rb

https://github.com/cloudfoundry/bosh/issues/700
https://travis-ci.org/cloudfoundry/bosh/jobs/41606159

Apparently we fixed this for Docker/GoCD by putting global defaults in the Dockerfile, but this didn't fix Travis.",,83247218,story,[],Travis needs git user/email,553935.0,"[553935, 1550486]",956238,1266616,bug,2015-01-17T01:25:30Z,https://www.pivotaltracker.com/story/show/83247218
2015-01-21T20:24:46Z,2014-10-15T18:46:54Z,accepted,,,2.0,80764862,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'disk-pools1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-04T19:40:34Z', 'id': 9111434, 'updated_at': '2014-08-18T03:50:54Z'}]",user should be able to use ebs volume encryption via disk pool cloud properties,344.0,[344],956238,81882,feature,2015-01-21T20:24:48Z,https://www.pivotaltracker.com/story/show/80764862
2015-01-21T23:17:35Z,2014-12-19T00:07:52Z,accepted,,"```
if cpi provides ephemeral disk:
  if disk exists:
    -> ok
  if disk does not exist:
    if agent should use root disk:
       if there is enough space on the root disk:
         -> ok
       else
         -> fail
    if agent cannot use root disk:
      -> fail

if cpi does not provide ephemeral disk
  if agent should use root disk:
     if there is enough space on the root disk:
       -> ok
     else
       -> fail
  if agent cannot use root disk:
    -> fail
```",2.0,84893144,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should fail bootstrapping if ephemeral storage cannot be found,553935.0,"[553935, 687691]",956238,81882,feature,2015-01-21T23:17:36Z,https://www.pivotaltracker.com/story/show/84893144
2015-01-24T00:07:11Z,2015-01-22T19:18:37Z,accepted,,looks like current tenant is broken,,86731316,story,[],Recreate ci-centos-manual tenant on Mirantis,553935.0,"[553935, 687691]",956238,553935,chore,2015-01-24T00:07:11Z,https://www.pivotaltracker.com/story/show/86731316
2015-01-25T20:00:00Z,2014-12-25T03:56:39Z,accepted,,"e.g. 

```
      - host: xxx.x.x.x
        user: ---
        password: ---
        datacenters:
          - name: BOSH_DC
            vm_folder:       some-folder/fold2/SYSTEM_MICRO_VSPHERE_dmitriy2_VMs #<-------
            template_folder: some-folder2/fold/SYSTEM_MICRO_VSPHERE_dmitriy2_Templates #<-------
            disk_path:       SYSTEM_MICRO_VSPHERE_Disks
            datastore_pattern:            jalapeno
            persistent_datastore_pattern: jalapeno
            allow_mixed_datastores: true
            clusters:
              - BOSH_CL
```

resulted in 

```
  Started deploy micro bosh > Creating VM from sc-7b2d16fd-9c2d-45cf-947a-93d5b5af3532/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_vsphere_cpi-1.2801.0/lib/cloud/vsphere/vm_creator.rb:23:in `create': Could not find stemcell: sc-7b2d16fd-9c2d-45cf-947a-93d5b5af3532 (RuntimeError)
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_vsphere_cpi-1.2801.0/lib/cloud/vsphere/cloud.rb:201:in `block in create_vm'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_common-1.2801.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_vsphere_cpi-1.2801.0/lib/cloud/vsphere/cloud.rb:191:in `create_vm'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:243:in `create_vm'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:123:in `block in create'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:122:in `create'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli_plugin_micro-1.2801.0/lib/bosh/cli/commands/micro.rb:179:in `perform'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2801.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2801.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2801.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2801.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
```",2.0,85137994,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere cpi should create vms and templates folders that are ***nested more than 1 level***,553935.0,"[553935, 1550486]",956238,81882,feature,2015-01-28T00:00:16Z,https://www.pivotaltracker.com/story/show/85137994
2015-01-27T19:40:55Z,2015-01-07T20:39:26Z,accepted,,,1.0,85649286,story,[],do not ask for user confirmation when uploading a release,687691.0,[687691],956238,81882,feature,2015-01-27T19:40:56Z,https://www.pivotaltracker.com/story/show/85649286
2015-01-27T19:55:12Z,2015-01-20T17:51:54Z,accepted,,"ubuntu: openssl: 1.0.1f-1ubuntu2.8
centos: do we have a new version?
",0.0,86540636,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump os images for ubuntu and centos for USN-2459-1,553935.0,"[553935, 344]",956238,81882,feature,2015-01-27T19:55:12Z,https://www.pivotaltracker.com/story/show/86540636
2015-01-27T22:11:24Z,2015-01-20T01:09:30Z,accepted,,"To reproduce:

1. Create release with errand that does not require any packages.
2. Create a manifest that has only enough available IPs to run an errand.
3. Deploy.
4. Run the errand. (the errand will fail to recreate the vm because there will be no available IPs)

```
Director task 4
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started binding instance vms > fake-errand-name/0. Done (00:00:00)

  Started updating job fake-errand-name > fake-errand-name/0 (canary). Done (00:00:05)

  Started running errand > fake-errand-name/0. Done (00:00:01)

  Started fetching logs for fake-errand-name/0 > Finding and packing log files. Done (00:00:01)

  Started deleting errand instances fake-errand-name > 85638. Done (00:00:00)

Error 130010:  asked for a dynamic IP but there were no more available
```

Note: If errand requires compilation it seems that compilation vm also does not release IP.",,86477758,story,[],Bosh should release network reservation when recreating vm after running errand,553935.0,"[553935, 344]",956238,344,bug,2015-01-27T22:11:25Z,https://www.pivotaltracker.com/story/show/86477758
2015-01-28T01:42:11Z,2015-01-12T19:46:37Z,accepted,,"ask @dk for more logs

it seems that clean up of errand is not succeeding. ive seen this before on other infrastructures

```
Director Version : 1.2690.5.0
...snip...
D, [2015-01-07T13:47:11.284883 #26334] [task:821] DEBUG -- : Deleted lock: lock:deployment:app-autoscaling-dbf0dcd71112600cbc08
E, [2015-01-07T13:47:11.285065 #26334] [task:821] ERROR -- : Resource pool `deploy-autoscaling' does not contain an allocated VM with the cid `urn:vcloud:vm:9a4703f9-cb26-4840-9df7-c12aed743b1e'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/deployment_plan/resource_pool.rb:152:in `deallocate_vm'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/errand/job_manager.rb:56:in `block in deallocate_vms'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/errand/job_manager.rb:56:in `each'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/errand/job_manager.rb:56:in `deallocate_vms'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/errand/job_manager.rb:46:in `delete_instances'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/jobs/run_errand.rb:101:in `delete_instances'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/jobs/run_errand.rb:81:in `with_updated_instances'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/jobs/run_errand.rb:56:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/jobs/run_errand.rb:55:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/job_runner.rb:104:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh_common-1.2690.5.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.5.0/bin/bosh-director-worker:78:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,85936794,story,[],investigate issue with running an errand,344.0,"[344, 553935]",956238,81882,bug,2015-01-28T03:03:19Z,https://www.pivotaltracker.com/story/show/85936794
2015-01-28T17:46:19Z,2015-01-26T22:50:34Z,accepted,,https://github.com/cloudfoundry/bosh/pull/734,1.0,86960650,story,[],PR 'Update default mysql engine to 5.5.40a for AWS RDS',553935.0,[553935],956238,81882,feature,2015-01-28T17:46:19Z,https://www.pivotaltracker.com/story/show/86960650
2015-01-28T17:56:26Z,2015-01-14T18:51:19Z,accepted,,,,86138176,story,[],Clean /var/vcap/data at the end of stemcell building (micro building stage),553935.0,[553935],956238,553935,bug,2015-01-28T17:56:26Z,https://www.pivotaltracker.com/story/show/86138176
2015-01-28T17:56:29Z,2015-01-07T19:55:37Z,accepted,,https://github.com/cloudfoundry/bosh/pull/725,2.0,85645682,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Allow openstack configurations that have a single storage AZ but multiple compute AZ's',553935.0,"[553935, 344, 1246128]",956238,81882,feature,2015-01-28T17:56:51Z,https://www.pivotaltracker.com/story/show/85645682
2015-01-28T22:26:54Z,2015-01-12T17:29:19Z,accepted,,make sure bosh upload release works with bosh.io releases,2.0,85920454,story,[],bosh upload stemcell & bosh upload release should follow redirects,1348450.0,[1348450],956238,81882,feature,2015-01-28T22:26:54Z,https://www.pivotaltracker.com/story/show/85920454
2015-01-31T02:51:50Z,2015-01-27T18:12:38Z,accepted,,"- this does not affect ubuntu trusty: http://www.ubuntu.com/usn/usn-2485-1/
- figure out how if this needs to be fixed for centos 6.6
",1.0,87029068,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",bump images for centos for CVE-2015-0235,1550486.0,"[1550486, 344]",956238,81882,feature,2015-01-31T03:38:08Z,https://www.pivotaltracker.com/story/show/87029068
2015-01-31T03:07:06Z,2015-01-30T19:36:12Z,accepted,,"bosh-stemcell/lib/bosh/stemcell/builder_options.rb used to hardcode bosh- and prepend to all stemcell names.
Now only the name bosh-stemcell/lib/bosh/stemcell/definition.rb generates is used.

We need them to start with bosh- again.

Broken in 2827. Was working in 2824.",,87321366,story,[],Stemcell names (in manifest) should start with bosh-,344.0,[344],956238,1266616,bug,2015-01-31T03:07:13Z,https://www.pivotaltracker.com/story/show/87321366
2015-01-31T04:08:16Z,2015-01-22T21:10:43Z,accepted,,"As an OpenStack CF Admin, I should be able to leverage Fog's availability zone functionality.   Both the openstack CPI and fog supports availability zone but the problem is that Bosh still uses fog v1.23 and availability zones were introduced later in fog v1.27:

Fog:  https://github.com/fog/fog/blob/27eec8700f4b6b990442bc9715dc27edc6b47a66/lib/fog/openstack/requests/compute/create_volume.rb#L14

Bosh: https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb#L416",2.0,86742104,story,[],Update Fog in Openstack CPI to v1.27,553935.0,[553935],956238,1210852,feature,2015-01-31T04:08:17Z,https://www.pivotaltracker.com/story/show/86742104
2015-02-02T22:25:11Z,2015-02-02T18:51:09Z,accepted,,,,87445784,story,[],Get Jonathan onboarded,553935.0,[553935],956238,553935,chore,2015-02-02T22:25:11Z,https://www.pivotaltracker.com/story/show/87445784
2015-02-03T01:45:48Z,2015-01-22T17:18:30Z,accepted,,Currently running lifecycle tests fails because Mirantis fails to delete volumes at the end of lifecycle. Those volumes get into weird state so we cannot delete them from the console. We have an open ticket with them and they suggested to do some debugging with nova client.,,86719600,story,[],Migrate Openstack lifecycle tests to Mirantis,1348450.0,[1348450],956238,553935,chore,2015-02-03T01:45:49Z,https://www.pivotaltracker.com/story/show/86719600
2015-02-03T23:06:18Z,2015-02-02T18:23:56Z,accepted,,**BLOCKED**: awaiting response from support team,,87442342,story,[],setup logrotate on minrantis dcs,119.0,"[119, 553935]",956238,81882,chore,2015-02-03T23:06:19Z,https://www.pivotaltracker.com/story/show/87442342
2015-02-03T23:58:18Z,2015-01-21T22:59:45Z,accepted,,"When running agent tests there is an assertion about the default network.

To reproduce connect ethernet to different network (display)

@karlisenberg can include the stack trace",,86662662,story,[],Agent unit tests should not depend on the network setup,553935.0,"[553935, 119]",956238,553935,chore,2015-02-03T23:58:18Z,https://www.pivotaltracker.com/story/show/86662662
2015-02-04T18:07:24Z,2015-01-28T00:47:12Z,accepted,,It is not the way we configure drs rules. They are getting configured on resource pool.,,87059914,story,[],remove validation for drs_rules config in vsphere cpi code,553935.0,[553935],956238,553935,chore,2015-02-04T18:07:21Z,https://www.pivotaltracker.com/story/show/87059914
2015-02-04T18:44:56Z,2015-01-27T20:24:36Z,accepted,,,,87041424,story,[],remove bosh-warden-cpi from the main bosh repo,344.0,[344],956238,81882,chore,2015-02-11T02:36:32Z,https://www.pivotaltracker.com/story/show/87041424
2015-02-04T20:05:45Z,2015-01-27T00:48:09Z,accepted,,"bosh-lite on magellan

```
$ bosh deployments

+-----------------+-------------------+-------------------------------------------------+
| Name            | Release(s)        | Stemcell(s)                                     |
+-----------------+-------------------+-------------------------------------------------+
| cf-warden       | cf/194            | bosh-warden-boshlite-ubuntu-trusty-go_agent/389 |
|                 | cf/195            |                                                 |
+-----------------+-------------------+-------------------------------------------------+
| cf-warden-mysql | cf-mysql/16+dev.3 | bosh-warden-boshlite-ubuntu-trusty-go_agent/389 |
+-----------------+-------------------+-------------------------------------------------+

$ bosh releases

+----------+-----------+-------------+
| Name     | Versions  | Commit Hash |
+----------+-----------+-------------+
| cf       | 194*      | d2466074+   |
|          | 195*      | 1b7bdc4d+   |
| cf-mysql | 16+dev.2  | 09799748    |
|          | 16+dev.3* | 44471d63    |
+----------+-----------+-------------+
(*) Currently deployed
(+) Uncommitted changes

Releases total: 2
```",0.0,86969072,story,[],investigate magellan cf deploy: bosh reports two versions of cf-release are deployed,553935.0,[553935],956238,58676,feature,2015-02-04T20:05:44Z,https://www.pivotaltracker.com/story/show/86969072
2015-02-04T21:50:46Z,2015-01-16T18:54:33Z,accepted,,"Code is here: https://github.com/pivotal-cf-experimental/boshng
use minimal cf manifest that dk emailed
up to a day",1.0,86329700,story,[],Spike: set up minimal cf using toy bosh dependency resolution,1348450.0,[1348450],956238,81882,feature,2015-02-04T21:50:44Z,https://www.pivotaltracker.com/story/show/86329700
2015-02-10T22:45:52Z,2015-01-14T19:17:00Z,accepted,,"agent currently restarts when it sees that sv start monit fails. it should just re-run that command until 10 times and wait 1 second in between:

```
2015-01-14_18:04:45.92576 [File System] 2015/01/14 18:04:45 DEBUG - Symlinking oldPath /etc/sv/monit with newPath /etc/service/monit
2015-01-14_18:04:45.92577 [File System] 2015/01/14 18:04:45 DEBUG - Checking if file exists /etc/service
2015-01-14_18:04:45.92587 [Cmd Runner] 2015/01/14 18:04:45 DEBUG - Running command: sv start monit
2015-01-14_18:04:45.93823 [Cmd Runner] 2015/01/14 18:04:45 DEBUG - Stdout: warning: monit: unable to open supervise/ok: file does not exist
2015-01-14_18:04:45.93824 [Cmd Runner] 2015/01/14 18:04:45 DEBUG - Stderr: 
2015-01-14_18:04:45.93825 [Cmd Runner] 2015/01/14 18:04:45 DEBUG - Successful: false (1)
2015-01-14_18:04:45.93825 [main] 2015/01/14 18:04:45 ERROR - App setup Running bootstrap: Starting monit: Shelling out to sv: Running command: 'sv start monit', stdout: 'warning: monit: unable to open supervise/ok: file does not exist
2015-01-14_18:04:45.93825 ', stderr: '': exit status 1
2015-01-14_18:04:45.96747 [main] 2015/01/14 18:04:45 DEBUG - Starting agent
```

success (from the restart of the agent):
```
2015-01-14_18:04:56.64577 [Cmd Runner] 2015/01/14 18:04:56 DEBUG - Running command: sv start monit
2015-01-14_18:04:57.06800 [Cmd Runner] 2015/01/14 18:04:57 DEBUG - Stdout: ok: run: monit: (pid 1540) 1s, normally down
2015-01-14_18:04:57.06803 [Cmd Runner] 2015/01/14 18:04:57 DEBUG - Stderr:
```",2.0,86141174,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh-agent should be more permissive on how it starts up monit,553935.0,"[553935, 119]",956238,553935,feature,2015-02-10T22:45:42Z,https://www.pivotaltracker.com/story/show/86141174
2015-02-10T22:49:03Z,2014-12-22T22:06:04Z,accepted,,"- add a build that produces raw openstack stemcell
- add -raw in the name and make sure that stemcell.MF contains correct `disk_format: raw`
- stemcell.MF has name in two places",4.0,85045340,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",user should be able to download raw (disk format) openstack stemcell,687691.0,"[687691, 553935]",956238,81882,feature,2015-02-10T22:48:54Z,https://www.pivotaltracker.com/story/show/85045340
2015-02-11T19:18:55Z,2015-02-11T18:53:28Z,accepted,,Make sure CI AWS slaves have the same version we have locally. See https://github.com/cloudfoundry/internal-docs/blob/master/bosh/ci/aws_slaves.md,,88169124,story,[],Fix Berkshelf local diffs in bosh-stemcell,687691.0,[687691],956238,687691,chore,2015-02-11T19:18:56Z,https://www.pivotaltracker.com/story/show/88169124
2015-02-12T01:29:06Z,2015-02-09T18:05:19Z,accepted,,,1.0,87961378,story,[],bump ubuntu & centos stemcells,344.0,[344],956238,81882,feature,2015-02-12T01:29:07Z,https://www.pivotaltracker.com/story/show/87961378
2015-02-12T18:55:02Z,2015-02-11T22:44:06Z,accepted,,,,88191822,story,[],Remove all traces of lucid and centos 6.5 from stemcell building,687691.0,[687691],956238,687691,chore,2015-02-13T21:20:15Z,https://www.pivotaltracker.com/story/show/88191822
2015-02-12T23:41:37Z,2015-02-05T00:29:33Z,accepted,,see dk for specific ports,2.0,87679284,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh-agent should listen for syslog and monit notifs on localhost instead of *,344.0,[344],956238,81882,feature,2015-02-12T23:41:37Z,https://www.pivotaltracker.com/story/show/87679284
2015-02-13T01:33:59Z,2015-02-12T20:10:42Z,accepted,,so we don't have to go dig into the bucket to update the readme and json,,88269746,story,[],ci:publish_os_image_in_vm should print the new s3 version,687691.0,[687691],956238,687691,chore,2015-02-13T21:20:20Z,https://www.pivotaltracker.com/story/show/88269746
2015-02-13T22:44:11Z,2015-02-09T18:16:42Z,accepted,,,0.0,87962388,story,[],stemcells should have localhost resolvable to 127.0.0.1 without dns,1348450.0,[1348450],956238,553935,feature,2015-02-14T00:15:20Z,https://www.pivotaltracker.com/story/show/87962388
2015-02-13T23:50:50Z,2015-02-05T00:40:10Z,accepted,,,1.0,87680054,story,"[{'name': 'harden stemcells', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-01-13T08:17:27Z', 'id': 10469328, 'updated_at': '2015-01-13T08:17:27Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",disable postfix service on ubuntu stemcells ,687691.0,[687691],956238,81882,feature,2015-02-13T23:50:51Z,https://www.pivotaltracker.com/story/show/87680054
2015-02-13T23:51:29Z,2014-09-08T18:21:29Z,accepted,,"See https://www.pivotaltracker.com/story/show/78402580, the investigation story.",4.0,78402176,story,[],agent should use configuration file instead of -I to configure agent,553935.0,[553935],956238,81882,feature,2015-02-20T03:36:22Z,https://www.pivotaltracker.com/story/show/78402176
2015-02-14T00:28:36Z,2015-02-10T17:50:21Z,accepted,,"from https://groups.google.com/a/cloudfoundry.org/forum/#!topic/vcap-dev/eYc139-tai0:

The SSH server is configured to support Cipher Block Chaining (CBC) encryption. This may allow an attacker to recover the plaintext message from the ciphertext. 
The SSH server is configured to allow either MD5 or 96-bit MAC algorithms, both of which are considered weak.",2.0,88065724,story,"[{'name': 'harden stemcells', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-01-13T08:17:27Z', 'id': 10469328, 'updated_at': '2015-01-13T08:17:27Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",harden default sshd config,687691.0,[687691],956238,81882,feature,2015-02-14T00:28:53Z,https://www.pivotaltracker.com/story/show/88065724
2015-02-15T20:00:00Z,2015-01-23T00:06:28Z,accepted,,,2.0,86755066,story,[],Release build artifacts should be cached outside of the Release source tree.,553935.0,[553935],956238,1348450,feature,2015-02-17T17:17:13Z,https://www.pivotaltracker.com/story/show/86755066
2015-02-15T20:00:00Z,2014-12-29T20:44:22Z,accepted,,"looks like it is only reproducible on bluebox. @dk runtime confirmed that enabling serial logging in grub.conf and adding the following kernel parameters fixes console log issue on bluebox.

http://docs.openstack.org/admin-guide-cloud/content/section_compute-empty-log-output.html",2.0,85236116,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",Enable console logs on openstack for bluebox openstack,344.0,[344],956238,81882,feature,2015-02-17T22:15:56Z,https://www.pivotaltracker.com/story/show/85236116
2015-02-17T18:08:33Z,2015-02-17T17:19:16Z,accepted,,,,88531732,story,[],Get Marco onboarded,553935.0,[553935],956238,553935,chore,2015-02-17T18:09:07Z,https://www.pivotaltracker.com/story/show/88531732
2015-02-17T19:03:20Z,2014-10-21T19:14:10Z,accepted,,"Investigate flakey root device mounting on AWS (see email support thread)
[Case 1248495831] New instances that we could not SSH or ping

try to reproduce it if you can
",1.0,81134820,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",investigation of stemcells not starting in auto-deploy builds on aws,,[],956238,81882,feature,2015-02-17T20:14:48Z,https://www.pivotaltracker.com/story/show/81134820
2015-02-19T17:45:20Z,2015-02-10T18:04:52Z,accepted,,"- audit which paths are touched by the agent's compile_package
- make sure /var/vcap/data/compile is cleaned up
- confirm with new bosh-micro-cli",2.0,88067602,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should clean up compilation artifacts before and after compiling,687691.0,[687691],956238,81882,feature,2015-02-19T17:45:20Z,https://www.pivotaltracker.com/story/show/88067602
2015-02-19T19:02:55Z,2014-10-09T19:15:59Z,accepted,,"(pair with Maria to find out a bunch about vsphere cpi)

Disk model fields:
- uuid (cid)
- size
- datacenter
- datastore

Size is saved during create_disk and used by attach_disk to lazily create the disk.

Datacenter is stored on the disk, but only one datacenter is supported (globally configured).

Datastore is globally configured as a pattern, and the cpi determines which datastore to use based on an algorithm in `vsphere/resources.place` when creating the VM in `vm_creator.create`. So it is not always consistent.

Proposed Solution:
- Use a lookup vsphere API call to find the disk by id, instead of exact path.
- Check all datastores that match the specified (in manifest) datastore pattern.
- Move creation of disk from attach_disk to create_disk, using the vm_cid to determine locality.

Note:
- Compare speed of DB usage vs speed without DB usage",1.0,80405524,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",investigate vsphere cpi to not use database,553935.0,[553935],956238,81882,feature,2015-02-19T19:02:27Z,https://www.pivotaltracker.com/story/show/80405524
2015-02-20T03:25:41Z,2015-02-18T22:15:13Z,accepted,,,0.0,88658048,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",bump ubuntu os image,553935.0,[553935],956238,81882,feature,2015-02-20T03:26:27Z,https://www.pivotaltracker.com/story/show/88658048
2015-02-20T17:35:45Z,2015-02-20T01:40:46Z,accepted,,"Ubuntu 14.04.1 LTS
Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
Last login: Thu Feb 19 20:17:18 2015 from 209.234.137.222
vcap@417c0021-7868-479a-754f-3a22ce693dd8:~$ sudo us
[sudo] password for vcap:
sudo: us: command not found
vcap@417c0021-7868-479a-754f-3a22ce693dd8:~$ sudo su
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# ls -la /sbin/
Display all 159 possibilities? (y or n)
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# ls -la /sbin/dhclient
dhclient         dhclient-script
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# ls -la /sbin/dhclient
-rwxr-xr-x 1 root root 1668160 Apr  7  2014 /sbin/dhclient
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# dhclient3
bash: dhclient3: command not found
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# ls -la /etc/dhcp/dhclient.conf
-rw-r--r-- 1 root root 361 Feb 19 06:29 /etc/dhcp/dhclient.conf
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap# ifup --version
ifup version 0.7.47.2ubuntu4.1
Copyright (c) 1999-2009 Anthony Towns
Copyright (c) 2010-2013 Andrew Shadura

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.
root@417c0021-7868-479a-754f-3a22ce693dd8:/home/vcap#",,88756674,story,[],remove unnecessary conditiionals in ubuntu net manager,687691.0,[687691],956238,81882,chore,2015-02-20T17:35:45Z,https://www.pivotaltracker.com/story/show/88756674
2015-02-20T20:17:22Z,2014-12-17T22:44:18Z,accepted,,https://github.com/cloudfoundry/bosh/pull/708,4.0,84810956,story,[],PR 'Add code and tests to include master LICENSE file in root of release',553935.0,"[553935, 1550486]",956238,81882,feature,2015-02-20T20:19:48Z,https://www.pivotaltracker.com/story/show/84810956
2015-02-20T20:23:16Z,2015-02-17T17:17:57Z,accepted,,,,88531576,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]","PR 'Fix #753: undefined local variable or method ""name"" in archive_repository.rb'",553935.0,[553935],956238,81882,bug,2015-02-20T20:23:17Z,https://www.pivotaltracker.com/story/show/88531576
2015-02-23T18:06:52Z,2015-02-20T18:10:03Z,accepted,,,,88804126,story,[],investigate failing upload of env.json,553935.0,"[553935, 81882]",956238,553935,chore,2015-02-23T18:06:53Z,https://www.pivotaltracker.com/story/show/88804126
2015-02-24T19:39:07Z,2015-01-28T23:23:57Z,accepted,,timebox 1 day,4.0,87160666,story,[],Faster bosh CLI startup [was: look for plugins in a different way to speed bosh-cli when a lot of gems are installed],119.0,[119],956238,81882,feature,2015-02-27T02:35:59Z,https://www.pivotaltracker.com/story/show/87160666
2015-02-26T20:13:42Z,2015-02-23T19:37:04Z,accepted,,"mysql's text is 2^16, should be using larger text",,88934748,story,[],user should be able to deploy very long (> 2^16) manifest to the director and run errands,553935.0,[553935],956238,81882,bug,2015-02-26T20:14:12Z,https://www.pivotaltracker.com/story/show/88934748
2015-02-26T20:14:51Z,2015-02-24T18:03:14Z,accepted,,,0.0,89023378,story,[],Audit other database fields for text type,553935.0,[553935],956238,553935,feature,2015-02-26T20:15:00Z,https://www.pivotaltracker.com/story/show/89023378
2015-02-27T00:22:34Z,2015-02-26T18:16:17Z,accepted,,,,89217448,story,[],Remove lucid assets,553935.0,"[553935, 687691]",956238,553935,chore,2015-02-27T00:22:49Z,https://www.pivotaltracker.com/story/show/89217448
2015-02-27T20:15:36Z,2015-02-20T20:26:15Z,accepted,,,0.0,88815496,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty to 14.04.2,553935.0,"[553935, 687691]",956238,81882,feature,2015-02-27T20:15:36Z,https://www.pivotaltracker.com/story/show/88815496
2015-02-28T00:38:51Z,2015-02-26T22:39:03Z,accepted,,"If you have two keys in your agent and ~/.ssh/id_rsa exists all of your tries are used up before you get prompted for a password:

```
debug1: Authentications that can continue: publickey,password
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /Users/pivotal/.ssh/id_rsa_bosh
debug1: Authentications that can continue: publickey,password
debug1: Offering RSA public key: /Volumes/mmb/id_rsa
debug1: Authentications that can continue: publickey,password
debug1: Offering RSA public key: /Users/pivotal/.ssh/id_rsa
Received disconnect from 10.85.7.98: 2: Too many authentication failures for vcap
```

This happens on vSphere where we use password auth.",,89242880,story,[],Can't login to stemcell if have you a few keys in your ssh-agent.,81882.0,[81882],956238,756869,bug,2015-04-13T22:30:45Z,https://www.pivotaltracker.com/story/show/89242880
2015-03-04T19:30:03Z,2015-03-04T17:42:25Z,accepted,,"- http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2015-0204
- https://gist.github.com/martinseener/d50473228719a9554e6a",0.0,89629096,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",update bosh/stemcells for freak' openssl,553935.0,"[553935, 344]",956238,81882,feature,2015-03-04T19:30:03Z,https://www.pivotaltracker.com/story/show/89629096
2015-03-04T20:18:17Z,2015-03-02T22:17:55Z,accepted,,"this is due to the inclusion of the bosh-dev and bosh-stemcell's .gemspec as the bosh projects direct dependencies.   

Why?

do we need those 2 library sets to be gems or can they just be moved directly into the parent project?",,89461946,story,[],Fix Your Gemfile lists the gem <xxx> more than once. errors,119.0,[119],956238,119,chore,2015-03-04T20:18:35Z,https://www.pivotaltracker.com/story/show/89461946
2015-03-06T21:32:44Z,2015-02-19T01:47:38Z,accepted,,"push to branch until #88722984
- lifecycle test",,88670576,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",Investigate if DRS works with vsphere cpi since it is not using DB,553935.0,"[553935, 119]",956238,553935,chore,2015-03-06T21:33:02Z,https://www.pivotaltracker.com/story/show/88670576
2015-03-11T18:14:39Z,2015-03-04T20:22:29Z,accepted,,no need to leak password length information,0.0,89645148,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",Don't echo '*' as password characters for `bosh login`,687691.0,[687691],956238,687691,feature,2015-03-11T18:14:41Z,https://www.pivotaltracker.com/story/show/89645148
2015-03-11T18:56:41Z,2014-10-15T18:51:38Z,accepted,,currently health monitor crashes when it fails to get deployments info from the director.,2.0,80765224,story,"[{'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",health monitor process should keep on running even if it cannot authenticate with director,553935.0,[553935],956238,81882,feature,2015-03-11T18:56:53Z,https://www.pivotaltracker.com/story/show/80765224
2015-03-11T19:01:27Z,2015-03-09T23:57:46Z,accepted,,see issue and send pr toward: https://github.com/pivotal-sprout/sprout-jetbrains-editors/issues/1 ,,89970708,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}, {'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",Get Idea to run against java 8,119.0,[119],956238,119,chore,2015-03-11T19:01:28Z,https://www.pivotaltracker.com/story/show/89970708
2015-03-11T19:01:34Z,2015-03-07T02:09:00Z,accepted,,,,89842908,story,"[{'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",Update sproutwrap for bosh team,119.0,"[119, 1612190]",956238,119,chore,2015-03-11T19:01:35Z,https://www.pivotaltracker.com/story/show/89842908
2015-03-11T19:06:56Z,2015-03-06T20:03:16Z,accepted,,,,89822466,story,"[{'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",nicer RubyMine setup,1348450.0,[1348450],956238,1348450,chore,2015-03-11T19:06:57Z,https://www.pivotaltracker.com/story/show/89822466
2015-03-11T21:19:29Z,2015-03-11T21:19:27Z,accepted,,,,90143904,story,[],better boshcfg -- autogen manifests,1348450.0,[1348450],956238,1348450,chore,2015-03-11T21:19:30Z,https://www.pivotaltracker.com/story/show/90143904
2015-03-12T23:01:47Z,2015-03-09T21:38:34Z,accepted,,"we currently require go v1.3.3 to be installed on the workstations but should be using the latest go. 

Figure out why we currently need 1.3.3 and upgrade the various tools to use the latest version

Things to look at are the stemcell-builder, specs, ci

**Note**: there is a story #89971052 to sprout go v1.3.3 If that story was completed before this then we should also undo that in our sprout-wrap so that we start installing the latest go version",,89961234,story,"[{'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",upgrade go to latest stable go,119.0,[119],956238,119,chore,2015-03-12T23:01:47Z,https://www.pivotaltracker.com/story/show/89961234
2015-03-12T23:03:38Z,2015-03-12T00:36:32Z,accepted,,"it should be an alias to 'duet-commit -v'

the sprout-git::aliases recipe supports this feature",,90156260,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}, {'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",have git ci show changes in the commit window,119.0,[119],956238,119,chore,2015-03-12T23:03:38Z,https://www.pivotaltracker.com/story/show/90156260
2015-03-12T23:10:11Z,2015-03-11T21:49:43Z,accepted,,,,90146116,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}, {'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]", install cmake via homebrew,119.0,[119],956238,119,chore,2015-03-12T23:10:12Z,https://www.pivotaltracker.com/story/show/90146116
2015-03-16T19:24:19Z,2015-03-16T19:24:17Z,accepted,,"Do not install the go plugin from jetbrains in the ide. 
Download  https://github.com/go-lang-plugin-org/go-lang-idea-plugin/releases/download/0.9.16-alpha.9/google-go-language.jar
Install plugin from disk. 

Add the Go SDK to the project settings  -  /usr/local/Cellar/go/1.4.2/libexec/
Add GOPATH/src GOPATH/pkg as classpaths and sourcepaths in the SDK
Add GOPATH/src as Source Root to ‘go’ module
In Project Settings -> Project -> Project SDK, select Go SDK
In Project Settings -> Module -> go -> Dependencies -> Module SDK, select Go SDK
invalidate idea cache and restart",,90443652,story,"[{'name': 'tech debt', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-06T23:18:31Z', 'id': 11048290, 'updated_at': '2015-03-09T17:17:18Z'}]",Set up go project in IDEA,119.0,[119],956238,119,chore,2015-03-20T22:40:28Z,https://www.pivotaltracker.com/story/show/90443652
2015-03-16T21:36:55Z,2015-02-26T18:09:28Z,accepted,,"- usn-2519-1
- usn-2517-1",0.0,89216658,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu os images,553935.0,"[553935, 687691, 344]",956238,81882,feature,2015-03-16T21:37:03Z,https://www.pivotaltracker.com/story/show/89216658
2015-03-17T16:38:23Z,2015-03-09T23:31:06Z,accepted,,"We write it in the config file on the stemcell and parse it into a struct in the agent, but we no longer read that value -- we look at the networks themselves to figure out whether theyre manual or dhcp",,89968898,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",remove NetworkingType configuration from the bosh-agent and stemcell building stages,687691.0,"[687691, 1602644]",956238,81882,chore,2015-03-17T16:38:22Z,https://www.pivotaltracker.com/story/show/89968898
2015-03-18T00:54:44Z,2015-02-19T01:45:52Z,accepted,,push to branch until #88722984,4.0,88670492,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere create_vm call does not use database,553935.0,"[553935, 687691]",956238,553935,feature,2015-03-18T00:54:56Z,https://www.pivotaltracker.com/story/show/88670492
2015-03-18T00:54:48Z,2015-02-24T19:27:14Z,accepted,,,1.0,89031694,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere has_disk and delete_disk are not using database,553935.0,[553935],956238,553935,feature,2015-03-18T00:55:04Z,https://www.pivotaltracker.com/story/show/89031694
2015-03-18T00:54:50Z,2015-02-19T01:45:37Z,accepted,,push to branch until #88722984,2.0,88670486,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere attach_disk call does not use database,553935.0,[553935],956238,553935,feature,2015-03-18T00:54:50Z,https://www.pivotaltracker.com/story/show/88670486
2015-03-18T01:03:51Z,2015-03-17T16:44:18Z,accepted,,Add it to gitignore?,,90525244,story,[],Remove .bundle/config from bosh root,1550486.0,[1550486],956238,553935,chore,2015-03-18T01:03:52Z,https://www.pivotaltracker.com/story/show/90525244
2015-03-19T01:02:54Z,2015-01-16T03:13:39Z,accepted,,reason is already logged. include that info in the error message.,2.0,86273644,story,"[{'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]","improve ""no available resources"" error message to state clearly which resource is in trouble ",344.0,"[344, 1348450]",956238,81882,feature,2015-03-19T01:03:22Z,https://www.pivotaltracker.com/story/show/86273644
2015-03-19T01:05:39Z,2015-02-19T01:46:24Z,accepted,,push to branch until #88722984,1.0,88670512,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",remove database from vsphere cpi completely,687691.0,"[687691, 344]",956238,553935,feature,2015-03-19T01:05:45Z,https://www.pivotaltracker.com/story/show/88670512
2015-03-20T18:03:00Z,2015-03-13T18:50:01Z,accepted,,,0.0,90307340,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]","Bootstrapper listens on some port with insecure HTTPS, accepts a tarball, and expands it into a temp dir.",1348450.0,[1348450],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307340
2015-03-20T18:03:29Z,2015-03-13T18:50:01Z,accepted,,,1.0,90307342,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper runs some executable with a well-known name within tarball extraction dir as root,206015.0,[206015],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307342
2015-03-20T22:02:17Z,2015-03-13T18:50:01Z,accepted,,,2.0,90307344,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper verifies client against provided CA.,119.0,[119],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307344
2015-03-20T22:05:40Z,2015-03-13T18:50:01Z,accepted,,,1.0,90307346,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper identifies itself with provided private key for incoming (server) connections.,119.0,[119],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307346
2015-03-20T22:05:47Z,2015-03-13T18:50:01Z,accepted,,,1.0,90307348,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper HTTPS server rejects clients not permitted by metadata config (cn restrictions?).,1348450.0,[1348450],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307348
2015-03-23T18:03:35Z,2015-03-23T17:30:28Z,accepted,,,,90927094,story,[],Get Ted onboarded,553935.0,[553935],956238,553935,chore,2015-03-23T18:03:21Z,https://www.pivotaltracker.com/story/show/90927094
2015-03-23T18:43:59Z,2015-03-12T16:34:35Z,accepted,,coverage for work done in #88722984,,90207298,story,[],Add integration coverage for VSphere::Client#find_disk,1550486.0,[1550486],956238,687691,chore,2015-03-23T18:44:00Z,https://www.pivotaltracker.com/story/show/90207298
2015-03-23T18:55:08Z,2015-03-19T17:59:06Z,accepted,,http://www.ubuntu.com/usn/usn-2537-1/,2.0,90715696,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump os images for openssl sec vulnerabilities for ubuntu,1348450.0,[1348450],956238,81882,feature,2015-03-24T15:05:23Z,https://www.pivotaltracker.com/story/show/90715696
2015-03-23T22:32:46Z,2015-03-20T20:51:54Z,accepted,,"It is ignored, the env variable overrides the default",,90814144,story,[],Fix promote artifacts to be able to promote centos-6,553935.0,"[553935, 878715]",956238,553935,chore,2015-03-23T22:32:30Z,https://www.pivotaltracker.com/story/show/90814144
2015-03-24T21:01:40Z,2015-03-23T16:28:41Z,accepted,,https://github.com/cloudfoundry/bosh/pull/777 - unfortunately cf-release is now 5.1gb ,0.0,90919456,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR Bump max nginx upload to 10GB to account for a large 5gb+ cf-release,553935.0,"[553935, 878715]",956238,81882,feature,2015-03-24T21:01:41Z,https://www.pivotaltracker.com/story/show/90919456
2015-03-24T21:02:31Z,2015-03-23T20:35:25Z,accepted,,- http://www.ubuntu.com/usn/usn-2540-1/,0.0,90949274,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump os images for gnutls sec vulnerabilities for ubuntu,553935.0,"[553935, 878715]",956238,81882,feature,2015-03-24T21:02:32Z,https://www.pivotaltracker.com/story/show/90949274
2015-03-24T21:02:56Z,2015-03-23T22:03:49Z,accepted,,Ops manager fails to deploy with 2889. They are using sausage environment (esxi 5.5).,,90957438,story,[],vsphere cpi should not fail to get properties in esxi 5.5,553935.0,"[553935, 878715]",956238,1017737,bug,2015-03-25T20:55:11Z,https://www.pivotaltracker.com/story/show/90957438
2015-03-26T22:48:09Z,2015-03-25T22:24:01Z,accepted,,,1.0,91144222,story,[],build 2865.1 with an updated os image but with older vsphere cpi,553935.0,[553935],956238,81882,feature,2015-03-26T22:48:09Z,https://www.pivotaltracker.com/story/show/91144222
2015-03-26T22:59:44Z,2015-03-26T22:51:07Z,accepted,,,,91236150,story,[],retry unmounting raw image in image_install_grub,553935.0,[553935],956238,553935,chore,2015-03-26T22:59:40Z,https://www.pivotaltracker.com/story/show/91236150
2015-03-26T23:47:04Z,2015-03-26T22:48:41Z,accepted,,,,91236048,story,[],build 2865.1 raw stemcell for openstack without going thru bats,553935.0,[553935],956238,81882,chore,2015-03-26T23:46:59Z,https://www.pivotaltracker.com/story/show/91236048
2015-03-27T20:54:58Z,2015-02-28T00:17:47Z,accepted,,"Currently, the OS image builder pulls the latest linux headers which causes the subsequent tests to fail https://github.com/cloudfoundry/bosh/blob/master/bosh-stemcell/spec/os_image/ubuntu_trusty_spec.rb#L193 This requires to go back and fix tests to successfully build OS image. Instead we should specify which version we want to pull and make our tests to verify that that version is installed.",0.0,89329508,story,[],do not check which kernel version is installed in os image / stemcell specs,553935.0,[553935],956238,553935,feature,2015-03-27T20:54:58Z,https://www.pivotaltracker.com/story/show/89329508
2015-03-28T02:44:20Z,2015-03-09T23:48:32Z,accepted,,"final bit of work from licensing.
- keep saving LIC/NOT to the blobstore as a single tarball (as is today)
- put LIC/NOT into release tarball as files in the root of the release tarball

tar tf release-0.1.tgz
NOTICE
LICENSE
release.MF
packages/
jobs/",1.0,89970006,story,"[{'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",LICENSE and NOTICE files should be directly inside the generated release tarball,1550486.0,"[1550486, 344, 687691]",956238,81882,feature,2015-03-28T02:44:20Z,https://www.pivotaltracker.com/story/show/89970006
2015-03-30T21:01:13Z,2015-03-12T20:58:52Z,accepted,,"right now you only get a new agent if you run the rake task, which is really confusing when you're trying to run a focused integration spec.

can we make this a before(:all) in spec helper or something instead of a rake task step?
",,90233362,story,[],Build the agent when running bosh integration specs,553935.0,[553935],956238,687691,chore,2015-03-30T21:01:14Z,https://www.pivotaltracker.com/story/show/90233362
2015-03-30T22:17:15Z,2015-03-17T00:14:41Z,accepted,,Since we added individual Gemfiles to sub-projects CI started failing since rubygems.org sometimes returns 404 on existing gems. Even though we run `bundle install --local` bundler still reaches rubygems for some reason,,90468300,story,[],CI builds should not try to contact rubygems,553935.0,[553935],956238,553935,chore,2015-03-30T22:17:16Z,https://www.pivotaltracker.com/story/show/90468300
2015-03-30T22:23:35Z,2015-03-30T16:27:26Z,accepted,,,,91416670,story,[],Get Aaron onboarded,553935.0,[553935],956238,553935,chore,2015-03-30T22:23:36Z,https://www.pivotaltracker.com/story/show/91416670
2015-04-02T17:30:54Z,2015-02-17T18:05:50Z,accepted,,https://github.com/cloudfoundry/gosigar/pull/20,1.0,88536892,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Skip /proc/stat lines which are too short',553935.0,"[553935, 878715]",956238,81882,feature,2015-04-02T17:30:54Z,https://www.pivotaltracker.com/story/show/88536892
2015-04-02T19:29:26Z,2015-03-13T18:50:01Z,accepted,,,1.0,90307352,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]","Caller of PUT /self-update can tell when installation has finished, and status (200 if ok or 422 if failure).",687691.0,"[687691, 119]",956238,1348450,feature,2015-04-02T19:29:26Z,https://www.pivotaltracker.com/story/show/90307352
2015-04-03T00:34:04Z,2015-03-25T22:23:34Z,accepted,,"setup: 2 datastore [ds1, ds2]
persistent disk is in the second one (ds2).
currently we believe that because folder in the first ds does not exist, vs",,91144208,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",vsphere cpi should tolerate if disks is not found in the first datastore but is found in the later one,687691.0,[687691],956238,81882,bug,2015-04-03T00:34:18Z,https://www.pivotaltracker.com/story/show/91144208
2015-04-03T19:05:29Z,2015-02-19T18:41:22Z,accepted,,see internal docs for how to access vmware env,2.0,88722984,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",confirm new vsphere disk lookups work on vsan,553935.0,"[553935, 119]",956238,81882,feature,2015-04-03T19:05:29Z,https://www.pivotaltracker.com/story/show/88722984
2015-04-03T22:48:20Z,2015-03-20T17:00:00Z,accepted,,We should either run the script in pipeline to recreate datacenter. Or we should try using blue box,,90794792,story,[],fix openstack ci env,553935.0,"[553935, 878715]",956238,553935,chore,2015-04-03T22:48:21Z,https://www.pivotaltracker.com/story/show/90794792
2015-04-08T01:44:22Z,2015-04-06T18:48:02Z,accepted,,"Currently for Openstack centos, we explicitly remove 70-persistent-net.rules https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/system_openstack_network_centos/apply.sh#L9 

Mirantis is suggesting to remove it for ubuntu.",,91877126,story,[],Investigate if we need to remove persistent net rules for ubuntu,553935.0,[553935],956238,344,chore,2015-04-08T01:44:22Z,https://www.pivotaltracker.com/story/show/91877126
2015-04-08T01:44:29Z,2015-04-06T18:50:39Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/system_openstack_network_centos/apply.sh#L19

Do we need this?",,91877382,story,[],Investigate if we need to remove eth0 configuration from openstack centos stemcell,344.0,[344],956238,344,chore,2015-04-08T01:44:29Z,https://www.pivotaltracker.com/story/show/91877382
2015-04-10T02:01:45Z,2015-03-30T21:47:39Z,accepted,,,2.0,91451020,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user can save cloud config to the Director via `bosh update cloud-config`,687691.0,[687691],956238,81882,feature,2015-04-10T02:01:52Z,https://www.pivotaltracker.com/story/show/91451020
2015-04-10T02:02:31Z,2015-03-30T21:47:59Z,accepted,,"return the latest config
if no cloud config is present, return empty
allow upload of empty cloud config
",2.0,91451038,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user can view cloud config uploaded to the Director via `bosh cloud-config`,1406536.0,"[1406536, 687691]",956238,81882,feature,2015-04-10T02:02:31Z,https://www.pivotaltracker.com/story/show/91451038
2015-04-10T02:05:47Z,2015-03-30T21:48:26Z,accepted,,To accept: deployment row in the DB has the right cloud_config_id after bosh deploying,2.0,91451074,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user can run bosh deploy and see that latest cloud config is referenced by the deployment,1406536.0,[1406536],956238,81882,feature,2015-04-10T02:05:47Z,https://www.pivotaltracker.com/story/show/91451074
2015-04-10T02:17:11Z,2015-01-26T17:30:45Z,accepted,,"https://www.pivotaltracker.com/story/show/79810782 <- instigation story. 
- 0, 5, 10, 15, 15, 15 ... (see other backoff operations)
- confirm code with used volume and see it timeout after 10 mins - did we get rate limited while we were doing other CPI requests?
- confirm that resurecctor still works with instances with persistent disk (time to write a bats test, pls)
- retry on rate limit error",4.0,86929142,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",AWS CPI should wait for upto 10 mins for attach_disk to succeed if attach api call returns VolumeInUse,553935.0,[553935],956238,81882,feature,2015-04-10T02:19:30Z,https://www.pivotaltracker.com/story/show/86929142
2015-04-10T03:01:59Z,2015-03-13T18:50:02Z,accepted,,,1.0,90307374,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper in active mode downloads tarball from provided url (with key) and expands/runs it.,687691.0,[687691],956238,81882,feature,2015-04-10T03:02:00Z,https://www.pivotaltracker.com/story/show/90307374
2015-04-11T00:55:05Z,2014-12-19T00:18:40Z,accepted,,"acceptance: user can use manual networking (type: manual with subnets) on an openstack without dhcp server and agent should assign ips to nics based on what bosh decided. this might not work with http metadata service so only config-drive configuration would be supported (pending investigation of https://www.pivotaltracker.com/story/show/84893322).

http://docs.cloudfoundry.org/bosh/networks.html",4.0,84893840,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",agent should be able to assign network configuration statically (instead of dhcp) for manual networks on openstack,553935.0,"[553935, 1633158, 878715]",956238,81882,feature,2015-04-11T00:55:05Z,https://www.pivotaltracker.com/story/show/84893840
2015-04-13T23:38:49Z,2015-04-03T18:00:19Z,accepted,,,1.0,91782848,story,[],bosh delete deployment X should succeed if deployment does not exist,344.0,"[344, 1550486]",956238,81882,feature,2015-04-13T23:38:50Z,https://www.pivotaltracker.com/story/show/91782848
2015-04-14T18:39:45Z,2015-03-09T23:54:18Z,accepted,,"In the IDE see: *Tools* -> *Create Command-Line Launcher...*

Create an issue/pr toward https://github.com/pivotal-sprout/sprout-jetbrains-editors",,89970526,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",investigate and install the 'mine' and 'idea' executables when the ide is installed,119.0,[119],956238,119,chore,2015-04-14T18:49:17Z,https://www.pivotaltracker.com/story/show/89970526
2015-04-14T19:05:20Z,2015-04-13T23:51:32Z,accepted,,"java -version claims java 1.7 even though we brew cask installed java (1.8)

Some of the symlinks to java paths are different on dolores vs other machines.

Why did this happen?
How does brew cask install java?",,92396306,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Fix java on Dolores,119.0,[119],956238,553935,chore,2015-04-14T19:05:20Z,https://www.pivotaltracker.com/story/show/92396306
2015-04-14T23:23:23Z,2015-04-14T20:09:04Z,accepted,,"It causes CI to fail when comparing output


```
[WARNING] Loading the cli took 8.1 seconds, consider cleaning your gem environment
```",,92469114,story,[],Don't print CLI load time warning in non-interactive mode,119.0,[119],956238,553935,chore,2015-04-22T15:23:00Z,https://www.pivotaltracker.com/story/show/92469114
2015-04-14T23:58:07Z,2015-03-30T21:49:06Z,accepted,,"- via bosh deployments command
- still allow deployments not to reference any cloud config
- show 'none' when cloud config is not used by that deployment
- show 'outdated' when deployment does not reference latest cloud config
- show 'latest' when deployment uses latest cloud config
- check to see old cli works against new director
- check to see new cli works against old director",4.0,91451152,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user can see which deployments reference latest cloud config,878715.0,[878715],956238,81882,feature,2015-04-14T23:58:15Z,https://www.pivotaltracker.com/story/show/91451152
2015-04-15T18:10:06Z,2015-04-13T16:43:52Z,accepted,,"The cleaner is running before every BATs. Looks like it is not working any more. 

```
I, [2015-04-13T16:15:29.426172 #42099]  INFO : Deleting vApp bosh-acceptance-vapp.
rake aborted!
NoMethodError: undefined method `href' for nil:NilClass
/mnt/ci-tmp/bat_micro_vcloud_ubuntu_trusty_go_agent/ruby/2.1.0/gems/ruby_vcloud_sdk-0.7.1/lib/ruby_vcloud_sdk/vapp.rb:32:in `delete'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/vcloud/micro_bosh_deployment_cleaner.rb:48:in `delete_vapp'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/vcloud/micro_bosh_deployment_cleaner.rb:19:in `clean'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/bat/runner.rb:35:in `deploy_bats_microbosh'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/bat_helper.rb:66:in `deploy_bats_microbosh'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/bat_helper.rb:55:in `deploy_microbosh_and_run_bats'
/mnt/jenkins/workspace/bat_micro_vcloud_ubuntu_trusty_go_agent/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:116:in `block (3 levels) in <top (required)>'
Tasks: TOP => spec:system:micro
```",,92359286,story,[],Fix vlcoud cleaner,344.0,"[344, 1406536]",956238,553935,chore,2015-04-22T15:23:00Z,https://www.pivotaltracker.com/story/show/92359286
2015-04-16T18:02:44Z,2015-04-08T16:42:13Z,accepted,,,2.0,92055160,story,[],Agent should configure eth0 as dhcp for retrieving registry metadata over HTTP,553935.0,"[553935, 1633158]",956238,553935,feature,2015-04-16T18:02:45Z,https://www.pivotaltracker.com/story/show/92055160
2015-04-17T22:59:04Z,2014-12-02T21:42:09Z,accepted,,this also probably affects new bosh-micro command: https://github.com/cloudfoundry/bosh/issues/699. ,1.0,83798776,story,[],investigate 'Restrictive cli umask on 'bosh micro deploy' breaks Bosh-micro',344.0,[344],956238,81882,feature,2015-04-17T22:59:05Z,https://www.pivotaltracker.com/story/show/83798776
2015-04-20T18:38:26Z,2015-04-16T18:40:32Z,accepted,,,,92640114,story,[],Investigate Build Failures 17 April 2015,1406536.0,"[1406536, 1633158]",956238,1406536,chore,2015-04-20T18:38:26Z,https://www.pivotaltracker.com/story/show/92640114
2015-04-22T17:23:56Z,2015-04-22T16:23:53Z,accepted,,,,93030276,story,[],Get Luke onboarded,553935.0,[553935],956238,553935,chore,2015-04-22T17:23:55Z,https://www.pivotaltracker.com/story/show/93030276
2015-04-22T22:19:19Z,2015-02-24T21:33:13Z,accepted,,"- talk with marco on how to make it easier
- instead of importing use copy ami?
- check up on the thread for community
- use aws api to list all regions?
- make sure cpi uses correct region
- change one of the test suite to run against a diff region",4.0,89044786,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",produce aws light stemcells for all regions,344.0,"[344, 878715, 1406536]",956238,81882,feature,2015-04-22T22:19:19Z,https://www.pivotaltracker.com/story/show/89044786
2015-04-22T23:58:42Z,2015-04-09T22:00:47Z,accepted,,,1.0,92178938,story,[],director should show validation error for manual networks if gateway is not specified,1541728.0,[1541728],956238,81882,feature,2015-04-22T23:58:49Z,https://www.pivotaltracker.com/story/show/92178938
2015-04-23T00:11:30Z,2015-03-12T16:54:29Z,accepted,,"do we develop out of ~/go ~/workspace/<project-name>

do we use a global GO_PATH or the DirEnv project to manage GO_PATH?

get sprout to install dependencies and install the projects using `go get <project>` rather than `git clone`",,90209450,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Figure out how to setup go environment for development,206015.0,"[206015, 1348450]",956238,119,chore,2015-04-23T00:11:30Z,https://www.pivotaltracker.com/story/show/90209450
2015-04-23T20:15:49Z,2015-04-02T18:30:04Z,accepted,,"When running the file in isolation the spec is fast, but as part of rspec spec/unit it takes 90s. what gives?",,91719478,story,[],Fix 'bosh-director/spec/unit/jobs/delete_deployment_spec.rb:76' to not take 90s when running all the director unit tests,878715.0,[878715],956238,687691,chore,2015-04-27T21:39:29Z,https://www.pivotaltracker.com/story/show/91719478
2015-04-23T20:56:46Z,2015-02-19T20:12:44Z,accepted,,"On our local vBlock environments, there are ~1200 BOSH-deployed VMs, all configured to run cron.daily at 6:25UTC. This causes a spike in resource consumption that can be stressful to certain operations (e.g. it causes the cf-mysql-release to break, and copying files between VMs slows down by two orders of magnitude). It would be safer to distribute the timing of cron.daily to avoid this spike.",2.0,88733114,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]","ubuntu cron.daily should not all run at the same time, to avoid localized resource spiking",1541728.0,[1541728],956238,81882,feature,2015-04-23T20:56:46Z,https://www.pivotaltracker.com/story/show/88733114
2015-04-23T21:05:20Z,2014-07-17T20:52:14Z,accepted,,,,75227366,story,[],Remove `Digest::Digest is deprecated; use Digest` noise,1550486.0,"[1550486, 878715]",956238,1406536,chore,2015-04-27T21:39:29Z,https://www.pivotaltracker.com/story/show/75227366
2015-04-23T21:09:28Z,2015-03-25T22:27:20Z,accepted,,"originally story was ""vsphere cpi should be able to find persistent datastores when disk requested is a smaller size than available space in the ds""",,91144398,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",include detailed info about datastores during vsphere cpi operations,1633158.0,"[1633158, 119]",956238,81882,bug,2015-04-23T21:09:28Z,https://www.pivotaltracker.com/story/show/91144398
2015-04-23T21:11:46Z,2015-04-14T17:41:15Z,accepted,,"The latest go plugin that works well is in alpha. It requires Intellij 14.1, which is not currently available via brew.

Add details docs in the sprout-wrap README for how to install/setup plugin etc. including keybindings/colors",,92456062,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Add Manual steps for installing Intellij with go ,119.0,[119],956238,119,chore,2015-04-23T21:11:47Z,https://www.pivotaltracker.com/story/show/92456062
2015-04-24T19:16:59Z,2015-03-27T18:26:15Z,accepted,,"Right now, the Ubuntu Trusty stemcell (in a tarball) weighs in at 447MB and the RHEL 7 stemcell is 661MB.

Up to this point, we haven't paid much attention to the size of the CentOS and RHEL stemcells. There are probably a bunch of packages installed on them that are not really needed.",0.0,91294022,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",CentOS and RHEL stemcells can probably be made smaller,1406536.0,"[1406536, 1550486]",956238,1541728,feature,2015-04-27T21:39:27Z,https://www.pivotaltracker.com/story/show/91294022
2015-04-27T18:24:46Z,2015-04-24T15:36:51Z,accepted,,"vSphere CentOS BATs consistently fail after creating a new CentOS 7 stemcell, investigate the cause and find a fix ",,93213492,story,[],Investigate vSphere CentOS BATs failure,1406536.0,[1406536],956238,1406536,chore,2015-04-27T18:25:06Z,https://www.pivotaltracker.com/story/show/93213492
2015-04-27T18:25:36Z,2015-04-23T21:14:09Z,accepted,,CentOS OpenStack BATs fail on CI with a stacktrace during microBOSH deployment. When trying to deploy the microBOSH manually from a CI slave the process blows up with an error in Net/SSH,,93154278,story,[],Investigate OpenStack CentOS BATs failures,1406536.0,[1406536],956238,1406536,chore,2015-04-27T18:25:46Z,https://www.pivotaltracker.com/story/show/93154278
2015-04-27T22:11:35Z,2015-04-15T21:31:27Z,accepted,,request a new datacenter on mirantis express env,2.0,92567876,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",list out problems found from running bats on juno openstack on mirantis,553935.0,"[553935, 1655862]",956238,81882,feature,2015-04-27T22:11:35Z,https://www.pivotaltracker.com/story/show/92567876
2015-04-27T23:57:58Z,2015-04-13T18:11:43Z,accepted,,systemd KillMode=process,,92368344,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}]",When runit is restarted bosh-agent and monit processes should still be running on CentOS 7,553935.0,[553935],956238,553935,bug,2015-04-27T23:57:58Z,https://www.pivotaltracker.com/story/show/92368344
2015-04-28T19:04:03Z,2015-04-27T18:26:38Z,accepted,,,,93365290,story,[],Try to move build #2949 and commit sha 46bdbc6bfc51bbb3a6d0ff57ab51a8abc0f2edba through the pipeline,1406536.0,"[1406536, 1633158]",956238,1406536,chore,2015-04-28T19:04:14Z,https://www.pivotaltracker.com/story/show/93365290
2015-04-28T21:55:35Z,2015-04-21T17:19:55Z,accepted,,"get all 4 builds running in parallel

check largest c3, m3

check smaller c3s and multiple workers
",,92939602,story,[],Profile integration tests on concourse for speed,1633158.0,"[1633158, 687691]",956238,1406536,chore,2015-04-28T21:55:46Z,https://www.pivotaltracker.com/story/show/92939602
2015-04-29T16:41:45Z,2014-10-24T19:18:31Z,accepted,,"currently using Hetzner

This will require setting up a new tenant on Mirantis under the jenkins user, similar to the other 4 tenants used by the BATs.",,81395478,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Migrate openstack_cpi_lifecycle jenkins build to another openstack system,878715.0,[878715],956238,1266616,chore,2015-04-29T16:41:45Z,https://www.pivotaltracker.com/story/show/81395478
2015-04-29T17:29:28Z,2015-03-30T17:34:52Z,accepted,,"consistent with existing persistent_disk directive on the job.

seen in 2865.1

opsman specifies a persistent disk pool for each job, even if there is no persistent disk assigned. For the jobs with no persistent disk, the disk_size is set to 0

When we redeploy, bosh thinks there is a change to the persistent disk for these jobs, and updates the instance (bringing it down and back up unnecessarily)

We expected that bosh would leave the instance up and running since there was no change.",,91424354,story,[],bosh deploy should not attach disk (delete existing disks) when disk size specified via a disk pool is 0,553935.0,"[553935, 1655862]",956238,1165386,bug,2015-04-29T17:29:28Z,https://www.pivotaltracker.com/story/show/91424354
2015-04-30T20:55:47Z,2015-04-29T23:18:03Z,accepted,,,,93583928,story,[],Fix the login spec timeout in http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent_ruby_2_1/472/,1655862.0,"[1655862, 1550486]",956238,687691,chore,2015-04-30T20:55:48Z,https://www.pivotaltracker.com/story/show/93583928
2015-04-30T22:17:56Z,2015-04-29T23:33:52Z,accepted,,,,93584832,story,[],Update go to 1.4.2 for bosh-init pipeline,119.0,[119],956238,553935,chore,2015-04-30T22:27:42Z,https://www.pivotaltracker.com/story/show/93584832
2015-04-30T23:43:14Z,2015-04-30T18:50:06Z,accepted,," wrong number of arguments (4 for 5)
       Test directory: /var/lib/jenkins/jobs/bosh_integration_go_agent_ruby_2_1/workspace/bosh/tmp/integration-tests-workspace/pid-367/spec-20150430-367-1usz3rk
       Sandbox directory: /var/lib/jenkins/jobs/bosh_integration_go_agent_ruby_2_1/workspace/bosh/tmp/integration-tests-workspace/pid-367
     # ./bosh-dev/lib/bosh/dev/sandbox/socket_connector.rb:7:in `initialize'
     # ./spec/support/local_file_server.rb:28:in `new'
     # ./spec/support/local_file_server.rb:28:in `initialize'
     # ./spec/integration/cli_stemcell_spec.rb:88:in `new'
     # ./spec/integration/cli_stemcell_spec.rb:88:in `block (4 levels) in <top (required)>'
     # ./spec/integration/cli_stemcell_spec.rb:91:in `block (4 levels) in <top (required)>",,93650020,story,[],Fix build Failure: http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent_ruby_2_1/477/console,1655862.0,"[1655862, 1550486]",956238,1550486,chore,2015-04-30T23:43:14Z,https://www.pivotaltracker.com/story/show/93650020
2015-05-01T00:22:44Z,2015-04-29T17:52:38Z,accepted,,"The job was set to time out after 60 minutes, which we started hitting -- bumped to 90 for now but we should figure out why it's so slow (and lower the timeout again if possible)

See http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent/buildTimeTrend
",,93558412,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Figure out why bosh integration specs went from 20-30 min to 60+ minutes on jenkins,1550486.0,"[1550486, 1655862]",956238,687691,chore,2015-05-01T00:22:44Z,https://www.pivotaltracker.com/story/show/93558412
2015-05-01T00:26:56Z,2015-04-30T21:42:18Z,accepted,,"nats.log:

```
[2015-04-30 13:04:55 -0700, ""Starting nats-server version 0.5.0.beta.12 on port 61201""]

[2015-04-30 13:08:48 -0700, ""Server exiting..""]
```

build error (from http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent/645/consoleFull)
Note that the timestamp shows the connection attempt prior to the startup time.

```
E, [2015-04-30T13:04:52.292562 #20899] ERROR : Failed to connect to nats: #<Errno::ECONNREFUSED: Connection refused - connect(2)> host=localhost port=61201
      properly outputs progress (FAILED - 1)
```",,93662808,story,[],Integration tests should wait for nats to come up,119.0,[119],956238,1550486,chore,2015-05-01T00:26:56Z,https://www.pivotaltracker.com/story/show/93662808
2015-05-01T06:11:56Z,2015-04-29T16:53:05Z,accepted,,,2.0,93552812,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",run bats & lifecycle tests on vsphere 6,344.0,"[344, 119]",956238,81882,feature,2015-05-05T01:06:40Z,https://www.pivotaltracker.com/story/show/93552812
2015-05-01T17:42:26Z,2015-04-22T19:08:30Z,accepted,,"- use official debian/rhel repos to make **apt/yum pull down official binaries** from rsyslog site (http://www.rsyslog.com/downloads/download-other/)
- make sure relp works
- make sure imklog works (kernel logging)
- remove custom built syslog package",2.0,93045744,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",update rsyslog to version 8,1541728.0,[1541728],956238,81882,feature,2015-05-01T17:42:43Z,https://www.pivotaltracker.com/story/show/93045744
2015-05-01T17:43:55Z,2014-12-18T00:37:51Z,accepted,,"- http://seclists.org/oss-sec/2014/q4/1052 (CVE-2014-9090)
- https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-9090
- http://www.ubuntu.com/usn/usn-2443-1/",0.0,84817388,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",bump os images for CVE-2014-9090,81882.0,[81882],956238,81882,feature,2015-05-01T17:43:55Z,https://www.pivotaltracker.com/story/show/84817388
2015-05-01T17:43:55Z,2015-04-30T18:10:15Z,accepted,,"- http://www.ubuntu.com/usn/usn-2589-1/
- https://bugzilla.redhat.com/show_bug.cgi?id=1203712
",1.0,93646438,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",bump ubuntu trusty and centos os images [usn-2589-1],1655862.0,"[1655862, 1550486]",956238,81882,feature,2015-05-01T17:44:06Z,https://www.pivotaltracker.com/story/show/93646438
2015-05-01T17:44:42Z,2015-04-10T05:49:46Z,accepted,,"Bumps version to next final version and produces a tarball with updated info:

```
$ bosh finalize release name-0+dev.21.tgz --with-tarball
Release version: 1
Release tarball: name-1.tgz
```

- if --name is specified use given name
- if --name is not specified, use name used in the tarball
- if --version is specified use given version
- if --version is specified and is already taken, show an dup ver error
- if --version is not specified, follow BOSH bumping semantics (next biggest final version for that release name)

Assumes that it reads blobstore creds, runs in the release directory.

See https://github.com/cloudfoundry/bosh-notes/blob/master/release-creation.md for future direction.",4.0,92202592,story,[],user should be able to pass through a release tarball through their CI and eventually turn that exact set of bits into a final release,1541728.0,[1541728],956238,81882,feature,2015-05-01T17:44:42Z,https://www.pivotaltracker.com/story/show/92202592
2015-05-01T18:25:58Z,2015-05-01T17:47:40Z,accepted,,"It looks like agent unit tests in travis use bin/ci which is no longer exists https://travis-ci.org/cloudfoundry/bosh-agent/builds/59581265

We should fix that",,93711878,story,[],FIx agent travis unit tests,1550486.0,[1550486],956238,553935,chore,2015-05-01T18:25:59Z,https://www.pivotaltracker.com/story/show/93711878
2015-05-01T18:58:18Z,2015-05-01T16:56:23Z,accepted,,,,93708714,story,[],Rebuild and publish bosh/integration Docker image,1550486.0,"[1550486, 1655862]",956238,1550486,chore,2015-05-01T18:58:18Z,https://www.pivotaltracker.com/story/show/93708714
2015-05-01T22:07:46Z,2015-05-01T19:24:12Z,accepted,,"If there are scenarios where this is true, present to @dk to see if we have new vsphere cpi stories to account for them.",,93719762,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}, {'name': 'vsphere-loc-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:16:59Z', 'id': 11514338, 'updated_at': '2015-04-27T17:16:59Z'}]",figure out if there are vcenter configurations where we would not be able to move a disk from ds1 to ds2 (talk to mark kropf),81882.0,[81882],956238,119,chore,2015-05-01T22:07:47Z,https://www.pivotaltracker.com/story/show/93719762
2015-05-04T21:34:43Z,2015-04-30T21:45:13Z,accepted,,"It is possible that DB proxy is failing but there are not logs saved.

See failures here: http://bosh-jenkins.cf-app.com:8080/job/bosh_integration_go_agent/645/consoleFull#",,93663054,story,[],Save proxy logs in sandbox in integration tests,1541728.0,[1541728],956238,1550486,chore,2015-05-04T21:35:09Z,https://www.pivotaltracker.com/story/show/93663054
2015-05-05T00:17:44Z,2015-05-01T20:48:14Z,accepted,,"https://travis-ci.org/cloudfoundry/bosh-agent/builds/60872001
http://bosh-jenkins.cf-app.com:8080/job/bosh_unit/2690/consoleFull",,93725076,story,[],Fix bosh-agent unit tests on Travis and Jenkins,687691.0,[687691],956238,1550486,chore,2015-05-05T00:17:44Z,https://www.pivotaltracker.com/story/show/93725076
2015-05-05T01:10:38Z,2015-03-05T22:39:03Z,accepted,,"`bosh logs <job> <index>` should return all log files in `/var/vcap/sys/log` regardless of file extension. Right now we only return files that end in `.log`.

The cli `logs` help should mention that `--all` is deprecated and the cli shouldn't try to pass it as a filter (current agent doesn't expect an `all` filter)

 

Original bug report:

We recently asked a support engineer to grab the logs from a riak-cs deployment using `bosh logs`. We noticed that some of the logs weren't in the tarball, and tried `bosh logs --all`, but still couldn't get them all. Naming log files to end in `.log` caused them to be included, but we were unable to fetch logs with other extensions.

We've seen this against a bosh lite as well as PCF microbosh.

Being tracked in #89752748",,89753100,story,[],`bosh logs` should return all logs,687691.0,[687691],956238,1550416,bug,2015-05-05T01:10:54Z,https://www.pivotaltracker.com/story/show/89753100
2015-05-05T01:11:20Z,2015-04-13T19:02:36Z,accepted,,relevant PR: https://github.com/cloudfoundry/bosh/pull/761/files,,92373304,story,[],--filter for bosh logs should be honored,1541728.0,[1541728],956238,81882,bug,2015-05-05T01:11:31Z,https://www.pivotaltracker.com/story/show/92373304
2015-05-05T01:13:06Z,2015-03-30T21:54:37Z,accepted,,,1.0,91451730,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user does not have to specify cloud_properties on resource pool and default to empty Hash,553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-05T01:13:06Z,https://www.pivotaltracker.com/story/show/91451730
2015-05-05T01:13:47Z,2015-03-30T21:54:50Z,accepted,,"- dynamic network
- manual network subnet
- vip network",1.0,91451746,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user does not have to specify cloud_properties on network subnet and default to empty Hash,553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-05T01:13:47Z,https://www.pivotaltracker.com/story/show/91451746
2015-05-05T01:14:00Z,2015-03-30T21:55:10Z,accepted,,,1.0,91451776,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user does not have to specify cloud_properties on compilation and default to empty Hash,553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-05T01:14:01Z,https://www.pivotaltracker.com/story/show/91451776
2015-05-05T01:14:55Z,2015-04-22T00:33:29Z,accepted,,"in deployment_spec_parser.rb:

```
        # Uncomment when integration test fixed
        # raise ""No resource pools specified."" if @resource_pools.empty?
```

It's over a year old -- let's try to uncomment it?",2.0,92973932,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",Ensure resource pools are validated to be non-empty,553935.0,[553935],956238,687691,feature,2015-05-05T01:14:59Z,https://www.pivotaltracker.com/story/show/92973932
2015-05-05T04:35:38Z,2015-03-30T21:52:11Z,accepted,,,2.0,91451404,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]","user should see an error message if deployment manifest contains any disk pool, resource pool, network, compilation when cloud config exists on the director",553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-05T04:35:38Z,https://www.pivotaltracker.com/story/show/91451404
2015-05-05T04:35:56Z,2015-03-30T21:53:11Z,accepted,,"- missing properties on disk pools, resource pools, etc.
- deployment manifest references non-existent disk pools, etc.
- errors should be shown during `bosh update cloud-config ...` and should not save new cloud-config",1.0,91451502,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user should see usual error messages when cloud resources are not specified correctly,553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-05T04:35:56Z,https://www.pivotaltracker.com/story/show/91451502
2015-05-05T04:36:02Z,2015-03-30T21:51:20Z,accepted,,,4.0,91451310,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]","user can run `bosh deploy` with a deployment that references (but doesn't itself contain) a disk pool, resource pool, network, compilation from cloud config",553935.0,[553935],956238,81882,feature,2015-05-05T04:36:03Z,https://www.pivotaltracker.com/story/show/91451310
2015-05-05T04:36:04Z,2015-03-30T21:51:41Z,accepted,,,,91451336,story,"[{'name': 'cloud-config', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-30T21:55:57Z', 'id': 11268106, 'updated_at': '2015-03-30T21:55:57Z'}]",user can provide cloud configuration in a separate manifest,,[],956238,81882,release,2015-05-05T04:36:04Z,https://www.pivotaltracker.com/story/show/91451336
2015-05-06T00:47:41Z,2015-05-05T21:50:47Z,accepted,,"It should not have **two** ""release.MF"" files:
```
tar tvf bosh_cli/spec/assets/valid_release.tgz
...
-rw-r--r--  0 oleg    staff     483 Jan 26  2011 release.MF
-rw-r--r--  0 pivotal staff     487 Aug  1  2014 release.MF
```",,93945766,story,[],"update ""valid_release.tgz"" to be valid",1550486.0,[1550486],956238,1550486,chore,2015-05-07T00:27:16Z,https://www.pivotaltracker.com/story/show/93945766
2015-05-06T01:07:29Z,2015-05-05T18:57:36Z,accepted,,...to pick up the multiple pipeline capability,,93931062,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Upgrade concourse on our production/main instance,1633158.0,"[1633158, 1550486]",956238,1550486,chore,2015-05-06T01:07:29Z,https://www.pivotaltracker.com/story/show/93931062
2015-05-07T00:59:20Z,2015-05-05T04:47:19Z,accepted,,"i can only think of a single way to get shared networking working in an efficient way: maintain state in db with uniqueness constraints. 

see https://github.com/cloudfoundry/bosh-notes/blob/master/global-networking.md

- existing vms with assigned IPs
- new VMs coming in after user scaled
etc.",1.0,93869458,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",spike usage of ip allocations,553935.0,"[553935, 1655862]",956238,81882,feature,2015-05-07T00:59:20Z,https://www.pivotaltracker.com/story/show/93869458
2015-05-07T13:42:42Z,2015-04-30T15:04:59Z,accepted,,"There are a number of places that could be simplified and/or deleted now that we no longer produce CentOS 6 stemcells. For example, apply.sh stages, the stage_collection.rb, the bosh-stemcell README.md, and various rspec files.",,93629214,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",remove centos 6 support from stemcell builder,1541728.0,[1541728],956238,1541728,chore,2015-05-07T13:42:43Z,https://www.pivotaltracker.com/story/show/93629214
2015-05-07T21:18:12Z,2015-04-28T21:22:59Z,accepted,,,,93481960,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",unit tests run in concourse pipeline,1550486.0,[1550486],956238,1633158,chore,2015-05-15T03:05:43Z,https://www.pivotaltracker.com/story/show/93481960
2015-05-08T16:41:39Z,2015-04-28T21:22:36Z,accepted,,"the full 1.9.3, 2.1.2, postgres, mysql matrix",,93481926,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",integration tests run in a concourse pipeline,1550486.0,"[1550486, 1633158]",956238,1633158,chore,2015-05-15T03:05:45Z,https://www.pivotaltracker.com/story/show/93481926
2015-05-08T19:04:41Z,2015-05-08T16:38:36Z,accepted,," Failed to connect to nats: #<Errno::ECONNREFUSED: Connection refused - connect(2)> host=localhost port=61801

https://main.bosh-ci.cf-app.com/pipelines/main/jobs/test-integration/builds/28",,94189482,story,[],Fix flaky nats connection in integration tests,1550486.0,"[1550486, 553935]",956238,1550486,chore,2015-05-08T19:04:42Z,https://www.pivotaltracker.com/story/show/94189482
2015-05-08T20:51:10Z,2015-05-08T19:17:13Z,accepted,,,,94202116,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Improve CI performance,1550486.0,[1550486],956238,1550486,chore,2015-05-08T20:51:10Z,https://www.pivotaltracker.com/story/show/94202116
2015-05-08T22:09:01Z,2015-04-28T21:26:31Z,accepted,,"once we have unit and integration tests running in concourse, we can remove them from gocd",,93482218,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",kill duplicate gocd builds,1550486.0,"[1550486, 1633158]",956238,1633158,chore,2015-05-08T22:09:02Z,https://www.pivotaltracker.com/story/show/93482218
2015-05-08T22:26:52Z,2015-05-05T18:58:29Z,accepted,,,,93931136,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",kill unit and integration suites from jenkins,1550486.0,"[1550486, 553935]",956238,1550486,chore,2015-05-08T22:27:08Z,https://www.pivotaltracker.com/story/show/93931136
2015-05-08T22:35:34Z,2015-05-07T21:51:52Z,accepted,,,,94134392,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",kill unit and integration suite from checkman,1550486.0,[1550486],956238,1550486,chore,2015-05-08T22:35:46Z,https://www.pivotaltracker.com/story/show/94134392
2015-05-08T23:33:50Z,2015-04-30T19:00:31Z,accepted,,The flag is a no-op,1.0,93650844,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",Running `bosh logs --all` should print a deprecation warning,1541728.0,[1541728],956238,687691,feature,2015-05-08T23:33:50Z,https://www.pivotaltracker.com/story/show/93650844
2015-05-08T23:40:00Z,2015-05-05T01:32:43Z,accepted,,"Technically, it successfully deploys (in that it applies the update), but then fails to acquire a lock near the end, causing it to fail.

The failure stems from https://github.com/cloudfoundry/bosh/blob/ac625bbb70d9e58946e0b369cad22e9c64554a24/bosh-director/lib/bosh/director/deployment_plan/planner.rb#L164

This happens when deploying a second release version to a failing deployment, because there are two active release versions, resulting in trying to acquire the same release lock twice (or N times, as you attempt to deploy it multiple times). I hand-patched this on our director by adding a .uniq to the array of release names, which worked.",,93864338,story,[],BOSH director v2962 cannot deploy over failing deployments,1541728.0,[1541728],956238,381857,bug,2015-05-08T23:40:00Z,https://www.pivotaltracker.com/story/show/93864338
2015-05-08T23:49:15Z,2015-04-23T20:51:27Z,accepted,,"it seems that default networks are selected from all networks in CentosNetManager. Compare that to UbuntuNetManager which select it from non vip networks.

Correct thing to do: select from non-vip networks (like UbuntuNetManager)",,93152336,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}]",agent should select dns addresses from non-vip networks,1541728.0,[1541728],956238,81882,bug,2015-05-08T23:49:35Z,https://www.pivotaltracker.com/story/show/93152336
2015-05-09T00:17:39Z,2015-05-07T21:17:23Z,accepted,,"Add this task to test-unit.yml:
ci:publish_coverage_report

Will also need some kind of credentials.",,94132000,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Add coverage to unit specs in Concourse,1550486.0,[1550486],956238,1550486,chore,2015-05-09T00:17:40Z,https://www.pivotaltracker.com/story/show/94132000
2015-05-09T00:18:25Z,2015-05-05T18:56:59Z,accepted,,,,93931010,story,[],Resolve the following test timing issues,1550486.0,"[1550486, 553935]",956238,1550486,chore,2015-05-15T03:05:42Z,https://www.pivotaltracker.com/story/show/93931010
2015-05-13T19:22:04Z,2015-05-08T18:21:26Z,accepted,,"- determine if pem formatted strings covers all cases in how people typically configure additional certs on the vm
  - multiple cert
  - one cert
  - possible formats
  - does it work for linux/windows?",1.0,94197624,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",investigate cert installation procedures,1541728.0,[1541728],956238,81882,feature,2015-05-13T19:22:05Z,https://www.pivotaltracker.com/story/show/94197624
2015-05-14T17:58:00Z,2015-05-07T01:18:36Z,accepted,,,0.0,94047466,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",update ntp on the bosh-lite vm,81882.0,[81882],956238,81882,feature,2015-05-14T17:58:20Z,https://www.pivotaltracker.com/story/show/94047466
2015-05-14T19:52:31Z,2015-05-13T22:44:42Z,accepted,,"Failing in production CI, not elsewhere. May be due to the combination of:
- the underlying linux
- a PTY bug in ruby
- starting postgres as non-privileged
",,94546576,story,[],Fix concourse failure on postgres-backed integration,1550486.0,"[1550486, 1655862]",956238,1550486,chore,2015-05-14T19:52:31Z,https://www.pivotaltracker.com/story/show/94546576
2015-05-14T19:54:23Z,2015-05-08T19:17:33Z,accepted,,,,94202132,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Add workers to main CI,1550486.0,"[1550486, 553935]",956238,1550486,chore,2015-05-14T19:54:23Z,https://www.pivotaltracker.com/story/show/94202132
2015-05-14T23:25:15Z,2015-05-14T19:38:05Z,accepted,,"<https://main.bosh-ci.cf-app.com/pipelines/main/jobs/unit-2.1/builds/17>

bundler is in fact installed on the system.",,94615456,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]","Fix CI pipeline unit job flakiness: ""bundle: command not found""",1550486.0,[1550486],956238,1550486,chore,2015-05-14T23:25:15Z,https://www.pivotaltracker.com/story/show/94615456
2015-05-15T18:24:26Z,2015-03-17T04:07:17Z,accepted,,"They are currently version 7 which is compatible with vSphere 4.

We would like to use new features like nested virtualization but they are not supported by this hardware version.

See http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1010675",0.0,90476838,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",Update virtual hardware version in vSphere stemcells,1406536.0,[1406536],956238,756869,feature,2015-05-15T18:24:27Z,https://www.pivotaltracker.com/story/show/90476838
2015-05-16T00:11:21Z,2015-04-22T23:25:50Z,accepted,,"`Floating IP 10.116.5.208 not allocated` error was raised and VM was left without BOSH realizing it's there.

```
E, [2015-04-21 20:22:47 #16576] [task:8] ERROR -- DirectorJobRunner: Floating IP 10.116.5.208 not allocated
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/helpers.rb:20:in `cloud_error'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/vip_network.rb:45:in `block in configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/helpers.rb:26:in `with_openstack'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/vip_network.rb:32:in `configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/network_configurator.rb:89:in `configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/cloud.rb:309:in `block in create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2922.0/lib/cloud/openstack/cloud.rb:217:in `create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/vm_creator.rb:41:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/resource_pool_updater.rb:51:in `create_missing_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/resource_pool_updater.rb:34:in `block (4 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/resource_pool_updater.rb:32:in `block (3 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/resource_pool_updater.rb:31:in `block (2 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-04-21 20:22:47 #16576] [task:8] DEBUG -- DirectorJobRunner: (0.000145s) BEGIN
D, [2015-04-21 20:22:47 #16576] [task:8] DEBUG -- DirectorJobRunner: (0.001155s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-04-21 20:22:47.445698+0000', ""description"" = 'create deployment', ""result"" = 'Floating IP 10.116.5.208 not allocated', ""output"" = '/var/vcap/store/director/tasks/8', ""checkpoint_time"" = '2015-04-21 20:22:45.993239+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 8)
D, [2015-04-21 20:22:47 #16576] [task:8] DEBUG -- DirectorJobRunner: (0.004474s) COMMIT
I, [2015-04-21 20:22:47 #16576] []  INFO -- DirectorJobRunner: Task took 13 minutes 33.73205044400004 seconds to process.

Task 8 error
```",,93066396,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",openstack cpi should delete created vms if networking configuration fails at the end of the create_vm cpi call,553935.0,"[553935, 1655862]",956238,81882,bug,2015-05-16T00:12:54Z,https://www.pivotaltracker.com/story/show/93066396
2015-05-18T17:15:01Z,2015-05-09T01:15:43Z,accepted,,"- currently it seems that it keeps the sha1 of the original tarball used to hold the two LICENSE and NOTICE files. that's not going to match sha1 of the new tarball that we should created from LICENSE/NOTICE files found in the release tarball. 

- we should reconstruct license tarball to include LICENSE/NOTICE files [1] and upload this tarball to the blobstore. what happens currently is it just picks up a single file (LICENSE) and uploads it.

[1] https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/resources/license.rb#L27",,94222918,story,[],fix `finalize release` command to properly save off LICENSE/NOTICE files from the release tarball,1541728.0,[1541728],956238,119,bug,2015-05-18T17:15:09Z,https://www.pivotaltracker.com/story/show/94222918
2015-05-18T18:39:51Z,2015-04-01T17:17:08Z,accepted,,"vsphere: 1 cluster, 2 resource pools
bosh: 2 resource pools, each use different resource pool under the same cluster",2.0,91621196,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",different bosh resource pools can use different vsphere resource pools under the same cluster,553935.0,"[553935, 1655862]",956238,81882,feature,2015-05-18T18:40:03Z,https://www.pivotaltracker.com/story/show/91621196
2015-05-18T18:42:13Z,2015-05-14T16:46:05Z,accepted,,"Hi all,

I'm trying to deploy a job with 2 NICs (internal and external networks) to expose the job to Internet.
I get empty resolv.conf every time when I add second interface. When I use only one network my resolv.conf looks as expected. Any ideas?

my manifest - https://gist.github.com/prysmakou/71eb75d984b62aa8cc2d
Or NAT is only one solution on vSphere?

vSphere/BOSH 2969",,94598074,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",2 nics on a job on vsphere do not get dns options,1406536.0,[1406536],956238,81882,chore,2015-05-18T18:42:14Z,https://www.pivotaltracker.com/story/show/94598074
2015-05-18T21:08:14Z,2015-04-27T17:38:12Z,accepted,,- this is needed to support non-shared datastore AZ configuration,4.0,93358220,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}, {'name': 'vsphere-loc-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:16:59Z', 'id': 11514338, 'updated_at': '2015-04-27T17:16:59Z'}]",vsphere cpi should pick persistent/ephemeral datastores that are available to the hosts in the cluster,553935.0,"[553935, 1633158]",956238,81882,feature,2015-05-18T21:08:23Z,https://www.pivotaltracker.com/story/show/93358220
2015-05-18T21:08:15Z,2015-05-01T19:02:10Z,accepted,,,1.0,93718276,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}, {'name': 'vsphere-loc-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:16:59Z', 'id': 11514338, 'updated_at': '2015-04-27T17:16:59Z'}]",create_disk should account for the vm location and only select datastores that are available in the cluster vm is running,553935.0,"[553935, 1655862]",956238,119,feature,2015-05-18T21:08:32Z,https://www.pivotaltracker.com/story/show/93718276
2015-05-18T22:45:25Z,2015-04-01T02:05:33Z,accepted,,"https://github.com/cloudfoundry/bosh-agent/blob/master/jobsupervisor/monit/http_client.go

Retry client logic does not close the body.",,91563414,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",leaking connection when receiving non-200 monit response,1541728.0,[1541728],956238,1519906,bug,2015-05-18T22:45:25Z,https://www.pivotaltracker.com/story/show/91563414
2015-05-21T01:18:28Z,2015-05-20T20:10:24Z,accepted,,,,95088852,story,"[{'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Upgrade concourse to version 0.51,1550486.0,[1550486],956238,1550486,chore,2015-06-25T00:01:22Z,https://www.pivotaltracker.com/story/show/95088852
2015-05-21T19:23:05Z,2015-05-07T18:06:35Z,accepted,,"Director was only using cloud config, no deployments without cloud config",4.0,94116458,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",user can deploy a deployment that is using cloud config and get a next available IP address,553935.0,"[553935, 344, 1655862]",956238,81882,feature,2015-06-24T19:24:27Z,https://www.pivotaltracker.com/story/show/94116458
2015-05-21T21:51:03Z,2015-05-21T21:00:41Z,accepted,,,,95196182,story,[],add Github -> Tracker integration to all bosh managed tracker projects/repos,119.0,[119],956238,119,chore,2015-05-21T21:51:05Z,https://www.pivotaltracker.com/story/show/95196182
2015-05-21T21:56:27Z,2015-02-18T00:48:37Z,accepted,,https://github.com/cloudfoundry/bosh/pull/749,1.0,88572736,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Check for user and password also in DAV blobstores',1602644.0,[1602644],956238,81882,feature,2015-05-21T21:56:27Z,https://www.pivotaltracker.com/story/show/88572736
2015-05-22T00:30:26Z,2015-05-15T22:40:49Z,accepted,,also the authors should be reset every night,,94713704,story,[],setup workstation with git duet --global by default,119.0,[119],956238,119,chore,2015-05-22T00:30:26Z,https://www.pivotaltracker.com/story/show/94713704
2015-05-22T01:02:28Z,2015-05-18T18:18:32Z,accepted,,"- user updates cloud-config -> nothing happens to any vm
- user runs bosh deploy -> vm gets moved from now reserved IP to a next available IP
  - only VMs in this deploy
  - do not move any VM that is 'ok'",2.0,94877124,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",When reserved IPs change in cloud config VM should get a new IP,553935.0,"[553935, 878715]",956238,553935,feature,2015-05-22T01:02:29Z,https://www.pivotaltracker.com/story/show/94877124
2015-05-22T01:02:40Z,2015-05-19T22:00:54Z,accepted,,"we believe this should be the case, let's verify that",1.0,95004048,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Deployments that were never deployed with cloud config should be resurrected with original deployment,553935.0,"[553935, 878715]",956238,553935,feature,2015-05-22T01:02:40Z,https://www.pivotaltracker.com/story/show/95004048
2015-05-22T01:21:52Z,2015-05-14T22:13:00Z,accepted,,,0.0,94627846,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",make sure that assigned IPs to the VMs on manual networks never change,553935.0,[553935],956238,81882,feature,2015-05-22T01:21:53Z,https://www.pivotaltracker.com/story/show/94627846
2015-05-22T17:43:51Z,2015-05-14T21:46:34Z,accepted,,"Notes of interest:
- Should improve rendering performance.
- Introduces GUID into working dir path, which means we'll need to update scripts, manifests, etc. to know nothing about the working path (which is a good thing).",,94626090,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Update main concourse to 0.50,81882.0,[81882],956238,1550486,chore,2015-05-22T17:43:57Z,https://www.pivotaltracker.com/story/show/94626090
2015-05-22T21:16:02Z,2015-05-15T22:38:55Z,accepted,,,,94713596,story,[],figure out if we want to install bosh for bats tests,81882.0,[81882],956238,553935,chore,2015-10-08T00:21:31Z,https://www.pivotaltracker.com/story/show/94713596
2015-05-22T23:51:17Z,2015-05-21T23:05:41Z,accepted,,,,95205028,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}]",migrate bosh-agent unit tests to main bosh pipeline,553935.0,"[553935, 1495236]",956238,553935,chore,2015-05-22T23:51:18Z,https://www.pivotaltracker.com/story/show/95205028
2015-05-23T00:03:03Z,2015-05-22T18:10:18Z,accepted,,,,95271454,story,"[{'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",bosh cli version should be printed within task scripts,1550486.0,[1550486],956238,1550486,chore,2015-05-23T00:03:03Z,https://www.pivotaltracker.com/story/show/95271454
2015-05-23T00:32:17Z,2015-05-15T22:13:47Z,accepted,,"Try until no more IPs are available
",1.0,94712466,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Deployments with cloud config that run in parallel can reserve next available IP without race conditions,553935.0,"[553935, 1655862]",956238,553935,feature,2015-06-24T19:24:28Z,https://www.pivotaltracker.com/story/show/94712466
2015-05-26T23:56:45Z,2015-05-15T22:20:28Z,accepted,,"First test:
1) deploy before cloud config with reserved static ip on network A
2) upload cloud config  which also defines network A
3) deploy a second deployment with a static IP from network A
4) second deployment fails

Second test:
1) deploy before cloud config with reserved automatic ip range on network A
2) upload cloud config  which also defines network A
3) deploy a second deployment with automatic from network A
4) second deployment gets next available ip outside of first deployments range",4.0,94712796,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",new deployments that use cloud config should not try using IPs that were given out to deployments that are still not using cloud config,553935.0,"[553935, 878715, 1495236]",956238,553935,feature,2015-06-24T19:24:28Z,https://www.pivotaltracker.com/story/show/94712796
2015-05-27T00:18:42Z,2015-05-05T04:40:11Z,accepted,,"- user specifies same `static_ips: [x]` on two vms in two different deployments on the same manual network
- include name of the deployment that owns it",2.0,93869286,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",user should see an error message if two deployments try to request same static IP,553935.0,"[553935, 1655862]",956238,81882,feature,2015-06-24T19:24:28Z,https://www.pivotaltracker.com/story/show/93869286
2015-05-27T16:33:47Z,2015-05-26T23:10:56Z,accepted,,,,95483298,story,[],Get Kam onboarded,553935.0,[553935],956238,553935,chore,2015-05-27T16:33:48Z,https://www.pivotaltracker.com/story/show/95483298
2015-05-27T16:33:51Z,2015-05-26T23:11:06Z,accepted,,,,95483308,story,[],Get Tim onboarded,553935.0,[553935],956238,553935,chore,2015-05-27T16:33:51Z,https://www.pivotaltracker.com/story/show/95483308
2015-05-28T16:58:33Z,2015-05-27T17:55:13Z,accepted,,trigger: true is not default,,95552738,story,[],Fix bosh-agent to trigger on push,553935.0,"[553935, 1550486]",956238,553935,chore,2015-05-28T16:58:33Z,https://www.pivotaltracker.com/story/show/95552738
2015-05-28T23:48:42Z,2015-05-20T21:34:34Z,accepted,,"create new concourse pipeline for bosh-utils
Develop to master promote",,95096898,story,[],Run tests for bosh-utils,553935.0,"[553935, 1550486]",956238,81882,chore,2015-05-28T23:48:47Z,https://www.pivotaltracker.com/story/show/95096898
2015-05-29T00:31:47Z,2015-05-21T23:54:42Z,accepted,,so we can easily see the latest all-green sha,,95207888,story,[],main ci pipeline should have a job at the end that requires all the integrations to have passed,553935.0,"[553935, 1550486]",956238,687691,chore,2015-05-29T00:31:47Z,https://www.pivotaltracker.com/story/show/95207888
2015-05-29T00:56:41Z,2015-05-15T17:55:42Z,accepted,,For some reason it appears that the configuration specified in test-integration.yml is not being respected.,,94688780,story,[],"Fix main pipeline integration suite on concourse to actually log WARN level, not INFO",553935.0,"[553935, 1550486]",956238,1550486,chore,2015-05-29T00:56:41Z,https://www.pivotaltracker.com/story/show/94688780
2015-05-29T01:02:37Z,2015-05-21T23:06:54Z,accepted,,- delete from travis completely (history),,95205112,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}]",kill bosh-init travis,553935.0,"[553935, 1550486]",956238,553935,chore,2015-05-29T01:02:37Z,https://www.pivotaltracker.com/story/show/95205112
2015-06-01T18:37:06Z,2015-05-21T23:05:29Z,accepted,,"Brian moved vsphere BATs 
- new builds already exist",,95205014,story,[],Migrate bosh jenkins vsphere lifecycle + bats to nested ESXi,553935.0,"[553935, 1550486]",956238,553935,chore,2015-06-01T18:37:06Z,https://www.pivotaltracker.com/story/show/95205014
2015-06-02T18:13:50Z,2015-05-28T15:57:44Z,accepted,,"The script at https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/bosh_go_agent/apply.sh#L27 depends on the script at https://github.com/cloudfoundry/bosh-agent/blob/master/bin/build to cleanly rebuild the DAV blobstore CLI, which has recently been moved into the new bosh-utils repo.

There are two problems that need to be fixed:

1. The line that's supposed to build github.com/cloudfoundry/bosh-utils/davcli/main is commented out. Uncommenting it doesn't help (maybe because davcli is missing from Godeps?).

1. The bosh-agent/bin/build script doesn't clean the output directory before attempting the rebuild. If the build fails or doesn't happen for some reason (see previous point), the script picks up whatever was in the bosh-agent/out dir and packages it with the stemcell. In my case, this was an OS X binary that got packaged into an Ubuntu stemcell.",,95634510,story,[],bosh-blobstore-dav does not build correctly on stemcells,553935.0,"[553935, 1550486]",956238,1541728,bug,2015-06-02T18:13:50Z,https://www.pivotaltracker.com/story/show/95634510
2015-06-02T18:20:57Z,2015-05-28T21:17:27Z,accepted,,,0.0,95664106,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump stemcell os images,81882.0,[81882],956238,81882,feature,2015-06-02T18:21:08Z,https://www.pivotaltracker.com/story/show/95664106
2015-06-04T02:56:52Z,2015-06-02T01:14:19Z,accepted,,,,95931628,story,"[{'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",configure local openstack env for tom/edward,1386874.0,"[1386874, 1495236]",956238,81882,chore,2015-06-08T22:55:32Z,https://www.pivotaltracker.com/story/show/95931628
2015-06-05T17:58:47Z,2015-04-30T00:48:54Z,accepted,,,1.0,93587846,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",CLI should retry on OpenSSL::X509::StoreError,344.0,[344],956238,553935,feature,2015-06-05T17:58:48Z,https://www.pivotaltracker.com/story/show/93587846
2015-06-05T18:25:07Z,2015-04-08T20:48:05Z,accepted,,"- MAKE SURE that multiple create_vm/create_stemcell calls can run at the same time i.e. ignore AlreadyExists errors
- rdr environment in vcloud?

This is similar in spirit to the work that was done in the vSphere CPI to create the enclosing folder for a deployment

In Ops Manager currently we do the following:

[web/app/models/tempest/vcloud/vcloud_infrastructure.rb:37-40](https://github.com/pivotal-cf/installation/blob/master/web/app/models/tempest/vcloud/vcloud_infrastructure.rb#L37-L40)
```ruby
module Tempest
  module Vcloud
    class VcloudInfrastructure < BaseInfrastructure
# ...
      def prepare_to_deploy_microbosh
        manager = VcloudClients::VdcManager.new(iaas_configuration)
        manager.create_catalog(vdc.catalog)
      end
# ...
    end
  end
end
```

[gems/vcloud_clients/lib/vcloud_clients/vdc_manager.rb:10](https://github.com/pivotal-cf/installation/blob/master/gems/vcloud_clients/lib/vcloud_clients/vdc_manager.rb#L10)",4.0,92079290,story,"[{'name': 'vcloud', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-10T00:51:45Z', 'id': 11363370, 'updated_at': '2015-04-10T00:51:45Z'}]",vCloud CPI should create vApp template and catalog when creating a deployment,1541728.0,[1541728],956238,1054467,feature,2015-08-19T18:42:39Z,https://www.pivotaltracker.com/story/show/92079290
2015-06-05T19:02:08Z,2015-05-27T17:37:49Z,accepted,,"- to avoid improper escaping via yaml rendering
- already doing for vsphere",1.0,95550578,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]","wrap openstack endpoint, user, password, and tenant to be properly escaped",553935.0,"[553935, 1550486]",956238,81882,feature,2015-06-05T19:02:18Z,https://www.pivotaltracker.com/story/show/95550578
2015-06-06T00:14:12Z,2015-05-26T18:48:07Z,accepted,,"see https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/api/task_remover.rb#L9

FileUtils.rm_rf(task.output) will fail if task.output is not present. we should skip file deletion if task.output is nil or empty.",,95458780,story,[],task deletion should succeed even if old tasks did not have output files,553935.0,"[553935, 1550486]",956238,81882,bug,2015-06-06T00:14:12Z,https://www.pivotaltracker.com/story/show/95458780
2015-06-06T00:32:33Z,2015-05-05T04:42:47Z,accepted,,"- make one deployment with one vm and deploy
- make another deployment with one vm and deploy
- delete first deployment
- increase number of vms in the second deployment and see that additional vms get ips that were originally used by the first deployment

---
- for static and dynamic IPs",2.0,93869364,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",user can deploy a VM with an IP once it is released from a different deployment ,878715.0,"[878715, 1550486]",956238,81882,feature,2015-06-24T19:24:29Z,https://www.pivotaltracker.com/story/show/93869364
2015-06-06T00:32:33Z,2015-05-21T19:45:16Z,accepted,,currently it seems that IPs are not released when VMs are deleted but rather are deleted when deployment is deleted,2.0,95189000,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",delete deployment command should release IPs as soon as the VM is gone,878715.0,[878715],956238,81882,feature,2015-06-06T00:32:33Z,https://www.pivotaltracker.com/story/show/95189000
2015-06-09T00:22:26Z,2015-06-08T17:14:10Z,accepted,,"To save configuration for jenkins, looks like its broken",,96468836,story,[],Fix bosh-jenkins-config plugin,553935.0,[553935],956238,553935,chore,2015-06-09T00:22:26Z,https://www.pivotaltracker.com/story/show/96468836
2015-06-09T19:25:40Z,2015-06-09T00:16:09Z,accepted,,,,96508118,story,[],FIx bosh_vcloud_cpi,553935.0,[553935],956238,553935,chore,2015-06-09T19:37:30Z,https://www.pivotaltracker.com/story/show/96508118
2015-06-09T19:25:45Z,2015-06-08T21:14:43Z,accepted,,,,96495254,story,[],Get Scott and Megan onboarded,553935.0,[553935],956238,553935,chore,2015-06-09T19:25:46Z,https://www.pivotaltracker.com/story/show/96495254
2015-06-09T19:26:43Z,2015-06-09T19:26:34Z,accepted,,"Seems like there is a race condition git pull && git push, so retry for couple times
",,96593172,story,[],Retry pushing deployments bosh in auto-deployer,553935.0,"[553935, 1569704]",956238,553935,chore,2015-06-09T19:26:44Z,https://www.pivotaltracker.com/story/show/96593172
2015-06-09T22:24:47Z,2015-06-08T22:58:12Z,accepted,,,,96504012,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Update unit & integration suites to use the vendor/cache gems (speed up bundle install),119.0,[119],956238,1550486,chore,2015-06-09T22:24:48Z,https://www.pivotaltracker.com/story/show/96504012
2015-06-10T02:21:09Z,2015-06-04T18:16:27Z,accepted,,following line: https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L60 does not properly pass in region info hence when using elb client (https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/instance.rb#L88) it complains that it cannot find the instance.,,96220232,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",aws cpi should find instances for attaching to an elb from the correct region,553935.0,[553935],956238,81882,bug,2015-06-10T02:21:45Z,https://www.pivotaltracker.com/story/show/96220232
2015-06-10T15:29:12Z,2015-05-26T19:16:35Z,accepted,,"- what features
- what failures may result with a vm that does not belong to any instance (consistency model)
- 2 days max",4.0,95463414,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",investigate scenarios / use cases when we have a vm that's not associated with an instance,878715.0,"[878715, 687691]",956238,81882,feature,2015-06-10T15:29:12Z,https://www.pivotaltracker.com/story/show/95463414
2015-06-10T15:31:24Z,2015-06-04T18:34:29Z,accepted,,"First we want to understand when VMs are created when this config is provided (current behavior).  We will likely remove this functionality no matter what. Note that we will not blow up if the size entry is in a manifest, but will ignore it.",2.0,96222024,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Deprecate/Disable Resource Pool size configuration (Review),878715.0,"[878715, 687691]",956238,1550486,feature,2015-06-24T19:24:30Z,https://www.pivotaltracker.com/story/show/96222024
2015-06-10T15:33:55Z,2015-06-02T18:47:52Z,accepted,,Looks like bumping bosh-utils with vendored dependencies does not work.,,96010984,story,[],Bump bosh-utils in bosh-init/bosh-agent,1541728.0,[1541728],956238,553935,chore,2015-06-10T15:33:47Z,https://www.pivotaltracker.com/story/show/96010984
2015-06-10T18:56:19Z,2015-06-10T18:55:37Z,accepted,,,,96696034,story,[],Fix redis logs to go to stdout,553935.0,[553935],956238,553935,chore,2015-06-10T18:56:19Z,https://www.pivotaltracker.com/story/show/96696034
2015-06-10T20:42:04Z,2015-06-10T17:49:00Z,accepted,,https://support.bluebox.net/tickets/3EAB5E,,96688792,story,[],Failing to create volume from image source on BlueBox,553935.0,[553935],956238,553935,chore,2015-06-10T20:42:05Z,https://www.pivotaltracker.com/story/show/96688792
2015-06-10T22:34:00Z,2015-05-27T22:30:38Z,accepted,,"We running tests with go 1.4, but ship agent that is built with 1.3",,95579698,story,[],Update go to 1.4 in stemcell builder,553935.0,"[553935, 1569704]",956238,553935,chore,2015-06-10T22:34:12Z,https://www.pivotaltracker.com/story/show/95579698
2015-06-11T17:57:14Z,2015-06-05T18:11:00Z,accepted,,- maintenance update to address lua problem,1.0,96308676,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",update redis to latest version,119.0,[119],956238,81882,feature,2015-06-11T17:57:24Z,https://www.pivotaltracker.com/story/show/96308676
2015-06-11T17:57:50Z,2015-06-09T21:01:02Z,accepted,,"re https://github.com/cloudfoundry/bosh_vcloud_cpi/pull/14
- cannot repro on our environments",1.0,96601998,story,"[{'name': 'vcloud', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-10T00:51:45Z', 'id': 11363370, 'updated_at': '2015-04-10T00:51:45Z'}]",pull in vcloud changes to deal with pre-running state of tasks,553935.0,"[553935, 1569704]",956238,81882,feature,2015-06-11T17:57:50Z,https://www.pivotaltracker.com/story/show/96601998
2015-06-12T00:09:29Z,2015-06-10T23:47:31Z,accepted,,"kernel problem with ipv4 ping
http://www.ubuntu.com/usn/usn-2635-1/",0.0,96724548,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",update ubuntu trusty for USN-2635-1,553935.0,[553935],956238,81882,feature,2015-06-12T00:09:36Z,https://www.pivotaltracker.com/story/show/96724548
2015-06-12T22:06:50Z,2015-05-27T23:54:08Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb#L306 calls destroy but destroy call is non-blocking. we need to wait for the server to be deleted before returning from the cpi since director will try to reuse the manual network ip. 

- other places do something like this: https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb#L331
- in addition to waiting if waiting for the server fails we should still raise original error but log the failure from waiting.",,95584442,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",openstack cpi should wait for the server to be deleted ,1386874.0,"[1386874, 1495236]",956238,81882,bug,2015-06-12T22:06:51Z,https://www.pivotaltracker.com/story/show/95584442
2015-06-14T19:00:00Z,2015-05-05T16:52:07Z,accepted,,"- introduce new agent message `update_settings` that takes format below and returns ""updated"" or error
- this call is needed instead of using apply since other agent actions use blobstore (e.g. compile_package) and if blobstore is used it may have custom cert-ed endpoint
- make update_settings async!
- (later on this call will include ntp settings, blobstore settings, etc.)
- make sure default cert bundles are still there untouched
- at this point certs user can add/replace certs from the vm

input:

```
{
  ""cert"": ""..."" # pem formatted string
  ""blobstore"": ""..."" # later
  ""ntp"": ""..."" # later
}
```",4.0,93919326,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user should be able to add a cert bundle on a single agent on ubuntu,1541728.0,[1541728],956238,81882,feature,2015-06-16T23:29:32Z,https://www.pivotaltracker.com/story/show/93919326
2015-06-14T19:00:00Z,2015-05-05T16:52:20Z,accepted,,"- make sure default cert bundles are still there untouched
- at this point certs user can add/replace certs from the vm",2.0,93919346,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user should be able to configure cert bundle on a single agent on centos,1541728.0,[1541728],956238,81882,feature,2015-06-16T23:43:27Z,https://www.pivotaltracker.com/story/show/93919346
2015-06-14T19:00:00Z,2015-04-01T21:02:03Z,accepted,,"- add director configuration for cert bundle as a property
- call update_settings agent call after vm is created
- include a warning in the task event log if agent returns unkown rpc method for update_settings
- any place where create vm gets called (resurrector, normal flow, net config recreate etc.)",4.0,91642850,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user can set cert on __new__ vms created by the director,1541728.0,[1541728],956238,81882,feature,2015-06-16T23:44:49Z,https://www.pivotaltracker.com/story/show/91642850
2015-06-14T19:00:00Z,2015-05-05T17:01:18Z,accepted,,"- update_settings call will include nil or empty cert key
- make sure default cert bundles are still there untouched
- at this point certs user can add/remove/replace certs from the vm",2.0,93920456,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user should be able to unconfigure a previously provided cert on a single agent,1541728.0,[1541728],956238,81882,feature,2015-06-17T18:39:27Z,https://www.pivotaltracker.com/story/show/93920456
2015-06-15T18:32:04Z,2015-06-11T18:35:35Z,accepted,,"http://www.ubuntu.com/usn/usn-2639-1/
- expecting: libssl1.0.0: 1.0.1f-1ubuntu2.15",0.0,96811440,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",update ubuntu trusty for USN-2639-1,553935.0,[553935],956238,81882,feature,2015-06-15T18:32:05Z,https://www.pivotaltracker.com/story/show/96811440
2015-06-17T18:35:52Z,2015-06-16T16:41:07Z,accepted,,http://www.ubuntu.com/usn/usn-2646-1,1.0,97122542,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu kernel for USN-2646-1,553935.0,[553935],956238,81882,feature,2015-06-17T18:35:58Z,https://www.pivotaltracker.com/story/show/97122542
2015-06-18T16:51:30Z,2015-06-17T18:50:33Z,accepted,,,1.0,97232314,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",figure out how to create and use trusted cert chains,1541728.0,[1541728],956238,81882,feature,2015-06-18T16:51:30Z,https://www.pivotaltracker.com/story/show/97232314
2015-06-19T19:01:46Z,2015-05-06T21:29:55Z,accepted,,"- creds shared via email
- default_security_groups key
- security_groups key",2.0,94035130,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh should be able to use security-group ID in addition to using security-group NAME,1386874.0,"[1386874, 1495236]",956238,1054467,feature,2015-06-19T19:01:47Z,https://www.pivotaltracker.com/story/show/94035130
2015-06-19T19:50:23Z,2015-06-19T19:08:51Z,accepted,,,,97420956,story,[],revert 'aws cpi's snapshot_disk should not wait until it's completed and rather should wait for 'pending' state',553935.0,[553935],956238,81882,chore,2015-06-19T19:50:25Z,https://www.pivotaltracker.com/story/show/97420956
2015-06-20T00:07:42Z,2015-06-08T05:52:59Z,accepted,,"The latest openstack code needs the volume size to be set as well as the volume id when choosing boot vm from volume, at present, in the CPI, the volume id will be set while the volume size is empty.",,96415928,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",The latest openstack requires the volume_size to be set when trying to boot vm from volume,1386874.0,"[1386874, 1495236]",956238,1386874,bug,2015-06-20T00:07:42Z,https://www.pivotaltracker.com/story/show/96415928
2015-06-22T21:39:38Z,2015-06-19T23:00:35Z,accepted,,,,97438370,story,[],Publish artifacts to pipeline should fail if develop does not include last commit from master,553935.0,[553935],956238,553935,chore,2015-06-22T21:39:38Z,https://www.pivotaltracker.com/story/show/97438370
2015-06-22T23:01:06Z,2015-06-22T21:43:13Z,accepted,,"Some builds fail like this: 

 ERROR: bosh -n -c /tmp/build/8ed8e73b-f36e-4934-5fb9-df8b287755bb/bosh-src/tmp/integration-tests-workspace/pid-8096/client-sandbox/bosh_config.yml vms --details failed with output:
       Acting as user 'test' on 'Test Director'
       Deployment `simple'
       
       Director task 79
       Error 100: undefined method `credentials' for nil:NilClass
       
       Task 79 error
       Failed to fetch VMs information from director",,97581952,story,[],Investigate flaky get_vms  on concourse,553935.0,[553935],956238,553935,chore,2015-06-22T23:01:06Z,https://www.pivotaltracker.com/story/show/97581952
2015-06-23T00:50:27Z,2015-06-11T00:14:15Z,accepted,,"Currently bosh uses snapshot_disk CPI method to take an EBS volume snapshot at certain points; however, sometimes it times out because the volume is large. Currently bosh tries to wait until snapshot is complete. I believe that AWS docs [1] recommend to only wait until status is pending and then continue on. For this story let's switch snapshotting to wait for either 'complete' or 'pending' (https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L352)

[1] ""You can remount and use your volume while the snapshot status is pending."" from http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html",1.0,96726132,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",aws cpi's snapshot_disk should not wait until it's completed and rather should wait for 'pending' state,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-06-23T00:50:27Z,https://www.pivotaltracker.com/story/show/96726132
2015-06-23T18:30:30Z,2015-06-19T17:27:39Z,accepted,,https://github.com/cloudfoundry/bosh/pull/855/files,1.0,97405092,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Fix release tarball command to properly handle whitespace in file paths,687691.0,[687691],956238,81882,feature,2015-06-23T18:30:31Z,https://www.pivotaltracker.com/story/show/97405092
2015-06-23T18:37:23Z,2015-06-06T00:13:54Z,accepted,,/var/vcap/bosh/etc/stemcell_git_sha1 just like we have /var/vcap/bosh/etc/stemcell_version,2.0,96332276,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",include sha1 in the built stemcell ,1355110.0,[1355110],956238,81882,feature,2015-06-23T18:37:23Z,https://www.pivotaltracker.com/story/show/96332276
2015-06-23T18:42:51Z,2015-06-18T19:29:05Z,accepted,,"- db migration: leave it as null
  - on upload stemcell: leave it as null if OS key not present
  - +-----------------------------------------+---------------+---------+--------------------+
	| Name                                    | OS            | Version | CID                |
	+-----------------------------------------+---------------+------------------------------+
	| bosh-aws-xen-hvm-ubuntu-trusty-go_agent | ubuntu-trusty | 2972    | ami-f0544998 light |
	| bosh-aws-xen-hvm-ubuntu-trusty-go_agent |               | 2971    | ami-f0544997 light |
  - n/a consistencey? -dk",4.0,97328742,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can see OS when running bosh stemcells,1541728.0,"[1541728, 1355110, 948679]",956238,81882,feature,2015-06-23T18:42:59Z,https://www.pivotaltracker.com/story/show/97328742
2015-06-23T18:48:20Z,2015-06-22T17:37:54Z,accepted,,http://www.ubuntu.com/usn/usn-2644-2/,0.0,97555196,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty for usn-2644-2,553935.0,[553935],956238,553935,feature,2015-06-23T18:48:21Z,https://www.pivotaltracker.com/story/show/97555196
2015-06-23T18:54:20Z,2015-06-19T17:27:56Z,accepted,,https://github.com/cloudfoundry/bosh/pull/854,0.0,97405108,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",Print newlines around slow loading warning to stderr instead of stdout (fixes #852),553935.0,"[553935, 1569704]",956238,81882,feature,2015-06-23T18:54:20Z,https://www.pivotaltracker.com/story/show/97405108
2015-06-23T19:16:43Z,2015-06-19T17:11:10Z,accepted,,,1.0,97403588,story,"[{'name': 'vcloud', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-10T00:51:45Z', 'id': 11363370, 'updated_at': '2015-04-10T00:51:45Z'}]","escape vcloud url, user, password, org, dc, vapp, media, metadata key, description",687691.0,"[687691, 1655862]",956238,687691,feature,2015-06-23T19:16:43Z,https://www.pivotaltracker.com/story/show/97403588
2015-06-23T19:16:59Z,2015-06-13T00:46:11Z,accepted,,"use JSON.dumpa

the `gsub` doesn't work consistently across ruby/yaml versions",,96930436,story,[],remove dump_yaml from director.yml.erb.erb since it does not properly work ,687691.0,"[687691, 1655862]",956238,81882,bug,2015-06-23T19:16:59Z,https://www.pivotaltracker.com/story/show/96930436
2015-06-23T20:32:46Z,2015-03-02T22:05:53Z,accepted,,"It would be nice to not depend on our own (old) fork. Unfortunately if we upgrade tests fail due to https://github.com/defunkt/fakefs/issues/266 -- we could work around it, wait for a fix, or submit our own pull request",,89460604,story,[],Bump FakeFS so we don't rely on our fork any more,1550486.0,[1550486],956238,687691,chore,2015-06-23T20:46:04Z,https://www.pivotaltracker.com/story/show/89460604
2015-06-23T20:37:13Z,2015-06-03T21:27:11Z,accepted,,"ensure nested dependencies are not getting vendored in

Describe workflows for using Vendor
- adding a new package
- adding a package to a dependent package
- updating a package
- adding packaged from a repo you are already importing packages from
- removing packages
",,96133554,story,[],move bosh-init & bosh-agent to Vendor from Godeps,553935.0,[553935],956238,553935,chore,2015-06-23T20:37:13Z,https://www.pivotaltracker.com/story/show/96133554
2015-06-23T22:44:50Z,2015-06-19T00:13:45Z,accepted,,https://github.com/cloudfoundry/bosh/pull/856,0.0,97349584,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",allow to set hardware virtualization on vsphere vms,553935.0,"[553935, 1569704]",956238,81882,feature,2015-06-23T22:44:50Z,https://www.pivotaltracker.com/story/show/97349584
2015-06-24T00:17:22Z,2015-04-01T21:03:20Z,accepted,,"- cert might be added, removed or updated
- make sure that vms that already have the matching certs do not have their running jobs restarted",4.0,91643024,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user should be able to run a `bosh deploy` and vms that do not have a matching ca cert bundle should be updated with a new cert bundle,1541728.0,[1541728],956238,81882,feature,2015-06-24T00:17:32Z,https://www.pivotaltracker.com/story/show/91643024
2015-06-24T00:36:53Z,2015-06-17T22:24:58Z,accepted,,"""@dkalinin: we figured out the cert issue, it's because the 'update-ca-certificates' command should be run with the -f option, this clears the broken sym links which seems to be the reason the CAcert file is not found @jfuerth""",1.0,97250538,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",make sure chain certs can be used after system has be updated with new certs,1541728.0,[1541728],956238,81882,feature,2015-06-24T00:36:54Z,https://www.pivotaltracker.com/story/show/97250538
2015-06-24T00:36:58Z,2015-05-05T17:02:17Z,accepted,,,,93920538,story,"[{'name': 'ca-bundle', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-01T21:03:50Z', 'id': 11292210, 'updated_at': '2015-04-01T21:03:50Z'}]",user can configure director created vms with a custom cert,,[],956238,81882,release,2015-06-24T00:36:58Z,https://www.pivotaltracker.com/story/show/93920538
2015-06-24T23:09:12Z,2015-06-11T17:02:33Z,accepted,,"- private key path
- stemcell
- release",,96801172,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",all paths resolved from the manifest should be relative to the manifest,1569704.0,"[1569704, 1550486, 687691]",956238,81882,bug,2015-06-24T23:09:13Z,https://www.pivotaltracker.com/story/show/96801172
2015-06-24T23:13:07Z,2015-06-18T19:28:36Z,accepted,,"- db migration: leave it as null
  - on upload stemcell: leave it as null if OS key not present
  - +-----------------------------------------+---------------+---------+--------------------+
	| Name                                    | OS            | Version | CID                |
	+-----------------------------------------+---------------+------------------------------+
	| bosh-aws-xen-hvm-ubuntu-trusty-go_agent | ubuntu-trusty | 2972    | ami-f0544998 light |
	| bosh-aws-xen-hvm-ubuntu-trusty-go_agent |               | 2971    | ami-f0544997 light |
  - n/a consistencey? -dk",2.0,97328712,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]","user can see OS in the stemcell metadata for all stemcells (e.g. centos-7, ubuntu-trusty)",1541728.0,"[1541728, 1355110, 948679]",956238,81882,feature,2015-06-24T23:13:08Z,https://www.pivotaltracker.com/story/show/97328712
2015-06-24T23:31:04Z,2015-06-18T19:30:25Z,accepted,,- validate format of the args for the CLI command (string with a slash in the middle),1.0,97328848,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user sees an error before director task runs about the format of the release and stemcell refs,1355110.0,"[1355110, 948679]",956238,81882,feature,2015-06-24T23:31:24Z,https://www.pivotaltracker.com/story/show/97328848
2015-06-24T23:31:09Z,2015-06-18T19:30:00Z,accepted,,"- `bosh export release release-name/release-version (os type)/stemcell-version`
- assumes that some deployment is targetted (for backwards compatibility / recovery)",2.0,97328802,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can run export release command and see that args are validated and empty director task completes,1541728.0,"[1541728, 948679]",956238,81882,feature,2015-06-24T23:31:16Z,https://www.pivotaltracker.com/story/show/97328802
2015-06-24T23:51:37Z,2015-06-22T18:29:42Z,accepted,,https://github.com/cloudfoundry/bosh/pull/858,2.0,97561020,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in 'Do not remove instance from ELBs when terminating instances',553935.0,[553935],956238,81882,feature,2015-06-24T23:51:37Z,https://www.pivotaltracker.com/story/show/97561020
2015-06-25T00:25:15Z,2015-06-24T19:32:12Z,accepted,,,,97759678,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",configure bosh-lite to have resurrector properly working,81882.0,[81882],956238,81882,bug,2015-06-25T00:25:17Z,https://www.pivotaltracker.com/story/show/97759678
2015-06-25T00:27:45Z,2015-06-19T19:15:45Z,accepted,,"```
before:
     Error 100: Received unknown error from cpi: Unknown with message The volume 'vol-76aeef98' does not exist.
wanted:
     Error 100: Unknown CPI error 'Unknown' with message 'The volume 'vol-76aeef98' does not exist'.
```",1.0,97421420,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",improve external cpi error message,687691.0,[687691],956238,81882,feature,2015-06-25T00:27:46Z,https://www.pivotaltracker.com/story/show/97421420
2015-06-26T20:49:57Z,2015-06-17T17:46:26Z,accepted,,"We've moved this to main concourse deployment, and the secrets from bosh-init-ci to bosh-concourse-ci on 6/17/15. After we're sure we won't need any history from the old deployment/pipeline we should delete them.",,97225572,story,[],Delete old bosh-init concourse deployment and repo,687691.0,[687691],956238,687691,chore,2015-06-26T20:49:57Z,https://www.pivotaltracker.com/story/show/97225572
2015-06-29T23:20:06Z,2015-06-18T19:30:42Z,accepted,,- user sees an error from director task,2.0,97328860,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",director task validates release presence on the director,1355110.0,"[1355110, 948679]",956238,81882,feature,2015-06-29T23:20:07Z,https://www.pivotaltracker.com/story/show/97328860
2015-06-29T23:20:12Z,2015-06-18T19:31:02Z,accepted,,"- user sees an error from director task when there is no stemcell that matches os/version criteria
- DO NOT show error when there is more than one stemcell match",1.0,97328892,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",director task validates stemcell presence on the director,1355110.0,[1355110],956238,81882,feature,2015-06-29T23:20:12Z,https://www.pivotaltracker.com/story/show/97328892
2015-06-29T23:20:51Z,2015-06-18T19:31:40Z,accepted,,,1.0,97328938,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user sees an error if deployment is not set or deployment does not exist on the director,1541728.0,"[1541728, 1355110]",956238,81882,feature,2015-06-29T23:20:51Z,https://www.pivotaltracker.com/story/show/97328938
2015-06-30T14:02:22Z,2015-06-24T22:04:20Z,accepted,,"seems like putting bundler into gemfile might work:

```
jtarchie [3:08 PM] 
Did a quick test of putting your bundler version in you `Gemfile`. It actually gives some usable output.

```ot@9941ab857587:/app# bundle --version
Bundler version 1.10.4
root@9941ab857587:/app# cat Gemfile
source 'https://rubygems.org'

gem 'bundler', '1.10.5'
root@9941ab857587:/app# bundlke
bash: bundlke: command not found
root@9941ab857587:/app# bundle
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and installing your bundle as root will break this application for all non-root users on this machine.
Fetching gem metadata from https://rubygems.org/..
Fetching version metadata from https://rubygems.org/.
Resolving dependencies...
Bundler could not find compatible versions for gem ""bundler"":
  In Gemfile:
    bundler (= 1.10.5) ruby

  Current Bundler version:
    bundler (1.10.4)
This Gemfile requires a different version of Bundler.
Perhaps you need to update Bundler by running `gem install bundler`?
Could not find gem 'bundler (= 1.10.5) ruby in any of the sources
root@9941ab857587:/app# gem update bundler
Updating installed gems
Updating bundler
Fetching: bundler-1.10.5.gem (100%)
Successfully installed bundler-1.10.5
Gems updated: bundler
root@9941ab857587:/app# bundle
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and installing your bundle as root will break this application for all non-root users on this machine.
Fetching gem metadata from https://rubygems.org/..
Fetching version metadata from https://rubygems.org/.
Resolving dependencies...
Using bundler 1.10.5
Bundle complete! 1 Gemfile dependency, 1 gem now installed.
Bundled gems are installed into /usr/local/bundle.
root@9941ab857587:/app#
```",,97772836,story,[],Enforce same bundler version on CI and local machine,948679.0,"[948679, 1355110]",956238,553935,chore,2015-07-01T22:20:56Z,https://www.pivotaltracker.com/story/show/97772836
2015-06-30T18:46:05Z,2015-06-17T23:56:58Z,accepted,,"```
Are you sure you want to deploy this version? (type 'yes' to continue): yes

Compilation
No changes

Update
No changes

Resource pools
small_errand
  + cloud_properties: {}
  + name: small_errand
  + network: errand1
  stemcell
    + name: bosh-warden-boshlite-ubuntu-trusty-go_agent
    + version: 2776
Stemcell update has been detected. Are you sure you want to update stemcells? (type 'yes' to continue): yes
Stemcell update seems to be inconsistent with current deployment. Please carefully review changes above.
Are you sure this configuration is correct? (type 'yes' to continue):
```

In trying to get to a state where this warning is not displayed, I did update the stemcell. However the update failed, saying the stemcell already exists. After this, the warning was the same as above. 

```
$ bosh upload stemcell https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent

Using remote stemcell `https://bosh.io/d/stemcells/bosh-warden-boshlite-ubuntu-trusty-go_agent'

Director task 27
  Started update stemcell
  Started update stemcell > Downloading remote stemcell[WARNING] cannot access director, trying 4 more times...
[WARNING] cannot access director, trying 3 more times...
. Done (01:18:14)
  Started update stemcell > Extracting stemcell archive. Done (00:00:02)
  Started update stemcell > Verifying stemcell manifest. Done (00:00:00)
  Started update stemcell > Checking if this stemcell already exists. Failed: Stemcell `bosh-warden-boshlite-ubuntu-trusty-go_agent/2776' already exists (00:00:00)

Error 50002: Stemcell `bosh-warden-boshlite-ubuntu-trusty-go_agent/2776' already exists

Task 27 error

For a more detailed error report, run: bosh task 27 --debug
```

Why does the message say ""Stemcell update has been detected"" when there was no stemcell change?

What does the warning ""Stemcell update seems to be inconsistent with current deployment."" mean? This message is confusing.",,97255178,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]","when a new resource pool is introduced, very confusing warning message",553935.0,"[553935, 1426194]",956238,58676,bug,2015-06-30T18:46:05Z,https://www.pivotaltracker.com/story/show/97255178
2015-06-30T18:46:59Z,2015-06-22T22:57:38Z,accepted,,"Only show 1 yes/no prompt at the end of all diff-ing before kicking off deploy.

```
Releases
cf
  ± version:
    - 211.90.0
    + 211.94.0
Release version has changed: 211.90.0 -> 211.94.0
Are you sure you want to deploy this version? (type 'yes' to continue): yes

Compilation
No changes

Update
No changes

Resource pools
small_z1
  stemcell
    version type changed: String -> Fixnum
      - 2992
      + 2992
Stemcell update has been detected. Are you sure you want to update stemcells? (type 'yes' to continue): yes
```",1.0,97587148,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",remove annoying yes/no prompts,553935.0,"[553935, 1426194]",956238,1429768,feature,2015-06-30T18:46:59Z,https://www.pivotaltracker.com/story/show/97587148
2015-06-30T22:01:06Z,2015-06-30T17:32:18Z,accepted,,,,98125470,story,[],Move agent integration tests to concourse,553935.0,[553935],956238,553935,chore,2015-06-30T22:01:06Z,https://www.pivotaltracker.com/story/show/98125470
2015-06-30T22:04:29Z,2015-06-19T17:28:43Z,accepted,,https://github.com/cloudfoundry/bosh/pull/849,1.0,97405168,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",take in dmitriy's suggestion for '[centos stemcell] /var/vcap/bosh/bin not in PATH for root user',553935.0,"[553935, 1569704, 1426194]",956238,81882,feature,2015-06-30T22:04:30Z,https://www.pivotaltracker.com/story/show/97405168
2015-06-30T22:11:23Z,2015-06-29T18:16:43Z,accepted,,http://www.ubuntu.com/usn/usn-2657-1/,0.0,98036222,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty stemcell for USN-2657-1,553935.0,"[553935, 1426194]",956238,81882,feature,2015-07-01T22:20:54Z,https://www.pivotaltracker.com/story/show/98036222
2015-06-30T22:16:32Z,2015-06-24T00:03:55Z,accepted,,"currently nginx generates 1024 rsa keys. we need to bump that to at least 2048. https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/nginx_ctl#L25

might want to also bump in bosh-dev/assets/sandbox/ca/generate.sh 

nothing to do about existing keys",0.0,97684304,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",generate higher length rsa keys for the director nginx,344.0,[344],956238,81882,feature,2015-06-30T22:16:40Z,https://www.pivotaltracker.com/story/show/97684304
2015-06-30T23:11:29Z,2015-06-25T21:03:42Z,accepted,,- configure 2 new builds in jenkins,1.0,97852378,story,"[{'name': 'vcloud', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-10T00:51:45Z', 'id': 11363370, 'updated_at': '2015-04-10T00:51:45Z'}]",verify vcloud lifecycle/bats against a newer vcloud environment,553935.0,[553935],956238,81882,feature,2015-06-30T23:11:49Z,https://www.pivotaltracker.com/story/show/97852378
2015-07-01T23:07:25Z,2015-06-26T16:58:35Z,accepted,,"```
PATH=""$PWD/lol"" out/bosh-init deploy /Users/pivotal/workspace/deployments-bosh/aws_init_manifest.yml
Deployment manifest: '/Users/pivotal/workspace/deployments-bosh/aws_init_manifest.yml'
Deployment state: '/Users/pivotal/workspace/deployments-bosh/aws_init_manifest-state.json'

Started validating
  Downloading release 'bosh'... Skipped [Found in local cache] (00:00:00)
  Validating release 'bosh'... Finished (00:00:03)
  Downloading release 'bosh-aws-cpi'... Skipped [Found in local cache] (00:00:00)
  Validating release 'bosh-aws-cpi'... Finished (00:00:00)
  Validating cpi release... Finished (00:00:00)
  Validating deployment manifest... Finished (00:00:00)
  Downloading stemcell... Skipped [Found in local cache] (00:00:00)
  Validating stemcell... Finished (00:00:00)
Finished validating (00:00:03)

Started installing CPI
  Compiling package 'ruby_aws_cpi/7903f3a543e8ab35fd980a0ca74f9151282d56b2'... Finished (00:00:00)
  Compiling package 'bosh_aws_cpi/a977bbf166f255505f692163e1715d0347a790e7'... Finished (00:00:00)
  Installing packages... Finished (00:00:04)
  Rendering job templates... Failed (00:00:00)
```
 In the above case ruby was not on the path.
The output stops without showing why it failed.
",1.0,97909284,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should show an error if ruby is not found on the path,344.0,[344],956238,81882,feature,2015-07-01T23:07:25Z,https://www.pivotaltracker.com/story/show/97909284
2015-07-02T00:01:44Z,2015-07-01T16:54:28Z,accepted,,,,98208726,story,[],bosh-init and bosh-agent builds should fail if there are unchecked errors,687691.0,[687691],956238,687691,chore,2015-07-02T00:01:44Z,https://www.pivotaltracker.com/story/show/98208726
2015-07-02T00:04:41Z,2015-06-25T14:40:16Z,accepted,,,,97818630,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",move bosh-agent integration suite to concourse,687691.0,[687691],956238,1550486,chore,2015-07-02T00:04:41Z,https://www.pivotaltracker.com/story/show/97818630
2015-07-02T00:33:23Z,2015-06-26T16:41:11Z,accepted,,"- let's retry get_state and get_task 10 times, sleep 0.5 in between

```
Command 'deploy' failed:
  Deploying:
    Building state for instance 'worker/0':
      Compiling job package dependencies for instance 'worker/0':
        Compiling job package dependencies:
          Remotely compiling package 'golang_1.3' with the agent:
            Sending 'compile_package' to the agent:
              Sending 'get_task' to the agent:
                Performing request to agent endpoint 'https://mbus:vXqdFNcRG9ybP@10.85.33.10:6868/agent':
                  Performing POST request:
                    Post https://mbus:vXqdFNcRG9ybP@10.85.33.10:6868/agent: read tcp 10.85.33.10:6868: connection reset by peer
pivotal@mcf-build:~/workspace/bosh-concourse-ci/vsphere$ bosh-init deploy vsphere-worker-init-manifest.yml
Deployment manifest: '/home/pivotal/workspace/bosh-concourse-ci/vsphere/vsphere-worker-init-manifest.yml'
Deployment state: '/home/pivotal/workspace/bosh-concourse-ci/vsphere/vsphere-worker-init-manifest-state.json'
```

simply retrying moves past the error, so I feel like bosh-init should have just retried for me.",2.0,97907894,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should retry sending commands to agent,344.0,[344],956238,1406536,feature,2015-07-02T00:33:24Z,https://www.pivotaltracker.com/story/show/97907894
2015-07-02T00:33:27Z,2015-06-26T16:58:53Z,accepted,,,,97909296,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init binary is productionized,,[],956238,81882,release,2015-07-02T00:33:34Z,https://www.pivotaltracker.com/story/show/97909296
2015-07-02T00:33:38Z,2015-06-23T18:07:45Z,accepted,,"After introducing internal dependencies, go vet tries to vet vendored dependencies

Could not find an easy way to exclude directory from go vet",,97657552,story,[],"Enable govet for bosh-init, bosh-agent",687691.0,[687691],956238,553935,chore,2015-07-02T00:46:12Z,https://www.pivotaltracker.com/story/show/97657552
2015-07-02T00:34:50Z,2015-06-25T17:11:36Z,accepted,,- http://www.ubuntu.com/usn/usn-2653-1/ - python,1.0,97832822,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty for usn-2653-1,1426194.0,[1426194],956238,81882,feature,2015-07-02T00:34:50Z,https://www.pivotaltracker.com/story/show/97832822
2015-07-02T00:34:57Z,2015-05-21T21:46:24Z,accepted,,"- errands
- deletion of deployments
- scaling up/down",2.0,95199966,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",try to break global networking usage,687691.0,"[687691, 1687782]",956238,81882,feature,2015-07-02T00:34:57Z,https://www.pivotaltracker.com/story/show/95199966
2015-07-03T00:35:40Z,2015-02-18T00:46:05Z,accepted,,https://github.com/cloudfoundry/bosh/pull/735,2.0,88572368,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",PR 'Copy out the stemcell packages list and add it to stemcell tgz',553935.0,"[553935, 1426194]",956238,81882,feature,2015-07-03T00:35:41Z,https://www.pivotaltracker.com/story/show/88572368
2015-07-03T00:35:53Z,2015-06-02T00:37:44Z,accepted,,"... useful for garden for updated btrfs and overlay

- currently utopic is used: https://github.com/cloudfoundry/bosh/blob/77a6987bcf40ac91ed211e7048cbd8b3fd134d44/stemcell_builder/stages/system_kernel/apply.sh#L13

- vivid
- tests should reference specific package name (do not include version)",1.0,95930102,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",switch ubuntu trusty stemcells to 3.19 kernel,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-03T00:35:53Z,https://www.pivotaltracker.com/story/show/95930102
2015-07-06T22:18:05Z,2015-05-15T00:56:55Z,accepted,,"- which tests cover this? bosh-agent integration?
- do we need to change bats?
- https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/bosh_openstack_agent_settings/apply.sh#L10",2.0,94636274,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",allow centos stemcells to repartition root disk just like for ubuntu stemcells on openstack,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-06T22:18:06Z,https://www.pivotaltracker.com/story/show/94636274
2015-07-06T23:43:26Z,2015-02-18T00:46:31Z,accepted,,https://github.com/cloudfoundry/bosh/pull/742,1.0,88572382,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Make job state change request to accept empty body with content-length',1655862.0,[1655862],956238,81882,feature,2015-07-06T23:43:26Z,https://www.pivotaltracker.com/story/show/88572382
2015-07-07T17:14:47Z,2015-07-06T22:46:24Z,accepted,,"not dolores.

we have a bosh-agent aws account now. go change `bosh-concourse-ci/pipelines/bosh-agent/secrets.yml` with the new account credentials.",,98510562,story,[],move bosh-agent integration tests to run against the right AWS env,687691.0,[687691],956238,687691,chore,2015-07-07T17:14:47Z,https://www.pivotaltracker.com/story/show/98510562
2015-07-07T18:31:12Z,2015-07-06T22:37:15Z,accepted,,"just download once, skip if file exists

Do we need to do all that: https://github.com/cloudfoundry/bosh/blob/master/bosh-dev/lib/bosh/dev/download_adapter.rb#L19

Can we just shell out to wget/curl?
",,98510142,story,[],auto-deploy should not download stemcells for each of its step,687691.0,"[687691, 1687782]",956238,553935,chore,2015-07-07T18:31:13Z,https://www.pivotaltracker.com/story/show/98510142
2015-07-07T23:50:37Z,2015-03-31T18:01:55Z,accepted,,"currently you have to delete vm reference from the database in cases when vm is stuck and there is nothing bosh can do about it. this happens bunch of times on aws.

Expected updates to cck:

```
VM blah-123 has unresponsive agent:
  1. Skip for now
  2. Reboot VM
  3. Recreate VM
  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)

VM blah-124 is missing:
  1. Skip for now
  2. Recreate VM
  3. Delete VM reference
```

- ""Delete VM reference (forceful; may result in IP conflict)"" option should not check for cid presence.
- add an integration test",2.0,91529600,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",bosh cck should allow to delete vm reference even if vm is still around,1655862.0,"[1655862, 1426194]",956238,81882,feature,2015-07-07T23:50:37Z,https://www.pivotaltracker.com/story/show/91529600
2015-07-08T00:17:49Z,2015-06-24T23:42:24Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/maintenance.rb#L69

currently this overwrites the list every iteration instead of appending to delete list --- fix that",,97778902,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",cleanup command should properly pick out stemcells to delete,1655862.0,[1655862],956238,81882,bug,2015-07-08T00:17:49Z,https://www.pivotaltracker.com/story/show/97778902
2015-07-08T01:09:43Z,2015-07-07T17:14:21Z,accepted,,http://www.ubuntu.com/usn/usn-2667-1/,1.0,98574826,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty kernel [USN-2667-1],553935.0,"[553935, 1655862]",956238,553935,feature,2015-07-08T01:09:44Z,https://www.pivotaltracker.com/story/show/98574826
2015-07-08T01:56:37Z,2015-06-29T21:57:05Z,accepted,,if not array of strings raise an error,1.0,98058920,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can specify array of provides/requires names in the release,553935.0,"[553935, 1426194]",956238,81882,feature,2015-07-08T01:56:37Z,https://www.pivotaltracker.com/story/show/98058920
2015-07-08T02:14:03Z,2015-06-29T21:57:25Z,accepted,,"where X is deployment name, Y is deployment job, Z is release job, ZZ is link name
e.g. {name: blah, release: blah, links: {db: X.Y.Z.ZZ}}",1.0,98058952,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user must provide link source for each release job requirement as a string in the deployment manifest,553935.0,"[553935, 1687782]",956238,81882,feature,2015-07-08T02:14:03Z,https://www.pivotaltracker.com/story/show/98058952
2015-07-08T02:18:23Z,2015-06-29T21:57:45Z,accepted,,"- validate that link is in above format (X.Y.Z.ZZ)
- validate that link source exists within this deployment (so X is effectivly this deployments name)
   - currently do not support cross deployment links",2.0,98058972,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user must provide findable link source,553935.0,"[553935, 1687782]",956238,81882,feature,2015-07-08T02:18:23Z,https://www.pivotaltracker.com/story/show/98058972
2015-07-08T20:38:15Z,2015-06-26T16:34:12Z,accepted,,it looks like it is stuck sometimes and our concourse builds just hang,,97907314,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",tcp_proxy should be killed when we try to kill it,1355110.0,"[1355110, 948679]",956238,553935,chore,2015-10-09T18:08:26Z,https://www.pivotaltracker.com/story/show/97907314
2015-07-09T21:26:21Z,2015-07-09T17:29:05Z,accepted,,"Takes forever now, pain for development cycle",,98755470,story,[],Add argument to build_light rake task to only build in one region,1426194.0,"[1426194, 553935]",956238,553935,chore,2015-07-09T21:26:21Z,https://www.pivotaltracker.com/story/show/98755470
2015-07-09T21:54:20Z,2015-07-09T19:17:27Z,accepted,,we see a number of CI failures due to network flakiness when downloading nginx. maybe this task can retry a few times before giving up,,98764200,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",`spec:integration:install_dependencies` should retry,687691.0,"[687691, 1687782]",956238,687691,chore,2015-10-09T18:08:35Z,https://www.pivotaltracker.com/story/show/98764200
2015-07-10T17:59:52Z,2015-07-06T22:29:35Z,accepted,,"during bosh upload release

```
provides:
- db
- db

provides:
- db2 # name is implicitly db2
- {name: db2, type: db}
```",2.0,98509734,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user sees an error message if provided links are not uniquely named,553935.0,"[553935, 1655862]",956238,81882,feature,2015-07-10T17:59:52Z,https://www.pivotaltracker.com/story/show/98509734
2015-07-10T19:41:54Z,2015-07-06T22:29:04Z,accepted,,,2.0,98509710,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can specify `cluster.job.link-name` as a link source (relative to current deployment),553935.0,[553935],956238,81882,feature,2015-07-10T19:41:54Z,https://www.pivotaltracker.com/story/show/98509710
2015-07-10T19:57:29Z,2015-06-29T21:58:11Z,accepted,,"for each network have a field called address (include either IP for manual or vip network, DNS name for dynamic network)",2.0,98059016,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can retrieve network info about each release job instance,553935.0,"[553935, 1655862]",956238,81882,feature,2015-07-10T19:57:29Z,https://www.pivotaltracker.com/story/show/98059016
2015-07-10T19:57:37Z,2015-06-29T21:57:59Z,accepted,,"array of hashes with each hash containing name, index",2.0,98058994,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can retrieve nodes of another release job via link(requires-name.nodes) in its templates,553935.0,"[553935, 1655862]",956238,81882,feature,2015-07-10T19:57:37Z,https://www.pivotaltracker.com/story/show/98058994
2015-07-10T21:52:38Z,2015-07-10T19:06:43Z,accepted,,,,98848148,story,[],jenkins scm sync should work,687691.0,[687691],956238,687691,chore,2015-07-10T21:52:38Z,https://www.pivotaltracker.com/story/show/98848148
2015-07-10T22:09:37Z,2015-07-06T22:29:50Z,accepted,,"  - take link info (only net so far) from DB and snapshot at the time of the deploy
  - raise error if network information is not available
  - no access to live running VMs in a different deployment should happen (dont call to agents)",8.0,98509752,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can deploy a manifest with a link source from a different deployment,553935.0,"[553935, 1655862]",956238,81882,feature,2015-07-10T22:09:37Z,https://www.pivotaltracker.com/story/show/98509752
2015-07-10T23:17:59Z,2015-06-23T22:30:17Z,accepted,,"When calculating reserved ranges by deployments that were deployed before cloud config, we look at deployment manifest of all deployments in database. If there is an ongoing deployment it won't have a manifest saved until it is finished. That causes current deployment to fail.

Possible solution might be to ignore such deployments and not treat them as legacy.

```
E, [2015-06-23 14:54:54 #79302] [task:4] ERROR -- DirectorJobRunner: no implicit conversion of nil into String
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:370:in `parse'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:370:in `parse_stream'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:318:in `parse'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:245:in `load'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/global_network_resolver.rb:32:in `add_networks_from_deployment'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/global_network_resolver.rb:24:in `block in legacy_ranges'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/global_network_resolver.rb:23:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/global_network_resolver.rb:23:in `legacy_ranges'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/global_network_resolver.rb:13:in `reserved_legacy_ranges'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/manual_network.rb:22:in `initialize'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/cloud_manifest_parser.rb:36:in `new'
```",,97679986,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]","When running deployments at the same time, second deployment fails to calculate network ranges ",687691.0,[687691],956238,687691,bug,2015-07-10T23:17:59Z,https://www.pivotaltracker.com/story/show/97679986
2015-07-10T23:17:59Z,2015-06-24T18:24:36Z,accepted,,"Right now on the global-net branch, if you run an errand while a deploy is running, if there aren't enough IPs available by the time the errand job runs it fails confusingly:

```
...
Error 100: undefined method `cid' for nil:NilClass
```

we expect to see the original error:

```
...
asked for a dynamic IP but there were no more available
```

but we want to log the cleanup error because users care if the IAAS is in a weird state.

full backtrace (for incorrect failure):
```
I, [2015-06-24T11:22:34.345137 #84865]  INFO :         ******************** start Director task 5 ********************
        Acting as user 'test' on 'Test Director'

Director task 5
...
blahblahblah
...
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: Starting to prepare for deployment
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000144s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000123s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000310s) SELECT * FROM ""instances"" WHERE ((""deployment_id"" = 1) AND (""job"" = 'errand_job') AND (""index"" = 0)) LIMIT 1
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000139s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000155s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000234s) SELECT COUNT(*) AS ""count"" FROM ""instances"" WHERE ((""deployment_id"" = 1) AND (""job"" = 'errand_job') AND (""index"" = 0)) LIMIT 1
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000155s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000123s) BEGIN
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000436s) INSERT INTO ""instances"" (""deployment_id"", ""job"", ""index"", ""state"") VALUES (1, 'errand_job', 0, 'started') RETURNING *
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000722s) COMMIT
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: ResourcePool `a' - Adding allocated VM (index=0)
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000182s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000142s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000217s) SELECT * FROM ""instances"" WHERE ((""deployment_id"" = 1) AND (""job"" = 'errand_job') AND (""index"" = 1)) LIMIT 1
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000123s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000133s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000192s) SELECT COUNT(*) AS ""count"" FROM ""instances"" WHERE ((""deployment_id"" = 1) AND (""job"" = 'errand_job') AND (""index"" = 1)) LIMIT 1
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000125s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000106s) BEGIN
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000249s) INSERT INTO ""instances"" (""deployment_id"", ""job"", ""index"", ""state"") VALUES (1, 'errand_job', 1, 'started') RETURNING *
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000454s) COMMIT
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: ResourcePool `a' - Adding allocated VM (index=1)
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000130s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000128s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000273s) SELECT ""address"" FROM ""ip_addresses"" WHERE (""network_name"" = 'a')
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000190s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000121s) BEGIN
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000299s) INSERT INTO ""ip_addresses"" (""address"", ""network_name"", ""deployment_id"") VALUES (3232235780, 'a', 1) RETURNING *
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000422s) COMMIT
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000116s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000141s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000166s) SELECT ""address"" FROM ""ip_addresses"" WHERE (""network_name"" = 'a')
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: Deleting instances
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: Starting to delete job instances
I, [2015-06-24 11:22:32 #85956] [task:5]  INFO -- DirectorJobRunner: Deleting errand instances
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Creating new thread
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Creating new thread
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Waiting for tasks to complete
D, [2015-06-24 11:22:32 #85956] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: undefined method `cid' for nil:NilClass - /Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:24:in `delete_instance'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:17:in `block (3 levels) in delete_instances'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `call'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-06-24 11:22:32 #85956] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: undefined method `cid' for nil:NilClass - /Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:24:in `delete_instance'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:17:in `block (3 levels) in delete_instances'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `call'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-06-24 11:22:32 #85956] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up
D, [2015-06-24 11:22:32 #85956] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Shutting down pool
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:errand
D, [2015-06-24 11:22:32 #85956] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:errand
E, [2015-06-24 11:22:32 #85956] [task:5] ERROR -- DirectorJobRunner: undefined method `cid' for nil:NilClass
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:24:in `delete_instance'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/instance_deleter.rb:17:in `block (3 levels) in delete_instances'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `call'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000143s) SELECT NULL
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000110s) BEGIN
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000236s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-06-24 11:22:32.682122-0700', ""description"" = 'run errand errand_job from deployment errand', ""result"" = 'undefined method `cid'' for nil:NilClass', ""output"" = '/Users/pivotal/workspace/bosh/tmp/integration-tests-workspace/pid-84865/sandbox/boshdir/tasks/5', ""checkpoint_time"" = '2015-06-24 11:22:32.588657-0700', ""type"" = 'run_errand', ""username"" = 'test' WHERE (""id"" = 5)
D, [2015-06-24 11:22:32 #85956] [task:5] DEBUG -- DirectorJobRunner: (0.000410s) COMMIT
I, [2015-06-24 11:22:32 #85956] []  INFO -- DirectorJobRunner: Task took 0.097232 seconds to process.

Task 5 error

        ******************** end Director task 5 ********************
```",,97753192,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]","When an errand fails and then cleanup fails, raise the original error and log the error cleaning up instances",687691.0,"[687691, 1426194]",956238,687691,bug,2015-07-10T23:17:59Z,https://www.pivotaltracker.com/story/show/97753192
2015-07-10T23:22:58Z,2015-07-02T22:04:33Z,accepted,,- make sure hm server only listens on 127.0.0.1,2.0,98327500,story,"[{'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",hm should listen only localhost,687691.0,[687691],956238,81882,feature,2015-07-10T23:23:00Z,https://www.pivotaltracker.com/story/show/98327500
2015-07-10T23:24:15Z,2015-07-02T19:27:18Z,accepted,,"- remove hm.http properties except hm.http.port
- make sure /healthz still works",1.0,98316752,story,"[{'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",remove health monitor varz endpoint,687691.0,"[687691, 1426194]",956238,81882,feature,2015-07-10T23:24:15Z,https://www.pivotaltracker.com/story/show/98316752
2015-07-10T23:26:05Z,2015-07-02T22:36:48Z,accepted,,,1.0,98329310,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",finalize release command should not sync blobs,687691.0,[687691],956238,81882,feature,2015-07-10T23:26:17Z,https://www.pivotaltracker.com/story/show/98329310
2015-07-10T23:28:57Z,2015-07-01T16:58:58Z,accepted,,"compilation vms are created with resource pool ""package_compiler"" which is a bad thing(tm)

```
D, [2015-07-01 16:03:18 #32397] [task:27617] DEBUG -- DirectorJobRunner: (0.000599s) SELECT * FROM `vms` WHERE `id` = 11332
I, [2015-07-01 16:03:18 #32397] [task:27617]  INFO -- DirectorJobRunner: missing_vm 11332: Recreate VM using last known apply spec
E, [2015-07-01 16:03:18 #32397] [task:27617] ERROR -- DirectorJobRunner: Error resolving problem `6116': undefined method `fetch' for ""package_compiler"":String
E, [2015-07-01 16:03:18 #32397] [task:27617] ERROR -- DirectorJobRunner: /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/cloudcheck_helper.rb:95:in `recreate_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_handlers/missing_vm.rb:22:in `block (2 levels) in <class:MissingVM>'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:59:in `block in apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:21:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:58:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:42:in `block in apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `block in each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block (2 levels) in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:89:in `_execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `block in execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:801:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:171:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:140:in `fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/problem_resolver.rb:33:in `apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:38:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:37:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2992.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2992.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>
```",,98209102,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]","bosh cck should not fail with undefined method `fetch' for ""package_compiler"" when compilation vms are in unresponsive state",687691.0,[687691],956238,1550416,bug,2015-07-10T23:29:05Z,https://www.pivotaltracker.com/story/show/98209102
2015-07-10T23:57:16Z,2015-07-10T21:53:39Z,accepted,,,,98859492,story,[],fix the BATs,687691.0,[687691],956238,687691,chore,2015-07-10T23:57:16Z,https://www.pivotaltracker.com/story/show/98859492
2015-07-11T00:03:44Z,2015-06-26T20:49:45Z,accepted,,"If my compilation pool has 1 worker, and my job has one instance, I should be able to deploy that job with only 1 dynamic IP available

Right now the instance IP is reserved before the compilation happens so there isn't an available IP for the compilation VM. Instead, compile packages, release the IP, and then reserve it for the instance VM.",1.0,97926768,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Compilation VM IPs can be reused by job VMs when using dynamic IP reservation,553935.0,[553935],956238,553935,feature,2015-07-11T00:03:44Z,https://www.pivotaltracker.com/story/show/97926768
2015-07-13T21:50:19Z,2015-07-13T19:42:34Z,accepted,,"Allowing insecure file locking could allow for sensitive data to be viewed or edited by an unauthorized user.
---
None
---
SV-50478r1_rule
---
F-43626r1_fix
---
By default the NFS server requires secure file-lock requests, which require credentials from the client in order to lock a file. Most NFS clients send credentials with file lock requests, however, there are a few clients that do not send credentials when requesting a file-lock, allowing the client to only be able to lock world-readable files. To get around this, the ""insecure_locks"" option can be used so these clients can access the desired export. This poses a security risk by potentially allowing the client access to data for which it does not have authorization. Remove any instances of the ""insecure_locks"" option from the file ""/etc/exports"".
---
C-46239r1_chk
---
To verify insecure file locking has been disabled, run the following command: 

# grep insecure_locks /etc/exports


If there is output, this is a finding.",0.0,98977848,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38677] [high] The NFS server must not have the insecure file locking option enabled.,81882.0,[81882],956238,81882,feature,2015-07-14T00:01:42Z,https://www.pivotaltracker.com/story/show/98977848
2015-07-13T22:04:35Z,2015-07-13T19:42:40Z,accepted,,"Installing software updates is a fundamental mitigation against the exploitation of publicly-known vulnerabilities.
---
None
---
SV-50281r1_rule
---
F-43426r1_fix
---
If the system is joined to the Red Hat Network, a Red Hat Satellite Server, or a yum server, run the following command to install updates: 

# yum update

If the system is not configured to use one of these sources, updates (in the form of RPM packages) can be manually downloaded from the Red Hat Network and installed using ""rpm"".
---
C-46036r1_chk
---
If the system is joined to the Red Hat Network, a Red Hat Satellite Server, or a yum server which provides updates, invoking the following command will indicate if updates are available: 

# yum check-update

If the system is not configured to update from one of these sources, run the following command to list when each package was last updated: 

$ rpm -qa -last

Compare this to Red Hat Security Advisories (RHSA) listed at https://access.redhat.com/security/updates/active/ to determine whether the system is missing applicable security and bugfix  updates. 
If updates are not installed, this is a finding.",0.0,98978128,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38481] [medium] System security patches and updates must be installed and up-to-date.,81882.0,[81882],956238,81882,feature,2015-07-14T00:01:42Z,https://www.pivotaltracker.com/story/show/98978128
2015-07-14T00:00:50Z,2015-07-13T19:42:38Z,accepted,,"The ""/tmp"" partition is used as temporary storage by many programs. Placing ""/tmp"" in its own partition enables the setting of more restrictive mount options, which can help protect programs which use it.
---
None
---
SV-50255r1_rule
---
F-43387r1_fix
---
The ""/tmp"" directory is a world-writable directory used for temporary file storage. Ensure it has its own partition or logical volume at installation time, or migrate it using LVM.
---
C-45997r1_chk
---
Run the following command to determine if ""/tmp"" is on its own partition or logical volume: 

$ mount | grep ""on /tmp ""

If ""/tmp"" has its own partition or volume group, a line will be returned. 
If no line is returned, this is a finding.",0.0,98978020,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38455] [low] The system must use a separate file system for /tmp.,81882.0,[81882],956238,81882,feature,2015-07-14T00:01:42Z,https://www.pivotaltracker.com/story/show/98978020
2015-07-15T16:40:08Z,2015-07-14T22:52:55Z,accepted,,"http://bosh-jenkins.cf-app.com:8080/job/bat_micro_aws_centos_go_agent/563/
http://bosh-jenkins.cf-app.com:8080/job/bat_micro_aws_centos_go_agent/564/",,99093096,story,[],figure out why the BATs are failing for CentOS on AWS,687691.0,[687691],956238,687691,chore,2015-07-15T16:40:08Z,https://www.pivotaltracker.com/story/show/99093096
2015-07-15T18:02:18Z,2015-06-30T16:29:46Z,accepted,,"- increase number of times to 10. can we sleep more in between attempts?

Keeps failing periodically:

```
Syncing blobs...
postgres/postgres-9.0.3-1.amd64.tar.gz 
downloading 15.5Mdownloading 15.5M (0%)downloading 15.5M (25%)downloading 15.5M (25%)downloading 15.5M (31%)downloading 15.5M (65%)downloading 15.5M (100%)downloadedpostgres/postgres-9.0.3-1.i386.tar.gz 
downloading 14.4Mdownloading 14.4M (0%)downloading 14.4M (21%)downloading 14.4M (22%)downloading 14.4M (40%)downloading 14.4M (47%)downloading 14.4M (84%)downloading 14.4M (100%)downloadedpostgres/postgresql-9.0.3.tar.gz 
downloading 17.5Mdownloading 17.5M (0%)downloading 17.5M (27%)downloading 17.5M (65%)downloading 17.5M (71%)downloading 17.5M (72%)downloading 17.5M (100%)downloadedmysql/server-5.1.62-rel13.3-435-Linux-x86... 
/tmp/build/2070c507-9533-4e85-7250-2966ebe81a88/bosh-src/ v3:in `rescue in get': Failed to fetch object, underlying error: #<HTTPClient::ReceiveTimeoutError: execution expired> /opt/rubies/ruby-1.9.3-p547/lib/ruby/1.9.1/openssl/buffering.rb:121:in `sysread' (Bosh::Blobstore::BlobstoreError)
/opt/rubies/ruby-1.9.3-p547/lib/ruby/1.9.1/openssl/buffering.rb:121:in `readpartial'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient/s
```",1.0,98120310,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",increase number of times we retry downloading blobs,687691.0,"[687691, 1569704]",956238,553935,feature,2015-07-15T18:02:18Z,https://www.pivotaltracker.com/story/show/98120310
2015-07-15T18:02:34Z,2015-07-10T21:13:05Z,accepted,,provider aws,1.0,98856908,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",investigate how to make bosh-lite work with vagrant 1.7,1426194.0,"[1426194, 1687782]",956238,81882,feature,2015-07-15T18:02:34Z,https://www.pivotaltracker.com/story/show/98856908
2015-07-15T20:00:48Z,2015-07-02T22:06:58Z,accepted,,- also make sure apt-get is configured securely for ubuntu,2.0,98327626,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38476 - Vendor-provided cryptographic certificates must be installed to verify the integrity of system software.,553935.0,"[553935, 1687782]",956238,81882,feature,2015-07-15T20:00:59Z,https://www.pivotaltracker.com/story/show/98327626
2015-07-15T20:01:21Z,2015-07-02T21:51:31Z,accepted,,- verify that apt-get and yum use signatures,1.0,98326716,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38462 - The RPM package management tool must cryptographically verify the authenticity of all software packages during installation.,553935.0,[553935],956238,81882,feature,2015-07-15T20:01:22Z,https://www.pivotaltracker.com/story/show/98326716
2015-07-15T22:27:05Z,2015-07-02T21:55:06Z,accepted,,even though system-auth does not include nullok find out what's in common-auth (grep -R nullok /etc/pam.d/),2.0,98326914,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38497 - The system must not have accounts configured with blank or null passwords.,1426194.0,"[1426194, 553935]",956238,81882,feature,2015-07-15T22:27:05Z,https://www.pivotaltracker.com/story/show/98326914
2015-07-15T22:28:07Z,2015-07-02T21:49:09Z,accepted,,"To ensure the system is configured to log a message instead of rebooting the system when Ctrl-Alt-Delete is pressed, ensure the following line is in ""/etc/init/control-alt-delete.override"":

exec /usr/bin/logger -p security.info ""Control-Alt-Delete pressed""

If the system is not configured to block the shutdown command when Ctrl-Alt-Delete is pressed, this is a finding. ",2.0,98326584,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38668 - disable Ctrl-Alt-Delete key sequence,1426194.0,"[1426194, 687691]",956238,81882,feature,2015-07-15T22:28:08Z,https://www.pivotaltracker.com/story/show/98326584
2015-07-15T22:28:27Z,2015-07-02T22:08:29Z,accepted,,,1.0,98327690,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38491 - There must be no .rhosts or hosts.equiv files on the system.,1426194.0,"[1426194, 553935]",956238,81882,feature,2015-07-15T22:28:37Z,https://www.pivotaltracker.com/story/show/98327690
2015-07-15T22:31:12Z,2015-07-02T23:03:01Z,accepted,,,1.0,98331008,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38614 - The SSH daemon must not allow authentication using an empty password.,1426194.0,"[1426194, 553935]",956238,81882,feature,2015-07-15T22:31:13Z,https://www.pivotaltracker.com/story/show/98331008
2015-07-15T22:50:34Z,2015-07-02T22:17:06Z,accepted,,,1.0,98328066,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38607 - The SSH daemon must be configured to use only the SSHv2 protocol.,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:50:34Z,https://www.pivotaltracker.com/story/show/98328066
2015-07-15T22:51:15Z,2015-07-02T23:01:55Z,accepted,,- make sure tftp is not installed,1.0,98330944,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38701 - The TFTP daemon must operate in secure mode which provides access only to a single directory on the host file system.,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:51:17Z,https://www.pivotaltracker.com/story/show/98330944
2015-07-15T22:51:38Z,2015-07-02T22:38:15Z,accepted,,,1.0,98329372,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38587 - The telnet-server package must not be installed & V-38589 - The telnet daemon must not be running.,1426194.0,"[1426194, 553935]",956238,81882,feature,2015-07-15T22:51:39Z,https://www.pivotaltracker.com/story/show/98329372
2015-07-15T22:52:19Z,2015-07-02T22:25:03Z,accepted,,,2.0,98328576,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38598 - The rexecd service must not be running.,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:52:20Z,https://www.pivotaltracker.com/story/show/98328576
2015-07-15T22:52:26Z,2015-07-02T22:22:50Z,accepted,,,2.0,98328344,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38591 - The rsh-server package must not be installed.,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:52:32Z,https://www.pivotaltracker.com/story/show/98328344
2015-07-15T22:53:29Z,2015-07-02T22:20:07Z,accepted,,,1.0,98328184,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38594 - The rshd service must not be running,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:53:30Z,https://www.pivotaltracker.com/story/show/98328184
2015-07-15T22:53:40Z,2015-07-02T22:18:59Z,accepted,,,1.0,98328150,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",V-38602 - The rlogind service must not be running.,1426194.0,[1426194],956238,81882,feature,2015-07-15T22:53:40Z,https://www.pivotaltracker.com/story/show/98328150
2015-07-15T22:53:59Z,2015-06-29T16:58:44Z,accepted,,,,98027948,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",job links can be used to share networking information,,[],956238,81882,release,2015-07-15T22:53:59Z,https://www.pivotaltracker.com/story/show/98027948
2015-07-15T23:04:10Z,2015-07-08T20:48:04Z,accepted,,"for https://github.com/cloudfoundry/bosh/pull/860

see http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html",4.0,98687412,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Ubuntu and CentoOS stemcells should have ixgbevf version > 2.14,1687782.0,"[1687782, 687691]",956238,687691,feature,2015-07-15T23:04:11Z,https://www.pivotaltracker.com/story/show/98687412
2015-07-15T23:21:39Z,2015-07-03T00:53:10Z,accepted,,"bosh recreate joba 0
bosh recreate joba 1 

currently step number 2 fails to start recreating.",,98334962,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",bosh recreate multiple times should not require --force flag,687691.0,"[687691, 1687782]",956238,81882,bug,2015-07-15T23:21:39Z,https://www.pivotaltracker.com/story/show/98334962
2015-07-15T23:22:13Z,2015-07-07T18:53:29Z,accepted,,"seems like auth_required is missing?

```
ruby 2.1.2p95 idora in ~
○ → bosh status
Config
             /Users/pivotal/.bosh_config

Director
  Name       bosh-with-uaa
  URL        https://54.86.93.98:25555
  Version    1.3010.0 (00000000)
  User       not logged in # <--------------------------------
  UUID       9d00a46b-e311-4b41-8916-16f0e5a92e21
  CPI        cpi
  dns        disabled
  compiled_package_cache disabled
  snapshots  disabled

Deployment
  Manifest   /Users/pivotal/workspace/deployments-aws/idora/deployments/micro-with-uaa/dummy.yml

ruby 2.1.2p95 idora in ~
○ → bosh vm resurrection off
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/client/uaa/token_provider.rb:21:in `username': undefined method `username' for nil:NilClass (NoMethodError)
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/client/credentials.rb:10:in `username'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/base_command.rb:127:in `show_current_state'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/commands/vm.rb:12:in `resurrection_state'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3004.0/bin/bosh:39:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
```",,98583896,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",bosh vm resurrection off command should indicate that you must be logged in to continue instead of raising an error,687691.0,[687691],956238,81882,bug,2015-07-15T23:22:13Z,https://www.pivotaltracker.com/story/show/98583896
2015-07-15T23:22:23Z,2015-07-02T23:13:06Z,accepted,,,,98331380,story,"[{'name': 'hd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-02T23:15:07Z', 'id': 12121424, 'updated_at': '2015-07-02T23:16:21Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",extra hardening of stemcell - part 1 - high,,[],956238,81882,release,2015-07-15T23:22:23Z,https://www.pivotaltracker.com/story/show/98331380
2015-07-16T00:53:19Z,2015-07-10T19:29:07Z,accepted,,"if job does not require X, providing X in the links section should result in an error.",2.0,98849862,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",raise an error if unknown link is specified in the deployment manifest,1426194.0,"[1426194, 1687782]",956238,81882,feature,2015-07-16T00:53:19Z,https://www.pivotaltracker.com/story/show/98849862
2015-07-16T01:16:54Z,2015-07-06T22:29:19Z,accepted,,"  - introduce name for links in addition to specifying type
  - make sure it's link-name and not link-type
  - if matches the name, use name
  - do not match link types (but links without a name have implicit name that is their type)",4.0,98509720,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",user can specify `deployment.cluster.job.link-name` as a link source,1687782.0,"[1687782, 1426194]",956238,81882,feature,2015-07-16T01:16:54Z,https://www.pivotaltracker.com/story/show/98509720
2015-07-17T00:35:23Z,2015-07-14T19:23:10Z,accepted,,,,99070392,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Cloud check should operate on instance,1569704.0,[1569704],956238,1569704,chore,2015-07-17T00:35:24Z,https://www.pivotaltracker.com/story/show/99070392
2015-07-21T01:20:14Z,2015-07-17T23:27:15Z,accepted,,"https://gist.github.com/venables/4205617

https://github.com/cloudfoundry/bosh/blob/f429182032be13b6cf8c1feda3ffe0536d2a2664/bosh_cli/lib/cli/archive_builder.rb#L58

This is blocking CF MEGA",,99344554,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",Bosh does not copy symlink files correctly when destination does not exist,553935.0,"[553935, 1569704]",956238,1120042,bug,2015-07-21T01:20:14Z,https://www.pivotaltracker.com/story/show/99344554
2015-07-21T21:06:20Z,2015-07-16T23:33:39Z,accepted,,,,99270966,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",bind_existing_vm should not handle network reservations,553935.0,"[553935, 1569704]",956238,553935,chore,2015-07-22T01:01:34Z,https://www.pivotaltracker.com/story/show/99270966
2015-07-21T21:16:47Z,2015-07-14T19:00:00Z,accepted,,,,99066694,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",All network reservations should be happening via Instance,553935.0,[553935],956238,1569704,chore,2015-07-21T21:16:47Z,https://www.pivotaltracker.com/story/show/99066694
2015-07-21T22:16:10Z,2015-07-14T19:07:47Z,accepted,,"- copy cf-release in tmp-bosh-cf-release
- inline cloud_controller and loggregator jobs (currently submoduled)
- modify to use links for ip addressing
- deploy and see it succeed",4.0,99067422,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",deploy cf-mysql-release with current links feature,1687782.0,"[1687782, 687691]",956238,81882,feature,2015-07-21T22:16:10Z,https://www.pivotaltracker.com/story/show/99067422
2015-07-21T22:16:42Z,2015-06-30T18:01:04Z,accepted,,"include -v for eject to show more info.
https://github.com/cloudfoundry/bosh-agent/blob/ebf9a36dfddf783ffae5ae3748787d259a29374a/platform/cdrom/linux_cdrom.go#L53",1.0,98128438,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",turn on verbose logging for eject,344.0,[344],956238,81882,feature,2015-07-21T22:16:44Z,https://www.pivotaltracker.com/story/show/98128438
2015-07-22T18:48:56Z,2015-06-24T21:28:58Z,accepted,,"export release: '#{release_name}/#{release_version}' for '#{stemcell_os}/#{stemcell_version}'

quotes around release and stemcell info ^^^",1.0,97770302,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",export release director task description should include quotes,1355110.0,"[1355110, 631325]",956238,81882,feature,2015-07-22T18:48:56Z,https://www.pivotaltracker.com/story/show/97770302
2015-07-22T19:12:28Z,2015-06-18T19:32:19Z,accepted,,"- look at the task result to find blobstore_id
- use fetch_resource api call?
- save a file into a current working directory called release-name-version-""on""-os-stemcell-version.tgz",2.0,97328978,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can run export release command and cli should automatically download exported release once director task completes,1355110.0,[1355110],956238,81882,feature,2015-07-22T19:12:28Z,https://www.pivotaltracker.com/story/show/97328978
2015-07-22T19:12:33Z,2015-06-18T19:31:59Z,accepted,,"- build up release.MF
- save the compiled release into the blobstore as a tarball
- include blobstore_id and sha1 in the export release task result",4.0,97328966,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can run export release command and see that tarball is made for the compiled release,948679.0,"[948679, 1355110]",956238,81882,feature,2015-07-22T19:12:33Z,https://www.pivotaltracker.com/story/show/97328966
2015-07-23T00:03:54Z,2015-06-18T19:31:25Z,accepted,,"- based on the rules of the targetted deployment
- pick up the locks for the deployment, release and selected stemcell
- select first stemcell based on OS and stemcell version
- issue compile_package commands to the agent that were spawn up
- does not aggregate anything into a tarball",4.0,97328920,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can run export release command and see that *all* non-compiled packages are compiled,948679.0,"[948679, 631325, 1541728]",956238,81882,feature,2015-07-23T00:03:54Z,https://www.pivotaltracker.com/story/show/97328920
2015-07-24T22:06:07Z,2015-07-13T19:42:39Z,accepted,,"SSH trust relationships mean a compromise on one host can allow an attacker to move trivially to other hosts.
---
None
---
SV-50413r1_rule
---
F-43560r1_fix
---
SSH's cryptographic host-based authentication is more secure than "".rhosts"" authentication, since hosts are cryptographically authenticated. However, it is not recommended that hosts unilaterally trust one another, even within an organization. 

To disable host-based authentication, add or correct the following line in ""/etc/ssh/sshd_config"": 

HostbasedAuthentication no
---
C-46170r1_chk
---
To determine how the SSH daemon's ""HostbasedAuthentication"" option is set, run the following command: 

# grep -i HostbasedAuthentication /etc/ssh/sshd_config

If no line, a commented line, or a line indicating the value ""no"" is returned, then the required value is set. 
If the required value is not set, this is a finding.",1.0,98978078,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38612] [medium] The SSH daemon must not allow host-based authentication,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:06:07Z,https://www.pivotaltracker.com/story/show/98978078
2015-07-24T22:06:25Z,2015-07-13T19:42:46Z,accepted,,"The warning message reinforces policy awareness during the logon process and facilitates possible legal action against attackers. Alternatively, systems whose ownership should not be obvious should ensure usage of a banner that does not provide easy attribution.
---
None
---
SV-50416r1_rule
---
F-43563r1_fix
---
To enable the warning banner and ensure it is consistent across the system, add or correct the following line in ""/etc/ssh/sshd_config"": 

Banner /etc/issue

Another section contains information on how to create an appropriate system-wide warning banner.
---
C-46173r1_chk
---
To determine how the SSH daemon's ""Banner"" option is set, run the following command: 

# grep -i Banner /etc/ssh/sshd_config

If a line indicating /etc/issue is returned, then the required value is set. 
If the required value is not set, this is a finding.",1.0,98978372,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38615] [medium] The SSH daemon must be configured with the login banner.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:06:26Z,https://www.pivotaltracker.com/story/show/98978372
2015-07-24T22:06:48Z,2015-07-13T19:42:46Z,accepted,,"SSH trust relationships mean a compromise on one host can allow an attacker to move trivially to other hosts.
---
None
---
SV-50412r1_rule
---
F-43559r1_fix
---
SSH can emulate the behavior of the obsolete rsh command in allowing users to enable insecure access to their accounts via "".rhosts"" files. 

To ensure this behavior is disabled, add or correct the following line in ""/etc/ssh/sshd_config"": 

IgnoreRhosts yes
---
C-46169r1_chk
---
To determine how the SSH daemon's ""IgnoreRhosts"" option is set, run the following command: 

# grep -i IgnoreRhosts /etc/ssh/sshd_config

If no line, a commented line, or a line indicating the value ""yes"" is returned, then the required value is set. 
If the required value is not set, this is a finding.",1.0,98978378,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38611] [medium] The SSH daemon must ignore .rhosts files.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:06:48Z,https://www.pivotaltracker.com/story/show/98978378
2015-07-24T22:07:30Z,2015-07-13T19:42:35Z,accepted,,"Causing idle users to be automatically logged out guards against compromises one system leading trivially to compromises on another.
---
None
---
SV-50409r1_rule
---
F-43556r1_fix
---
SSH allows administrators to set an idle timeout interval. After this interval has passed, the idle user will be automatically logged out. 

To set an idle timeout interval, edit the following line in ""/etc/ssh/sshd_config"" as follows: 

ClientAliveInterval [interval]

The timeout [interval] is given in seconds. To have a timeout of 15 minutes, set [interval] to 900. 

If a shorter timeout has already been set for the login shell, that value will preempt any SSH setting made here. Keep in mind that some processes may stop SSH from correctly detecting that the user is idle.
---
C-46167r1_chk
---
Run the following command to see what the timeout interval is: 

# grep ClientAliveInterval /etc/ssh/sshd_config

If properly configured, the output should be: 

ClientAliveInterval 900


If it is not, this is a finding.",1.0,98977896,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38608] [low] The SSH daemon must set a timeout interval on idle sessions.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:07:31Z,https://www.pivotaltracker.com/story/show/98977896
2015-07-24T22:08:02Z,2015-07-13T19:42:39Z,accepted,,"SSH environment options potentially allow users to bypass access restriction in some configurations.
---
None
---
SV-50417r1_rule
---
F-43565r1_fix
---
To ensure users are not able to present environment options to the SSH daemon, add or correct the following line in ""/etc/ssh/sshd_config"": 

PermitUserEnvironment no
---
C-46175r1_chk
---
To ensure users are not able to present environment daemons, run the following command: 

# grep PermitUserEnvironment /etc/ssh/sshd_config

If properly configured, output should be: 

PermitUserEnvironment no


If it is not, this is a finding.",1.0,98978072,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38616] [low] The SSH daemon must not permit user environment settings.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:08:02Z,https://www.pivotaltracker.com/story/show/98978072
2015-07-24T22:08:23Z,2015-07-08T02:27:39Z,accepted,,"```
Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing package compilation > Finding packages to compile. Done (00:00:00) > Binding instance networks. Done (00:00:00)
  Started preparing dns > Binding DNS. Done (00:00:00) > new_one/2
```",,98614932,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",formatting on bosh deploy is not correct,1426194.0,"[1426194, 553935]",956238,81882,bug,2015-07-24T22:08:44Z,https://www.pivotaltracker.com/story/show/98614932
2015-07-24T22:08:38Z,2015-07-13T19:42:39Z,accepted,,"This ensures a user login will be terminated as soon as the ""ClientAliveCountMax"" is reached.
---
None
---
SV-50411r1_rule
---
F-43558r1_fix
---
To ensure the SSH idle timeout occurs precisely when the ""ClientAliveCountMax"" is set, edit ""/etc/ssh/sshd_config"" as follows: 

ClientAliveCountMax 0
---
C-46168r1_chk
---
To ensure the SSH idle timeout will occur when the ""ClientAliveCountMax"" is set, run the following command: 

# grep ClientAliveCountMax /etc/ssh/sshd_config

If properly configured, output should be: 

ClientAliveCountMax 0


If it is not, this is a finding.",1.0,98978076,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38610] [low] The SSH daemon must set a timeout count on idle sessions.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:08:39Z,https://www.pivotaltracker.com/story/show/98978076
2015-07-24T22:09:08Z,2015-07-13T19:42:46Z,accepted,,"Approved algorithms should impart some level of confidence in their implementation. These are also required for compliance.
---
None
---
SV-50418r1_rule
---
F-43566r1_fix
---
Limit the ciphers to those algorithms which are FIPS-approved. Counter (CTR) mode is also preferred over cipher-block chaining (CBC) mode. The following line in ""/etc/ssh/sshd_config"" demonstrates use of FIPS-approved ciphers: 

Ciphers aes128-ctr,aes192-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes192-cbc,aes256-cbc

The man page ""sshd_config(5)"" contains a list of supported ciphers.
---
C-46176r1_chk
---
Only FIPS-approved ciphers should be used. To verify that only FIPS-approved ciphers are in use, run the following command: 

# grep Ciphers /etc/ssh/sshd_config

The output should contain only those ciphers which are FIPS-approved, namely, the AES and 3DES ciphers. 
If that is not the case, this is a finding.",1.0,98978376,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38617] [medium] The SSH daemon must be configured to use only FIPS 140-2 approved ciphers.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-07-24T22:09:08Z,https://www.pivotaltracker.com/story/show/98978376
2015-07-25T00:28:47Z,2015-07-08T21:02:42Z,accepted,,"When creating a new VM:
 - For new HVM stemcells the VM should have sriov_net_support enabled (AMI flag)
 - For new PV stemcells the VM should have sriov_net_support disabled
 - For old stemcells it should be disabled

https://github.com/cloudfoundry/bosh/pull/860
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html",2.0,98688776,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",HVM stemcells should have sriov_net_support enabled by default,687691.0,"[687691, 1426194]",956238,687691,feature,2015-07-25T00:28:47Z,https://www.pivotaltracker.com/story/show/98688776
2015-07-25T00:29:09Z,2015-07-20T16:31:14Z,accepted,,"- verify ubuntu, centos, openstack w/dhcp, openstack w/o dhcp",2.0,99433450,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",openstack machines should be able to be configured to use dhcp or not (defaults to use dhcp),553935.0,[553935],956238,81882,feature,2015-07-25T00:29:09Z,https://www.pivotaltracker.com/story/show/99433450
2015-07-25T00:29:37Z,2015-07-20T16:30:51Z,accepted,,"- modify cpi to pass in dhcp=bool flag into bootstrapping settings for the agent
- modify agent to account for dhcp flag from the cpi when determining if network should be configured with dhcp (IsDHCP: https://github.com/cloudfoundry/bosh-agent/blob/2bd678b50040a4196b90a1f5eb16327dd63523f5/settings/settings.go#L191)
- verify that when aws machine has dhcp running when it boot up (centos and ubuntu)",4.0,99433414,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}]",aws machines should always use dhcp,553935.0,[553935],956238,81882,feature,2015-07-25T00:29:37Z,https://www.pivotaltracker.com/story/show/99433414
2015-07-27T16:52:00Z,2015-07-21T20:45:12Z,accepted,,"```
ubuntu@ip-172-31-4-119:~$  bosh status
/home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require': cannot load such file -- yajl (LoadError)
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli/commands/release/export_release.rb:3:in `<top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:119:in `block in <top (required)>'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:118:in `each'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:118:in `<top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/bin/bosh:3:in `block in <top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/benchmark.rb:294:in `realtime'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/bin/bosh:3:in `<top (required)>'
	from /home/ubuntu/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /home/ubuntu/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
ubuntu@ip-172-31-4-119:~$ bosh
/home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require': cannot load such file -- yajl (LoadError)
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli/commands/release/export_release.rb:3:in `<top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:119:in `block in <top (required)>'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:118:in `each'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli.rb:118:in `<top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/bin/bosh:3:in `block in <top (required)>'
	from /home/ubuntu/.rubies/ruby-2.1.2/lib/ruby/2.1.0/benchmark.rb:294:in `realtime'
	from /home/ubuntu/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/bin/bosh:3:in `<top (required)>'
	from /home/ubuntu/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /home/ubuntu/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
ubuntu@ip-172-31-4-119:~$ gem install bosh_cli --no-rdoc --no-ri
```",,99555792,story,[],bosh_cli explodes on start,553935.0,"[553935, 1406536]",956238,1406536,bug,2015-07-27T16:52:10Z,https://www.pivotaltracker.com/story/show/99555792
2015-07-27T17:12:48Z,2015-07-16T23:12:36Z,accepted,,"As a BOSH user
I want ""Acting as user user on deployment deployment..."" output to go to stderr
So that when I pipe output from bosh download manifest to another Unix utility, then I can get the valid yaml. 

Given a bosh deployment
When I pipe output from bosh download manifest to another Unix utility, 
Then I can get the valid yaml and not extra


Notes: 

Hi folks, Tushar and I noticed that the following output generated some extra words. We believe that it would be appropriate for messages such as ""Acting as user...blah "" to be output to stderr instead of stdout. 

In a previuos version of BOSH ( we don't know which ), this used to work  as expected. Perhaps the output used to go to stderr, or was not present at that time. 

```
± |master ✗| → bosh --version
BOSH 1.3012.0
± |master ✗| → bosh download manifest cf-cfapps-io2-diego 2>/dev/null| head
Acting as user 'admin' on deployment 'cf-cfapps-io2-diego' on 'vpc-bosh-run-pivotal-io'
---
compilation:
  cloud_properties:
    availability_zone: us-east-1d
    instance_type: c3.large
  network: diego1
  reuse_compilation_vms: true
  workers: 16

```",,99269906,story,[],"do not output ""Acting as user user on deployment deployment..."" for download manifest",344.0,"[344, 1426194]",956238,1524644,bug,2015-07-27T17:12:49Z,https://www.pivotaltracker.com/story/show/99269906
2015-07-27T17:52:21Z,2015-07-24T00:44:32Z,accepted,,,,99751638,story,[],UAA tests should use pre-built war file,1426194.0,"[1426194, 553935]",956238,1426194,chore,2015-07-27T17:52:22Z,https://www.pivotaltracker.com/story/show/99751638
2015-07-27T19:08:01Z,2015-07-27T16:35:15Z,accepted,,,,99920300,story,[],Get Rupa onboarded,553935.0,[553935],956238,553935,chore,2015-07-27T19:08:02Z,https://www.pivotaltracker.com/story/show/99920300
2015-07-27T23:29:57Z,2015-07-16T00:45:30Z,accepted,,"Director task 7
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)",,99188896,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",deployment name should be shown,1687782.0,[1687782],956238,1550486,bug,2015-07-27T23:29:57Z,https://www.pivotaltracker.com/story/show/99188896
2015-07-28T00:11:23Z,2015-06-30T23:57:38Z,accepted,,,1.0,98156318,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",add a timestamp and task id to ip addresses when they get acquired,1426194.0,"[1426194, 553935]",956238,81882,feature,2015-07-28T00:11:23Z,https://www.pivotaltracker.com/story/show/98156318
2015-07-28T00:11:41Z,2015-07-08T02:28:02Z,accepted,,,,98614942,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",creation of vm should never include null for disk locality,344.0,[344],956238,81882,bug,2015-07-28T00:11:41Z,https://www.pivotaltracker.com/story/show/98614942
2015-07-28T00:25:42Z,2015-07-16T00:53:44Z,accepted,,particularly i noticed that extra provided links were validated after vm was recreated.,2.0,99189180,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",all link resolution must happen before making any changes to the deployments,1687782.0,"[1687782, 687691]",956238,81882,feature,2015-07-28T00:25:42Z,https://www.pivotaltracker.com/story/show/99189180
2015-07-28T19:07:24Z,2015-07-24T16:45:01Z,accepted,,"http://www.ubuntu.com/usn/usn-2684-1/
expected kernel: linux-image-3.19.0-23-generic 3.19.0-23.24",1.0,99796096,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2684-1] bump os images for ubuntu,1550486.0,[1550486],956238,81882,feature,2015-07-28T19:07:25Z,https://www.pivotaltracker.com/story/show/99796096
2015-07-29T01:50:20Z,2015-07-28T16:41:05Z,accepted,,"http://www.ubuntu.com/usn/usn-2690-1/
expected: linux-image-3.19.0-25-generic 3.19.0-25.26~14.04.1",0.0,100037592,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2690-1] bump ubuntu trusty stemcell,553935.0,[553935],956238,81882,feature,2015-07-29T01:50:25Z,https://www.pivotaltracker.com/story/show/100037592
2015-07-29T18:58:32Z,2015-07-29T18:52:43Z,accepted,,,,100143774,story,[],Why is the global-net CI all busted,687691.0,"[687691, 1655862]",956238,687691,chore,2015-07-29T18:58:33Z,https://www.pivotaltracker.com/story/show/100143774
2015-07-30T00:47:32Z,2015-07-29T18:57:43Z,accepted,,"Commented out in `7178a38552a50f38d368d0c80303844072e8fb17`

We should fix this or delete it...",,100144184,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Reenable syslog agent test,1550486.0,"[1550486, 1591058]",956238,687691,chore,2015-08-11T00:43:48Z,https://www.pivotaltracker.com/story/show/100144184
2015-07-30T01:02:00Z,2015-05-14T22:12:41Z,accepted,,to reproduce this release and reserve IP in a tight loop,2.0,94627828,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",investigate if rapid manual IP reuse is still a problem on EC2,1426194.0,"[1426194, 1687782]",956238,81882,feature,2015-07-30T01:02:00Z,https://www.pivotaltracker.com/story/show/94627828
2015-07-30T01:11:40Z,2015-07-23T21:11:46Z,accepted,,"- name is a required String
- cloud_properties is a required Hash; with empty hash as a default
- keep availability_zones key optional

For example:

```
availability_zones: # <----- top level in the cloud-config
- name: z1
  cloud_properties: {availability_zone: us-east-1a}
- name: z2
  cloud_properties: {availability_zone: us-east-1c}

networks:
...
```",2.0,99740004,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can specify list of availability zones,1687782.0,[1687782],956238,81882,feature,2015-07-30T01:11:40Z,https://www.pivotaltracker.com/story/show/99740004
2015-07-30T01:11:47Z,2015-07-23T21:11:58Z,accepted,,,1.0,99740018,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user should see an error message if duplicate names are specified for AZs,1687782.0,"[1687782, 687691]",956238,81882,feature,2015-07-30T01:11:47Z,https://www.pivotaltracker.com/story/show/99740018
2015-07-30T01:13:56Z,2015-07-23T21:12:12Z,accepted,,"- error if availability zone referenced is not found
- keep available_zone key optional",2.0,99740040,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can specify availability zones key for a manual network's subnets,1687782.0,"[1687782, 687691, 1655862]",956238,81882,feature,2015-07-30T01:13:56Z,https://www.pivotaltracker.com/story/show/99740040
2015-07-30T01:16:02Z,2015-07-23T21:12:29Z,accepted,,"- error if availability zone referenced is not found at the top level az key
- availablity zones key on the job is optional",2.0,99740064,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can specify availability zones on a deployment job,687691.0,"[687691, 1655862]",956238,81882,feature,2015-07-30T01:16:02Z,https://www.pivotaltracker.com/story/show/99740064
2015-07-30T01:31:45Z,2015-07-23T20:56:17Z,accepted,,"so that compilation workers do not select IPs that are already taken. 

this is important for non-db-based-ip-selection.",,99738586,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",binding of networks should happen before we spin up compilation workers,553935.0,"[553935, 1017727]",956238,81882,bug,2015-07-30T01:31:45Z,https://www.pivotaltracker.com/story/show/99738586
2015-07-30T01:35:39Z,2015-06-03T19:07:36Z,accepted,,"```
Failed creating bound missing vms > mysql-partition-null-az/0: Timed out pinging to 1bf5d248-c097-4586-a6d4-bd3683b575e4 after 600 seconds (00:12:08)
Failed creating bound missing vms (00:12:08)

Error 450002: Timed out pinging to 655cb5c3-1ef2-4bab-8ba4-45de8b89a96c after 600 seconds
```

Then you'll see error messages like this:

```
Error 130008: `ha_proxy-partition-null-az/0' asked for a static IP http://67.xx.xx.xx but it's already reserved/in use
```

Currently, the object modeling around instances is poorly encapsulated. As a result, we have code duplication and inconsistencies. Refactor the modeling of running jobs and their associated IaaS resources to clearly encapsulate responsibility.  We want to move in the following direction:

- Instance object that holds assumed state, actual state, and desired state 
- A State object is a collection of DB-backed dumb data objects (no behavior). VMs, Ip Addresses, Disks, Job Templates, etc. 
- State objects can be diffed. 
- AssumedState is a State object, generated from DB models. 
- DesiredState is a State object generated from the parsed manifest. 
- ActualState is a State object generated from AgentState.",4.0,96116066,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",director should never create a vm that is not assigned to an instance,1569704.0,"[1569704, 878715, 553935]",956238,81882,feature,2015-07-30T01:35:39Z,https://www.pivotaltracker.com/story/show/96116066
2015-07-30T01:40:54Z,2015-07-01T00:07:10Z,accepted,,ip used to be in the reserved section and it was removed from there. ,1.0,98156764,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",unreserved ip can be used for instances,553935.0,"[553935, 1017727]",956238,81882,feature,2015-07-30T01:40:54Z,https://www.pivotaltracker.com/story/show/98156764
2015-07-30T02:01:55Z,2015-06-30T23:57:22Z,accepted,,"debug log. make sure log starts with something consistent and greppable
- add released, acquired, retried to be acquired (during acquire race)
- add log for when ip needs to move and a reason why",2.0,98156312,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]","operator should see logs when ip are released, acquired, retried to be acquired (during acquire race)",1426194.0,"[1426194, 553935, 1017727]",956238,81882,feature,2015-07-30T02:01:56Z,https://www.pivotaltracker.com/story/show/98156312
2015-07-30T02:02:10Z,2015-07-14T18:56:21Z,accepted,,,1.0,99066324,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Compilation should use Instance to manage VM lifecycle,1569704.0,[1569704],956238,1569704,feature,2015-07-30T02:02:10Z,https://www.pivotaltracker.com/story/show/99066324
2015-07-30T18:42:22Z,2015-07-29T16:27:55Z,accepted,,"During our publish and promote process in our jenkins pipeline the process includes an automated step of making an Adding final release commit on to master. 
This is a problem when we manually kick off a publish build of a specific SHA that does not have the latest master since it was automatically commited by jenkins. We would like to detect this earlier by having the publish step fail if the SHA that the build was ran on does not have the latest master. ",,100131476,story,[],promote candidate build should fail if commit does not include latest master,1426194.0,"[1426194, 1017727]",956238,553935,chore,2015-07-30T18:42:23Z,https://www.pivotaltracker.com/story/show/100131476
2015-07-30T20:48:36Z,2015-07-29T17:09:48Z,accepted,,"http://www.ubuntu.com/usn/usn-2694-1/
Expected: libpcre3 1:8.31-2ubuntu2.1

Also update pcre package used by the nginx module in bosh release.",1.0,100134842,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[usn-2694-1] bump ubuntu trusty stemcell for libpcre,553935.0,[553935],956238,81882,feature,2015-07-30T20:48:37Z,https://www.pivotaltracker.com/story/show/100134842
2015-07-30T21:32:09Z,2015-07-13T21:21:24Z,accepted,,"`bosh_common/lib/common/errors.rb`

We probably want to catch these errors without having to rescue `Exception`, since lots of Exceptions aren't necessarily recoverable. By default `begin...rescue...end` will rescue `StandardError`, and we probably want that to include RetryCountExceeded errors",,98987988,story,[],Bosh::Common::RetryCountExceeded should inherit from StandardError not Exception,1426194.0,"[1426194, 1017727]",956238,687691,chore,2015-07-30T21:32:25Z,https://www.pivotaltracker.com/story/show/98987988
2015-07-31T00:53:01Z,2015-07-31T00:47:56Z,accepted,,,,100259984,story,[],add integration test for bosh deploy --recreate to spec/integration/recreate_spec.rb,553935.0,[553935],956238,81882,chore,2015-07-31T00:53:01Z,https://www.pivotaltracker.com/story/show/100259984
2015-07-31T01:09:17Z,2015-07-13T19:42:45Z,accepted,,"Using interactive boot, the console user could disable auditing, firewalls, or other services, weakening system security.
---
None
---
SV-50389r1_rule
---
F-43536r1_fix
---
To disable the ability for users to perform interactive startups, edit the file ""/etc/sysconfig/init"". Add or correct the line: 

PROMPT=no

The ""PROMPT"" option allows the console user to perform an interactive system startup, in which it is possible to select the set of services which are started on boot.
---
C-46146r1_chk
---
To check whether interactive boot is disabled, run the following command: 

$ grep PROMPT /etc/sysconfig/init

If interactive boot is disabled, the output will show: 

PROMPT=no


If it does not, this is a finding.",1.0,98978336,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38588] [medium] The system must not permit interactive boot [not possible],1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-07-31T01:09:19Z,https://www.pivotaltracker.com/story/show/98978336
2015-07-31T16:37:55Z,2015-07-30T17:39:30Z,accepted,,"http://www.ubuntu.com/usn/usn-2698-1
expected: libsqlite3-0 3.8.2-1ubuntu2.1",1.0,100228724,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[usn-2698-1] bump ubuntu trusty for libsqlite3,687691.0,[687691],956238,81882,feature,2015-07-31T16:37:56Z,https://www.pivotaltracker.com/story/show/100228724
2015-07-31T16:41:53Z,2015-07-27T18:12:40Z,accepted,,"The director sent an empty-string blobstore ID to the agent on a compilation VM. This led to a misleading error message:

```
D, [2015-07-27 17:43:45 #6109] [compile_package(go-lang-1.4.2/7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d, bosh-aws-xen-hvm-centos-7-go_agent/3001)] DEBUG -- DirectorJobRunner: SENT: agent.2eb14cc3-190d-4d9e-b93a-23748e2455dc {""method"":""compile_package"",""arguments"":[null,null,""go-lang-1.4.2"",""7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d.1"",{}],""reply_to"":""director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.1003274d-6ac7-453d-8bd9-07a0eb494094""}
D, [2015-07-27 17:43:45 #6109] [] DEBUG -- DirectorJobRunner: RECEIVED: director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.1003274d-6ac7-453d-8bd9-07a0eb494094 {""value"":{""agent_task_id"":""62134187-9143-49bf-75e4-c1ceaf9affe6"",""state"":""running""}}
D, [2015-07-27 17:43:45 #6109] [compile_package(go-lang-1.4.2/7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d, bosh-aws-xen-hvm-centos-7-go_agent/3001)] DEBUG -- DirectorJobRunner: SENT: agent.2eb14cc3-190d-4d9e-b93a-23748e2455dc {""method"":""get_task"",""arguments"":[""62134187-9143-49bf-75e4-c1ceaf9affe6""],""reply_to"":""director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.d01186b3-0f93-426b-9e37-bf4748031ded""}
D, [2015-07-27 17:43:45 #6109] [] DEBUG -- DirectorJobRunner: RECEIVED: director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.d01186b3-0f93-426b-9e37-bf4748031ded {""value"":{""agent_task_id"":""62134187-9143-49bf-75e4-c1ceaf9affe6"",""state"":""running""}}

[ ... task polling omitted ... ]

D, [2015-07-27 17:43:48 #6109] [compile_package(go-lang-1.4.2/7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d, bosh-aws-xen-hvm-centos-7-go_agent/3001)] DEBUG -- DirectorJobRunner: SENT: agent.2eb14cc3-190d-4d9e-b93a-23748e2455dc {""method"":""get_task"",""arguments"":[""62134187-9143-49bf-75e4-c1ceaf9affe6""],""reply_to"":""director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.d983a453-8a95-4823-b01c-23399dd1057a""}
D, [2015-07-27 17:43:48 #6109] [] DEBUG -- DirectorJobRunner: RECEIVED: director.4f8b9d4c-29e7-4b4d-80cb-c006f4d648fd.d983a453-8a95-4823-b01c-23399dd1057a {""exception"":{""message"":""Action Failed get_task: Task 62134187-9143-49bf-75e4-c1ceaf9affe6 result: Compiling package go-lang-1.4.2: Fetching package go-lang-1.4.2: Fetching package blob : Getting blob from inner blobstore: Getting blob from inner blobstore: Shelling out to bosh-blobstore-dav cli: Running command: 'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json get  /var/vcap/data/tmp/bosh-blobstore-externalBlobstore-Get441305173', stdout: 'Error running app - Getting dav blob : Wrong response code: 404; body: \u003chtml\u003e\r\n\u003chead\u003e\u003ctitle\u003e404 Not Found\u003c/title\u003e\u003c/head\u003e\r\n\u003cbody bgcolor=\""white\""\u003e\r\n\u003ccenter\u003e\u003ch1\u003e404 Not Found\u003c/h1\u003e\u003c/center\u003e\r\n\u003chr\u003e\u003ccenter\u003enginx\u003c/center\u003e\r\n\u003c/body\u003e\r\n\u003c/html\u003e\r\n', stderr: '': exit status 1""}}
```

Note the extra space in the `bosh-blobstore-dav` command where the blobstore ID should have been. Instead of reporting a 404 error, `bosh-blobstore-dav` should have exited immediately with a ""not enough arguments"" or ""incorrect usage"" error.",,99929594,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh-blobstore-dav should validate its command-line arguments,1426194.0,"[1426194, 1687782]",956238,948679,bug,2015-07-31T16:41:53Z,https://www.pivotaltracker.com/story/show/99929594
2015-07-31T16:42:25Z,2015-07-29T18:47:43Z,accepted,,"libpq, ruby. let's keep it for redis for now.",1.0,100143260,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Remove -j from bosh release,1550486.0,[1550486],956238,553935,feature,2015-07-31T16:42:35Z,https://www.pivotaltracker.com/story/show/100143260
2015-07-31T17:54:50Z,2015-07-10T17:51:57Z,accepted,,"- copy cf-release in tmp-bosh-cf-release
- inline cloud_controller and loggregator jobs (currently submoduled)
- modify to use links for ip addressing
- deploy and see it succeed",4.0,98842526,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",deploy cf-release with current links feature,1550486.0,[1550486],956238,81882,feature,2015-07-31T17:54:50Z,https://www.pivotaltracker.com/story/show/98842526
2015-07-31T19:08:42Z,2015-07-22T23:27:14Z,accepted,,"should not try to fetch director task result if task did not succeed?

```
ruby 2.1.2p95 idora in ~
○ → bosh export release bosh/185+dev.2 ubuntu-trusty/3016

Director task 506
Error 30006: Release version `bosh/185+dev.2' doesn't exist

Task 506 error

For a more detailed error report, run: bosh task 506 --debug
/Users/pivotal/.gem/ruby/2.1.2/gems/yajl-ruby-1.2.1/lib/yajl.rb:37:in `parse': input must be a string or IO (Yajl::ParseError)
	from /Users/pivotal/.gem/ruby/2.1.2/gems/yajl-ruby-1.2.1/lib/yajl.rb:37:in `parse'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli/commands/release/export_release.rb:27:in `export'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.3019.0/bin/bosh:16:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
```",1.0,99659682,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",cli should exit with error without trying to fetch compiled release tarball if director task did not succeed,631325.0,[631325],956238,81882,feature,2015-07-31T19:08:51Z,https://www.pivotaltracker.com/story/show/99659682
2015-07-31T19:10:38Z,2015-07-22T18:13:41Z,accepted,,"either blobstore_id and sha1 are both null or both present.

https://github.com/cloudfoundry/bosh/commit/0b6de3cb88eec149b4f440b9f268092de58944d1#diff-33ccbd3374b48c5a02d72ada9cd6582aR21",1.0,99633782,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",stricter validation of package model,631325.0,"[631325, 1355110]",956238,81882,feature,2015-07-31T19:10:44Z,https://www.pivotaltracker.com/story/show/99633782
2015-08-03T20:52:48Z,2015-07-21T01:43:55Z,accepted,,"issue: https://github.com/cloudfoundry/bosh/issues/724

if ReleaseInvalidArchive is raised in [1] @release_path is never set so release_dir will never be cleaned up.

https://github.com/cloudfoundry/bosh/blob/2840387d687a44569edd9cdc2e49ecba129634b8/bosh-director/lib/bosh/director/jobs/update_release.rb#L53

[1] https://github.com/cloudfoundry/bosh/blob/2840387d687a44569edd9cdc2e49ecba129634b8/bosh-director/lib/bosh/director/jobs/update_release.rb#L88",,99479992,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",clean up failed release upload when ReleaseInvalidArchive is raised,1386874.0,[1386874],956238,81882,bug,2015-08-03T20:52:48Z,https://www.pivotaltracker.com/story/show/99479992
2015-08-03T20:53:57Z,2015-05-18T16:37:27Z,accepted,,"benchmark deployments and identify bottlenecks and memory consumption.  DO NOT MAKE CHANGES IN THIS STORY!
- /8?
- try multiple deployments
- use m3.medium",2.0,94851742,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",try to run out of memory when allocating/counting ip addresses,1426194.0,"[1426194, 1017727]",956238,553935,feature,2015-08-03T20:53:57Z,https://www.pivotaltracker.com/story/show/94851742
2015-08-04T00:44:53Z,2015-07-30T20:35:25Z,accepted,,,,100245270,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",investigate why recreate does not create when global net is turned on,553935.0,"[553935, 1715614]",956238,81882,bug,2015-08-04T00:44:53Z,https://www.pivotaltracker.com/story/show/100245270
2015-08-04T01:03:13Z,2015-06-30T23:49:10Z,accepted,,"first deploy
ordered
- jobA has .10
- jobB has .11

2nd deploy
ordered
jobA has .11
jobB has .10

2nd deploy should result in an error since there is no way jobB can release .11 address before we update jobA.

seems to work (should not work) when using cloud-config and correctly fails with legacy manifests.

there may be a pending test.",0.0,98155606,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",deployment with two static ips should not swap ip reservation in cases when two jobs are still needed https://www.pivotaltracker.com/story/show/98155606,553935.0,"[553935, 1715614]",956238,81882,feature,2015-08-04T01:03:13Z,https://www.pivotaltracker.com/story/show/98155606
2015-08-04T02:11:22Z,2015-06-30T23:53:30Z,accepted,,"include deployment name, instance name and index.",1.0,98156130,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",user sees ip is already taken error which includes information about who holds that ip,553935.0,"[553935, 1715614]",956238,81882,feature,2015-08-04T02:11:22Z,https://www.pivotaltracker.com/story/show/98156130
2015-08-04T02:21:28Z,2015-06-30T17:52:36Z,accepted,,"Test already written in  ip reservations spec, grep for story id

This applies to both dynamically reserved IPs and statically reserved IPs",0.0,98127770,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]","when deploying deployment A  failed to create vm (no vm in database), deployment B should not reserve IP of that VM",553935.0,[553935],956238,687691,feature,2015-08-04T02:21:28Z,https://www.pivotaltracker.com/story/show/98127770
2015-08-04T03:12:18Z,2015-06-08T19:04:13Z,accepted,,,2.0,96481326,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",director should maintain same manual network ip for instances that were resurrected,553935.0,[553935],956238,81882,feature,2015-08-04T03:12:18Z,https://www.pivotaltracker.com/story/show/96481326
2015-08-04T03:42:23Z,2015-07-23T21:12:38Z,accepted,,"Given a cloud config
```
availability_zones:
  - { name: zone1, cloud_properties: {...} }
  - { name: zone2, cloud_properties: {...} }
networks:
  - name: foo-net
    subnets:
      - range: ...
        availability_zone: zone1
    type: manual
...
```
And a deployment manifest
```
jobs:
  - name: foo-job
     networks: [foo-net]
     availability_zones: [zone1, zone2]
...
```
When I deploy I should see an error:

""Job 'foo-job' refers to an availability zone 'zone2' but 'foo-net' has no subnet in that zone""

Ignore VIP networks when validating AZs. Ignore dynamic networks for now until #99740512",2.0,99740078,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can see an error message when a deployment job in two AZs uses a network that does not have subnet in that AZ,687691.0,"[687691, 1655862]",956238,81882,feature,2015-08-04T03:42:23Z,https://www.pivotaltracker.com/story/show/99740078
2015-08-04T04:11:53Z,2015-05-16T01:28:42Z,accepted,,"- delete both vm and persistent disk
- bosh cck should show two errors 

```
Problem 1 of 2: VM with cloud ID `vm-9f7df3fb-de79-4c06-9d03-9dbcef3e21cf' missing.
  1. Ignore problem
  2. Recreate VM using last known apply spec
  3. Delete VM reference (DANGEROUS!)
Please choose a resolution [1 - 3]: 3

Problem 2 of 2: Disk `disk-3d3a7ab1-5127-4e3b-8d8e-f41d1b1dffef' (dummy1/0, 10000M) is missing.
  1. Ignore problem
  2. Delete disk reference (DANGEROUS!)
Please choose a resolution [1 - 2]: 2
```

- resolving second problem (missing disk) should ignore if vm is deleted

from https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/problem_handlers/missing_disk.rb#L49

```
E, [2015-05-16 01:24:34 #17764] [task:56] ERROR -- DirectorJobRunner: Error resolving problem `2': undefined method `cid' for nil:NilClass
E, [2015-05-16 01:24:34 #17764] [task:56] ERROR -- DirectorJobRunner: /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_handlers/missing_disk.rb:49:in `delete_disk_reference'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_handlers/missing_disk.rb:40:in `block (2 levels) in <class:MissingDisk>'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_resolver.rb:59:in `block in apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2941.0/lib/bosh/director/event_log.rb:97:in `call'
```",1.0,94719748,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",vm deletion should not prevent users from using cck to delete disks,1495236.0,[1495236],956238,81882,feature,2015-08-04T04:11:54Z,https://www.pivotaltracker.com/story/show/94719748
2015-08-04T04:33:27Z,2015-07-23T21:14:26Z,accepted,,"- create_vm CPI call gets sum of AZ's cloud_properties and resource pool's cloud_properties
- take az's cloud_properties hash and merge resource pool's cloud_properties hash on top of that

don't worry about keeping instances in the same AZ when scaling up/down

place instances as evenly as possible. if instances don't divide evenly into the number of AZs, place extra in first AZs in list.",4.0,99740204,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can see deployment job VMs get AZ assigned to them based on specific AZ,1655862.0,"[1655862, 687691]",956238,81882,feature,2015-08-04T04:33:27Z,https://www.pivotaltracker.com/story/show/99740204
2015-08-04T17:49:52Z,2015-08-04T16:01:34Z,accepted,,"The integration tests occasionally hang when loading the Ruby Sequel connection_pool extension.

Maria's analysis was helpful. We will add retry logic as she suggests, in hopes it makes the sandbox startup in the tests more reliable:

I just looked at hanging integration test. There are 2 issues, one is that we loop forever until worker starts up. Second is that worker fails to start up, because it fails to connect to DB.

Looking in worker debug logs:

```
/opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `initialize': PG::Error: timeout expired (Sequel::DatabaseConnectionError)
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `new'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `connect'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool.rb:94:in `make_new'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:164:in `make_new'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:137:in `available'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:127:in `block in acquire'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `block in sync'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `synchronize'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `sync'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:126:in `acquire'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:85:in `acquire'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:94:in `hold'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:452:in `server_version'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:1319:in `server_version'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:1279:in `select_clause_methods'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:835:in `clause_sql'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:136:in `select_sql'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:141:in `sql'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:299:in `valid_connection_sql'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:67:in `extended'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:107:in `extend'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:107:in `block in <module:Sequel>'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:147:in `call'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:147:in `block in extension'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:145:in `each'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:145:in `extension'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:280:in `extension'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/lib/bosh/director/config.rb:174:in `configure_db'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/lib/bosh/director/config.rb:131:in `configure'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/lib/bosh/director/config.rb:416:in `configure_evil_config_singleton!'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/lib/bosh/director/app.rb:26:in `initialize'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/bin/bosh-director-worker:52:in `new'
        from /tmp/build/dd149a0c-f860-4f5e-566e-bc570e026015/bosh-src/bosh-director/bin/bosh-director-worker:52:in `<top (required)>'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `load'
        from /opt/rubies/ruby-2.1.6/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `<main>'
```

Looks like it's failing to initialize extension: https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/config.rb#L174

Should not that use retry logic to connect to DB?",,100544528,story,[],Add retry logic to loading sequel connection_pool extension,948679.0,"[948679, 1541728]",956238,1541728,chore,2015-08-04T17:49:43Z,https://www.pivotaltracker.com/story/show/100544528
2015-08-05T21:25:43Z,2015-07-29T18:51:37Z,accepted,,pcre.org,1.0,100143662,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Bump pcre blob in bosh release,1550486.0,[1550486],956238,553935,feature,2015-08-05T21:25:51Z,https://www.pivotaltracker.com/story/show/100143662
2015-08-06T22:37:02Z,2015-07-23T21:55:21Z,accepted,,If everyone is creating/deleting IPs it's going to be really hard to know what's going on,,99743700,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",only IpProvider should deal with Models::IpAddress,553935.0,[553935],956238,687691,chore,2015-08-06T22:37:02Z,https://www.pivotaltracker.com/story/show/99743700
2015-08-07T23:17:54Z,2015-07-23T22:30:10Z,accepted,,"Seems this has been done for the director template at https://www.pivotaltracker.com/story/show/95550578, but not for the registry. Hit this problem today in a customer PoC",,99745628,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]","wrap openstack endpoint, user, password, and tenant to be properly escaped at the bosh registry templates",1426194.0,"[1426194, 1017727]",956238,1015645,bug,2015-08-07T23:18:16Z,https://www.pivotaltracker.com/story/show/99745628
2015-08-10T17:59:44Z,2015-08-01T00:15:40Z,accepted,,"what happens when ip address was found (and save failed) and then it was not found (via find)
https://github.com/cloudfoundry/bosh/blob/global-net/bosh-director/lib/bosh/director/deployment_plan/ip_provider/database_ip_provider.rb#L104-L124",,100344028,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",race in reserving ip for an instance,553935.0,"[553935, 1655862]",956238,81882,bug,2015-08-10T17:59:44Z,https://www.pivotaltracker.com/story/show/100344028
2015-08-10T18:42:14Z,2015-08-06T23:38:44Z,accepted,,,,100786948,story,[],remove the 2hr keep alive on concourse worker containers,1687782.0,"[1687782, 1715614]",956238,344,chore,2015-08-10T18:42:14Z,https://www.pivotaltracker.com/story/show/100786948
2015-08-10T18:42:47Z,2015-08-04T02:36:27Z,accepted,,"Looks like we loop forever right now.

https://github.com/cloudfoundry/bosh/blob/master/bosh-dev/lib/bosh/dev/sandbox/services/director_service.rb#L79

5 min?",,100480768,story,[],Integration tests should fail if workers are failing to start within some timeout,344.0,"[344, 1017727]",956238,553935,chore,2015-08-10T18:42:47Z,https://www.pivotaltracker.com/story/show/100480768
2015-08-10T23:31:45Z,2015-07-20T19:17:14Z,accepted,,- if has task_id then async,2.0,99449778,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",director should accept all tasks to be async even if it's sync task,1569704.0,"[1569704, 1700364]",956238,81882,feature,2015-08-21T17:07:11Z,https://www.pivotaltracker.com/story/show/99449778
2015-08-11T00:17:45Z,2015-08-10T22:35:05Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-1.9-postgres/builds/73,,100987584,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Investigate flaky failure,553935.0,"[553935, 344]",956238,553935,chore,2015-08-11T00:17:45Z,https://www.pivotaltracker.com/story/show/100987584
2015-08-11T03:03:25Z,2015-07-02T00:43:26Z,accepted,,ideally without spinning up VMs up front,2.0,98242592,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user sees an error that there is no way to deploy release version X on stemcell X because it does not have source packages,948679.0,"[948679, 1541728]",956238,81882,feature,2015-08-11T03:03:25Z,https://www.pivotaltracker.com/story/show/98242592
2015-08-11T03:04:01Z,2015-07-02T00:43:15Z,accepted,,"- assumes that stemcell is already uploaded
- raise an error if no stemcell matched the criteria e.g. ""No stemcell matches OS 'os-blah' version '3646'. Please upload stemcell matching this criteria and run this command again.""
- populate compiled packages for all matching stemcells
- make sure that uploading compiled release does not exit 1 because of validations (in CLI or Director)",4.0,98242586,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]","user can use upload release command to import compiled packages into the Director from a compiled release tarball, such that deploy command does not try to compile these packages during the deploy",631325.0,"[631325, 1355110]",956238,81882,feature,2015-08-13T12:59:47Z,https://www.pivotaltracker.com/story/show/98242586
2015-08-11T03:04:21Z,2015-07-02T00:42:59Z,accepted,,"- jobs should be saved into the blobstore
- actions should be shown in the event log",2.0,98242576,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can use upload release command to import release jobs into the Director from a compiled release tarball,1355110.0,"[1355110, 631325]",956238,81882,feature,2015-08-11T03:04:22Z,https://www.pivotaltracker.com/story/show/98242576
2015-08-11T03:04:24Z,2015-06-03T23:13:10Z,accepted,,,,96142292,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",releases can be --exported-- in their compiled form for a specific stemcell version,,[],956238,81882,release,2015-08-11T03:04:24Z,https://www.pivotaltracker.com/story/show/96142292
2015-08-11T03:19:32Z,2015-07-28T21:54:43Z,accepted,,"To reproduce:

1. `bosh upload release my_compiled_release.tgz`
2. `bosh delete release my_compiled_release/0.99`

Expected behaviour: the release and all its compiled packages are deleted

Actual behaviour: `Error 30009: Can't delete release: no implicit conversion of nil into String`

Stack trace from director:

```
E, [2015-07-28 21:47:24 #14232] [task:17] ERROR -- DirectorJobRunner: Can't delete release: no implicit conversion of nil into String
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/delete_release.rb:198:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,100066722,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",can't delete a release that was uploaded as a compiled package,1355110.0,"[1355110, 631325]",956238,1541728,bug,2015-08-11T03:19:32Z,https://www.pivotaltracker.com/story/show/100066722
2015-08-11T03:44:46Z,2015-07-22T23:58:16Z,accepted,,otherwise release exported is not the release that's being compiled,,99661174,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",export release director task must error if release version specified does not match release version specified in the manifest,631325.0,[631325],956238,81882,bug,2015-08-11T03:44:47Z,https://www.pivotaltracker.com/story/show/99661174
2015-08-11T04:10:40Z,2015-08-04T00:44:35Z,accepted,,"it takes really long time between those two steps.

  Started preparing deployment > Binding deployment. Done (00:00:00)

???

  Started preparing deployment > Binding releases. Done (00:00:00)",1.0,100476824,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",what's happening between binding deployment and binding releases,553935.0,"[553935, 1655862]",956238,81882,feature,2015-08-11T04:10:41Z,https://www.pivotaltracker.com/story/show/100476824
2015-08-11T04:19:41Z,2015-08-04T03:13:46Z,accepted,,,1.0,100481786,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",vms should have metadata tags set on them,553935.0,"[553935, 1655862]",956238,81882,feature,2015-08-11T04:19:41Z,https://www.pivotaltracker.com/story/show/100481786
2015-08-11T05:27:26Z,2015-06-04T23:08:16Z,accepted,,,2.0,96245208,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",director should maintain same manual network ip for instances that were hard stopped (detached) and then started,553935.0,"[553935, 1655862]",956238,81882,feature,2015-08-11T05:27:26Z,https://www.pivotaltracker.com/story/show/96245208
2015-08-11T05:32:40Z,2015-08-04T17:31:54Z,accepted,,"@calebamiles reported an issue they were seeing in their BATs, network reservation was failing, but there was no error. After investigation we found that BOSH failed to find subnet. In this case the error will not be set: https://github.com/cloudfoundry/bosh/blob/ede389a2e112e1b4f2dbc4495c08977da4439483/bosh-director/lib/bosh/director/deployment_plan/manual_network.rb#L48

This code was changed on global-network branch and should be updated there.

```
Networks
default
  subnets
    10.231.2.0/26
      ± dns: 
        + 127.0.0.5
second
  subnets
    10.231.3.0/26
      ± dns: 
        + 127.0.0.5

Jobs
batlight
  ± networks: 
    - {""name""=>""default"", ""default""=>[""dns"", ""gateway""], ""static_ips""=>[""10.231.2.7""]}
    + {""name""=>""default"", ""default""=>[""dns"", ""gateway""], ""static_ips""=>[""10.231.3.7""]}

Properties
No changes


Deploying
---------
Deployment name: `deployment'
Director name: `micro'

Director task 90
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)

  Started preparing deployment
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Failed: `batlight/0' failed to reserve static IP 10.231.3.7:  (00:00:00)

Error 130010: `batlight/0' failed to reserve static IP 10.231.3.7: 

Task 90 error
```",1.0,100573032,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",show descriptive error when failing to find subnet for the ip,1655862.0,"[1655862, 553935]",956238,553935,feature,2015-08-11T05:32:54Z,https://www.pivotaltracker.com/story/show/100573032
2015-08-12T23:12:39Z,2015-07-30T23:35:37Z,accepted,,"#100144184 / `68076bdd6d2c8c012e11b9b815bf5ccf3e1d447d` fixed the tests for the agent ci pipeline, but not the bosh pipeline.

had to revert to get security fix out.

one fix is to also mess with what user the bosh unit tests are run as. Colin wonders if this is really a unit test if it cares so much about the context in which it's run?",,100257616,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",re-reenable syslog agent tests,1591058.0,"[1591058, 1550486]",956238,687691,chore,2015-08-12T23:11:49Z,https://www.pivotaltracker.com/story/show/100257616
2015-08-13T05:11:04Z,2015-07-30T18:16:52Z,accepted,,"/var/vcap/bosh/etc/stemcell_version
/var/vcap/bosh/etc/stemcell_git_sha1

Running on stemcell version 'blah' (git: blah)
Running on stemcell version '?' (git: ?)

https://github.com/cloudfoundry/bosh-agent/blob/ebf9a36dfddf783ffae5ae3748787d259a29374a/main/agent.go",2.0,100231820,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",include stemcell version and sha optionally in logs,344.0,"[344, 1017727]",956238,81882,feature,2015-08-13T05:11:04Z,https://www.pivotaltracker.com/story/show/100231820
2015-08-13T05:11:27Z,2015-04-07T20:23:13Z,accepted,,"insert at the top of https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/bin/bosh

```
#!/usr/bin/env ruby

if ENV[""BOSH_USE_BUNDLER""]
  gemfile_path = File.join(File.dirname(__FILE__), ""bosh.Gemfile"")
  unless File.exists?(gemfile_path)
    require ""fileutils""
    FileUtils.mkdir_p(File.dirname(gemfile_path))
    File.write(gemfile_path, ""gem 'bosh_cli'"")
    print ""\nOptimizing gem configuration...\n\n""
    ENV[""BUNDLE_GEMFILE""] = gemfile_path
    require ""bundler/setup""
  else
    rubyopt = [ENV[""RUBYOPT""]].compact
    if rubyopt.empty? || rubyopt.first !~ /-rbundler\/setup/
      rubyopt.unshift(""-rbundler/setup"")
      ENV[""BUNDLE_GEMFILE""] = gemfile_path
      ENV[""RUBYOPT""] = rubyopt.join("" "")
      Kernel.exec($0, *ARGV)
    end
  end
end
```

Unanswered questions:
- figure out unique id per bosh_cli gem installed (different rubies)
- how does bosh micro plugin work?
- how do other plugins work",2.0,91981394,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",make bosh cli faster by optimizing loading of gems,344.0,"[344, 1017727]",956238,344,feature,2015-08-20T16:51:22Z,https://www.pivotaltracker.com/story/show/91981394
2015-08-13T05:15:33Z,2015-08-05T01:37:17Z,accepted,,copy of `bosh vms`. all flags should work; however unlike `bosh vms` command `bosh instances` command should only show vms for the currently targeted deployment. use the same api call that `bosh vms` uses. also instead of 'Job/Index' show 'Instance'.,1.0,100611746,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",introduce `bosh instances` command,1602644.0,[1602644],956238,81882,feature,2015-08-13T05:15:33Z,https://www.pivotaltracker.com/story/show/100611746
2015-08-13T05:17:43Z,2015-08-03T16:34:04Z,accepted,,https://github.com/cloudfoundry/bosh/pull/898,2.0,100433438,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR '[Power Support] Update build configuration files to support IBM Power platform for ruby and libpq packages during packaging.',344.0,[344],956238,81882,feature,2015-08-13T05:17:43Z,https://www.pivotaltracker.com/story/show/100433438
2015-08-13T05:23:19Z,2015-07-13T19:42:42Z,accepted,,"If Bluetooth functionality must be disabled, preventing the kernel from loading the kernel module provides an additional safeguard against its activation.
---
None
---
SV-50483r3_rule
---
F-43631r3_fix
---
The kernel's module loading system can be configured to prevent loading of the Bluetooth module. Add the following to the appropriate ""/etc/modprobe.d"" configuration file to prevent the loading of the Bluetooth module: 

install net-pf-31 /bin/true
install bluetooth /bin/true
---
C-46244r3_chk
---
If the system is configured to prevent the loading of the ""bluetooth"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r bluetooth /etc/modprobe.conf /etc/modprobe.d

If the system is configured to prevent the loading of the ""net-pf-31"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r net-pf-31 /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978220,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38682] [medium] The Bluetooth kernel module must be disabled,1495236.0,[1495236],956238,81882,feature,2015-08-13T05:23:20Z,https://www.pivotaltracker.com/story/show/98978220
2015-08-13T05:23:58Z,2015-07-13T19:42:44Z,accepted,,"Disabling the ""bluetooth"" service prevents the system from attempting connections to Bluetooth devices, which entails some security risk. Nevertheless, variation in this risk decision may be expected due to the utility of Bluetooth connectivity and its limited range.
---
None
---
SV-50492r2_rule
---
F-43640r1_fix
---
The ""bluetooth"" service can be disabled with the following command: 

# chkconfig bluetooth off



# service bluetooth stop
---
C-46253r3_chk
---
To check that the ""bluetooth"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""bluetooth"" --list

Output should indicate the ""bluetooth"" service has either not been installed or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""bluetooth"" --list
""bluetooth"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off


If the service is configured to run, this is a finding.",1.0,98978272,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38691] [medium] The Bluetooth service must be disabled,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-08-13T05:23:58Z,https://www.pivotaltracker.com/story/show/98978272
2015-08-13T05:24:47Z,2015-08-04T01:40:35Z,accepted,,"something short in `bosh/docs`

discuss:
- static ip -> dynamic ip release
- 2 jobs with 2 static ips, swap fail, remove static ip + keep swapped ip",1.0,100479154,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",create a doc describing why and how ips are allocated and released,553935.0,"[553935, 1017727]",956238,81882,feature,2015-08-13T05:24:47Z,https://www.pivotaltracker.com/story/show/100479154
2015-08-13T06:23:06Z,2015-07-21T01:45:57Z,accepted,,"remove https://github.com/cloudfoundry/bosh/blob/2840387d687a44569edd9cdc2e49ecba129634b8/bosh-director/lib/bosh/director/jobs/update_release.rb#L60.

write an integration test.",1.0,99480078,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",allow to run bosh upload release --rebase even when there are no changes,1386874.0,[1386874],956238,81882,feature,2015-08-13T06:23:06Z,https://www.pivotaltracker.com/story/show/99480078
2015-08-13T17:34:16Z,2015-08-06T16:05:36Z,accepted,,,,100745804,story,[],Switch builds to nested-esxi for vsphere,1687782.0,"[1687782, 1591058]",956238,553935,chore,2015-08-13T17:34:16Z,https://www.pivotaltracker.com/story/show/100745804
2015-08-13T20:35:16Z,2015-08-12T23:57:44Z,accepted,,,,101171036,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Audit all pending specs in global networking,553935.0,[553935],956238,553935,chore,2015-08-13T20:35:16Z,https://www.pivotaltracker.com/story/show/101171036
2015-08-14T20:13:32Z,2015-08-12T14:09:02Z,accepted,,"The release asset valid_release.tgz is old and doesn't have fingerprints in its manifest. We need to upgrade the tests using it to newer assets that support fingerprints. 
dk: ""Let's keep one test with the old artifact and use new artifacts for all the others."" ",,101123674,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",upgrade integration tests using the valid_release.tgz that lacks fingerprints to use newer assets except for one test,1355110.0,[1355110],956238,1355110,chore,2015-08-14T20:13:32Z,https://www.pivotaltracker.com/story/show/101123674
2015-08-14T22:41:27Z,2015-08-12T03:32:51Z,accepted,,Postgres throws an error which we don't catch,1.0,101095268,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Race condition when running multiple deployments,553935.0,"[553935, 1017727]",956238,553935,feature,2015-08-14T22:41:27Z,https://www.pivotaltracker.com/story/show/101095268
2015-08-14T23:13:58Z,2015-07-13T19:42:46Z,accepted,,"This prevents attackers with physical access from trivially bypassing security on the machine and gaining root access. Such accesses are further prevented by configuring the bootloader password.
---
None
---
SV-50387r1_rule
---
F-43534r1_fix
---
Single-user mode is intended as a system recovery method, providing a single user root access to the system by providing a boot option at startup. By default, no authentication is performed if single-user mode is selected. 

To require entry of the root password even if the system is started in single-user mode, add or correct the following line in the file ""/etc/sysconfig/init"": 

SINGLE=/sbin/sulogin
---
C-46145r1_chk
---
To check if authentication is required for single-user mode, run the following command: 

$ grep SINGLE /etc/sysconfig/init

The output should be the following: 

SINGLE=/sbin/sulogin


If the output is different, this is a finding.",1.0,98978368,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38586] [medium] The system must require authentication upon booting into single-user and maintenance modes [centos only],1386874.0,[1386874],956238,81882,feature,2015-08-14T23:13:58Z,https://www.pivotaltracker.com/story/show/98978368
2015-08-14T23:14:37Z,2015-08-04T22:33:21Z,accepted,,we do not populate DNS for network object (https://github.com/cloudfoundry/bosh-init/blob/master/deployment/manifest/parser.go#L244-L247). for manual network its subnets get DNS set on https://github.com/cloudfoundry/bosh-init/blob/master/deployment/manifest/parser.go#L261-L266.,,100601012,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",bosh-init should pass dns settings for dynamic networks to cpi,344.0,[344],956238,1406536,bug,2015-08-14T23:14:48Z,https://www.pivotaltracker.com/story/show/100601012
2015-08-14T23:27:47Z,2015-07-31T00:41:50Z,accepted,,director.allow_unspecified_gateway should default to false. when set to true gateway field on the manual network subnet can be skipped. this will be used on bosh-lites to be backwards compatible.,2.0,100259808,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",provide director config option that ignores setting gateway on manual net subnets,344.0,"[344, 1655862, 1017727]",956238,81882,feature,2015-08-14T23:28:06Z,https://www.pivotaltracker.com/story/show/100259808
2015-08-15T00:46:51Z,2015-08-14T16:54:57Z,accepted,,"http://www.ubuntu.com/usn/usn-2710-1/
expected: openssh-server 1:6.6p1-2ubuntu2.2",2.0,101297474,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2710-1] bump ubuntu for OpenSSH vulnerabilities,1550486.0,"[1550486, 1655862]",956238,81882,feature,2015-08-15T00:52:48Z,https://www.pivotaltracker.com/story/show/101297474
2015-08-17T14:15:57Z,2015-06-29T16:59:42Z,accepted,,,,98028018,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",releases can be --imported-- in their compiled form for a specific stemcell version,,[],956238,81882,release,2015-08-17T14:15:57Z,https://www.pivotaltracker.com/story/show/98028018
2015-08-17T16:44:56Z,2015-08-04T20:25:18Z,accepted,,,0.0,100589702,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Add integration tests for failing deploys,553935.0,[553935],956238,553935,feature,2015-08-17T16:44:56Z,https://www.pivotaltracker.com/story/show/100589702
2015-08-17T18:13:29Z,2015-08-17T17:59:05Z,accepted,,,,101427062,story,[],Jenkins should be able to optionally ignore checking that candidate contains the latest SHA on master,1017727.0,"[1017727, 1550486]",956238,1550486,chore,2015-08-18T00:17:01Z,https://www.pivotaltracker.com/story/show/101427062
2015-08-17T20:37:51Z,2015-08-11T05:25:12Z,accepted,,"I've used something like this:

```
Bosh::Director::Models::IpAddress.all.map{ |i| puts ""#{i.instance.deployment.name}.#{i.instance.job}/#{i.instance.index} - #{i.network_name} - #{NetAddr::CIDR.create(i.address)}"" }; nil
```

Useful:
- instance id
- network name
- human readable address",1.0,101006998,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",add debugging nicety to IpAddress model,553935.0,"[553935, 1017727]",956238,81882,feature,2015-08-17T20:37:53Z,https://www.pivotaltracker.com/story/show/101006998
2015-08-17T20:39:22Z,2015-08-10T17:24:16Z,accepted,,"D, [2015-08-10 16:45:57 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Adding reservation '{type=dynamic, ip=nil, network=default, instance=bosh/0, reserved=false}' for 'bosh/0' for network 'default'
-----> Need reservation ...

D, [2015-08-10 16:45:57 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Adding reservation '{type=dynamic, ip=nil, network=default, instance=bosh/1, reserved=false}' for 'bosh/1' for network 'default'
D, [2015-08-10 16:45:57 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Adding reservation '{type=static, ip=""10.10.0.8"", network=default, instance=bosh1/0, reserved=false}' for 'bosh1/0' for network 'default'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Creating instance network reservations from database for instance 'bosh/0'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Reserving ip '10.10.0.9' for existing instance 'bosh/0' for network 'default'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Reserving static ip '10.10.0.9' for manual network 'default'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration][database-ip-provider] Reserved dynamic ip '10.10.0.9' for network 'default' (10.10.0.0/24)
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Adding reservation '{ip=""10.10.0.9"", network=default, instance=bosh/0, reserved=true}' for 'bosh/0' for network 'default'
------> include type=??

D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Reserving ip '10.10.0.10' for existing instance 'bosh/0' for network 'default'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Reserving static ip '10.10.0.10' for manual network 'default'
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration][database-ip-provider] Reserved dynamic ip '10.10.0.10' for network 'default' (10.10.0.0/24)
D, [2015-08-10 16:46:08 #3064] [task:114] DEBUG -- DirectorJobRunner: [network-configuration] Adding reservation '{ip=""10.10.0.10"", network=default, instance=bosh/0, reserved=true}' for 'bosh/0' for network 'default'",1.0,100956926,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",improve log messages,553935.0,"[553935, 1017727]",956238,81882,feature,2015-08-17T20:39:22Z,https://www.pivotaltracker.com/story/show/100956926
2015-08-17T20:47:16Z,2015-07-30T20:41:56Z,accepted,,"id is a uniquely picked identifier for all instances in the director
id must not change for the duration of an instance",2.0,100245910,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",spec.id (string) should be available in the job templates,1687782.0,"[1687782, 1017727]",956238,81882,feature,2015-08-17T20:47:17Z,https://www.pivotaltracker.com/story/show/100245910
2015-08-17T22:32:29Z,2015-08-11T05:26:44Z,accepted,,migration,1.0,101007088,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",record if ip is dynamic or static on the ip address,553935.0,"[553935, 1017727]",956238,81882,feature,2015-08-17T22:32:29Z,https://www.pivotaltracker.com/story/show/101007088
2015-08-17T22:38:29Z,2015-08-12T18:33:43Z,accepted,,,,101148998,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",When instance is no longer using static IP it should get dynamic IP on next deploy,553935.0,"[553935, 1017727]",956238,553935,bug,2015-08-17T22:38:29Z,https://www.pivotaltracker.com/story/show/101148998
2015-08-17T23:05:09Z,2015-08-11T05:24:15Z,accepted,,"```
private -> dynamic 63 (stopped)
private -> static 66 (stopped)
63, and 66 are still reserved


irb(main):020:0> Bosh::Director::Models::IpAddress.all.map{ |i| puts ""#{i.instance.job}/#{i.instance.index} - #{i.network_name} - #{NetAddr::CIDR.create(i.address)}"" }; nil
db/0 - private - 10.10.0.62/32
db/1 - private - 10.10.0.63/32
db/2 - private - 10.10.0.64/32
rails/0 - private - 10.10.0.65/32
=> nil
irb(main):021:0> Bosh::Director::Models::IpAddress.all.map{ |i| puts ""#{i.instance.job}/#{i.instance.index} - #{i.network_name} - #{NetAddr::CIDR.create(i.address)}"" }; nil
db/0 - private - 10.10.0.62/32
db/1 - private - 10.10.0.63/32
db/2 - private - 10.10.0.64/32
rails/0 - private - 10.10.0.65/32
db/1 - private - 10.10.0.66/32


$ bosh start db 1
Acting as user 'admin' on deployment 'tiny-dummy' on 'micro'
You are about to start db/1
Detecting deployment changes
----------------------------
Start db/1? (type 'yes' to continue): yes
Performing `start db/1'...
Director task 26
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:09)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Failed: 'db/1' already has reservation for network 'private', IP 168427583 (00:00:00)
Error 130003: 'db/1' already has reservation for network 'private', IP 168427583
Task 26 error
For a more detailed error report, run: bosh task 26 --debug
```

manifest after static ips
```
name: tiny-dummy

director_uuid: 23db8d81-bfcd-4bb4-af1b-25d6cd72b843

releases:
- {name: dummy, version: latest}

update:
  canaries: 1
  max_in_flight: 1
  canary_watch_time: 1000 - 30000
  update_watch_time: 1000 - 30000

jobs:
- name: db
  templates:
  - name: dummy_with_properties
    release: dummy
  instances: 3
  # availability_zones: [z1, z2]
  resource_pool: z1
  networks:
  - {name: private, static_ips: [10.10.0.62, 10.10.0.66, 10.10.0.64]}

- name: rails
  templates:
  - name: dummy_with_package
    release: dummy
  instances: 1
  resource_pool: z1
  networks:
  - name: private
```

Steps to reproduce:
- Deploy with manifest
- bosh stop --hard db/1
- Update manifest to have static IP
- // bosh stop --hard db/1  ----> should be a no-op, but BOSH recreates the VM
- bosh start db/1  --force ----> This fails

Expected behaviour:
- `bosh start db/1 --force` should release dynamic IP and use the static IP from the manifest",,101006820,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",instance should not carry both dynamic and static ips,553935.0,"[553935, 1017727]",956238,81882,bug,2015-08-17T23:05:10Z,https://www.pivotaltracker.com/story/show/101006820
2015-08-17T23:27:47Z,2015-08-10T17:19:02Z,accepted,,"```
#<Bosh::Director::NetworkReservationAlreadyExists: 'bosh/0' already has reservation for network 'default', IP 168427529>
```
- make it user friendly format
- show new reservation info
- show old reservation info",1.0,100956334,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",fix error msg to show human friendly ip,553935.0,[553935],956238,81882,feature,2015-08-17T23:27:48Z,https://www.pivotaltracker.com/story/show/100956334
2015-08-18T17:27:30Z,2015-08-05T23:53:42Z,accepted,,"```
...
  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages
  Started compiling packages > golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7. Done (00:00:21)
  Started compiling packages > mariadb/755be9e717880707f032694c8217a89374c6fc85. Done (00:11:24)
  Started compiling packages > golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7
  Started compiling packages > Downloading 'golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7' from global cache. Done (00:00:01)
     Done compiling packages > golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7 (00:11:46)
  Started compiling packages > syslog_aggregator/078da6dcb999c1e6f5398a6eb739182ccb4aba25. Done (00:00:03)
  Started compiling packages > common/ba480a46c4b2aa9484fb24ed01a8649453573e6f. Done (00:00:03)
  Started compiling packages > golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7
  Started compiling packages > Downloading 'golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7' from global cache. Done (00:00:08)
     Done compiling packages > golang/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7 (00:11:53)
  Started compiling packages > python/4e255efa754d91b825476b57e111345f200944e1. Done (00:02:11)
  Started compiling packages > check/d6811f25e9d56428a9b942631c27c9b24f5064dc. Done (00:00:18)
  Started compiling packages > boost/3eb8bdb1abb7eff5b63c4c5bdb41c0a778925c31. Done (00:01:25)
  Started compiling packages > service-backup/394c081e415b664eebfcd34667cec522ce63a3e4. Done (00:00:09)
  Started compiling packages > xtrabackup/2e701e7a9e4241b28052d984733de36aae152275. Done (00:06:20)
  Started compiling packages > streaming-mysql-backup-tool/7790794dd4dc30dec5425b732dc0ce4eb4d900b9. Done (00:00:08)
  Started compiling packages > gra-log-purger/f02fa5774ab54dbb1b1c3702d03cb929b85d60e6. Done (00:00:07)
  Started compiling packages > galera-healthcheck/3da4dedbcd7d9f404a19e7720e226fd472002266. Done (00:00:08)
  Started compiling packages > mariadb_ctrl/7658290da98e2cad209456f174d3b9fa143c87fc. Done (00:00:08)
  Started compiling packages > scons/11e7ad3b28b43a96de3df7aa41afddde582fcc38. Done (00:00:04)
  Started compiling packages > galera/d15a1d2d15e5e7417278d4aa1b908566022b9623. Done (00:08:05)
  Started compiling packages > mariadb/43aa3547bc5a01dd51f1501e6b93c215dd7255e9. Done (00:11:04)
  Started compiling packages > xtrabackup/2e701e7a9e4241b28052d984733de36aae152275. Done (00:32:24)

  Started preparing dns > Binding DNS. Done (00:00:00)
...
```

golang definitely did not take 11min.",,100700270,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",cli should correctly calculate time diff for events,687691.0,[687691],956238,81882,bug,2015-08-18T17:27:30Z,https://www.pivotaltracker.com/story/show/100700270
2015-08-18T20:34:09Z,2014-07-25T21:57:20Z,accepted,,"use case: drain scripts might not execute correctly so user cannot successfully bosh deploy new set of drain scripts
bosh deploy --skip-drain # all jobs
bosh deploy --recreate --skip-drain job1,job2 # specific jobs",2.0,75747338,story,"[{'name': 'drain', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623538, 'updated_at': '2014-06-06T18:53:38Z'}, {'name': 'drain1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T08:09:20Z', 'id': 9116420, 'updated_at': '2014-08-05T08:09:20Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",user should be able to run a deploy without running drain scripts,553935.0,[553935],956238,81882,feature,2015-08-18T20:34:09Z,https://www.pivotaltracker.com/story/show/75747338
2015-08-18T20:43:46Z,2015-07-22T23:41:32Z,accepted,,"should include version of the job and package.
let's also log the full release.MF in the director debug log (add test for logging).

  Started copying jobs
  Started copying jobs > blobstore/d61a68c7a642b24a75de5920fa79af4bf3637bb3. Done (00:00:00)
  Started copying jobs > director/9f93dccbd2bfc81ee936b8c103ceaac19b79fcd5. Done (00:00:00)
  Started copying jobs > health_monitor/7688bcde1f2ff53c06ae00cdd7c1a4b11ddc1b90. Done (00:00:01)
  Started copying jobs > registry/3e499ab638b1092e1d5efa84b83f46c7f552dda1. Done (00:00:00)
     Done copying jobs (00:00:01)

  Started copying packages
  Started copying packages > blobstore/d61a68c7a642b24a75de5920fa79af4bf3637bb3. Done (00:00:00)
  Started copying packages > director/9f93dccbd2bfc81ee936b8c103ceaac19b79fcd5. Done (00:00:00)
  Started copying packages > health_monitor/7688bcde1f2ff53c06ae00cdd7c1a4b11ddc1b90. Done (00:00:01)
  Started copying packages > registry/3e499ab638b1092e1d5efa84b83f46c7f552dda1. Done (00:00:00)
     Done copying packages (00:00:01)",1.0,99660356,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",director task should show which jobs and packages are being included in the tarball,948679.0,"[948679, 631325]",956238,81882,feature,2015-08-18T20:43:46Z,https://www.pivotaltracker.com/story/show/99660356
2015-08-18T22:39:32Z,2015-08-11T02:58:16Z,accepted,,"currently raises: 

```
$ bosh export release bosh/193+dev.1 ubuntu-trusty/3031
Director task 14
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)
  Started preparing package compilation > Finding packages to compile. Done (00:00:00)
  Started compiling packages
  Started compiling packages > genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201. Failed: Action Failed get_task: Task f54004c6-55ce-428e-6f22-ffc97435126e result: Compiling package genisoimage: Fetching package genisoimage: Blobstore ID for package 'genisoimage' is empty (00:02:44)

Error 450001: Action Failed get_task: Task f54004c6-55ce-428e-6f22-ffc97435126e result: Compiling package genisoimage: Fetching package genisoimage: Blobstore ID for package 'genisoimage' is empty

Task 14 error
```

e.g. Error 60001: Can't deploy `bosh/193+dev.1': it is not compiled for `bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3031' and no source package is available",1.0,100999960,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",exporting a release that cannot be compiled because at least one source package is missing should raise a better error,948679.0,"[948679, 631325]",956238,81882,feature,2015-08-18T22:39:32Z,https://www.pivotaltracker.com/story/show/100999960
2015-08-18T23:03:07Z,2015-08-12T03:23:26Z,accepted,,,2.0,101095068,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",run a test suite that deploy multiple deployments at once,553935.0,"[553935, 1591058]",956238,81882,feature,2015-08-18T23:03:08Z,https://www.pivotaltracker.com/story/show/101095068
2015-08-18T23:09:55Z,2015-08-18T16:10:52Z,accepted,,"expected: openssh-server 1:6.6p1-2ubuntu2.3
expected: linux-image-3.19.0-26-generic 3.19.0-26.28~14.04.1",0.0,101510068,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2718-1 & USN-2710-2] bump ubuntu for kernel & openssh,553935.0,[553935],956238,81882,feature,2015-08-18T23:09:56Z,https://www.pivotaltracker.com/story/show/101510068
2015-08-19T01:15:33Z,2015-07-31T20:53:40Z,accepted,,,2.0,100334496,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",spec.availability_zone (string) should return AZ assigned to that instance,1591058.0,"[1591058, 1687782]",956238,81882,feature,2015-08-19T01:15:34Z,https://www.pivotaltracker.com/story/show/100334496
2015-08-19T01:37:41Z,2015-07-30T20:42:09Z,accepted,,"pick instance with 0th index

if there is no instance 0 don't set a bootstrap instance",2.0,100245928,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",spec.bootstrap (bool) should be set to true in the job templates for exactly one instance in the cluster,1687782.0,"[1687782, 1591058]",956238,81882,feature,2015-08-19T01:37:41Z,https://www.pivotaltracker.com/story/show/100245928
2015-08-19T01:46:32Z,2015-08-14T17:16:54Z,accepted,,"- agent should not received properties, links, availability_zone, persistent_disk_pool (also in the agent)
- spec should not receive rendered_templates_archive, configuration_hash",2.0,101299120,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",do not send unnecessary keys to template rendering or agent,1687782.0,"[1687782, 687691]",956238,81882,feature,2015-08-19T01:46:32Z,https://www.pivotaltracker.com/story/show/101299120
2015-08-19T22:20:44Z,2015-08-19T00:53:05Z,accepted,,"As a bosh-developer
I want to make sure that a release can be created with our code changes

Context:
While working on #101308280,  we found out that we were not able to create release with our rake task. We later discovered that this was caused by extra files left over from merging in pull request 902. We want to make sure we can detect these types of failure in the pipeline instead of future stories.",,101552268,story,"[{'name': 'bosh release', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T21:41:09Z', 'id': 8099334, 'updated_at': '2014-04-01T21:41:09Z'}, {'name': 'test improvements', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-03T19:17:21Z', 'id': 11310830, 'updated_at': '2015-04-03T19:17:21Z'}]",Add integration test to ensure that release can be created,1426194.0,[1426194],956238,1426194,chore,2015-08-19T22:20:44Z,https://www.pivotaltracker.com/story/show/101552268
2015-08-20T18:50:18Z,2015-08-14T19:19:48Z,accepted,,see how we produce same file for ubuntu,2.0,101308280,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",produce txt file with a list of packages installed,1591058.0,"[1591058, 1426194]",956238,81882,feature,2015-08-20T18:50:19Z,https://www.pivotaltracker.com/story/show/101308280
2015-08-20T18:50:58Z,2014-12-17T22:43:33Z,accepted,,"complete work from https://github.com/cloudfoundry/bosh/pull/676
even though integration tests pass, builds in ci might fail",2.0,84810910,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",PR 'Remove obsolete :os_image_version argument from stemcell:build',1602644.0,[1602644],956238,81882,feature,2015-08-20T18:50:58Z,https://www.pivotaltracker.com/story/show/84810910
2015-08-20T18:52:16Z,2015-07-22T23:37:48Z,accepted,,and all the compilation dependencies in the dependencies array.,1.0,99660228,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",remove blobstore and common packages from bosh release,1355110.0,"[1355110, 1541728]",956238,81882,feature,2015-08-20T18:52:17Z,https://www.pivotaltracker.com/story/show/99660228
2015-08-20T18:53:42Z,2015-08-03T16:31:33Z,accepted,,https://github.com/cloudfoundry/bosh/pull/902,4.0,100433220,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Add syslog forwarder plugin to HM',1591058.0,"[1591058, 1426194]",956238,81882,feature,2015-08-20T18:53:43Z,https://www.pivotaltracker.com/story/show/100433220
2015-08-20T19:19:43Z,2015-07-13T19:42:39Z,accepted,,"The ""/etc/passwd"" file contains information about the users that are configured on the system. Protection of this file is critical for system security.
---
None
---
SV-50250r1_rule
---
F-43395r1_fix
---
To properly set the owner of ""/etc/passwd"", run the command: 

# chown root /etc/passwd
---
C-46005r1_chk
---
To check the ownership of ""/etc/passwd"", run the command: 

$ ls -l /etc/passwd

If properly configured, the output should indicate the following owner: ""root"" 
If it does not, this is a finding.",1.0,98978092,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38450] [medium] The /etc/passwd file must be owned by root,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:19:44Z,https://www.pivotaltracker.com/story/show/98978092
2015-08-20T19:19:54Z,2015-07-13T19:42:39Z,accepted,,"The ""/etc/passwd"" file contains information about the users that are configured on the system. Protection of this file is critical for system security.
---
None
---
SV-50251r1_rule
---
F-43396r1_fix
---
To properly set the group owner of ""/etc/passwd"", run the command: 

# chgrp root /etc/passwd
---
C-46006r1_chk
---
To check the group ownership of ""/etc/passwd"", run the command: 

$ ls -l /etc/passwd

If properly configured, the output should indicate the following group-owner. ""root"" 
If it does not, this is a finding.",1.0,98978096,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38451] [medium] The /etc/passwd file must be group-owned by root.,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:19:54Z,https://www.pivotaltracker.com/story/show/98978096
2015-08-20T19:20:18Z,2015-07-13T19:42:39Z,accepted,,"The hashes for all user account passwords should be stored in the file ""/etc/shadow"" and never in ""/etc/passwd"", which is readable by all users.
---
None
---
SV-50300r1_rule
---
F-43446r1_fix
---
If any password hashes are stored in ""/etc/passwd"" (in the second field, instead of an ""x""), the cause of this misconfiguration should be investigated. The account should have its password reset and the hash should be properly stored, or the account should be deleted entirely.
---
C-46056r1_chk
---
To check that no password hashes are stored in ""/etc/passwd"", run the following command: 

# awk -F: '($2 != ""x"") {print}' /etc/passwd

If it produces any output, then a password hash is stored in ""/etc/passwd"". 
If any stored hashes are found in /etc/passwd, this is a finding.",1.0,98978090,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38499] [medium] The /etc/passwd file must not contain password hashes.,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:20:21Z,https://www.pivotaltracker.com/story/show/98978090
2015-08-20T19:20:51Z,2015-07-13T19:42:39Z,accepted,,"The ""/etc/group"" file contains information regarding groups that are configured on the system. Protection of this file is important for system security.
---
None
---
SV-50259r1_rule
---
F-43404r1_fix
---
To properly set the group owner of ""/etc/group"", run the command: 

# chgrp root /etc/group
---
C-46014r1_chk
---
To check the group ownership of ""/etc/group"", run the command: 

$ ls -l /etc/group

If properly configured, the output should indicate the following group-owner. ""root"" 
If it does not, this is a finding.",1.0,98978082,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38459] [medium] The /etc/group file must be group-owned by root.,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:20:51Z,https://www.pivotaltracker.com/story/show/98978082
2015-08-20T19:21:00Z,2015-07-13T19:42:39Z,accepted,,"The ""/etc/group"" file contains information regarding groups that are configured on the system. Protection of this file is important for system security.
---
None
---
SV-50258r1_rule
---
F-43403r1_fix
---
To properly set the owner of ""/etc/group"", run the command: 

# chown root /etc/group
---
C-46013r1_chk
---
To check the ownership of ""/etc/group"", run the command: 

$ ls -l /etc/group

If properly configured, the output should indicate the following owner: ""root"" 
If it does not, this is a finding.",1.0,98978098,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38458] [medium] The /etc/group file must be owned by root.,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:21:00Z,https://www.pivotaltracker.com/story/show/98978098
2015-08-20T19:22:40Z,2015-07-13T19:42:41Z,accepted,,"The ""/etc/group"" file contains information regarding groups that are configured on the system. Protection of this file is important for system security.
---
None
---
SV-50261r1_rule
---
F-43406r1_fix
---
To properly set the permissions of ""/etc/group"", run the command: 

# chmod 644 /etc/group
---
C-46015r1_chk
---
To check the permissions of ""/etc/group"", run the command: 

$ ls -l /etc/group

If properly configured, the output should indicate the following permissions: ""-rw-r--r--"" 
If it does not, this is a finding.",1.0,98978150,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38461] [medium] The /etc/group file must have mode 0644 or less permissive.,1495236.0,[1495236],956238,81882,feature,2015-08-20T19:22:40Z,https://www.pivotaltracker.com/story/show/98978150
2015-08-20T22:48:16Z,2015-07-31T19:37:30Z,accepted,,"```
$ bosh release bosh/184

+-----------+------------------------------------------+--------------------------------------+---------
| Job       | Fingerprint                              | Blobstore ID                         | SHA1
+-----------+------------------------------------------+--------------------------------------+---------
| registry  | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | 12645...
| lbpq      | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | 34r78...
| ruby      | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | 34r78...
| director  | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | wiy34...
+--------------------------------------------------------------- ------------------------------+---------

+-----------+------------------------------------------+------------------------+--------------------------------------+---------
| Package   | Fingerprint                              | Contents               | Blobstore ID                         | SHA1
+-----------+------------------------------------------+------------------------+--------------------------------------+---------
| registry  | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | source                 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | 12645...
|           |                                          | bosh-aws-xen-blah/2030 | 2194213f-9b55-4439-bbb2-df780e63abde | 34r78...
|           |                                          | bosh-aws-xen-blah/2031 | 2194213f-9b55-4439-bbb2-df780e63abde | 34r78...
| lbpq      | 6072480e8fd62bab8fd4e38149c403c18bd03e81 | source                 | 0be5319d-ba3f-479f-4fa7-d4a89087248b | wiy34...
|           |                                          | bosh-aws-xen-blah/2030 | 2194213f-9b55-4439-bbb2-df780e63abde | 43r45...
|           |                                          | bosh-aws-xen-blah/2031 | 2194213f-9b55-4439-bbb2-df780e63abde | 43r45...
+-------------------------------------------------------------------------------+--------------------------------------+---------
```

- Return error if release version is not found.
- show full sha1",2.0,100329018,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can inspect contents of a single release version,1541728.0,"[1541728, 948679]",956238,81882,feature,2015-08-20T22:48:17Z,https://www.pivotaltracker.com/story/show/100329018
2015-08-21T00:11:13Z,2015-08-20T22:20:27Z,accepted,,"Looks like with vsphere restart it is not working
",,101724750,story,[],Re-deploy vsphere concourse worker,553935.0,[553935],956238,553935,chore,2015-08-21T00:11:13Z,https://www.pivotaltracker.com/story/show/101724750
2015-08-21T18:09:07Z,2015-08-21T02:29:40Z,accepted,,,,101734812,story,[],ci should clean untracked files with `git clean -dff` after checking out,553935.0,"[553935, 1017727]",956238,81882,chore,2015-08-21T18:09:07Z,https://www.pivotaltracker.com/story/show/101734812
2015-08-21T18:09:14Z,2015-08-20T19:14:41Z,accepted,,,,101710382,story,[],keep only one ruby 1.9 build in ci for integration tests,553935.0,"[553935, 1017727]",956238,81882,chore,2015-08-21T18:09:15Z,https://www.pivotaltracker.com/story/show/101710382
2015-08-22T00:42:23Z,2015-08-05T20:47:09Z,accepted,,"`$ be rake spec:unit:ruby_gems` has some tests with deprecated expectations 
```
Deprecation Warnings:

Using `should_receive` from rspec-mocks' old `:should` syntax without explicitly enabling the syntax is deprecated. Use the new `:expect` syntax or explicitly enable `:should` instead. Called from /Users/pivotal/workspace/bosh/bosh_cli/spec/unit/core_ext_spec.rb:89:in `block (2 levels) in <top (required)>'.
```",,100687746,story,[],Clean up deprecated rspec expectations ,687691.0,"[687691, 1591058]",956238,687691,chore,2015-08-22T00:42:36Z,https://www.pivotaltracker.com/story/show/100687746
2015-08-25T17:06:14Z,2015-08-25T16:23:35Z,accepted,,,,101992498,story,[],Cycle BOSH concourse credentials,1550486.0,[1550486],956238,1550486,chore,2015-08-25T17:06:15Z,https://www.pivotaltracker.com/story/show/101992498
2015-08-25T22:29:55Z,2015-08-24T18:30:31Z,accepted,,,0.0,101915066,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",move bosh-lite ci pipeline to main ci,81882.0,[81882],956238,81882,feature,2015-08-25T22:29:55Z,https://www.pivotaltracker.com/story/show/101915066
2015-08-25T23:23:46Z,2015-08-18T00:02:46Z,accepted,,dummy cpi should acquire (atomically) ip during create_vm call. if any other create_vm call uses same ip it should raise an error. delete_vm call should release the ip.,1.0,101459190,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",add guards in the dummy cpi to disallow creation of a vm with already taken ips,1355110.0,"[1355110, 1541728]",956238,81882,feature,2015-08-25T23:23:46Z,https://www.pivotaltracker.com/story/show/101459190
2015-08-26T00:12:14Z,2015-06-24T19:21:26Z,accepted,,"- remove cli commands
- remove director jobs, api endpoints",2.0,97758728,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",remove compiled packages feature,948679.0,"[948679, 631325]",956238,81882,feature,2015-08-26T00:12:16Z,https://www.pivotaltracker.com/story/show/97758728
2015-08-26T00:16:02Z,2015-07-31T19:08:01Z,accepted,,"current: Error 30011: Release bosh/188+dev.1 not found in deployment bosh manifest
desired: Error 30011: Release version `bosh/188+dev.1' not found in deployment `bosh' manifest

current: Exported Release ...
desired: Exported release ...

current: Error 60001: Can't deploy `bosh/193+dev.1': it is not compiled for `bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3031' and no source package is available
desired: Error 60001: Can't deploy release `bosh/193+dev.1': it is not compiled for stemcell`bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3031' and no source package is available .... should we show which package cannot be compiled?",1.0,100326826,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",clean up messages,948679.0,"[948679, 631325]",956238,81882,feature,2015-08-26T00:16:02Z,https://www.pivotaltracker.com/story/show/100326826
2015-08-26T17:57:39Z,2015-08-11T03:16:33Z,accepted,,"```
$ bosh delete release bosh 193+dev.1
Acting as user 'admin' on 'micro'
Deleting `bosh/193+dev.1'
Are you sure? (type 'yes' to continue): yes

Director task 18
  Started deleting packages
  Started deleting packages > nats/6a31c7bb0d5ffa2a9f43c7fd7193193438e20e92. Done (00:00:00)
  Started deleting packages > genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201. Done (00:00:00)
  Started deleting packages > postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce. Done (00:00:00)
  Started deleting packages > mysql/e5309aed88f5cc662bc77988a31874461f7c4fb8. Done (00:00:00)
  Started deleting packages > powerdns/256336d00b1689138490c385c03ad3a8f54b4a9e. Done (00:00:00)
  Started deleting packages > redis/37eae530889cb9ef4e84f9c3d0827bab5ae5cb66. Done (00:00:00)
  Started deleting packages > libpq/6072480e8fd62bab8fd4e38149c403c18bd03e81. Done (00:00:00)
  Started deleting packages > ruby/8c1c0bba2f15f89e3129213e3877dd40e339592f. Done (00:00:01)
  Started deleting packages > director/3e0098402cb7017076ed0e5614f52587562f2d15. Done (00:00:00)
  Started deleting packages > health_monitor/3be559939ad1f2325f2d0176c283e5463b5551e8. Done (00:00:00)
  Started deleting packages > nginx/1d356bbd17ed8c349fd1053093222d78559687ec. Done (00:00:00)
  Started deleting packages > registry/0e07967d01bc01c9ac6c5845b22c23632a9ad913. Done (00:00:00)
     Done deleting packages (00:00:01)

  Started deleting templates # <------------------------------------------------- jobs
  Started deleting templates > postgres/90e43294de79754e659fc1731f515dc090f922f0. Done (00:00:00)
  Started deleting templates > blobstore/267710c9165f827561b14b020c1181b098df27cf. Done (00:00:00)
  Started deleting templates > nats/6850abe07a70b465a6ff7e2b7c133e07c534f220. Done (00:00:00)
  Started deleting templates > redis/ef9ead1616b5a92a3d2b7d673f6de2ea2a8fa23c. Done (00:00:00)
  Started deleting templates > powerdns/581a3eca97719ac0757e8c5028971fe1a37976df. Done (00:00:00)
  Started deleting templates > health_monitor/1f172d09b2eb1003ca8ec53d2786febae89689ca. Done (00:00:00)
  Started deleting templates > director/ff35b9bf29cf99d5c491ae552ab704cb633ef095. Done (00:00:00)
  Started deleting templates > registry/236e0320b7ba04fe011f216db120f89131cbc451. Done (00:00:00)
     Done deleting templates (00:00:00)

Task 18 done

Started		2015-08-11 03:14:38 UTC
Finished	2015-08-11 03:14:39 UTC
Duration	00:00:01

Deleted `bosh/193+dev.1'
```",1.0,101000378,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",fix 'Started deleting templates' to say 'Started deleting jobs',1355110.0,[1355110],956238,81882,feature,2015-08-26T17:57:40Z,https://www.pivotaltracker.com/story/show/101000378
2015-08-26T18:31:26Z,2015-07-02T00:43:49Z,accepted,,there is already some kind of matching going on via https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/api/controllers/packages_controller.rb#L6,4.0,98242606,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]","user sees that re-uploading compiled release second time does not perform any expensive actions (upload, blobstore access, etc.)",1355110.0,[1355110],956238,81882,feature,2015-08-26T18:31:27Z,https://www.pivotaltracker.com/story/show/98242606
2015-08-26T18:42:50Z,2015-08-12T23:59:28Z,accepted,,"If I run `bosh upload release --skip-if-exists foo.tgz` in parallel, they'll both attempt to upload, and one will fail.

I'd expect them both to succeed, and maybe one to do less work (doesn't really matter though). Both having to upload is reasonable as long as it works.",1.0,101171118,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",BOSH director should support concurrent release uploads,1355110.0,[1355110],956238,381857,feature,2015-08-26T18:42:51Z,https://www.pivotaltracker.com/story/show/101171118
2015-08-26T20:27:41Z,2015-07-02T00:43:36Z,accepted,,reverse should also work,2.0,98242598,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",user can upload non-compiled release after uploading compiled release to backfill source packages so that release can be deployed on other stemcells,1355110.0,"[1355110, 631325]",956238,81882,feature,2015-08-26T20:27:42Z,https://www.pivotaltracker.com/story/show/98242598
2015-08-26T20:39:59Z,2015-08-11T20:23:57Z,accepted,,"@dk 
step1: do a bosh deploy (The deployment is using an ubuntu stemcell in the manifest)
```
>bosh deploy
Acting as user 'admin' on deployment 'hello-go' on 'my-bosh'
Getting deployment properties from director...
Unable to get properties list from director, trying without it...
Cannot get current deployment information from director, possibly a new deployment
Please review all changes carefully

Deploying
---------
Are you sure you want to deploy? (type 'yes' to continue): yes

Director task 40
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)

  Started preparing deployment
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages
  Started compiling packages > go-lang-1.4.2/7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d. Done (00:06:07)
  Started compiling packages > hello-go/ed93a1dd9d6a041361eea29bddc2a0e1acbeafe2. Done (00:05:34)
     Done compiling packages (00:11:42)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started creating bound missing vms > infrastructure/0. Done (00:02:54)

  Started binding instance vms > hello-go/0. Done (00:00:00)

  Started preparing configuration > Binding configuration. Done (00:00:01)

  Started updating job hello-go > hello-go/0 (canary). Done (00:02:39)

Task 40 done

Started		2015-08-11 19:16:58 UTC
Finished	2015-08-11 19:34:14 UTC
Duration	00:17:16

Deployed `hello-go' to `my-bosh'
```
step2: export release this same release for centos
```
> bosh export release hello-go/2 centos-7/3012

Director task 41
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages
  Started compiling packages > go-lang-1.4.2/7d4bf6e5267a46d414af2b9a62e761c2e5f33a8d. Done (00:05:54)
  Started compiling packages > hello-go/ed93a1dd9d6a041361eea29bddc2a0e1acbeafe2. Failed: Unknown CPI error 'Unknown' with message 'Address 10.0.5.12 is in use.' (00:10:20)
   Failed compiling packages (00:16:14)

Error 100: Unknown CPI error 'Unknown' with message 'Address 10.0.5.12 is in use.'

Task 41 error

For a more detailed error report, run: bosh task 41 --debug
```
Bosh vms shows that the Address 10.0.5.12 is actually used by the one vm that is part of the hello-go deployment
```
bosh vms
Acting as user 'admin' on 'my-bosh'
Deployment `hello-go'

Director task 42

Task 42 done

+------------+---------+----------------+-----------+
| Job/index  | State   | Resource Pool  | IPs       |
+------------+---------+----------------+-----------+
| hello-go/0 | running | infrastructure | 10.0.5.12 |
+------------+---------+----------------+-----------+

VMs total: 1
```
When we do the bosh deploy command, its create two compilation vms with 10.0.5.13 and 10.0.5.14, but when we use export release it create a vm with 10.0.5.11 and fails on the 2nd one because its already used

here is the deployemnt manifest
```
---
name: hello-go
director_uuid: 620df94b-8435-48e0-81c1-a70d66803d3f
packages: {}
releases:
 - name: hello-go
   version: 2

compilation:
  workers: 1
  network: default
  cloud_properties:
    instance_type: ""m1.small""

update:
  canaries: 1
  canary_watch_time: 30000
  update_watch_time: 30000
  max_in_flight: 4

networks:
  - name: default
    type: manual
    subnets:
      - range: 10.0.5.0/24
        reserved: [10.0.5.2-10.0.5.10]
        static: [10.0.5.218-10.0.5.219]
        dns:
          - 10.0.0.2
        gateway: 10.0.5.1
        cloud_properties:
          name: random
          security_groups: bosh
          subnet: subnet-de256887
  - name: static
    type: vip
    cloud_properties: {}

resource_pools:
  - name: infrastructure
    network: default
    size: 1
    stemcell:
      name: bosh-aws-xen-ubuntu-trusty-go_agent
      version: 3019
    cloud_properties:
      instance_type: ""m1.small""

jobs:
  - name: hello-go
    release: hello-go
    template: hello-go
    instances: 1
    resource_pool: infrastructure
    persistent_disk: 1_024
    networks:
     - name: default
       default:
         - dns
         - gateway
```",,101074848,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",Export release command seems to be creating compilation VMs with an incorrect IP address,1355110.0,"[1355110, 1541728]",956238,1355110,bug,2015-08-26T20:40:00Z,https://www.pivotaltracker.com/story/show/101074848
2015-08-26T21:00:48Z,2015-07-28T16:08:44Z,accepted,,"To reproduce:

1. `bosh upload release` (this will upload my_release/1)
2. `bosh deploy` (deploys a deployment that references my_release/1)
3. `bosh export release my_release/1 my_os/1000`
4. `bosh delete deployment` (deletes the deployment from step 2)
5. `bosh delete release my_release/1`
6. `bosh upload release my_release-1-my_os-1000.tgz`
7. Create a new release `my_release/2`, ensuring at least one package in it has the same version hash as in my_release/1
8. `bosh upload release` (uploads my_release/2)
9. Update the deployment manifest from step 2 to reference my_release/2
10. `bosh deploy`


Expected behaviour: director should not attempt to compile the package that has already been uploaded in compiled form from my_release/1. Deployment should succeed.

Actual behaviour:  Step 10 fails with the following error when it tries to compile the package shared between my_release/1 and my_release/2. The error message will be similar to:

```
  Started compiling packages > go-lang-1.4.2/7920f1e239366b918ef532b6fe071417c16dbe23. Failed: Action Failed get_task: Task 2c994e04-3e51-4517-72ca-ea51091c8538 result: Compiling package go-lang-1.4.2: Fetching package go-lang-1.4.2: Fetching package blob : Getting blob from inner blobstore: Getting blob from inner blobstore: Shelling out to bosh-blobstore-dav cli: Running command: 'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json get  /var/vcap/data/tmp/bosh-blobstore-externalBlobstore-Get206219403', stdout: 'Error running app - Getting dav blob : Wrong response code: 404; body: <html>
<head><title>404 Not Found</title></head>
<body bgcolor=""white"">
<center><h1>404 Not Found</h1></center>
<hr><center>nginx</center>
</body>
</html>
', stderr: '': exit status 1 (00:00:34)
```

The error itself is a bit mileading; see [#99929594] for our analysis.",,100033484,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",deployment flow is broken when two release versions share a compiled package,948679.0,[948679],956238,1541728,bug,2015-08-26T21:00:48Z,https://www.pivotaltracker.com/story/show/100033484
2015-08-26T23:20:58Z,2015-08-18T00:00:27Z,accepted,,"```
D, [2015-08-17 23:48:59 #21746] [] DEBUG -- DirectorJobRunner: RECEIVED: director.130f4536-779d-43d6-807a-a0cef3f18895.ddcb6f62-8c4f-4e31-ad8d-50de8757a50f {""value"":""stopped""}
D, [2015-08-17 23:48:59 #21746] [] DEBUG -- DirectorJobRunner: RECEIVED: director.130f4536-779d-43d6-807a-a0cef3f18895.05e551fb-eb6c-49df-9dfd-8a0d8202dedf {""value"":""stopped""}
D, [2015-08-17 23:48:59 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.000127s) SELECT NULL
D, [2015-08-17 23:48:59 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000779s) SELECT NULL
D, [2015-08-17 23:48:59 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.000606s) SELECT * FROM ""persistent_disks"" WHERE (""persistent_disks"".""instance_id"" = 6)
D, [2015-08-17 23:48:59 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000788s) SELECT * FROM ""persistent_disks"" WHERE (""persistent_disks"".""instance_id"" = 10)
D, [2015-08-17 23:48:59 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.000556s) SELECT NULL
D, [2015-08-17 23:48:59 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000458s) SELECT NULL
D, [2015-08-17 23:48:59 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.000646s) SELECT * FROM ""rendered_templates_archives"" WHERE (""rendered_templates_archives"".""instance_id"" = 6)
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.001743s) SELECT * FROM ""rendered_templates_archives"" WHERE (""rendered_templates_archives"".""instance_id"" = 10)
D, [2015-08-17 23:49:00 #21746] [] DEBUG -- DirectorJobRunner: RECEIVED: director.130f4536-779d-43d6-807a-a0cef3f18895.d76a6b43-e98d-4f1f-8951-cfb73e361d1e {""value"":{""agent_task_id"":""4d98ae31-d3bf-4b3a-6608-edc331b532b7"",""state"":""running""}}
D, [2015-08-17 23:49:00 #21746] [delete_instance(db/2)] DEBUG -- DirectorJobRunner: SENT: agent.fd58cd38-7945-4f65-92dc-dbfeb56a0af0 {""method"":""get_task"",""arguments"":[""4d98ae31-d3bf-4b3a-6608-edc331b532b7""],""reply_to"":""director.130f4536-779d-43d6-807a-a0cef3f18895.82aeb8ee-ea0c-4a1d-9beb-9a336a71a036""}
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000118s) SELECT NULL
D, [2015-08-17 23:49:00 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.000100s) SELECT NULL
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.001005s) DELETE FROM ""rendered_templates_archives"" WHERE ""id"" = 31
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000090s) SELECT NULL
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000060s) BEGIN
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000763s) SELECT * FROM ""ip_addresses"" WHERE (""ip_addresses"".""instance_id"" = 10)
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000161s) DELETE FROM ""ip_addresses"" WHERE ""id"" = 10
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000987s) DELETE FROM ""instances"" WHERE ""id"" = 10
D, [2015-08-17 23:49:00 #21746] [delete_instance(db/0)] DEBUG -- DirectorJobRunner: (0.004131s) DELETE FROM ""rendered_templates_archives"" WHERE ""id"" = 20
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: (0.000985s) COMMIT
D, [2015-08-17 23:49:00 #21746] [delete_instance(db2/0)] DEBUG -- DirectorJobRunner: External CPI sending request: {""method"":""delete_vm"",""arguments"":[""i-a34f3e71""],""context"":{""director_uuid"":""f976642f-ade0-4d68-8660-2bba6817f10f""}} with command: /var/vcap/jobs/cpi/bin/cpi
```

at the end of this story nothing should call delete_vm except VmDeleter.",1.0,101459084,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",bosh delete deployment should deallocate ips only after deleting vms,553935.0,"[553935, 1655862]",956238,81882,feature,2015-08-26T23:20:58Z,https://www.pivotaltracker.com/story/show/101459084
2015-08-26T23:55:43Z,2015-08-21T00:12:10Z,accepted,,"When not using cloud config, compilation VM tries to obtain VM that is reserved by deployment",,101730164,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Fix export release on global-net with in-memory-ip-provider,553935.0,"[553935, 687691, 1655862]",956238,553935,bug,2015-08-26T23:55:43Z,https://www.pivotaltracker.com/story/show/101730164
2015-08-27T00:35:29Z,2015-07-23T21:15:25Z,accepted,,"- currently director will just pick first subnet
- shorter syntax should still work
- dns key should continue work on each subnet

We want to support both the old syntax:
```
networks:
- name: my-network
  type: dynamic
  dns:  [10.10.0.2]
  cloud_properties: {subnet: subnet-9be6c3f7}
```
and new syntax:
```
networks:
- name: my-network
  type: dynamic
  subnets:
  	-  dns:  [10.10.0.2]
  	   cloud_properties: {subnet: subnet-9be6c3f7}
```",4.0,99740512,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",user can specify subnets on dynamic network,1426194.0,"[1426194, 1591058, 344]",956238,81882,feature,2015-08-27T00:35:30Z,https://www.pivotaltracker.com/story/show/99740512
2015-08-27T21:00:55Z,2015-08-21T01:30:46Z,accepted,,"Since packages may now not have blobstore_id/sha1 we should not just pick up first package: https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/update_release.rb#L238-L241

why 'elseif package' if package is always there: https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/update_release.rb#L459",,101733112,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",reuse of existing packages that do not have sha1/blobstore does not work,631325.0,"[631325, 1355110]",956238,81882,bug,2015-08-27T21:00:56Z,https://www.pivotaltracker.com/story/show/101733112
2015-08-27T21:27:46Z,2015-08-25T22:23:05Z,accepted,,,1.0,102027976,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",allow default password for non-interactive ssh,553935.0,[553935],956238,81882,feature,2015-08-27T21:27:46Z,https://www.pivotaltracker.com/story/show/102027976
2015-08-27T21:59:02Z,2015-08-22T00:35:17Z,accepted,,"With 16 cores, postgres refused connections from integraiton tests: https://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/integration-1.9-postgres/builds/158",,101817290,story,[],Integration tests postgres should allow more than 8 clients,553935.0,[553935],956238,553935,chore,2015-08-27T21:59:02Z,https://www.pivotaltracker.com/story/show/101817290
2015-08-27T22:31:04Z,2015-08-26T18:53:59Z,accepted,,,,102098902,story,[],Migrate mushroom builds (auto-deploy) to giraffe,553935.0,[553935],956238,553935,chore,2015-08-27T22:31:04Z,https://www.pivotaltracker.com/story/show/102098902
2015-08-28T01:14:08Z,2015-07-13T19:42:43Z,accepted,,"The /etc/gshadow file contains group password hashes. Protection of this file is critical for system security.
---
None
---
SV-50249r1_rule
---
F-43394r1_fix
---
To properly set the permissions of ""/etc/gshadow"", run the command: 

# chmod 0000 /etc/gshadow
---
C-46004r1_chk
---
To check the permissions of ""/etc/gshadow"", run the command: 

$ ls -l /etc/gshadow

If properly configured, the output should indicate the following permissions: ""----------"" 
If it does not, this is a finding.",1.0,98978236,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38449] [medium] The /etc/gshadow file must have mode 0000.,1495236.0,[1495236],956238,81882,feature,2015-08-28T01:14:09Z,https://www.pivotaltracker.com/story/show/98978236
2015-08-28T01:14:21Z,2015-07-13T19:42:40Z,accepted,,"The ""/etc/gshadow"" file contains group password hashes. Protection of this file is critical for system security.
---
None
---
SV-50243r1_rule
---
F-43388r1_fix
---
To properly set the owner of ""/etc/gshadow"", run the command: 

# chown root /etc/gshadow
---
C-45998r1_chk
---
To check the ownership of ""/etc/gshadow"", run the command: 

$ ls -l /etc/gshadow

If properly configured, the output should indicate the following owner: ""root"" 
If it does not, this is a finding.",1.0,98978112,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38443] [medium] The /etc/gshadow file must be owned by root,1495236.0,[1495236],956238,81882,feature,2015-08-28T01:14:23Z,https://www.pivotaltracker.com/story/show/98978112
2015-08-28T01:14:32Z,2015-07-13T19:42:43Z,accepted,,"The ""/etc/gshadow"" file contains group password hashes. Protection of this file is critical for system security.
---
None
---
SV-50248r1_rule
---
F-43393r1_fix
---
To properly set the group owner of ""/etc/gshadow"", run the command: 

# chgrp root /etc/gshadow
---
C-46003r1_chk
---
To check the group ownership of ""/etc/gshadow"", run the command: 

$ ls -l /etc/gshadow

If properly configured, the output should indicate the following group-owner. ""root"" 
If it does not, this is a finding.",1.0,98978238,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38448] [medium] The /etc/gshadow file must be group-owned by root.,1495236.0,[1495236],956238,81882,feature,2015-08-28T01:14:33Z,https://www.pivotaltracker.com/story/show/98978238
2015-08-28T01:15:12Z,2015-07-13T19:42:39Z,accepted,,"Data in world-writable files can be modified by any user on the system. In almost all circumstances, files can be configured using a combination of user and group permissions to support whatever legitimate access is needed without the risk caused by world-writable files.
---
None
---
SV-50444r3_rule
---
F-43591r1_fix
---
It is generally a good idea to remove global (other) write access to a file when it is discovered. However, check with documentation for specific applications before making changes. Also, monitor for recurring world-writable files, as these may be symptoms of a misconfigured application or user account.
---
C-46202r3_chk
---
To find world-writable files, run the following command for each local partition [PART], excluding special filesystems such as /selinux, /proc, or /sys: 

# find [PART] -xdev -type f -perm -002

If there is output, this is a finding.",1.0,98978084,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38643] [medium] There must be no world-writable files on the system,1495236.0,[1495236],956238,81882,feature,2015-08-28T01:15:12Z,https://www.pivotaltracker.com/story/show/98978084
2015-08-28T01:17:21Z,2015-08-05T01:38:01Z,accepted,,"- show Disk CID as a column when --details is specified
- rename CID to VM CID column
- vm_state job includes active_disk_cid (https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/vm_state.rb#L75) = id of *active* persistent disk for that instance (https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/models/instance.rb#L21)
- n/a is shown if active_disk_cid is nil

```
$ bosh instances --details
+-----------------+--------------------+---------------+-----+------------+------------+--------------------------------------+--------------+
| Job/index       | State              | Resource Pool | IPs | VM CID     | Disk CID   | Agent ID                             | Resurrection |
+-----------------+--------------------+---------------+-----+------------+------------+--------------------------------------+--------------+
| tiny-dummy/kv84 | unresponsive agent |               |     | i-dc20e10e | vol-dc2bla | 5e1f2353-1f2b-421e-bb3e-aa82b3fd8089 | active       |
+-----------------+--------------------+---------------+-----+------------+------------+--------------------------------------+--------------+
```",1.0,100611896,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",show active disk cid in `bosh instances` output,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-08-28T01:17:21Z,https://www.pivotaltracker.com/story/show/100611896
2015-08-28T02:57:41Z,2015-08-25T17:18:52Z,accepted,,related to aws bug that keeps on adding instance_type to ephemeral_disk section.,,102000672,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",make sure cpi is not able to modify original params passed in to it,1550486.0,"[1550486, 1591058]",956238,81882,bug,2015-08-28T20:54:24Z,https://www.pivotaltracker.com/story/show/102000672
2015-08-28T17:10:37Z,2015-06-30T23:56:17Z,accepted,,,1.0,98156264,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]","given a vm is on two manual networks, if one ip needs to change (due to range change), make sure that only that ip is changed and other ip stays the same.",553935.0,"[553935, 1715614]",956238,81882,feature,2015-08-28T17:10:38Z,https://www.pivotaltracker.com/story/show/98156264
2015-08-28T17:18:53Z,2015-08-14T16:42:37Z,accepted,,,4.0,101296664,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",Changing IP address from static to dynamic (with cloud config updated to not include static) should keep same IP address,553935.0,"[553935, 687691]",956238,553935,feature,2015-08-28T17:18:54Z,https://www.pivotaltracker.com/story/show/101296664
2015-08-31T22:33:54Z,2015-08-31T21:47:13Z,accepted,,,,102420076,story,[],Get Victor onboarded,1017727.0,"[1017727, 1779224]",956238,1017727,chore,2015-08-31T22:33:54Z,https://www.pivotaltracker.com/story/show/102420076
2015-09-01T17:07:31Z,2015-08-31T23:04:13Z,accepted,,"http://www.ubuntu.com/usn/usn-2726-1
expected: 
Ubuntu 14.04 LTS:
  lib64expat1                     2.1.0-4ubuntu1.1 # not on stemcell
  libexpat1                       2.1.0-4ubuntu1.1",1.0,102424950,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2726-1] Expat vulnerability,1550486.0,[1550486],956238,81882,feature,2015-09-01T17:23:43Z,https://www.pivotaltracker.com/story/show/102424950
2015-09-01T18:35:42Z,2015-08-27T17:22:40Z,accepted,,"currently, it's using git clone of the local bosh project, which does not pull in uncommitted changes.",,102177536,story,[],Fix spec/integration/create_bosh_release_spec.rb to ensure that it uses local changes,1550486.0,"[1550486, 687691]",956238,1550486,chore,2015-09-01T18:35:42Z,https://www.pivotaltracker.com/story/show/102177536
2015-09-01T21:32:21Z,2015-08-05T00:28:47Z,accepted,,"branch from master, do the stuff, run the pipeline the special way

github checklist + some sort of doc (maybe)


scenarios:
 1. just os image bump
 1. os image bump + bosh code changes
 1. just bosh code changes? (gem bump?)

---

note that we do **NOT** need to run through concourse, as we're branching off of `master`, so it's already gone through everything

document:
- the changes to pipeline.yml
- the fact that the changes to pipeline.yml should **not** be committed
- the fact that we don't even need to go through the concourse pipeline at all, if the changes are OS kernel/package changes (no code change)
- how to get around any merge issues, e.g. when a gem like ""fog"", neighboring the bosh gems in Gemfile.lock, is updated

",4.0,100609430,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",create a doc and a checklist on how to cut a new stemcell version with a fix to bosh,1550486.0,[1550486],956238,81882,feature,2015-09-01T21:32:21Z,https://www.pivotaltracker.com/story/show/100609430
2015-09-01T22:38:26Z,2015-08-13T17:11:08Z,accepted,,see /var/vcap/bosh/etc/stemcell_version_git_sha1 for + in the stemcell. any stemcell from bosh.io has it. why....,,101223866,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",investigate why stemcells have + in git sha,1426194.0,[1426194],956238,81882,chore,2015-09-01T22:38:27Z,https://www.pivotaltracker.com/story/show/101223866
2015-09-02T21:59:03Z,2015-08-28T18:39:26Z,accepted,,"concourse takes -x to exclude files

temp size:
955M	tmp/",,102277058,story,[],rake fly:integration should exclude tmp folder,1550486.0,[1550486],956238,687691,chore,2015-09-02T21:59:03Z,https://www.pivotaltracker.com/story/show/102277058
2015-09-03T00:46:28Z,2015-09-02T21:43:08Z,accepted,,,,102624718,story,"[{'name': 'tech forum', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-02T21:43:15Z', 'id': 12666536, 'updated_at': '2015-09-02T21:43:15Z'}]",Put something from code climate onto the CI monitor,1426194.0,[1426194],956238,687691,chore,2015-09-03T00:46:29Z,https://www.pivotaltracker.com/story/show/102624718
2015-09-03T18:36:47Z,2015-09-02T21:41:56Z,accepted,,"what rules is it enforcing. which ones do we like? would it make sense to use just the Lint cop and not the Style cop?

Let's do a little bit of research and report back for the next tech forum.",,102624618,story,[],Review RuboCop rules,687691.0,[687691],956238,687691,chore,2015-09-03T18:36:47Z,https://www.pivotaltracker.com/story/show/102624618
2015-09-03T22:26:05Z,2015-08-19T20:33:35Z,accepted,,"- scripts that use software raid are fine with it?
- how should agent label disks and how to maintain consistent ordering after agent restarts?
- is it ok that software will be placed onto a separate ebs disk?
- on aws it's instance storage; on azure it's net disks. ",4.0,101630068,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",investigate disk labeling for exposing disks,1541728.0,"[1541728, 659629]",956238,81882,feature,2015-09-11T15:23:30Z,https://www.pivotaltracker.com/story/show/101630068
2015-09-03T22:43:02Z,2015-08-20T18:05:14Z,accepted,,"- should be move /usr, etc to an ephemeral partition (just like /var/vcap/data is)
- should we autogrow / on a bootup?
- does it work for ubuntu and centos?
- does each IaaS (aws, openstack, vsphere, vcloud, azure) allow to specify bigger root disk at runtime (in create_vm CPI call). I believe AWS does.
- what are the implications of having a bigger stemcell root image? do things become slow?

acceptance: put details into comments.",4.0,101703760,story,"[{'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",investigate how to allow package managers to use more space on the machine,1541728.0,"[1541728, 659629]",956238,81882,feature,2015-09-03T22:43:03Z,https://www.pivotaltracker.com/story/show/101703760
2015-09-04T16:47:57Z,2015-08-21T17:34:07Z,accepted,,- consider how to make ruby from docker images match to whatever we ship in the release,1.0,101786970,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",update bosh's ruby to 2.1.6,1550486.0,"[1550486, 553935]",956238,81882,feature,2015-09-04T16:47:57Z,https://www.pivotaltracker.com/story/show/101786970
2015-09-04T16:48:13Z,2015-09-03T22:43:33Z,accepted,,,2.0,102712952,story,"[{'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",find out minimum stemcell size for ambari install on aws,1541728.0,[1541728],956238,81882,feature,2015-09-04T16:48:14Z,https://www.pivotaltracker.com/story/show/102712952
2015-09-04T18:59:36Z,2015-09-02T19:14:25Z,accepted,,"change ""Compiled Release uploaded"" message to say ""Release Uploaded"" 

from slack:
@naderziada: ""Compiled Release uploaded"" is this useful for integration tests>
?

naderziada [2:32 PM] 
'Release uploaded’ was already there, so we just made it ‘Compiled Release uploaded’ when its a compiled release, some of the integration tests do check for it in the output
but it can be removed

dkalinin [2:32 PM] 
the reason why is it think it makes to not show distinction between compiled vs normal release at that point.

naderziada [2:34 PM] 
just say ‘Release uploaded’ in both cases?

dkalinin [2:36 PM] 
yeah. ill file a story…",,102612200,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",Remove distinction between compiled vs normal release upload message,948679.0,[948679],956238,1355110,bug,2015-09-04T18:59:46Z,https://www.pivotaltracker.com/story/show/102612200
2015-09-04T22:24:22Z,2015-08-27T00:01:19Z,accepted,,we should print cloud config before printing the manifest.,1.0,102120922,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'soloable', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-28T17:55:36Z', 'id': 12619366, 'updated_at': '2015-08-28T17:55:36Z'}]",print associated cloud config to the debug log,687691.0,[687691],956238,81882,feature,2015-09-04T22:24:22Z,https://www.pivotaltracker.com/story/show/102120922
2015-09-04T22:28:09Z,2015-08-28T02:58:40Z,accepted,,e.g. networks_changed? if true should output network_settings vs @current_state['networks'],1.0,102226976,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'soloable', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-28T17:55:36Z', 'id': 12619366, 'updated_at': '2015-08-28T17:55:36Z'}]",include detailed information about why changes for specific instance will happen,344.0,[344],956238,81882,feature,2015-09-04T22:28:09Z,https://www.pivotaltracker.com/story/show/102226976
2015-09-04T23:06:47Z,2015-08-21T18:38:47Z,accepted,,"Running: `bosh export release cf/215 ubuntu-trusty/3039`

```
Task 820 done

Started		2015-08-21 17:52:10 UTC
Finished	2015-08-21 18:21:14 UTC
Duration	00:29:04

release-cf-215-on-ubuntu-trusty-stemcell-... downloading...
Cannot download resource `afe9781b-3a16-4756-8b96-4bcf9930e037': HTTP status 502
```

Director logs:

```
root@46a58fcd-e4c0-467c-4569-8163ad58aa55:/home/vcap# grep -5 -R afe9781b-3a16-4756-8b96-4bcf9930e037 /var/vcap/sys/log/director/
/var/vcap/sys/log/director/error.log:2015/08/21 18:23:26 [error] 26059#0: *11721 upstream prematurely closed connection while reading response header from upstream, client: 209.234.137.222, server: , request: ""GET /resources/afe9781b-3a16-4756-8b96-4bcf9930e037 HTTP/1.1"", upstream: ""http://127.0.0.1:25556/resources/afe9781b-3a16-4756-8b96-4bcf9930e037"", host: ""some-host-name""
--
/var/vcap/sys/log/director/access.log-209.234.137.222 - admin [21/Aug/2015:18:22:59 +0000] ""GET /tasks/820 HTTP/1.1"" 200 149 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-209.234.137.222 - admin [21/Aug/2015:18:23:00 +0000] ""GET /tasks/820/output?type=event HTTP/1.1"" 416 25 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-209.234.137.222 - admin [21/Aug/2015:18:23:01 +0000] ""GET /tasks/820 HTTP/1.1"" 200 188 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-209.234.137.222 - admin [21/Aug/2015:18:23:02 +0000] ""GET /tasks/820/output?type=event HTTP/1.1"" 416 25 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-209.234.137.222 - admin [21/Aug/2015:18:23:02 +0000] ""GET /tasks/820/output?type=result HTTP/1.1"" 206 106 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log:209.234.137.222 - admin [21/Aug/2015:18:23:26 +0000] ""GET /resources/afe9781b-3a16-4756-8b96-4bcf9930e037 HTTP/1.1"" 502 166 ""-"" ""HTTPClient/1.0 (2.4.0, ruby 2.1.2 (2014-05-08))"" 24.170 24.170 .
/var/vcap/sys/log/director/access.log-127.0.0.1 - hm [21/Aug/2015:18:23:29 +0000] ""GET /deployments HTTP/1.1"" 200 315 ""-"" ""EventMachine HttpClient"" 0.007 0.007 .
/var/vcap/sys/log/director/access.log-127.0.0.1 - hm [21/Aug/2015:18:23:29 +0000] ""GET /deployments/bosh/vms HTTP/1.1"" 200 95 ""-"" ""EventMachine HttpClient"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-127.0.0.1 - hm [21/Aug/2015:18:24:29 +0000] ""GET /deployments HTTP/1.1"" 200 315 ""-"" ""EventMachine HttpClient"" 0.005 0.005 .
/var/vcap/sys/log/director/access.log-127.0.0.1 - hm [21/Aug/2015:18:24:29 +0000] ""GET /deployments/bosh/vms HTTP/1.1"" 200 95 ""-"" ""EventMachine HttpClient"" 0.003 0.003 .
/var/vcap/sys/log/director/access.log-127.0.0.1 - hm [21/Aug/2015:18:25:29 +0000] ""GET /deployments HTTP/1.1"" 200 315 ""-"" ""EventMachine HttpClient"" 0.006 0.006 .
--
/var/vcap/sys/log/director/director.stderr.log-209.234.137.222 - - [21/Aug/2015:18:22:59 +0000] ""GET /tasks/820 HTTP/1.0"" 200 149 0.0023
/var/vcap/sys/log/director/director.stderr.log-209.234.137.222 - - [21/Aug/2015:18:23:00 +0000] ""GET /tasks/820/output?type=event HTTP/1.0"" 416 25 0.0023
/var/vcap/sys/log/director/director.stderr.log-209.234.137.222 - - [21/Aug/2015:18:23:01 +0000] ""GET /tasks/820 HTTP/1.0"" 200 188 0.0022
/var/vcap/sys/log/director/director.stderr.log-209.234.137.222 - - [21/Aug/2015:18:23:02 +0000] ""GET /tasks/820/output?type=event HTTP/1.0"" 416 25 0.0024
/var/vcap/sys/log/director/director.stderr.log-209.234.137.222 - - [21/Aug/2015:18:23:02 +0000] ""GET /tasks/820/output?type=result HTTP/1.0"" 206 106 0.0023
/var/vcap/sys/log/director/director.stderr.log:209.234.137.222 - - [21/Aug/2015:18:23:26 +0000] ""GET /resources/afe9781b-3a16-4756-8b96-4bcf9930e037 HTTP/1.0"" 200 2519421337 24.0684
/var/vcap/sys/log/director/director.stderr.log-127.0.0.1 - - [21/Aug/2015:18:23:29 +0000] ""GET /deployments HTTP/1.0"" 200 315 0.0058
/var/vcap/sys/log/director/director.stderr.log-127.0.0.1 - - [21/Aug/2015:18:23:29 +0000] ""GET /deployments/bosh/vms HTTP/1.0"" 200 95 0.0030
/var/vcap/sys/log/director/director.stderr.log-127.0.0.1 - - [21/Aug/2015:18:24:29 +0000] ""GET /deployments HTTP/1.0"" 200 315 0.0054
/var/vcap/sys/log/director/director.stderr.log-127.0.0.1 - - [21/Aug/2015:18:24:29 +0000] ""GET /deployments/bosh/vms HTTP/1.0"" 200 95 0.0031
/var/vcap/sys/log/director/director.stderr.log-127.0.0.1 - - [21/Aug/2015:18:25:29 +0000] ""GET /deployments HTTP/1.0"" 200 315 0.0055
```",,101792504,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",cannot export compiled cf release; keep on failing to serve the download,1355110.0,"[1355110, 1541728]",956238,81882,bug,2015-09-04T23:07:01Z,https://www.pivotaltracker.com/story/show/101792504
2015-09-05T00:25:41Z,2015-08-28T17:59:27Z,accepted,,"DeploymentsController -  has routes based on index `get '/:deployment/jobs/:job/:index'`, `put /:deployment/jobs/:job/:index` etc.
AgentStateMigrator:67 - index is used to reconcile instances from a previously unfinished rename

AvailabilityZonePicker - favors reusing existing instances with the lowest index (to keep existing integration tests passing)
AvailabilityZonePicker - assigns indexes to new instances (for backward compatibility)

DeploymentPlan::Link - exposes index (and not node id). release authors may end up using index in their release.

`bosh ssh` requires uses specify the index of which instance they'd like to shell into

Inactive Disk: specifies the instance index of the inactive disk

OutOfSyncVm - Validation whether an instance is out of sync

ProblemScanner::VmScanStage - return which instance is out of sync",0.0,102273602,story,[],audit use of instance.index in director,344.0,[344],956238,344,feature,2015-09-05T00:25:41Z,https://www.pivotaltracker.com/story/show/102273602
2015-09-05T00:33:24Z,2015-08-03T17:38:37Z,accepted,,"- if specified on compilation use that az (merge that AZs cloud properties into compilation cloud properties)
- if not specified do not use any az

when no az is specified do whatever we do for instances that do not have az assigned",2.0,100440272,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",compilation VMs should be created in availability zone that was specified,1426194.0,"[1426194, 1655862]",956238,687691,feature,2015-09-05T00:33:24Z,https://www.pivotaltracker.com/story/show/100440272
2015-09-05T02:08:12Z,2015-08-20T19:34:25Z,accepted,,"- use bin/provision_cf.sh to get cf running
- do a bosh deploy of cf; once it succeeds, do another bosh deploy. see that cli shows diff even though nothing has changed.

dk: see that idora's bosh gets fixed because of this.",,101712062,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",fix director so that deployed cf does not report login.links properties were removed during a second deploy,553935.0,[553935],956238,81882,bug,2015-09-05T02:08:13Z,https://www.pivotaltracker.com/story/show/101712062
2015-09-06T00:35:51Z,2015-07-23T21:14:46Z,accepted,,"will require instances to save their AZ

for now gap in index will be introduced when azs are scaled down

Recreate as few instances as possible (moving instances unnecessarily is a waste of time, and more importantly requires losing/migrating persistent data). It doesn't matter which AZs have the extra instances if they don't divide evenly. It doesn't matter which instances get moved when re-balancing (as long as it's as few as possible).

Some scenarios:
1. given a job in 2 zones with 3 instances, we expect 
```
zone_1: job/0, job/2
zone_2: job/1
```
 when redeploying without zone_1 and still 3 instances, we expect:
```
zone_2: job/0, job/1, job/2
```
and the job/1 instance should not have been recreated (same agent_id and cid)

2. given a job in 2 zones with 3 instances, we expect 
```
zone_1: job/0, job/2
zone_2: job/1
```
when redeploying without zone_1 and only 1 instance we expect:
```
zone_2: job/1
```
and the job/1 instance should not have been recreated (same agent_id and cid)

3. given a job in 2 zones with 5 instances, we expect 
```
zone_1: job/0, job/2, job/4
zone_2: job/1, job/3
```
when redeploying with 3 zones and 5 instances we expect
```
zone_0: job/4
zone_1: job/0, job/2
zone_2: job/1, job/3
```
and the job/0, job/1, job/2, job/3 instances should not have been recreated

4. given a job with 2 zones and 4 instances
```
zone_1:	job/0 job/2
zone_2:  job/1 job/3
```
when redeploying with 3 zones and 6 instances 2 instances are created in the new AZ and the originals are untouched
```
zone_1:	job/0 job/2
zone_2:  job/1 job/3
zone_3: job/4 job/5
```

5. from 1 AZ with 2 instances
```
zone_b:	job/0 job/1
```
to 2 AZs with 3 instances
```
zone_a:  job/2
zone_b:	job/0 job/1
```",4.0,99740226,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can delete az and scale down instances at the same time and see that left over instances did not change AZs,1655862.0,"[1655862, 344]",956238,81882,feature,2015-09-06T00:35:51Z,https://www.pivotaltracker.com/story/show/99740226
2015-09-06T01:58:44Z,2015-08-21T17:03:24Z,accepted,,"- if there are 5 instances in one az and one more az is added for the job with instances kept at 5, director should keep all instances in that same az
- if there are 5 instances in one az and one more az is added for the job but instances are increased to 7, director should place 2 new instances into a new az",4.0,101784928,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",instances with persistent data should not be rebalanced when adding azs,1426194.0,"[1426194, 1655862, 344]",956238,81882,feature,2015-09-06T01:58:44Z,https://www.pivotaltracker.com/story/show/101784928
2015-09-06T02:37:20Z,2015-07-23T21:15:44Z,accepted,,"- error if availability zone referenced is not found
- keep availability_zone key optional
- dns remains optional
- dns/cloud_props should only be specified at either top or subnet level, not both, otherwise error

supported:

currently (network  = 1subnet):
```
networks:
- name: blah
  type: dynamic
  dns: [blah]
  cloud_properties: { ... }
```
with az support (network  = x subnet):
```
networks:
- name: blah
  type: dynamic
  subnets:
  - cloud_properties: { .. }
    dns: [4.4.4.4]
    availability_zone: z1
  - cloud_properties: { .. }
    dns: [8.8.8.8]
    availability_zone: z2
```
both scenarios must be supported. see subnet validation rules on manual network subnets.",2.0,99740530,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can specify availability zones for a dynamic network's subnets,687691.0,[687691],956238,81882,feature,2015-09-06T02:37:21Z,https://www.pivotaltracker.com/story/show/99740530
2015-09-06T02:57:44Z,2015-08-14T17:54:48Z,accepted,,"agent should send it to the hm in a heartbeat, hm should forward it to logger plugin
expectation: i should see instance id in the logs next to index, etc.",2.0,101302312,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",health monitor should include instance id in events it emits in addition to deployment/job/index,344.0,[344],956238,81882,feature,2015-09-06T02:57:44Z,https://www.pivotaltracker.com/story/show/101302312
2015-09-06T03:11:56Z,2015-08-28T23:24:09Z,accepted,,"currently we save az only after we create a vm. we should save the az on the instance at the time of creation of an instance. 

user should be able to run `bosh instances` right after director made its selection and before it started to create vms and see all az assigned.",2.0,102294534,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",ips allocated to a new instance in an az should be reused on subsequent deploy if current deploy failed in create_vm,344.0,[344],956238,81882,feature,2015-09-06T03:11:56Z,https://www.pivotaltracker.com/story/show/102294534
2015-09-06T03:23:57Z,2015-08-11T06:00:02Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/903a147e0ba49178fce104f9b764051e1966b745/bosh-director/lib/bosh/director/deployment_plan/existing_instance.rb#L17

```
Deploying
---------

Director task 40
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:11)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Failed: undefined method `apply_spec' for nil:NilClass (00:00:00)
Error 100: undefined method `apply_spec' for nil:NilClass
Task 40 error
For a more detailed error report, run: bosh task 40 --debug
```

planning:
1.
```
D, [2015-08-11 05:54:25 #9949] [task:40] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-08-11 05:54:25 #9949] [task:40] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
I, [2015-08-11 05:54:25 #9949] [task:40]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-08-11 05:54:25 #9949] [task:40] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""8a7280c2-581a-4423-bf6c-88648e656887"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director '23db8d81-bfcd-4bb4-af1b-25d6cd72b843': #<NoMethodError: undefined method `apply_spec' for nil:NilClass>"",""created_at"":1439272465}
E, [2015-08-11 05:54:25 #9949] [task:40] ERROR -- DirectorJobRunner: undefined method `apply_spec' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/existing_instance.rb:17:in `initialize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/existing_instance.rb:7:in `new'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/existing_instance.rb:7:in `create_from_model'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:40:in `block in plan_obsolete_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:39:in `map'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:39:in `plan_obsolete_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:139:in `block in prepare'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:202:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
```
2.
```
E, [2015-08-11 05:54:25 #9949] [task:40] ERROR -- DirectorJobRunner: undefined method `env' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/existing_instance.rb:18:in `initialize'
```

deletion:

```
D, [2015-08-11 06:07:52 #10358] [task:44] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""a430c37d-a035-4f0c-991d-3c021c2bad60"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director '23db8d81-bfcd-4bb4-af1b-25d6cd72b843': #<NoMethodError: undefined method `cid' for nil:NilClass>"",""created_at"":1439273272}
E, [2015-08-11 06:07:52 #10358] [task:44] ERROR -- DirectorJobRunner: undefined method `cid' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/instance_deleter.rb:80:in `delete_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/instance_deleter.rb:17:in `block (3 levels) in delete_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
```",,101008682,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",existing instance cannot be planned/deleted if it does not have a vm,553935.0,"[553935, 1017727]",956238,81882,bug,2015-09-06T03:23:57Z,https://www.pivotaltracker.com/story/show/101008682
2015-09-06T03:24:44Z,2015-08-27T01:55:07Z,accepted,,"```
irb(main):001:0> Bosh::Director::Models::Instance.all
=> [#<Bosh::Director::Models::Instance @values={:id=>4, :job=>""dummy"", :index=>0, :deployment_id=>4, :vm_id=>nil, :state=>""started"", :resurrection_paused=>false, :uuid=>""05f44e0e-c02a-4eda-8b55-e42799961f86"", :availability_zone=>nil, :cloud_properties=>nil, :compilation=>false}>]
irb(main):002:0> Bosh::Director::Models::IpAddress.all
=> []
irb(main):003:0> Bosh::Director::Models::Vm.all
=> []
irb(main):004:0>
```

```
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.001687s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT * FROM ""releases"" WHERE (""name"" = 'dummy') LIMIT 1
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT * FROM ""release_versions"" WHERE ((""release_versions"".""release_id"" = 11) AND (""version"" = '0+dev.42')) LIMIT 1
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Found release `dummy/0+dev.42'
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000000s) SELECT ""release_versions"".* FROM ""release_versions"" INNER JOIN ""deployments_release_versions"" ON ((""deployments_release_versions"".""release_version_id"" = ""release_versions"".""id"") AND (""deployments_release_versions"".""deployment_id"" = 4))
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Deleting lock: lock:release:dummy
D, [2015-08-27 01:53:32 #26199] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Deleted lock: lock:release:dummy
I, [2015-08-27 01:53:32 #26199] [task:55]  INFO -- DirectorJobRunner: Binding existing deployment
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000116s) SELECT NULL
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: (0.000392s) SELECT * FROM ""instances"" WHERE (""instances"".""deployment_id"" = 4)
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Waiting for tasks to complete
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Shutting down pool
I, [2015-08-27 01:53:32 #26199] [task:55]  INFO -- DirectorJobRunner: Existing desired instance: dummy/0 in az:
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: [network-configuration] Creating instance network reservations from agent state for instance 'dummy/0'
D, [2015-08-27 01:53:32 #26199] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
I, [2015-08-27 01:53:32 #26199] [task:55]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-08-27 01:53:32 #26199] [task:55] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""c6c300df-869d-4b43-9f14-ddba8f939b2c"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director 'e5896f90-c41c-44ab-8851-00df05b431f5': #<NoMethodError: undefined method `fetch' for nil:NilClass>"",""created_at"":1440640412}
E, [2015-08-27 01:53:32 #26199] [task:55] ERROR -- DirectorJobRunner: undefined method `fetch' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_network_reservations.rb:11:in `create_from_state'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance.rb:161:in `bind_existing_reservations'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance.rb:42:in `fetch_existing'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:66:in `block in desired_existing_instance_plans'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:63:in `map'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:63:in `desired_existing_instance_plans'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:33:in `plan_job_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:34:in `block (2 levels) in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:31:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:31:in `block in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:209:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:207:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:21:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner.rb:115:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:33:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:25:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
```",,102125266,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",bosh deploy with existing instance without vm should properly bind networks,1426194.0,[1426194],956238,81882,bug,2015-09-06T03:24:44Z,https://www.pivotaltracker.com/story/show/102125266
2015-09-06T03:48:04Z,2015-06-30T23:44:00Z,accepted,,if first deploy fails in the middle we need to make sure compilation instances are reclaimed and ips are released on subsequent deploy,2.0,98155340,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",make sure compilation instances are recreated/used/deleted upon finishing compilation in the next deploy,553935.0,"[553935, 1715614, 1687782]",956238,81882,feature,2015-09-06T03:48:04Z,https://www.pivotaltracker.com/story/show/98155340
2015-09-06T04:13:41Z,2015-08-17T19:20:18Z,accepted,,"- if run_script is not supported ignore the message and log (see prepare message for example)
- if run_script is supported then wait for it
- error out if run_script errors",2.0,101436550,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should see that pre-start scripts get run by the director during bosh deploy,948679.0,"[948679, 1541728]",956238,81882,feature,2015-09-06T04:13:41Z,https://www.pivotaltracker.com/story/show/101436550
2015-09-06T04:48:02Z,2015-07-20T19:22:14Z,accepted,,"- agent should introduce a new action called run_script async
  - args: name, options
- agent's run_script(pre-start) should run pre-start script for the first 
  - if script does not exist, return successfully (similarly to how drain does it: https://github.com/cloudfoundry/bosh-agent/blob/master/agent/action/drain.go#L106)
- set up same env as for run errand (https://github.com/cloudfoundry/bosh-agent/blob/master/agent/action/run_errand.go#L67)",4.0,99450208,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should see that first release job's pre-start script gets run before that job is started,1355110.0,"[1355110, 631325]",956238,81882,feature,2015-09-06T04:48:11Z,https://www.pivotaltracker.com/story/show/99450208
2015-09-06T05:24:23Z,2015-08-19T22:46:25Z,accepted,,,2.0,101640636,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pre-start action should be executed when vm is resurrected,948679.0,"[948679, 1541728]",956238,81882,feature,2015-09-06T05:24:24Z,https://www.pivotaltracker.com/story/show/101640636
2015-09-06T07:29:00Z,2015-08-26T19:38:57Z,accepted,,"given a dummy release
bosh create release ... & deploy it (release called dummy)
bosh export release

different director:
bosh create release --name dummy2 # this step seems to be important
bosh upload release
bosh upload release compiled
bosh upload release dev_releases/dummy-yml-for-the-same-release-that-was-used-for-compilation # ----> blows up

Currently bad tar error is raised, but we should raise tarball does not exist error so it's a bit more clear. For example:

```
  Started resolving package dependencies > Resolving package dependencies. Done (00:00:00)
  Started processing 2 existing packages > Processing 2 existing packages. Failed: ...
Error 100: Tarball '/var/vcap/data/tmp/director/d20150826-28825-1bbdw2u/packages/bad_package.tgz' does not exist
```

the underlyng root issue is that cli will not include package source for the second upload because it thinks that director has those packages since it returned fingerprints for that. seems like director should only return fingerprints for packages that have blob/sha1 for match_remote_packages call. same for match_remote_compiled_packages call?

Do we need to condition on where it's compiled release or not?

```
if tarball.compiled_release?
              package_matches = match_remote_compiled_packages(tarball.manifest)
            else
              package_matches = match_remote_packages(tarball.manifest)
            end
```",,102102608,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",backfilling of source does not work if there is another release that has exactly same contents,948679.0,"[948679, 1355110]",956238,81882,bug,2015-09-06T07:29:01Z,https://www.pivotaltracker.com/story/show/102102608
2015-09-06T08:14:01Z,2015-08-26T19:16:17Z,accepted,,"seems like i got into a problem and update_release decided to delete *used* release version: https://github.com/cloudfoundry/bosh/blob/a79e0091c9040bb12da50e76e222e9101f2072ad/bosh-director/lib/bosh/director/jobs/update_release.rb#L579-L585, but failed in the end since it's still fkey referenced by the deployment.

i dont think we should even try deleting it on error since deleting assets out of blobstore etc is a lengthy acitivity and error prone. let's make sure that if it fails the first time, let's say on a blobstore import, we can re-run bosh upload release and it finishes successfully.

also should we be more idempotent for each package/job: https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/update_release.rb#L140? currently if upload release fails, it will not allow to complete it next time if not all packages were created the first time?

delete release after invalid import should work in that case.",4.0,102100980,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",upload release should not delete prev uploaded release versions when error happens,948679.0,"[948679, 1355110]",956238,81882,feature,2015-09-06T08:14:01Z,https://www.pivotaltracker.com/story/show/102100980
2015-09-06T08:14:02Z,2015-08-26T23:50:28Z,accepted,,"Error should be something like this: Release version `blah/ver` is already uploaded with different content.

This error should be raised if release's git sha, uncommitted changes or fingerprint of a package/job does not match.

```
Director task 740
  Started extracting release > Extracting release. Done (00:00:00)

  Started verifying manifest > Verifying manifest. Done (00:00:00)

Error 100: release_id and version unique

Task 740 error

For a more detailed error report, run: bosh task 740 --debug/var/lib/jenkins/jobs/auto_deploy_vsphere_centos_go_agent/workspace/bosh/bosh-core/lib/bosh/core/shell.rb:52:in `report'
```",1.0,102120104,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",improve error message when upload release tries to add already existing release,948679.0,"[948679, 1355110]",956238,81882,feature,2015-09-06T08:14:02Z,https://www.pivotaltracker.com/story/show/102120104
2015-09-06T09:00:13Z,2015-05-05T04:51:39Z,accepted,,,,93869696,story,"[{'name': 'global-net', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:57:05Z', 'id': 11546910, 'updated_at': '2015-04-29T21:57:05Z'}]",users can share networks configured via cloud-config,,[],956238,81882,release,2015-09-06T09:00:13Z,https://www.pivotaltracker.com/story/show/93869696
2015-09-08T21:19:37Z,2015-09-08T18:22:25Z,accepted,,,,102934810,story,[],debug sigsegv problem,1017727.0,"[1017727, 1783496]",956238,1550486,chore,2015-09-08T21:19:47Z,https://www.pivotaltracker.com/story/show/102934810
2015-09-10T00:02:54Z,2015-09-09T06:42:47Z,accepted,,"http://www.ubuntu.com/usn/usn-2738-1
expected:  linux-image-3.19.0-28-generic   3.19.0-28.30",1.0,102975036,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2738-1] bump ubuntu for linux kernel vulnerability,344.0,[344],956238,81882,feature,2015-09-10T00:02:54Z,https://www.pivotaltracker.com/story/show/102975036
2015-09-11T00:10:57Z,2015-06-11T01:00:15Z,accepted,,"The main BOSH project now uses ""bosh/main"". No other BOSH projects should be using the ""bosh/integration"" image.",,96728480,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Delete bosh/integration image from Docker Hub,1550486.0,[1550486],956238,1550486,chore,2015-09-11T00:10:57Z,https://www.pivotaltracker.com/story/show/96728480
2015-09-11T18:53:36Z,2015-09-11T18:12:20Z,accepted,,,,103218146,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",Add integration test in the agent for the raw ephemeral labeling feature,1355110.0,"[1355110, 659629]",956238,1355110,chore,2015-09-14T19:49:17Z,https://www.pivotaltracker.com/story/show/103218146
2015-09-12T00:00:49Z,2015-08-14T21:46:39Z,accepted,,show doc link in the failure output,,101318096,story,[],Point to OS image doc when building stemcell fails,1550486.0,[1550486],956238,553935,chore,2015-09-12T00:00:50Z,https://www.pivotaltracker.com/story/show/101318096
2015-09-13T19:00:00Z,2015-09-01T23:29:35Z,accepted,,"- still support single db creation
- support multiple db creation via databases key
- still have only one user?

https://github.com/cloudfoundry/bosh/blob/master/release/jobs/postgres/templates/postgres_ctl.erb#L58-L65",2.0,102532550,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",allow creation of one or more databases by the postgres job via additional_databases key,344.0,[344],956238,81882,feature,2015-09-14T07:18:30Z,https://www.pivotaltracker.com/story/show/102532550
2015-09-14T06:43:37Z,2015-08-31T18:48:04Z,accepted,,,2.0,102400436,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",update bosh release to ruby 2.1.7,1550486.0,[1550486],956238,81882,feature,2015-09-14T06:43:38Z,https://www.pivotaltracker.com/story/show/102400436
2015-09-14T18:39:42Z,2015-02-09T22:21:08Z,accepted,,,,87992678,story,"[{'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",all director functionality can be used with uaa,,[],956238,81882,release,2015-09-14T18:39:42Z,https://www.pivotaltracker.com/story/show/87992678
2015-09-15T02:07:39Z,2014-02-26T18:29:49Z,accepted,,"It should not use the packages directory. This unnecessarily pollutes the ngnix job package directory

```
2014/02/26 18:16:24 [crit] 2410#0: *80 pwrite() ""/var/vcap/packages/nginx/client_body_temp/0000000004"" failed (28: No space left on device), client: 172.16.79.1, server: , request: ""POST /stemcells HTTP/1.1"", host: ""172.16.79.5:25555""
2014/02/26 18:20:47 [crit] 2409#0: *91 pwrite() ""/var/vcap/packages/nginx/client_body_temp/0000000005"" failed (28: No space left on device), client: 172.16.79.1, server: , request: ""POST /stemcells HTTP/1.1"", host: ""172.16.79.5:25555""
```

it should use /var/vcar/data/tmp/director/nginx (not created right now)",,66528580,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",nginx should not use /var/vcap/packages partition for temporary space when accepting uploads,1386874.0,"[1386874, 1495236, 81882]",956238,756869,bug,2015-09-15T02:07:39Z,https://www.pivotaltracker.com/story/show/66528580
2015-09-15T16:35:50Z,2015-09-10T17:49:27Z,accepted,,"http://www.ubuntu.com/usn/usn-2739-1
expected:  libfreetype6                    2.5.2-1ubuntu2.5",1.0,103119290,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2739-1] bump ubuntu for FreeType vulnerabilities,1550486.0,[1550486],956238,81882,feature,2015-09-15T16:35:51Z,https://www.pivotaltracker.com/story/show/103119290
2015-09-15T21:17:23Z,2015-09-04T16:48:46Z,accepted,,,1.0,102763380,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",increase root partition size only on aws stemcells to 10gb (because ambari so that it can install lots of things onto root partition) [NO CHANGES MADE],948679.0,"[948679, 631325]",956238,81882,feature,2015-09-15T21:17:32Z,https://www.pivotaltracker.com/story/show/102763380
2015-09-15T21:40:18Z,2015-07-21T17:52:35Z,accepted,,at this point it will be `protocol: 2`,1.0,99541126,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",director should include agent protocol version when sending messages to the agent,1541728.0,"[1541728, 1224348]",956238,81882,feature,2015-09-15T21:40:19Z,https://www.pivotaltracker.com/story/show/99541126
2015-09-15T23:11:08Z,2015-07-31T21:00:45Z,accepted,,since each job instance now has an id that does not change we should include it in the link.,1.0,100334928,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",links should include id for each one of the nodes,1426194.0,[1426194],956238,81882,feature,2015-09-15T23:11:08Z,https://www.pivotaltracker.com/story/show/100334928
2015-09-15T23:11:09Z,2015-07-31T20:55:04Z,accepted,,"```
nodes = [
  {
    name: string
    index: string
    availability_zone: string
    networks: { ... }
  }, 
...]
```",1.0,100334580,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",links should include AZ for each one of the nodes,,[],956238,81882,feature,2015-09-15T23:11:09Z,https://www.pivotaltracker.com/story/show/100334580
2015-09-15T23:17:56Z,2015-08-28T19:09:31Z,accepted,,"if there is no az then show n/a. name originally comes from cloud-config. it's already saved on the instance. add AZ column before resource pool. also `bosh vms` command. similarly to how we deal with disk_cid, do not show az column at all if az is not returned from the api.

Something like this:

```
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
| Instance                                | State   | AZ | Resource Pool | IPs          | VM CID     | Disk CID     | Agent ID                             | Resurrection |
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
| db/5a564456-de05-41c2-be64-96c62a57c9cc | running | z1 | z1            | 10.10.0.70   | i-9db04c48 | vol-394d7ad8 | 595f3b1b-e268-46b6-a6ae-5b55160c82ac | active       |
| db/617ddd75-f8e2-45bb-82bf-91bd1d837bae | running | z1 | z1            | 10.10.0.63   | i-57b04c82 | vol-364d7ad7 | 15506759-8057-478d-95da-7ec9382f2426 | active       |
| db/0a0ce4cb-b936-4e09-8061-8673baa37481 | running | z2 | z1            | 10.10.64.121 | i-67855bc7 | vol-2c9894d7 | 185cc7c4-4b35-40e9-a4d9-2ab7e736dc07 | active       |
| db/cadd49b2-03fa-4625-9092-4102c19b21ec | running | z2 | z1            | 10.10.64.122 | i-f4825c54 | vol-9598946e | 4b234ad5-bdc5-4192-a096-b0ff7d42a2b5 | active       |
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
```",2.0,102279332,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bosh instances command should show assigned az per instance,687691.0,"[687691, 1779224]",956238,81882,feature,2015-09-15T23:17:56Z,https://www.pivotaltracker.com/story/show/102279332
2015-09-15T23:38:57Z,2015-08-27T01:58:41Z,accepted,,"deletion should happen based on db state

```
D, [2015-08-27 01:57:32 #26269] [task:57] DEBUG -- DirectorJobRunner: (0.000598s) SELECT * FROM ""deployments"" WHERE (""name"" = 'tiny-dummy') LIMIT 1
D, [2015-08-27 01:57:32 #26269] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-08-27 01:57:32 #26269] [task:57] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-08-27 01:57:32 #26269] [task:57] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
E, [2015-08-27 01:57:32 #26269] [task:57] ERROR -- DirectorJobRunner: no implicit conversion of nil into String
/var/vcap/packages/ruby/lib/ruby/2.1.0/psych.rb:370:in `parse'
/var/vcap/packages/ruby/lib/ruby/2.1.0/psych.rb:370:in `parse_stream'
/var/vcap/packages/ruby/lib/ruby/2.1.0/psych.rb:318:in `parse'
/var/vcap/packages/ruby/lib/ruby/2.1.0/psych.rb:245:in `load'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:38:in `create_from_model'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/delete_deployment.rb:28:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/delete_deployment.rb:24:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,102125346,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user should be able to delete failed deployment,1426194.0,"[1426194, 687691]",956238,81882,bug,2015-09-15T23:38:57Z,https://www.pivotaltracker.com/story/show/102125346
2015-09-15T23:44:29Z,2015-08-21T18:52:36Z,accepted,,,1.0,101793914,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",run os image tests again at the end of the stemcell building,1386874.0,[1386874],956238,81882,feature,2015-09-15T23:44:29Z,https://www.pivotaltracker.com/story/show/101793914
2015-09-16T01:06:13Z,2015-09-06T01:58:27Z,accepted,,"- had 1 instance in az1, 4 instances in az2, 0 instances in az3
- removed az3 and scaled down to 4 instances
- bosh deploy results in error

```
D, [2015-09-06 01:54:35 #23889] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: Shutting down pool
D, [2015-09-06 01:54:35 #23889] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
I, [2015-09-06 01:54:35 #23889] [task:14145]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""f5879316-3f00-4f89-aa2a-3299a66cd645"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director 'f976642f-ade0-4d68-8660-2bba6817f10f': #<NoMethodError: undefined method `az=' for nil:NilClass>"",""created_at"":1441504475}
E, [2015-09-06 01:54:35 #23889] [task:14145] ERROR -- DirectorJobRunner: undefined method `az=' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/availability_zone_picker.rb:121:in `record_placement'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/availability_zone_picker.rb:33:in `block in place_instances_that_have_persistent_disk_in_existing_az'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/availability_zone_picker.rb:28:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/availability_zone_picker.rb:28:in `place_instances_that_have_persistent_disk_in_existing_az'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/availability_zone_picker.rb:10:in `place_and_match_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:14:in `plan_job_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:34:in `block (2 levels) in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:31:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:31:in `block in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:210:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:208:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:21:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner.rb:125:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:41:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: (0.000113s) SELECT NULL
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: (0.000062s) BEGIN
D, [2015-09-06 01:54:35 #23889] [task:14145] DEBUG -- DirectorJobRunner: (0.000468s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-09-06 01:54:35.441011+0000', ""description"" = 'create deployment', ""result"" = 'undefined method `az='' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/14145', ""checkpoint_time"" = '2015-09-06 01:54:35.229229+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 14145)
```

bosh instances --details

```
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
| Instance                                | State   | AZ | Resource Pool | IPs          | VM CID     | Disk CID     | Agent ID                             | Resurrection |
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
| db/473377eb-d18c-41ea-9a8c-e2cf247c036c | running | z1 | z1            | 10.10.0.63   | i-ba728f6f | vol-387542d9 | edc037da-1d36-4364-8af2-38aa187c7923 | active       |
| db/06a6c435-6126-4dcf-b57d-f75d3978f352 | running | z2 | z1            | 10.10.64.123 | i-cf74ab6f | vol-4dc1cdb6 | ffd8405e-fdcc-4d1d-bcbd-1393c5abd84f | active       |
| db/19caa899-5a62-464f-ae0b-bef0cab1aec0 | running | z2 | z1            | 10.10.64.125 | i-dd72ad7d | vol-93c1cd68 | 1c7709da-5460-430b-be46-005c1a93adb4 | active       |
| db/72a3d39e-5f90-45ff-8735-176cedda5275 | running | z2 | z1            | 10.10.64.124 | i-1171aeb1 | vol-60c1cd9b | 3a3fb589-b8c1-46b3-afa9-0aae98b04134 | active       |
| db/96644b60-6987-4311-935d-c0690c2bf547 | running | z2 | z1            | 10.10.64.122 | i-0372ada3 | vol-64222e9f | 0b21d850-557c-4888-8626-721a31080209 | active       |
+-----------------------------------------+---------+----+---------------+--------------+------------+--------------+--------------------------------------+--------------+
```",,102806036,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can scale down instances from a job in multiple azs,1779224.0,"[1779224, 687691, 1655862]",956238,81882,bug,2015-09-16T01:06:15Z,https://www.pivotaltracker.com/story/show/102806036
2015-09-16T01:35:45Z,2015-08-17T18:09:09Z,accepted,,,2.0,101428154,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","when az cloud properties change, instances in that az should be recreated",1779224.0,"[1779224, 344]",956238,81882,feature,2015-09-16T01:35:45Z,https://www.pivotaltracker.com/story/show/101428154
2015-09-16T20:47:15Z,2015-09-16T00:18:59Z,accepted,,,,103462656,story,[],Merge master into global-net,1017727.0,"[1017727, 1779224]",956238,1017727,chore,2015-09-16T20:47:15Z,https://www.pivotaltracker.com/story/show/103462656
2015-09-16T22:19:46Z,2015-08-21T17:31:31Z,accepted,,"- dump to stderr
- confirm that it shows up in the var/vcap/bosh/log/current
- use https://golang.org/pkg/runtime/#Stack
- example trapping signal in bosh-init",2.0,101786798,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",agent should dump goroutines to agent log when SIGSEGV is sent,1017727.0,[1017727],956238,81882,feature,2015-09-16T22:19:55Z,https://www.pivotaltracker.com/story/show/101786798
2015-09-16T22:51:55Z,2015-08-18T00:14:37Z,accepted,,"bosh create release --dir ~/workspace/my-release
bosh upload release --dir ~/workspace/my-release

- both the of the commands should operate on a release inside ~/workspace/my-release regardless which directory they run from.

helpful for bosh-load-tests

- use Chdir?",2.0,101459622,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",bosh create/upload release should accept --dir flag to a release directory,553935.0,"[553935, 1783496]",956238,81882,feature,2015-09-16T22:51:56Z,https://www.pivotaltracker.com/story/show/101459622
2015-09-16T23:07:23Z,2015-08-01T00:48:24Z,accepted,,"- agent should return process information (https://github.com/cloudfoundry/bosh-agent/blob/master/agent/action/get_state.go action)
  - response should include `processes => [{name => ""cloud_controller"", state =""running""}, {name => ""cloud_controller_worker"", state =""Execution failed""}]`
- director should save process info into result file (https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/vm_state.rb)

Modify the bosh agents to send additional information to director which will show the data in `bosh vms && bosh task --result`",2.0,100344708,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",Director should include process's running states in vm state results ,1495236.0,"[1495236, 1386874]",956238,1482982,feature,2015-09-16T23:07:24Z,https://www.pivotaltracker.com/story/show/100344708
2015-09-17T02:46:12Z,2015-09-16T22:23:53Z,accepted,,,,103551312,story,[],Upgrade concourse and workers to 0.63.0,81882.0,[81882],956238,344,chore,2015-09-17T02:46:12Z,https://www.pivotaltracker.com/story/show/103551312
2015-09-17T03:24:38Z,2015-07-20T19:25:59Z,accepted,,"- start all pre- start scripts at the same time
- wait for all to finish
- when one fails continue waiting for unfinished tasks
- no timeouts
- include info about which pre-start scripts ran and which one failed",4.0,99450484,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should see that all pre-start scripts run for each release job in parallel on the vm,948679.0,"[948679, 1355110]",956238,81882,feature,2015-09-17T03:24:47Z,https://www.pivotaltracker.com/story/show/99450484
2015-09-17T21:55:22Z,2015-09-06T08:15:12Z,accepted,,"Steps I think relate:
- add LOL error (see in comments)
- bosh upload release https://bosh.io/d/github.com/cloudfoundry/cf-release?v=215 which raised LOL
- remove LOL error
- bosh upload release https://bosh.io/d/github.com/cloudfoundry/cf-release?v=215 and saw ""Job `loggregator_trafficcontroller' is referencing a missing package `common'""

---

Following was the error while uploading:

```
±  |master ✗| → bosh upload release https://bosh.io/d/github.com/cloudfoundry/cf-release?v=215
Acting as user 'admin' on 'idora-aws'

Using remote release `https://bosh.io/d/github.com/cloudfoundry/cf-release?v=215'

Director task 25113
  Started downloading remote release > Downloading remote release. Done (00:00:30)

  Started extracting release > Extracting release. Done (00:00:16)

  Started verifying manifest > Verifying manifest. Done (00:00:00)

  Started resolving package dependencies > Resolving package dependencies. Done (00:00:00)

  Started creating new packages
  Started creating new packages > gnatsd/a0d6f5d3264aa8ecadb52d3bfa04540636800820. Done (00:00:00)
  Started creating new packages > buildpack_java/82fb7b325de57573e9ebddadf004b9dac5b49185. Done (00:00:00)
  Started creating new packages > statsd-injector/8c2166f3d82ffb8759d6eb409947121aca5123a1. Done (00:00:00)
  Started creating new packages > hm9000/140370f2fc22365ffe78a1e6bce6a122f0eb42dc. Done (00:00:00)
  Started creating new packages > gorouter/d0423a40ad0cfb3fb1cf8a719fa5939b397c1ad7. Done (00:00:00)
  Started creating new packages > buildpack_binary/e0c8736b073d83c2459519851b5736c288311d92. Done (00:00:00)
  Started creating new packages > acceptance-tests/35ceef5e97d612e791959f85315a141b8ac3576c. Done (00:00:00)
  Started creating new packages > nginx/79f6087cfeec22f16f9182b42d0a6697f7360c22. Done (00:00:00)
  Started creating new packages > etcd_metrics_server/64efbbfb5761d09a24dad21ecfebd8824b99d433. Done (00:00:00)
  Started creating new packages > golang1.4/f57ddbc8d55d7a0f08775bf76bb6a27dc98c7ea7. Done (00:00:01)
  Started creating new packages > cloud_controller_ng/e0c406ae04914096952686761972c1042d9d0f2c. Done (00:00:00)
  Started creating new packages > etcd/d43feb5cdad0809d109df0afe6cd3c315dc94a61. Done (00:00:00)
  Started creating new packages > debian_nfs_server/aac05f22582b2f9faa6840da056084ed15772594. Done (00:00:00)
  Started creating new packages > consul/14b83378b30a2b55a25e641e835af3e5c87a0d41. Done (00:00:00)
  Started creating new packages > mysqlclient/8b5d9ce287341048377997a9b3fe4ff3e6a1c68f. Done (00:00:01)
  Started creating new packages > golang1.3/e4b65bcb478d9bea1f9c92042346539713551a4a. Done (00:00:00)
  Started creating new packages > postgres-9.4.2/ac1c8a521594f9459ffede25b9d7e0308811f139. Done (00:00:00)
  Started creating new packages > buildpack_staticfile/f79dd915e8ee73b297ef3ae1d85b2895d5f5c106. Done (00:00:00)
  Started creating new packages > ruby-2.1.6/d198bfa976cf87710b0bfc39935175cf0ea7773f. Done (00:00:01)
  Started creating new packages > routing-api/de794a001b94979d68be7f456253efede93c75f7. Done (00:00:00)
  Started creating new packages > uaa/ec48f8d2f8e5ebe7b162ac7a513dff449d884e93. Done (00:00:02)
  Started creating new packages > haproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171. Done (00:00:00)
  Started creating new packages > collector/2644a3d4929a93b575dc61ebe28fd9b7a2bd1745. Done (00:00:00)
  Started creating new packages > dea_logging_agent/683bd2966840cf89a800caff2e7edfbf02e9abf7. Done (00:00:00)
  Started creating new packages > buildpack_php/75d5c3a906ae21afd4e6082ac9a193a67692dd4c. Done (00:00:05)
  Started creating new packages > smoke-tests/883089ce873dffbf0a3207432ca7c76c0b082394. Done (00:00:01)
  Started creating new packages > dea_next/4b8bfe8ee6fca394e55893fec98e2da30100ddc1. Done (00:00:01)
  Started creating new packages > buildpack_ruby/c0fb4858089e680c029eb4b3a10577ca780b1dfc. Done (00:00:05)
  Started creating new packages > postgres/b63fe0176a93609bd4ba44751ea490a3ee0f646c. Done (00:00:02)
  Started creating new packages > cli/6fb52c578aad523ba3c78bc350313d4aa4db7da9. Done (00:00:00)
  Started creating new packages > nginx_newrelic_plugin/da403e7a851ab33370ba79bcd6ae135db9f9e7f6. Done (00:00:00)
  Started creating new packages > buildpack_nodejs/a55b6669b5138c9d90720dd2dc678de48955560d. Done (00:00:01)
     Done creating new packages (00:00:20)

  Started processing 13 existing packages > Processing 13 existing packages. Done (00:00:00)

  Started creating new jobs
  Started creating new jobs > loggregator_trafficcontroller/5970ca72011219b6a3d10a71f74fa43337caef17. Failed: Job `loggregator_trafficcontroller' is referencing a missing package `common' (00:00:00)

Error 80003: Job `loggregator_trafficcontroller' is referencing a missing package `common'

Task 25113 error

For a more detailed error report, run: bosh task 25113 --debug
```


```
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000113s) SELECT NULL
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000109s) SELECT NULL
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000180s) SELECT * FROM ""templates"" WHERE (""fingerprint"" = 'e597e616003791ea243cf1d49880a90088c1129e')
I, [2015-09-06 08:12:43 #12449] [task:25113]  INFO -- DirectorJobRunner: Creating new template `loggregator_trafficcontroller/5970ca72011219b6a3d10a71f74fa43337caef17'
I, [2015-09-06 08:12:43 #12449] [task:25113]  INFO -- DirectorJobRunner: Creating job template `loggregator_trafficcontroller/5970ca72011219b6a3d10a71f74fa43337caef17' from provided bits
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: Deleting lock: lock:release:cf
D, [2015-09-06 08:12:43 #12449] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: Deleted lock: lock:release:cf
E, [2015-09-06 08:12:43 #12449] [task:25113] ERROR -- DirectorJobRunner: Job `loggregator_trafficcontroller' is referencing a missing package `common'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/release/release_job.rb:117:in `block in parse_package_names'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/release/release_job.rb:114:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/release/release_job.rb:114:in `parse_package_names'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/release/release_job.rb:24:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:551:in `create_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:541:in `block (2 levels) in create_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:539:in `block in create_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:537:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:537:in `create_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:526:in `process_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:145:in `process_release'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:49:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/lock_helper.rb:24:in `block in with_release_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/lock_helper.rb:36:in `with_release_locks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/lock_helper.rb:24:in `with_release_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/update_release.rb:49:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3068.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3068.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000332s) SELECT NULL
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000113s) BEGIN
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.000387s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-09-06 08:12:43.792379+0000', ""description"" = 'create release', ""result"" = 'Job `loggregator_trafficcontroller'' is referencing a missing package `common''', ""output"" = '/var/vcap/store/director/tasks/25113', ""checkpoint_time"" = '2015-09-06 08:12:36.314228+0000', ""type"" = 'update_release', ""username"" = 'admin' WHERE (""id"" = 25113)
D, [2015-09-06 08:12:43 #12449] [task:25113] DEBUG -- DirectorJobRunner: (0.001274s) COMMIT
I, [2015-09-06 08:12:43 #12449] []  INFO -- DirectorJobRunner: Task took 1 minute 7.4877417870000045 seconds to process.
```",,102809924,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]","while importing release, director should find proper package dependency",659629.0,"[659629, 1355110]",956238,81882,bug,2015-09-17T21:55:24Z,https://www.pivotaltracker.com/story/show/102809924
2015-09-17T22:00:04Z,2015-09-14T17:12:52Z,accepted,,"current:
| cloud_controller_ng/0 | running | z1
wanted:
| cloud_controller_ng/874rsdc834tjrbk (0) | running | z1

- only show this new format if director is returning id in the vm state result
- applied to instances and vms commands",4.0,103335462,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bosh instances command should show id instead of index,344.0,[344],956238,81882,feature,2015-09-17T22:00:04Z,https://www.pivotaltracker.com/story/show/103335462
2015-09-17T22:56:02Z,2015-08-21T01:58:29Z,accepted,,,,101734034,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",compiled releases mvp,,[],956238,81882,release,2015-09-17T22:56:02Z,https://www.pivotaltracker.com/story/show/101734034
2015-09-18T00:59:20Z,2015-09-14T16:49:17Z,accepted,,Build fails if we fail to download tomcat https://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/integration-2.1-postgres/builds/226,,103333254,story,[],Retry downloading integration test dependencies (or check it in?),1017727.0,"[1017727, 1779224]",956238,553935,chore,2015-09-18T00:59:20Z,https://www.pivotaltracker.com/story/show/103333254
2015-09-22T00:08:04Z,2015-09-18T18:23:56Z,accepted,,Expected: libicu 52.1-3ubuntu0.4,0.0,103697696,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2740-1] bump ubuntu trusty for ICU vulnerabilities,553935.0,"[553935, 1779224]",956238,81882,feature,2015-09-22T00:08:05Z,https://www.pivotaltracker.com/story/show/103697696
2015-09-22T00:49:43Z,2015-08-10T19:15:07Z,accepted,,"show star and a legend on bosh instances.

don't show anything on `bosh vms`",2.0,100969324,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",bosh instances cmd should show which node is bootstrap node,1783496.0,"[1783496, 1017727]",956238,81882,feature,2015-09-22T00:49:43Z,https://www.pivotaltracker.com/story/show/100969324
2015-09-22T04:13:27Z,2015-07-23T21:15:01Z,accepted,,"when i span a job over AZs and it uses a manual network, the correct subnet (based on an AZ) should be picked for created instances.
only applied to automatic IPs (not static IPs)",4.0,99740478,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can see that a deployment job VMs get IPs from AZ specific subnets of manual network,1017727.0,[1017727],956238,81882,feature,2015-09-22T04:13:27Z,https://www.pivotaltracker.com/story/show/99740478
2015-09-22T05:06:04Z,2015-07-23T21:15:52Z,accepted,,,2.0,99740542,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can see deployment job VMs get IPs from AZ specific subnets of dynamic network,553935.0,"[553935, 1783496]",956238,81882,feature,2015-09-22T05:06:04Z,https://www.pivotaltracker.com/story/show/99740542
2015-09-23T19:20:07Z,2015-09-22T22:33:01Z,accepted,,,,103957034,story,[],Merge master into global-net,1017727.0,"[1017727, 676405]",956238,1017727,chore,2015-09-23T19:20:07Z,https://www.pivotaltracker.com/story/show/103957034
2015-09-23T20:04:57Z,2015-09-14T19:55:36Z,accepted,,Verify that we can bosh ssh into a vm and see labeled raw ephemeral partitions,,103351430,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",BATS tests for raw ephemeral disk,1541728.0,"[1541728, 631325]",956238,659629,chore,2015-09-23T20:04:57Z,https://www.pivotaltracker.com/story/show/103351430
2015-09-24T04:52:59Z,2015-09-22T00:34:12Z,accepted,,currently we only run for centos,0.0,103867398,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",run bats os specific tests for ubuntu stemcells,553935.0,"[553935, 1779224]",956238,81882,feature,2015-09-24T04:52:59Z,https://www.pivotaltracker.com/story/show/103867398
2015-09-24T23:36:42Z,2015-09-24T22:15:42Z,accepted,,"membrane is failing validation in be rake spec:system:micro[vsphere,esxi,centos,7,manual,go,false,ovf]

+ bundle exec rake 'spec:system:micro[vsphere,esxi,centos,7,manual,go,false,ovf]'
I, [2015-09-24T11:06:42.692864 #21409]  INFO : CANDIDATE_BUILD_NUMBER is 3080. Using candidate build.
I, [2015-09-24T11:06:42.925812 #21409]  INFO : Downloading http://bosh-ci-pipeline.s3.amazonaws.com/3080/bosh-stemcell/vsphere/bosh-stemcell-3080-vsphere-esxi-centos-7-go_agent.tgz to /mnt/jenkins/workspace/bat_micro_vsphere_centos_go_agent_nested_esxi/ci-artifacts/vsphere/manual/centos/7/go/deployments/bosh-stemcell-3080-vsphere-esxi-centos-7-go_agent.tgz
rake aborted!
Membrane::SchemaValidationError: { properties => { networks => At index 1: { name => Expected static, given second } } }

We added additional network in bats test which is causing this failure we are not sure why this is in place so we are removing it",,104140302,story,[],Remove membrane manifest validation in bosh dev,1426194.0,[1426194],956238,1426194,chore,2015-09-24T23:36:42Z,https://www.pivotaltracker.com/story/show/104140302
2015-09-26T00:06:34Z,2015-09-25T15:06:12Z,accepted,,"PrepareNetworkChangeAction.Run starts a goroutine that sleeps for 1 second then calls os.Exit(). This gets invoked a bunch of times from prepare_network_change_test.go.

Depending on the (randomized) order the tests run in, this can cause the process running the tests to exit before all tests have run. In this case, Ginkgo reports ""Test Suite Passed"" even though not all the tests ran. In this case, it will even say ""Test Suite Passed"" if there were failing tests.",,104182256,story,[],Fix Action suite so tests don't call os.Exit(),1426194.0,"[1426194, 1783496]",956238,1541728,chore,2015-09-26T00:06:35Z,https://www.pivotaltracker.com/story/show/104182256
2015-09-26T00:22:40Z,2015-09-25T20:53:55Z,accepted,,"`spec/spec_helper.rb` invokes the agent build script before running the tests, which is a great help, but it doesn't check if the build was successful. On a dev machine, this usually means the tests will run against an older version of the agent left over from the last successful build.

When the agent build fails, the rspec tests should not run.",,104210248,story,[],sandbox startup should fail when compiling the agent fails,1426194.0,[1426194],956238,1541728,chore,2015-09-26T00:22:40Z,https://www.pivotaltracker.com/story/show/104210248
2015-09-26T00:52:14Z,2015-09-23T21:01:08Z,accepted,,"bosh vms
current:
blah_job/some-id-goes-here (0)
expected:
blah_job/0 (some-id-goes-here)",2.0,104037164,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",show ids in parenthesis and index after job,1426194.0,[1426194],956238,81882,feature,2015-09-26T00:52:14Z,https://www.pivotaltracker.com/story/show/104037164
2015-09-26T00:52:21Z,2015-09-15T22:10:48Z,accepted,,"test in integration

```
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:515:in `set_dataset': Model.set_dataset takes one of the following classes as an argument: Symbol, LiteralString, SQL::Identifier, SQL::QualifiedIdentifier, SQL::AliasedExpression, Dataset (Sequel::Error)
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model.rb:46:in `Model'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models/dns/domain.rb:4:in `<module:Dns>'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models/dns/domain.rb:3:in `<top (required)>'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models/dns.rb:3:in `require'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models/dns.rb:3:in `<top (required)>'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models.rb:24:in `require'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/models.rb:24:in `<top (required)>'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/config.rb:139:in `configure'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/config.rb:426:in `configure_evil_config_singleton!'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/app.rb:26:in `initialize'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-console:37:in `new'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-console:37:in `start'
	from /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-console:75:in `<top (required)>'
	from /var/vcap/packages/director/bin/bosh-director-console:16:in `load'
	from /var/vcap/packages/director/bin/bosh-director-console:16:in `<main>'
```",,103456084,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",user should be able to configure director without dns,344.0,[344],956238,81882,bug,2015-09-26T00:52:21Z,https://www.pivotaltracker.com/story/show/103456084
2015-09-26T20:49:33Z,2015-09-18T16:59:22Z,accepted,,if curious enough find out what generates them,2.0,103688782,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",double check that keys in /etc/ssh/ in the stemcell are removed via a test at the end of the stemcell builder,1426194.0,"[1426194, 1779224]",956238,81882,feature,2015-09-26T20:51:25Z,https://www.pivotaltracker.com/story/show/103688782
2015-09-26T20:51:48Z,2015-08-21T18:53:42Z,accepted,,hook into rspec somehow to find all test descriptions we ran and check that we have run expected stig tests,1.0,101793986,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",add a test that checks which stig tests we ran,1386874.0,[1386874],956238,81882,feature,2015-09-26T20:51:48Z,https://www.pivotaltracker.com/story/show/101793986
2015-09-28T20:37:20Z,2015-09-19T00:03:41Z,accepted,,"We install kernel in OS image step. Currently installing open-vm-tools pulls down latest kernel

```
....
=== Applying 'system_open_vm_tools' stage ===
....
Resolving Dependencies
--> Running transaction check
...
---> Package kernel-headers.x86_64 0:3.10.0-229.11.1.el7 will be updated
---> Package kernel-headers.x86_64 0:3.10.0-229.14.1.el7 will be an update
...
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package               Arch        Version                   Repository    Size
================================================================================
Installing:
 kernel                x86_64      3.10.0-229.14.1.el7       updates       31 M
 kernel-devel          x86_64      3.10.0-229.14.1.el7       updates      9.9 M
```

Try installing kernel-devel in OS image and see if open-vm-tools will be happy. Test this by using older OS image version.

may be add a test to make sure that only one kernel image is present at the end of stemcell building.",,103719482,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Installing open-vm-tools in stemcell should not pull latest kernel,1779224.0,[1779224],956238,553935,chore,2015-09-28T20:37:20Z,https://www.pivotaltracker.com/story/show/103719482
2015-09-28T23:33:52Z,2015-07-20T19:28:17Z,accepted,,"- make sure that stdout/stderr are not in memory.  We should set stderr/stdout to file handles: https://github.com/cloudfoundry/bosh-agent/blob/master/agent/cmdrunner/file_logging_cmd_runner.go#L90-L98
- do the same logging as we do for compilation (/var/vcap/sys/log/job/pre-start.log), but
  - do not delete all other logs in that directory (or remove previous pre-start.log) -> APPEND
  - use the name of the script for the log filenames
  - single log file vs stderr/stdout.log?
- show names of jobs that failed in the error in addition to the count
- do not include any output in the error",2.0,99450648,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should see some helpful logging for failed pre-start scripts,948679.0,"[948679, 631325]",956238,81882,feature,2015-09-28T23:33:52Z,https://www.pivotaltracker.com/story/show/99450648
2015-09-28T23:35:01Z,2015-09-15T01:21:35Z,accepted,,"see https://www.pivotaltracker.com/story/show/101724116 for more context
- use object_exists? on the blobstore client to find out if we need to recreate rendered templates archive",1.0,103373852,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh deploy --recreate should succeed if rendered templates archives are missing,1386874.0,[1386874],956238,81882,feature,2015-09-28T23:35:02Z,https://www.pivotaltracker.com/story/show/103373852
2015-09-28T23:58:01Z,2015-09-21T20:56:42Z,accepted,,,0.0,103849856,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Investigate runsvdir failure to start monit,553935.0,"[553935, 1779224]",956238,553935,feature,2015-09-28T23:58:01Z,https://www.pivotaltracker.com/story/show/103849856
2015-09-29T23:11:42Z,2015-09-29T16:21:13Z,accepted,,"  http://www.ubuntu.com/usn/usn-2751-1
expected:   linux-image-3.19.0-30-generic   3.19.0-30.33~14.04.1",1.0,104449794,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2751-1] bump trusty for Linux kernel (Vivid HWE) vulnerabilities,1550486.0,"[1550486, 344]",956238,81882,feature,2015-09-29T23:11:53Z,https://www.pivotaltracker.com/story/show/104449794
2015-09-29T23:14:31Z,2015-09-01T01:11:22Z,accepted,,aws instance profile credentials. in aws-sdk-go it's called ec2 role provider: https://github.com/aws/aws-sdk-go/blob/master/aws/credentials/ec2rolecreds/ec2_role_provider.go,2.0,102434808,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'aws-iam', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-14T22:31:43Z', 'id': 12765408, 'updated_at': '2015-09-14T22:32:08Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",allow s3cli to use instance profile provided credentials,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-09-29T23:14:31Z,https://www.pivotaltracker.com/story/show/102434808
2015-09-29T23:14:42Z,2015-09-15T17:18:58Z,accepted,,https://github.com/cloudfoundry/bosh/pull/941/files,2.0,103429904,story,"[{'name': 'aws-iam', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-14T22:31:43Z', 'id': 12765408, 'updated_at': '2015-09-14T22:32:08Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in director/blobstore/registry support for credential_source for aws creds,1426194.0,"[1426194, 1779224]",956238,81882,feature,2015-09-29T23:14:42Z,https://www.pivotaltracker.com/story/show/103429904
2015-09-30T18:56:09Z,2015-09-30T16:41:59Z,accepted,,,,104549866,story,[],Bosh load tests should show debug logs when deploy fails,553935.0,"[553935, 1779224]",956238,553935,chore,2015-09-30T18:56:09Z,https://www.pivotaltracker.com/story/show/104549866
2015-09-30T23:10:23Z,2015-09-23T21:06:43Z,accepted,,"- via set_vm_metadata
- stll include index",1.0,104037838,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",include id in vm metadata,1426194.0,"[1426194, 1779224]",956238,81882,feature,2015-09-30T23:10:23Z,https://www.pivotaltracker.com/story/show/104037838
2015-09-30T23:13:35Z,2015-09-26T00:45:46Z,accepted,,"currently shows: 
D, [2015-09-26 00:44:23 #8189] [task:14317] DEBUG -- DirectorJobRunner: Need to update instance 'db/0', changes: #<Set:0x007fee865f13e8>

Set is useless, should be comman separated list",0.0,104222772,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",better debug log for 'Need to update instance',553935.0,[553935],956238,81882,feature,2015-09-30T23:13:35Z,https://www.pivotaltracker.com/story/show/104222772
2015-10-01T00:36:37Z,2015-09-30T01:18:10Z,accepted,,apply spec will include new bootstrap,1.0,104494250,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Changing instance bootstrap should update instance,553935.0,"[553935, 1017727]",956238,553935,feature,2015-10-01T00:36:37Z,https://www.pivotaltracker.com/story/show/104494250
2015-10-01T00:38:27Z,2015-09-30T23:07:13Z,accepted,,,,104584902,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",flaky on global-net spec/integration/hm_stateful_spec.rb:13,553935.0,"[553935, 1779224]",956238,553935,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/104584902
2015-10-01T01:08:47Z,2015-08-10T19:09:13Z,accepted,,,2.0,100968824,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",spec.bootstrap should not be re-assigned if bootstrap VM is missing (e.g. terminate VM from iaas + cck),1017727.0,"[1017727, 1779224]",956238,81882,feature,2015-10-01T01:08:47Z,https://www.pivotaltracker.com/story/show/100968824
2015-10-01T01:08:55Z,2015-08-26T17:33:35Z,accepted,,"When you first deploy a bosh director with microbosh or bosh-init the VM has the blobstore locally instead of it being in s3 bucket. This can cause a out of memory error if there is too much in the blobstore. 

Change agent's builtin blobstore http server to stream data instead. memory consumption should be better.

There is fs.OpenFile which return io.Reader/io.Writer

relevant:
- https://github.com/cloudfoundry/bosh-agent/blob/585d2cc3a47129aa875738f09a26101ec6e0b1d1/micro/https_handler.go
- https://github.com/cloudfoundry/bosh-utils/blob/master/blobstore/blob_manager.go",2.0,102091246,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should not load full blob into memory,1426194.0,"[1426194, 1655862]",956238,81882,feature,2015-10-01T01:09:07Z,https://www.pivotaltracker.com/story/show/102091246
2015-10-01T02:03:42Z,2015-07-30T20:42:50Z,accepted,,"if job was in [z1, z2] and z1 is removed first instance of the z2 should be picked as bootstrap. if i redeploy with z1 restored, bootstrap instance should remain in z2 (save bootstrap on instance record).",2.0,100245974,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Release author should be able to maintain the same bootstrap node after rebalancing AZs (spec.bootstrap should be re-assigned if bootstrap instance is removed (e.g. if one AZ is removed)),1783496.0,"[1783496, 1017727, 676405]",956238,81882,feature,2015-10-01T02:03:42Z,https://www.pivotaltracker.com/story/show/100245974
2015-10-01T18:58:52Z,2015-09-30T18:46:34Z,accepted,,"http://www.ubuntu.com/usn/usn-2756-1/
  rpcbind                         0.2.1-2ubuntu2.2",1.0,104560886,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2756-1] bump ubuntu/centos for rpcbind vulnerability,1550486.0,[1550486],956238,81882,feature,2015-10-01T18:58:53Z,https://www.pivotaltracker.com/story/show/104560886
2015-10-01T19:28:46Z,2015-10-01T19:27:43Z,accepted,,"CPI team needs to have floating dependencies so that they can update their releases 
",,104660350,story,[],"bosh registry should not lock fog, fog-aws and aws-sdk",553935.0,[553935],956238,553935,chore,2015-10-01T19:28:46Z,https://www.pivotaltracker.com/story/show/104660350
2015-10-01T21:05:32Z,2015-09-28T17:57:51Z,accepted,,,,104348724,story,[],Configure agent pipeline to promote to master from develop,553935.0,"[553935, 1779224]",956238,553935,chore,2015-10-01T21:05:33Z,https://www.pivotaltracker.com/story/show/104348724
2015-10-01T21:10:00Z,2015-09-30T21:00:50Z,accepted,,,,104576206,story,[],Check that we don't leave artifacts in openstack,553935.0,"[553935, 1779224]",956238,553935,chore,2015-10-01T21:10:00Z,https://www.pivotaltracker.com/story/show/104576206
2015-10-01T21:45:31Z,2015-09-23T18:46:12Z,accepted,,,4.0,104026004,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",dynamic/static subnets can belong to multiple azs,344.0,"[344, 676405]",956238,81882,feature,2015-10-01T21:45:31Z,https://www.pivotaltracker.com/story/show/104026004
2015-10-01T22:13:01Z,2015-09-29T18:50:57Z,accepted,,"It is currently using the account for Dolores, which is not a great idea.",,104465912,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Reconfigure `bosh-agent` Concourse pipeline to use the appropriate AWS account,553935.0,[553935],956238,1550486,chore,2015-10-01T22:13:01Z,https://www.pivotaltracker.com/story/show/104465912
2015-10-01T22:14:33Z,2015-09-01T22:48:13Z,accepted,,"investigate what happen when gateway is involved. 

be backwards compatible at all levels.

bosh ssh should receive expected host fingerprint from the director which in turn should receive it from the agent. based on that value configure interactive/non-interactive ssh/scp to use that host fingerprint. should work for ubuntu/centos boxes.",4.0,102530088,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should be able to ssh into a recreated vm without being prompted about host fingerprint,1224348.0,[1224348],956238,81882,feature,2015-10-01T22:14:33Z,https://www.pivotaltracker.com/story/show/102530088
2015-10-01T22:14:52Z,2015-09-14T19:11:24Z,accepted,,"generate ephemeral key - is it slow?
ideally would not touch the disk",2.0,103347672,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should not have to have any ssh keys on their system when trying to bosh ssh into vms,1224348.0,[1224348],956238,81882,feature,2015-10-01T22:14:52Z,https://www.pivotaltracker.com/story/show/103347672
2015-10-01T22:40:52Z,2015-08-18T21:56:39Z,accepted,,"- make sure to properly clean up dns addresses after instance deleting (scaling, delete deployment, delete job)
- id.net.job.deployment.bosh
- is used in links in address field with index dns name
  -> switch using id based dns record in a link",4.0,101542644,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",director should create/delete dns entries for instances that use id in addition to creating ones with index,344.0,"[344, 1783496]",956238,81882,feature,2015-10-01T22:40:52Z,https://www.pivotaltracker.com/story/show/101542644
2015-10-01T22:41:00Z,2015-08-03T17:31:58Z,accepted,,"when we're recreating an instance using cloudcheck_helper, use the right cloud properties and network subnets based on saved AZ",2.0,100439216,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",when health monitor/cloud check recreates a vm the AZ stays the same,687691.0,"[687691, 1779224]",956238,687691,feature,2015-10-01T22:41:00Z,https://www.pivotaltracker.com/story/show/100439216
2015-10-01T23:38:20Z,2015-10-01T22:41:29Z,accepted,,,0.0,104675208,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",audit which instance properties are kept on instance vs vm (in the database),344.0,[344],956238,81882,feature,2015-10-01T23:38:20Z,https://www.pivotaltracker.com/story/show/104675208
2015-10-02T16:23:45Z,2015-09-18T00:06:39Z,accepted,,"Some tests fail with the following error. We need to add more debugging info like (what kind of job is running currently, that prevents resque to drain).

```
E, [2015-09-17T23:11:48.631356 #40048] ERROR : Resque queue failed to drain in 60 seconds. Resque.info: {:pending=>0,
 :processed=>9,
 :queues=>1,
 :workers=>3,
 :working=>1,
 :failed=>0,
 :servers=>[""redis://localhost:61600/0""],
 :environment=>""development""}

    exposes an availability zone for within the template spec for the instance (FAILED - 1)
```

https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-2.1-mysql/builds/72",,103642514,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Investigate resque drain failures,553935.0,[553935],956238,553935,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/103642514
2015-10-02T20:59:57Z,2015-09-24T21:35:15Z,accepted,,"kick off the request to the director, wait for the task to be accepted, do not wait for the task to finish.",1.0,104137744,story,"[{'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user using bosh ssh should not wait for ssh user to be deleted,1224348.0,"[1224348, 1441196, 659629]",956238,81882,feature,2015-10-02T20:59:58Z,https://www.pivotaltracker.com/story/show/104137744
2015-10-02T22:19:26Z,2015-09-17T22:59:23Z,accepted,,"Failing build: https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-1.9-postgres/builds/70

Some logs:
```
fails to delete release in use but deletes a different release

Failures:

  1) cli: cloudcheck properly delete VMs references for VMs with dead agents
     Failure/Error: fail(""Cloud check failed, output: #{output}"")
     RuntimeError:
       Cloud check failed, output: Performing cloud check...
       
       Director task 16
       Error 100: Unable to get deployment lock, maybe a deployment is in progress. Try again later.
       
       Task 16 error
       
       For a more detailed error report, run: bosh task 16 --debug
       
       Test directory: /tmp/build/f00f00b0-a155-47ea-47f0-cb4f22e440a9/bosh-src/tmp/integration-tests-workspace/pid-39737/spec-20150917-39737-w4qkv
       Sandbox directory: /tmp/build/f00f00b0-a155-47ea-47f0-cb4f22e440a9/bosh-src/tmp/integration-tests-workspace/pid-39737
     # ./spec/integration/cli_cck_spec.rb:90:in `bosh_run_cck_with_resolution'
     # ./spec/integration/cli_cck_spec.rb:44:in `block (2 levels) in <top (required)>'

Finished in 49 minutes 58 seconds (files took 3.38 seconds to load)
31 examples, 1 failure

Failed examples:

rspec ./spec/integration/cli_cck_spec.rb:32 # cli: cloudcheck properly delete VMs references for VMs with dead agents
```",,103639456,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Look into flaky cli_cck_spec integration test,553935.0,"[553935, 1813272]",956238,1783496,chore,2015-10-08T17:41:47Z,https://www.pivotaltracker.com/story/show/103639456
2015-10-02T22:33:28Z,2015-07-13T19:42:39Z,accepted,,"The ""root"" group is a highly-privileged group. Furthermore, the group-owner of this file should not have any access privileges anyway.
---
None
---
SV-50382r1_rule
---
F-43529r1_fix
---
The file ""/etc/grub.conf"" should be group-owned by the ""root"" group to prevent destruction or modification of the file. To properly set the group owner of ""/etc/grub.conf"", run the command: 

# chgrp root /etc/grub.conf
---
C-46139r1_chk
---
To check the group ownership of ""/etc/grub.conf"", run the command: 

$ ls -lL /etc/grub.conf

If properly configured, the output should indicate the following group-owner. ""root"" 
If it does not, this is a finding.",1.0,98978094,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38581] [medium] The system boot loader configuration file(s) must be group-owned by root,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-10-02T22:33:29Z,https://www.pivotaltracker.com/story/show/98978094
2015-10-02T22:33:33Z,2015-07-13T19:42:44Z,accepted,,"Password protection on the boot loader configuration ensures users with physical access cannot trivially alter important bootloader settings. These include which kernel to use, and whether to enter single-user mode.
---
None
---
SV-50386r1_rule
---
F-43533r1_fix
---
The grub boot loader should have password protection enabled to protect boot-time settings. To do so, select a password and then generate a hash from it by running the following command: 

# grub-crypt --sha-512

When prompted to enter a password, insert the following line into ""/etc/grub.conf"" immediately after the header comments. (Use the output from ""grub-crypt"" as the value of [password-hash]): 

password --encrypted [password-hash]
---
C-46143r1_chk
---
To verify the boot loader password has been set and encrypted, run the following command: 

# grep password /etc/grub.conf

The output should show the following: 

password --encrypted ""$6$[rest-of-the-password-hash]""


If it does not, this is a finding.",1.0,98978300,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38585] [medium] The system boot loader must require authentication,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-10-02T22:33:33Z,https://www.pivotaltracker.com/story/show/98978300
2015-10-02T22:33:35Z,2015-07-13T19:42:43Z,accepted,,"Only root should be able to modify important boot parameters.
---
None
---
SV-50380r1_rule
---
F-43527r1_fix
---
The file ""/etc/grub.conf"" should be owned by the ""root"" user to prevent destruction or modification of the file. To properly set the owner of ""/etc/grub.conf"", run the command: 

# chown root /etc/grub.conf
---
C-46137r1_chk
---
To check the ownership of ""/etc/grub.conf"", run the command: 

$ ls -lL /etc/grub.conf

If properly configured, the output should indicate the following owner: ""root"" 
If it does not, this is a finding.",1.0,98978246,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38579] [medium] The system boot loader configuration file(s) must be owned by root,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-10-02T22:33:37Z,https://www.pivotaltracker.com/story/show/98978246
2015-10-02T22:33:39Z,2015-07-13T19:42:45Z,accepted,,"Proper permissions ensure that only the root user can modify important boot parameters.
---
None
---
SV-50384r2_rule
---
F-43531r2_fix
---
File permissions for ""/boot/grub/grub.conf"" should be set to 600, which is the default. To properly set the permissions of ""/boot/grub/grub.conf"", run the command:

# chmod 600 /boot/grub/grub.conf

Boot partitions based on VFAT, NTFS, or other non-standard configurations may require alternative measures.
---
C-46141r2_chk
---
To check the permissions of /etc/grub.conf, run the command:

$ sudo ls -lL /etc/grub.conf

If properly configured, the output should indicate the following permissions: ""-rw-------""
If it does not, this is a finding. ",1.0,98978312,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38583] [medium] The system boot loader configuration file(s) must have mode 0600 or less permissive,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-10-02T22:33:41Z,https://www.pivotaltracker.com/story/show/98978312
2015-10-02T22:53:01Z,2015-08-01T00:46:19Z,accepted,,"`bosh instances --ps`  command should show details about each instance's processes 

Modify the cli to display data from bosh director. 

better formatting: https://github.com/cloudfoundry/bosh-notes/blob/master/cli-deployment-resources.md#processes

- process names are indented by two spaces under an instance name
- if --ps is specified each instance is separated by the bottom row border
- state for each process should be in a state column
- use processes array returned by the director

```
Task 21942 done

+------------------------------------+---------+---------------+--------------+
| Job/index                          | State   | Resource Pool | IPs          |
+------------------------------------+---------+---------------+--------------+
| router_z2/1                        | failing | router_z2     | 10.10.80.16  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| runner_z1/0                        | failing | runner_z1     | 10.10.17.31  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| runner_z2/3                        | failing | runner_z2     | 10.10.81.3   |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| stats_z1/0                         | failing | small_z1      | 10.10.17.21  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| uaa_z1/0                           | failing | medium_z1     | 10.10.17.22  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| uaa_z2/0                           | failing | medium_z2     | 10.10.81.18  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
```",2.0,100344680,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",Add additional output to `bosh instances` to display processes for each instance,1495236.0,"[1495236, 1386874]",956238,1482982,feature,2015-10-02T22:53:02Z,https://www.pivotaltracker.com/story/show/100344680
2015-10-02T23:00:59Z,2015-09-29T19:27:06Z,accepted,,"example, httpclient does not close connections:

lsof -i shows many connections

```
require 'httpclient'
100.times do

c = HTTPClient.new
c.ssl_config.verify_mode = OpenSSL::SSL::VERIFY_NONE
c.ssl_config.verify_callback = Proc.new {}
c.get_content('https://google.com')
sleep 4
end
```",,104469472,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",bosh cli should not keep around connections to the director,1779224.0,"[1779224, 1783496]",956238,81882,bug,2015-10-02T23:00:59Z,https://www.pivotaltracker.com/story/show/104469472
2015-10-02T23:05:15Z,2015-08-27T21:05:41Z,accepted,,- make sure this applies for all commands,2.0,102211582,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",--ca-cert cli flag on target should trigger verification for both director and uaa,553935.0,"[553935, 1655862, 1783496]",956238,81882,feature,2015-10-02T23:05:16Z,https://www.pivotaltracker.com/story/show/102211582
2015-10-02T23:27:25Z,2015-07-23T21:25:40Z,accepted,,,,99741218,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",director has basic *first class* AZ support,,[],956238,81882,release,2015-10-02T23:27:25Z,https://www.pivotaltracker.com/story/show/99741218
2015-10-03T00:19:41Z,2015-09-11T18:20:12Z,accepted,,"- do not delete the old stemcell
- call create_stemcell for a new stemcell
- save the stemcell cid into existing entry",1.0,103218714,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can run upload stemcell --fix which will import stemcell into iaas and replace db entry with new stemcell id,1386874.0,[1386874],956238,81882,feature,2015-10-03T00:19:41Z,https://www.pivotaltracker.com/story/show/103218714
2015-10-05T00:54:23Z,2015-09-15T20:54:06Z,accepted,,"should be consistent with ephemeral_disk option (http://bosh.io/docs/aws-cpi.html#resource-pools)

```
  cloud_properties:
    root_disk:
      type: gp2
      size: 10_000 # in megabytes
```",2.0,103448958,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'root-disk1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:15:09Z', 'id': 12776752, 'updated_at': '2015-09-15T21:15:09Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",allow root_disk configuration in resource pool cloud properties for aws machines,659629.0,"[659629, 1355110, 1441196]",956238,81882,feature,2015-10-05T00:54:25Z,https://www.pivotaltracker.com/story/show/103448958
2015-10-05T05:25:24Z,2014-05-05T18:12:47Z,accepted,,"- run drain scripts from all job templates in parallel
- depending on result value of the drain script follow dynamic or static steps (see director for existing behavior)
- return 0 from drain action if everything succeeds
- return err if any script failed
- even if one drain script failed continue running other drain scripts until they all finish, but then return err",4.0,70697490,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'drain', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623538, 'updated_at': '2014-06-06T18:53:38Z'}, {'name': 'drain1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T08:09:20Z', 'id': 9116420, 'updated_at': '2014-08-05T08:09:20Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh agent should drain multiple job templates,1541728.0,"[1541728, 631325]",956238,1210852,feature,2015-10-05T05:25:25Z,https://www.pivotaltracker.com/story/show/70697490
2015-10-05T05:36:26Z,2015-08-19T20:13:11Z,accepted,,"```
resource_pools:
- name: blah
  network: blah
  stemcell: { ... }
  cloud_properties:
    ephemeral_disk:
      type: gp2
      size: 20_000
    raw_instance_storage: true # <-----------
```

- if raw_instance_storage==true then we should create a separate ebs disk to hold bosh's ephemeral data (we already do that sometimes).
- if no ephemeral disk config is specified the add it with default size of 10gb/gp2 (we already this for t2.micro and others)
- possible configuration between CPI and agent:

```
""disks"" => {
   ""system"" => ""/dev/sda"",
   ""ephemeral"" => ""/dev/sdb"",
   ""persistent"" => { ... }
   ??? # <------------ raw ephemeral volumes
}
```",4.0,101628202,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can request access to all raw ephemeral storage on aws,1541728.0,"[1541728, 1224348]",956238,81882,feature,2015-10-05T05:36:27Z,https://www.pivotaltracker.com/story/show/101628202
2015-10-05T05:36:59Z,2015-09-16T01:48:07Z,accepted,,,,103465308,story,"[{'name': 'drain1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T08:09:20Z', 'id': 9116420, 'updated_at': '2014-08-05T08:09:20Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",multiple jobs are drained in parallel,,[],956238,81882,release,2015-10-05T05:36:59Z,https://www.pivotaltracker.com/story/show/103465308
2015-10-05T05:51:51Z,2015-08-20T22:10:23Z,accepted,,"- bosh deploy one vm
- go into the blobstore and delete rendered templates archive (db table should have blobstore_id)
- bosh deploy --recreate: currently fails because blobstore returns 404 not found

to do: ignore 404 not found errors when deleting rendered template archives (there is I believe Bosh::Blobstore::NotFound error)

https://github.com/cloudfoundry/bosh/issues/927",2.0,101724116,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh delete deployment should succeed if rendered templates archives are missing,1386874.0,[1386874],956238,81882,feature,2015-10-05T05:51:51Z,https://www.pivotaltracker.com/story/show/101724116
2015-10-05T06:54:38Z,2015-09-24T21:55:10Z,accepted,,"  - keep supporting disk_pools
  - persistent_disk_pool on a job can be specified as persistent_disk_type",2.0,104139026,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can specify disk_pools as disk_types,1426194.0,"[1426194, 1783496]",956238,81882,feature,2015-10-05T06:54:38Z,https://www.pivotaltracker.com/story/show/104139026
2015-10-05T06:54:43Z,2015-09-24T21:55:20Z,accepted,,"  - error if alias is duplicate
  - required: alias, version, os|name",2.0,104139040,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can specify stemcells in deployment manifest with a unique alias,1779224.0,"[1779224, 1751936]",956238,81882,feature,2015-10-05T06:54:43Z,https://www.pivotaltracker.com/story/show/104139040
2015-10-05T06:54:58Z,2015-09-24T21:56:08Z,accepted,,"  - error if name is duplicate
  - cloud_properties is optional",1.0,104139086,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can specify vm_types in cloud config,1426194.0,"[1426194, 1783496]",956238,81882,feature,2015-10-05T06:54:58Z,https://www.pivotaltracker.com/story/show/104139086
2015-10-05T18:15:46Z,2015-09-15T17:42:17Z,accepted,,"Example: https://github.com/cppforlife/bosh-warden-cpi-release/blob/master/update-deps
try the following vendor solution http://engineeredweb.com/blog/2015/go-1.5-vendor-handling/",,103431710,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",update go to 1.5 & switch bosh-agent vendor tool (10 line bash script),553935.0,"[553935, 1779224, 1813272]",956238,1426194,chore,2015-10-05T18:15:47Z,https://www.pivotaltracker.com/story/show/103431710
2015-10-05T22:24:06Z,2015-07-13T21:58:20Z,accepted,,,,98991294,story,"[{'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","extra hardening of stemcell - part 2 - ssh, boot, bluetooth, /etc",,[],956238,81882,release,2015-10-05T22:24:06Z,https://www.pivotaltracker.com/story/show/98991294
2015-10-06T16:39:35Z,2015-10-06T01:48:35Z,accepted,,we fixed stemcell cleaner. but toolsmiths report we are running out of space. check what is happening,,104963404,story,[],Clean up old stemcells on giraffe,553935.0,[553935],956238,553935,chore,2015-10-06T16:39:36Z,https://www.pivotaltracker.com/story/show/104963404
2015-10-06T18:16:31Z,2015-10-05T23:22:58Z,accepted,,"  http://www.ubuntu.com/usn/usn-2765-1
linux-image-3.19.0-30-generic 3.19.0-30.34~14.04.1",0.0,104957018,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",[USN-2765-1] bump ubuntu for Linux kernel (Vivid HWE) vulnerability,553935.0,"[553935, 1783496]",956238,81882,feature,2015-10-06T18:16:32Z,https://www.pivotaltracker.com/story/show/104957018
2015-10-06T22:32:37Z,2015-10-02T16:23:19Z,accepted,,"https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-2.1-mysql/builds/168

```
Failures:

  1) simultaneous deploys when there are enough IPs for two deployments with compilation allocates different IP to another deploy
     Failure/Error: expect(first_deployment_ips + second_deployment_ips).to match_array(
       expected collection contained:  [""192.168.1.2"", ""192.168.1.3""]
       actual collection contained:    [""192.168.1.2"", ""192.168.1.4""]
       the missing elements were:      [""192.168.1.3""]
       the extra elements were:        [""192.168.1.4""]
       
       Test directory: /tmp/build/ab19d2f5-5d55-45f7-7406-ac26ad677bef/bosh-src/tmp/integration-tests-workspace/pid-40064/spec-20151002-40064-1f9h6nc
       Sandbox directory: /tmp/build/ab19d2f5-5d55-45f7-7406-ac26ad677bef/bosh-src/tmp/integration-tests-workspace/pid-40064
     # ./spec/integration/global_networking/simultaneous_deploys_spec.rb:36:in `block (3 levels) in <top (required)>'

Finished in 37 minutes 26 seconds (files took 0.81582 seconds to load)
37 examples, 1 failure

Failed examples:

rspec ./spec/integration/global_networking/simultaneous_deploys_spec.rb:22 # simultaneous deploys when there are enough IPs for two deployments with compilation allocates different IP to another deploy
```",,104736594,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Investigate flaky mysql test in global-net,553935.0,"[553935, 1783496]",956238,553935,chore,2015-10-08T17:42:02Z,https://www.pivotaltracker.com/story/show/104736594
2015-10-07T01:44:14Z,2015-10-02T22:12:47Z,accepted,,,,104767856,story,[],investigate missing concourse director,1550486.0,[1550486],956238,1550486,chore,2015-10-07T01:44:14Z,https://www.pivotaltracker.com/story/show/104767856
2015-10-07T19:27:38Z,2015-10-05T18:01:39Z,accepted,,,,104925914,story,[],Add git commit hook to generate URL for story number,1813272.0,"[1813272, 1017727]",956238,1017727,chore,2015-10-07T19:27:38Z,https://www.pivotaltracker.com/story/show/104925914
2015-10-08T17:16:11Z,2015-10-07T18:54:42Z,accepted,,,,105132052,story,[],fix s3cli to work with swift after goamz upgrades,81882.0,[81882],956238,81882,bug,2015-10-08T17:16:13Z,https://www.pivotaltracker.com/story/show/105132052
2015-10-09T18:07:17Z,2015-10-06T16:49:34Z,accepted,,it has caching,,105019100,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Update concourse to 0.65.1,1550486.0,[1550486],956238,553935,chore,2015-10-09T18:07:17Z,https://www.pivotaltracker.com/story/show/105019100
2015-10-09T19:11:15Z,2015-10-07T17:11:28Z,accepted,,,1.0,105121972,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",lock down net-ssh to 2.9.2 so that cli can work on ruby1.9.3,344.0,[344],956238,81882,feature,2015-10-09T19:11:26Z,https://www.pivotaltracker.com/story/show/105121972
2015-10-11T19:00:00Z,2015-08-28T17:48:52Z,accepted,,"1. bootstrap node
2. instances in alphanum in bootstrap's az  (job_name+id)
3. az in alphanum (name)
4. instances in each remaining az in alphanum (job_name+id)",4.0,102272798,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",instances should always be updated in the same order,1017727.0,"[1017727, 1783496, 1813272]",956238,81882,feature,2015-10-12T08:07:37Z,https://www.pivotaltracker.com/story/show/102272798
2015-10-12T01:48:59Z,2015-09-15T00:53:15Z,accepted,,"- blow up if keys in migrated_jobs section contain az names that are not specified in availability_zones key
- blow up if job names do not exist in the db in the instances table or are not in migrated section for the job (decided not to blow up on this)
- if jobs names are in the current manifest -> blow up (cannot migrate if still trying to use them)
- if prev migrated job name is used in the manifest again, as long as there are no migrated_jobs sections that use that name, it's ok (just a normal new job)",2.0,103373048,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user should see validation errors when trying to migrate jobs in an invalid way,553935.0,[553935],956238,81882,feature,2015-10-12T01:48:59Z,https://www.pivotaltracker.com/story/show/103373048
2015-10-12T01:49:30Z,2015-09-01T22:24:18Z,accepted,,"```
D, [2015-09-01 22:20:28 #19842] [task:4223] DEBUG -- DirectorJobRunner: Allocating dynamic ip for manual network 'private'
D, [2015-09-01 22:20:28 #19842] [task:4223] DEBUG -- DirectorJobRunner: Trying to allocate a dynamic IP in subnet'#<Bosh::Director::DeploymentPlan::ManualNetworkSubnet:0x007efd3fd0c878 @network=#<Bosh::Director::DeploymentPlan::ManualNetwork:0x007efd3fd0f168 @name=""private"", @canonical_name=""private"", @logger=#<Bosh::Director::TaggedLogger:0x007efd3fcb90d8 @logger=<Logging::Logger:0x3f7e9fef7f60 name=""DirectorJobRunner"">, @tags=""[network-configuration]"">, @subnets=[#<Bosh::Director::DeploymentPlan::ManualNetworkSubnet:0x007efd3fd0c878 ...>, #<Bosh::Director::DeploymentPlan::ManualNetworkSubnet:0x007efd3fce6650 @network=#<Bosh::Director::DeploymentPlan::ManualNetwork:0x007efd3fd0f168 ...>, @range=#<NetAddr::CIDRv4:0x007efd3fce5a20 @ip=168443904, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967040, @network=168443904, @hostmask=255, @tag={}, @wildcard_mask=4294967040>, @netmask=""255.255.255.0"", @gateway=#<NetAddr::CIDRv4:0x007efd3fce5020 @ip=168443905, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967295, @network=168443905, @hostmask=0, @tag={}, @wildcard_mask=4294967295>, @dns=[""10.10.0.2""], @availability_zone=""z2"", @cloud_properties={""subnet""=>""subnet-eb8bd3ad""}, @ip_provider=#<Bosh::Director::DeploymentPlan::DatabaseIpProvider:0x007efd3fcbab18 @range=#<NetAddr::CIDRv4:0x007efd3fce5a20 @ip=168443904, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967040, @network=168443904, @hostmask=255, @tag={}, @wildcard_mask=4294967040>, @network_name=""private"", @network_desc=""network 'private' (10.10.64.0/24)"", @restricted_ips=#<Set: {168443905, 168443904, 168444159, 168443906, 168443907, 168443908, 168443909, 168443910, 168443911, 168443912, 168443913, 168443914, 168443915, 168443916, 168443917, 168443918, 168443919, 168443920, 168443921, 168443922, 168443923, 168443924, 168443925, 168443926, 168443927, 168443928, 168443929, 168443930, 168443931, 168443932, 168443933, 168443934, 168443935, 168443936, 168443937, 168443938, 168443939, 168443940, 168443941, 168443942, 168443943, 168443944, 168443945, 168443946, 168443947, 168443948, 168443949, 168443950, 168443951, 168443952, 168443953, 168443954, 168443955, 168443956, 168443957, 168443958, 168443959, 168443960, 168443961, 168443962, 168443963, 168443964, 168443965, 168443966, 168443967, 168443968, 168443969, 168443970, 168443971, 168443972, 168443973, 168443974, 168443975, 168443976, 168443977, 168443978, 168443979, 168443980, 168443981, 168443982, 168443983, 168443984, 168443985, 168443986, 168443987, 168443988, 168443989, 168443990, 168443991, 168443992, 168443993, 168443994, 168443995, 168443996, 168443997, 168443998, 168443999, 168444000, 168444001, 168444002, 168444003, 168444004, 168444005, 168444006, 168444007, 168444008, 168444009, 168444010, 168444011, 168444012, 168444013, 168444014, 168444015, 168444016, 168444017, 168444018, 168444019, 168444020, 168444021, 168444022, 168444023, 168444024, 168444034, 168444035, 168444036, 168444037, 168444038, 168444039, 168444040, 168444041, 168444042, 168444043, 168444044, 168444045, 168444046, 168444047, 168444048, 168444049, 168444050, 168444051, 168444052, 168444053, 168444054, 168444055, 168444056, 168444057, 168444058, 168444059, 168444060, 168444061, 168444062, 168444063, 168444064, 168444065, 168444066, 168444067, 168444068, 168444069, 168444070, 168444071, 168444072, 168444073, 168444074, 168444075, 168444076, 168444077, 168444078, 168444079, 168444080, 168444081, 168444082, 168444083, 168444084, 168444085, 168444086, 168444087, 168444088, 168444089, 168444090, 168444091, 168444092, 168444093, 168444094, 168444095, 168444096, 168444097, 168444098, 168444099, 168444100, 168444101, 168444102, 168444103, 168444104, 168444105, 168444106, 168444107, 168444108, 168444109, 168444110, 168444111, 168444112, 168444113, 168444114, 168444115, 168444116, 168444117, 168444118, 168444119, 168444120, 168444121, 168444122, 168444123, 168444124, 168444125, 168444126, 168444127, 168444128, 168444129, 168444130, 168444131, 168444132, 168444133, 168444134, 168444135, 168444136, 168444137, 168444138, 168444139, 168444140, 168444141, 168444142, 168444143, 168444144, 168444145, 168444146, 168444147, 168444148, 168444149, 168444150, 168444151, 168444152, 168444153, 168444154, 168444155, 168444156, 168444157, 168444158}>, @static_ips=#<Set: {}>, @logger=#<Bosh::Director::TaggedLogger:0x007efd3fcba8c0 @logger=<Logging::Logger:0x3f7e9fef7f60 name=""DirectorJobRunner"">, @tags=""[network-configuration][database-ip-provider]"">>>], @default_subnet=#<Bosh::Director::DeploymentPlan::ManualNetworkSubnet:0x007efd3fcba6e0 @network=#<Bosh::Director::DeploymentPlan::ManualNetwork:0x007efd3fd0f168 ...>, @range=#<NetAddr::CIDRv4:0x007efd3fcb9e70 @ip=0, @version=4, @address_len=32, @all_f=4294967295, @netmask=0, @network=0, @hostmask=4294967295, @tag={}, @wildcard_mask=0>, @netmask=""0.0.0.0"", @gateway=#<NetAddr::CIDRv4:0x007efd3fcb97b8 @ip=1, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967295, @network=1, @hostmask=0, @tag={}, @wildcard_mask=4294967295>, @dns=nil, @availability_zone=nil, @cloud_properties={}, @ip_provider=#<Bosh::Director::DeploymentPlan::DatabaseIpProvider:0x007efd3fcb94c0 @range=#<NetAddr::CIDRv4:0x007efd3fcb9e70 @ip=0, @version=4, @address_len=32, @all_f=4294967295, @netmask=0, @network=0, @hostmask=4294967295, @tag={}, @wildcard_mask=0>, @network_name=""private"", @network_desc=""network 'private' (0.0.0.0/0)"", @restricted_ips=#<Set: {1, 0, 4294967295}>, @static_ips=#<Set: {}>, @logger=#<Bosh::Director::TaggedLogger:0x007efd3fcb9290 @logger=<Logging::Logger:0x3f7e9fef7f60 name=""DirectorJobRunner"">, @tags=""[network-configuration][database-ip-provider]"">>>>, @range=#<NetAddr::CIDRv4:0x007efd3fd07d78 @ip=168427520, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967040, @network=168427520, @hostmask=255, @tag={}, @wildcard_mask=4294967040>, @netmask=""255.255.255.0"", @gateway=#<NetAddr::CIDRv4:0x007efd3fd076c0 @ip=168427521, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967295, @network=168427521, @hostmask=0, @tag={}, @wildcard_mask=4294967295>, @dns=[""10.10.0.2""], @availability_zone=""z1"", @cloud_properties={""subnet""=>""subnet-f2744a86""}, @ip_provider=#<Bosh::Director::DeploymentPlan::DatabaseIpProvider:0x007efd3fce6a10 @range=#<NetAddr::CIDRv4:0x007efd3fd07d78 @ip=168427520, @version=4, @address_len=32, @all_f=4294967295, @netmask=4294967040, @network=168427520, @hostmask=255, @tag={}, @wildcard_mask=4294967040>, @network_name=""private"", @network_desc=""network 'private' (10.10.0.0/24)"", @restricted_ips=#<Set: {168427521, 168427520, 168427775, 168427522, 168427523, 168427524, 168427525, 168427526, 168427527, 168427528, 168427529, 168427530, 168427531, 168427532, 168427533, 168427534, 168427535, 168427536, 168427537, 168427538, 168427539, 168427540, 168427541, 168427542, 168427543, 168427544, 168427545, 168427546, 168427547, 168427548, 168427549, 168427550, 168427551, 168427552, 168427553, 168427554, 168427555, 168427556, 168427557, 168427558, 168427559, 168427560, 168427561, 168427562, 168427563, 168427564, 168427565, 168427566, 168427567, 168427568, 168427569, 168427570, 168427571, 168427572, 168427573, 168427574, 168427575, 168427576, 168427577, 168427578, 168427579, 168427580, 168427581, 168427640, 168427641, 168427642, 168427643, 168427644, 168427645, 168427646, 168427647, 168427648, 168427649, 168427650, 168427651, 168427652, 168427653, 168427654, 168427655, 168427656, 168427657, 168427658, 168427659, 168427660, 168427661, 168427662, 168427663, 168427664, 168427665, 168427666, 168427667, 168427668, 168427669, 168427670, 168427671, 168427672, 168427673, 168427674, 168427675, 168427676, 168427677, 168427678, 168427679, 168427680, 168427681, 168427682, 168427683, 168427684, 168427685, 168427686, 168427687, 168427688, 168427689, 168427690, 168427691, 168427692, 168427693, 168427694, 168427695, 168427696, 168427697, 168427698, 168427699, 168427700, 168427701, 168427702, 168427703, 168427704, 168427705, 168427706, 168427707, 168427708, 168427709, 168427710, 168427711, 168427712, 168427713, 168427714, 168427715, 168427716, 168427717, 168427718, 168427719, 168427720, 168427721, 168427722, 168427723, 168427724, 168427725, 168427726, 168427727, 168427728, 168427729, 168427730, 168427731, 168427732, 168427733, 168427734, 168427735, 168427736, 168427737, 168427738, 168427739, 168427740, 168427741, 168427742, 168427743, 168427744, 168427745, 168427746, 168427747, 168427748, 168427749, 168427750, 168427751, 168427752, 168427753, 168427754, 168427755, 168427756, 168427757, 168427758, 168427759, 168427760, 168427761, 168427762, 168427763, 168427764, 168427765, 168427766, 168427767, 168427768, 168427769, 168427770, 168427771, 168427772, 168427773, 168427774}>, @static_ips=#<Set: {168427595, 168427596, 168427605, 168427606, 168427582, 168427584, 168427585, 168427586, 168427587, 168427588, 168427589, 168427620}>, @logger=#<Bosh::Director::TaggedLogger:0x007efd3fce6768 @logger=<Logging::Logger:0x3f7e9fef7f60 name=""DirectorJobRunner"">, @tags=""[network-configuration][database-ip-provider]"">>>'
D, [2015-09-01 22:20:28 #19842] [task:4223] DEBUG -- DirectorJobRunner: (0.000160s) SELECT NULL
```",1.0,102528706,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",improve logging output when allocating dynamic ip,344.0,[344],956238,81882,feature,2015-10-12T01:49:30Z,https://www.pivotaltracker.com/story/show/102528706
2015-10-12T02:33:30Z,2015-09-15T20:44:55Z,accepted,,"- lets enable this for now only for aws stemcells 
- for ubuntu and centos
- hopefully can be done as an isolated stage in stemcell builder",4.0,103448030,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'root-disk1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:15:09Z', 'id': 12776752, 'updated_at': '2015-09-15T21:15:09Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",grow root partition automatically on aws stemcells,659629.0,"[659629, 1355110]",956238,81882,feature,2015-10-12T02:33:31Z,https://www.pivotaltracker.com/story/show/103448030
2015-10-12T03:33:05Z,2015-09-24T21:55:27Z,accepted,,"  - pick first stemcell just like during export release
  - during bosh deploy (in binding stemcells stage)",2.0,104139050,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can see error message if os/version combo does not match any uploaded stemcell,1779224.0,"[1779224, 1783496]",956238,81882,feature,2015-10-12T03:33:05Z,https://www.pivotaltracker.com/story/show/104139050
2015-10-12T03:33:09Z,2015-09-24T21:55:45Z,accepted,,  - resolved in CLI just like for releases,4.0,104139064,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can use version: latest in stemcells section,1751936.0,"[1751936, 1779224, 1783496]",956238,81882,feature,2015-10-12T03:33:09Z,https://www.pivotaltracker.com/story/show/104139064
2015-10-12T04:22:37Z,2015-10-08T19:04:40Z,accepted,,"```
jobs:
- name: cloud_controller
  instances: 5
  stemcell: ...
  env: 
    bosh:
      password: ...
```

CPI in create_vm call should receive env hash for each instance in the job. if env changes we should recreate a vm.",0.0,105237766,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",accept agent env information from the job section,344.0,[344],956238,1779224,feature,2015-10-12T04:22:37Z,https://www.pivotaltracker.com/story/show/105237766
2015-10-12T04:22:42Z,2015-09-24T21:55:35Z,accepted,,,2.0,104139058,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can specify exact stemcell name to match,1779224.0,[1779224],956238,81882,feature,2015-10-12T04:22:42Z,https://www.pivotaltracker.com/story/show/104139058
2015-10-12T04:26:41Z,2015-09-15T20:54:38Z,accepted,,,,103449014,story,"[{'name': 'root-disk1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:15:09Z', 'id': 12776752, 'updated_at': '2015-09-15T21:15:09Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",aws machines can have bigger root disk,,[],956238,81882,release,2015-10-12T04:26:41Z,https://www.pivotaltracker.com/story/show/103449014
2015-10-12T04:31:14Z,2015-09-24T21:56:50Z,accepted,,"  - keep supporting resource_pool
  - vm_types+stemcell must be found",2.0,104139138,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",user can specify vm_type+stemcell instead of resource_pool on job,1426194.0,[1426194],956238,81882,feature,2015-10-12T04:31:14Z,https://www.pivotaltracker.com/story/show/104139138
2015-10-12T04:31:50Z,2015-09-24T21:57:04Z,accepted,,"- existence of a resource pool in cloud-config or dep man should raise an error when vm_types/stemcells are specified
- error if name/version does not match uploaded stemcell",1.0,104139162,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]", user can only specify either resource_pools or vm_types+stemcells,1426194.0,[1426194],956238,81882,feature,2015-10-12T04:31:50Z,https://www.pivotaltracker.com/story/show/104139162
2015-10-12T04:41:37Z,2015-08-19T20:23:10Z,accepted,,"- bosh-agent should label disks on bootstrap
- do not do anything to raw disks
- labels could be something like /dev/disk/by-X/bosh-ephemeral-1, /dev/disk/by-X/bosh-ephemeral-1, etc.",2.0,101629002,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",release author can find raw instance storage on the vm via disk labels on aws,659629.0,"[659629, 1355110]",956238,81882,feature,2015-10-12T04:41:38Z,https://www.pivotaltracker.com/story/show/101629002
2015-10-12T04:41:55Z,2015-08-19T19:52:56Z,accepted,,,,101626312,story,"[{'name': 'raw-eph', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-19T20:34:46Z', 'id': 12540628, 'updated_at': '2015-08-19T20:34:46Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",multiple ephemeral disks can be requested on aws,,[],956238,81882,release,2015-10-12T04:41:55Z,https://www.pivotaltracker.com/story/show/101626312
2015-10-12T05:02:10Z,2015-10-05T05:00:51Z,accepted,,"not sure what's going on here?
https://github.com/cloudfoundry/bosh/blob/2a4d3f813d9f2cb3fa8283230a27b2fcc5f59f4c/bosh-director/lib/bosh/director/api/controllers/deployments_controller.rb#L67
Add appropriate logging (+ test for it) to make state which jobs will be skipped.",,104861224,story,"[{'name': 'diego:ga', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-08T20:45:36Z', 'id': 12996482, 'updated_at': '2015-10-08T20:45:36Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'drain', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623538, 'updated_at': '2014-06-06T18:53:38Z'}]",do not always skip draining when stopping/restarting single job,1550486.0,"[1550486, 1813272]",956238,81882,bug,2015-10-12T05:02:11Z,https://www.pivotaltracker.com/story/show/104861224
2015-10-12T05:10:38Z,2015-10-05T18:17:45Z,accepted,,,2.0,104927662,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bosh instances/vms should show job name/index/bootstrap from DB not agent state to see correct information when agent is dead,553935.0,"[553935, 1783496]",956238,553935,feature,2015-10-12T05:10:38Z,https://www.pivotaltracker.com/story/show/104927662
2015-10-12T06:24:47Z,2015-09-24T21:57:18Z,accepted,,"  - for vm_types rely on cloud_properties (if names change, do not do anything)
  - if stemcell changes
  - switching between stemcell name vs os should not recreate if stemcell is the same
  - transition between res_pool -> vm_type/stemcell may result in recreation",4.0,104139178,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",VMs should be recreated if either stemcell or vm_type changes,1779224.0,[1779224],956238,81882,feature,2015-10-12T06:24:47Z,https://www.pivotaltracker.com/story/show/104139178
2015-10-12T06:46:32Z,2015-10-06T18:44:51Z,accepted,,,2.0,105029272,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]","when switching from using resource pool to vm_types/stemcells, vms should not be recreated",1779224.0,"[1779224, 344]",956238,81882,feature,2015-10-12T06:46:32Z,https://www.pivotaltracker.com/story/show/105029272
2015-10-12T06:46:35Z,2015-09-24T21:58:13Z,accepted,,,,104139226,story,"[{'name': 'cloud-config2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-24T21:58:20Z', 'id': 12864258, 'updated_at': '2015-09-24T21:58:20Z'}]",vm_types/disk_types are first class things,,[],956238,81882,release,2015-10-12T06:46:35Z,https://www.pivotaltracker.com/story/show/104139226
2015-10-13T00:42:51Z,2015-10-10T00:13:47Z,accepted,,Do we really need at least 4.0.0 now?,,105359524,story,[],Update README in bosh-stemcell for minimal version of vagrant berkshelf ,553935.0,[553935],956238,553935,chore,2015-10-13T00:42:51Z,https://www.pivotaltracker.com/story/show/105359524
2015-10-14T03:42:17Z,2015-10-02T23:35:34Z,accepted,,,1.0,104772954,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",add debug log for 'migrating job xx_z1/id (index) to xx/id (index)',1017727.0,[1017727],956238,81882,feature,2015-10-14T03:42:17Z,https://www.pivotaltracker.com/story/show/104772954
2015-10-14T19:45:10Z,2015-10-13T18:45:36Z,accepted,,"It's not clear how to build an HVM stemcell for AWS. Sometimes HVM stemcells are required and paravirtual will not suffice, so it would be useful for an easy-to-follow readme that shows how to build them.",,105596784,story,[],Update readme for stemcell builder to be more clear on HVM,659629.0,"[659629, 1441196]",956238,659629,chore,2015-10-14T19:45:10Z,https://www.pivotaltracker.com/story/show/105596784
2015-10-15T16:54:02Z,2015-10-05T18:58:46Z,accepted,,,,104932254,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",remove instance from db,1017727.0,"[1017727, 1813272, 1426194]",956238,81882,chore,2015-10-15T16:54:03Z,https://www.pivotaltracker.com/story/show/104932254
2015-10-15T17:39:26Z,2015-10-13T16:55:11Z,accepted,,"https://github.com/cloudfoundry/bosh/issues/980 (CVE-2015-5288, CVE-2015-5289)",1.0,105585496,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}]",update postgres to 9.0.23 (latest in 9.0.x),553935.0,"[553935, 1813272, 1115834]",956238,81882,feature,2015-10-15T17:43:17Z,https://www.pivotaltracker.com/story/show/105585496
2015-10-16T00:50:33Z,2015-10-01T22:38:42Z,accepted,,,,104675072,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",audit for NotImplemented error and comments from instance database file,1017727.0,[1017727],956238,81882,chore,2015-10-16T00:50:33Z,https://www.pivotaltracker.com/story/show/104675072
2015-10-16T22:47:57Z,2015-10-06T01:04:54Z,accepted,,"remove ' ! ' before parsing....

```
package main

import ""github.com/cloudfoundry/bosh-init/internal/gopkg.in/yaml.v2""

func main() {
	var man struct{}

	lol := `---
instances:
- :id: 1
  :name: micro-runtime1
  :uuid: bm-d0831cf3-cd6c-4e28-ab3a-5df5a2a0bdc9
  :stemcell_cid: ami-ba570bd2 light
  :stemcell_sha1: 2988c091528ffb18e8c1f61cfc2fe1c4b41b65f3
  :stemcell_name: light-bosh-stemcell-2859-aws-xen-hvm-ubuntu-trusty-go_agent
  :config_sha1: 0dead1752dc87ddc72fd3adf0b04588f6ea8c40e
  :vm_cid: i-1d6817e7
  :disk_cid: vol-c981e080
disks: []
registry_instances:
- :id: 1
  :instance_id: i-1d6817e7
  :settings: ! '{""vm"":{""name"":""...""}}'
`

	err := yaml.Unmarshal([]byte(lol), &man)
	if err != nil {
		panic(err.Error())
	}
}
```",,104962208,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",init should be able to migrate state files with !,1778582.0,"[1778582, 1783496]",956238,81882,bug,2015-10-16T22:48:08Z,https://www.pivotaltracker.com/story/show/104962208
2015-10-16T22:58:12Z,2015-08-12T15:54:15Z,accepted,,bootstrap == first not to update,,101133866,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",deployments are updated in a deterministic order according to bootstrap/ids/...,,[],956238,81882,release,2015-10-16T22:58:12Z,https://www.pivotaltracker.com/story/show/101133866
2015-10-17T00:20:15Z,2015-10-05T22:09:47Z,accepted,,"- can check in iaas that disks are still there
- no delete_disk cpi call should be made
- transact properly",2.0,104951640,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",user sees that persistent disks are not deleted during bosh deploy,344.0,"[344, 1813272]",956238,81882,feature,2015-10-17T00:20:15Z,https://www.pivotaltracker.com/story/show/104951640
2015-10-17T00:46:47Z,2015-10-14T00:28:33Z,accepted,,,2.0,105622386,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",bosh cck resolutions for inactive and missing disks should orphan disk,1813272.0,"[1813272, 1426194]",956238,553935,feature,2015-10-17T00:46:47Z,https://www.pivotaltracker.com/story/show/105622386
2015-10-17T00:48:33Z,2015-10-12T07:05:30Z,accepted,,"currently a stacktrace:

```
  Started preparing deployment > Binding existing deployment. Failed: undefined method `availability_zone_names' for nil:NilClass (00:00:00)
Error 100: undefined method `availability_zone_names' for nil:NilClass
```

```
D, [2015-10-12 07:03:22 #31281] [task:18001] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""efe75c51-4209-4186-bb2b-399a6f2431c4"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director 'f976642f-ade0-4d68-8660-2bba6817f10f': #<NoMethodError: undefined method `availability_zone_names' for nil:NilClass>"",""created_at"":1444633402}
E, [2015-10-12 07:03:22 #31281] [task:18001] ERROR -- DirectorJobRunner: undefined method `availability_zone_names' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:40:in `block in az_list_from_static_ips'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:38:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:38:in `az_list_from_static_ips'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:9:in `place_and_match_in'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/plan.rb:37:in `assign_zones'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/plan.rb:29:in `results'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/plan.rb:14:in `needed'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:13:in `plan_job_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:35:in `block (2 levels) in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:32:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:32:in `block in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:215:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:213:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:22:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner.rb:130:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:41:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:105:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",1.0,105440796,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can see error when subnet is not found for a static ip,344.0,[344],956238,81882,feature,2015-10-17T00:48:33Z,https://www.pivotaltracker.com/story/show/105440796
2015-10-17T00:59:19Z,2015-10-08T00:19:42Z,accepted,,,,105158784,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}, {'name': 'retro action item', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032000, 'updated_at': '2015-06-23T16:06:50Z'}]",Update the Concourse pipeline for Concourse to deploy custom workers,119.0,[119],956238,1550486,chore,2015-10-17T00:59:20Z,https://www.pivotaltracker.com/story/show/105158784
2015-10-17T02:01:46Z,2015-10-01T23:24:15Z,accepted,,"chad asks: where thou unit test?

```
D, [2015-10-01 23:22:14 #10365] [task:14430] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy-migration
I, [2015-10-01 23:22:14 #10365] [task:14430]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-10-01 23:22:14 #10365] [task:14430] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""8d395dd8-c2ef-4abc-9592-154d5a52abad"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy-migration' against Director 'f976642f-ade0-4d68-8660-2bba6817f10f': #<NameError: undefined local variable or method `network' for #<Bosh::Director::DeploymentPlan::JobNetworksParser:0x007f64ce6cfd38>>"",""created_at"":1443741734}
E, [2015-10-01 23:22:14 #10365] [task:14430] ERROR -- DirectorJobRunner: undefined local variable or method `network' for #<Bosh::Director::DeploymentPlan::JobNetworksParser:0x007f64ce6cfd38>
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_network_parser.rb:43:in `look_up_deployment_network'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_network_parser.rb:35:in `block in parse_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_network_parser.rb:31:in `map'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_network_parser.rb:31:in `parse_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_network_parser.rb:14:in `parse'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job_spec_parser.rb:42:in `parse'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job.rb:87:in `parse'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/deployment_spec_parser.rb:87:in `block in parse_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/deployment_spec_parser.rb:84:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/deployment_spec_parser.rb:84:in `parse_jobs'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/deployment_spec_parser.rb:23:in `parse'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:86:in `parse_from_manifest'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:51:in `block in create_from_manifest'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner_factory.rb:49:in `create_from_manifest'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:39:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,104677826,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",undefined local variable or method `network' for #<Bosh::Director::DeploymentPlan::JobNetworksParser:0x007f64ce6cfd38>,1017727.0,[1017727],956238,81882,bug,2015-10-17T02:01:46Z,https://www.pivotaltracker.com/story/show/104677826
2015-10-17T02:02:04Z,2015-09-30T22:50:04Z,accepted,,"I tried to remove az from a manifest and redeploy existing deployment.

```
D, [2015-09-30 22:47:29 #2411] [task:14364] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-09-30 22:47:29 #2411] [task:14364] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
I, [2015-09-30 22:47:29 #2411] [task:14364]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-09-30 22:47:29 #2411] [task:14364] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""71e54215-e792-41ce-810e-971f15954857"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director 'f976642f-ade0-4d68-8660-2bba6817f10f': #<NoMethodError: undefined method `availability_zone' for #<Bosh::Director::DeploymentPlan::InstanceFromDatabase:0x007fbef10f77f8>>"",""created_at"":1443653249}
E, [2015-09-30 22:47:29 #2411] [task:14364] ERROR -- DirectorJobRunner: undefined method `availability_zone' for #<Bosh::Director::DeploymentPlan::InstanceFromDatabase:0x007fbef10f77f8>
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/link.rb:17:in `block in spec'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/link.rb:15:in `map'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/link.rb:15:in `spec'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/link_lookup.rb:40:in `find_link_spec'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:33:in `block in resolve_required_links'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:21:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:21:in `resolve_required_links'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:13:in `block in resolve'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:12:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/links/links_resolver.rb:12:in `resolve'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:141:in `block (2 levels) in bind_links'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:140:in `block in bind_links'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:139:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:139:in `bind_links'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:72:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner.rb:130:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:41:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,104583828,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",undefined method `availability_zone',1017727.0,[1017727],956238,81882,bug,2015-10-17T02:02:05Z,https://www.pivotaltracker.com/story/show/104583828
2015-10-17T05:39:53Z,2015-08-19T00:55:53Z,accepted,,"- when migrating jobs into a job that already has instances, existing job instances should be preferred over instances that are being migrated, e.g...
  - if the number of available IPs is less than the number of instances, the deploy should fail
  - if the size of the pool is smaller than the number of instances, some migrating instances should be dropped

- when rebalancing across AZs, IP addresses, DNS records, etc. should be retained (so long as we are keeping the instance)

use same vm if possible (ie same network, same instance type, etc.) and always will have to re-render templates due to name change.

https://github.com/cloudfoundry/bosh-notes/blob/master/availability-zones.md#migrating-data-when-collapsing-deployment-jobs
",4.0,101552358,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",multiple jobs can be aggregated into a single job via `migrated_from` key (happy path),553935.0,"[553935, 1783496]",956238,81882,feature,2015-10-17T05:39:53Z,https://www.pivotaltracker.com/story/show/101552358
2015-10-17T06:51:05Z,2015-10-06T00:45:11Z,accepted,,"That means we migrate first before updating instance. Right now we modify that information in database during instance update.

Depends on #104961060 to be implemented first.",2.0,104961496,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","rendered job templates should have new migrated name, index, az and bootstrap",553935.0,"[553935, 1813272]",956238,553935,feature,2015-10-17T06:51:05Z,https://www.pivotaltracker.com/story/show/104961496
2015-10-17T06:53:14Z,2015-10-06T00:33:29Z,accepted,,"Right now we clean up dns records for old migrated job instance in instance updater when we update an instance with new job name. At that point it relies on instance job name in database to get dns record name to delete. Since we are going to update job name at the beginning of deploy when doing migration we can't rely on current job name later on. Ideally, instances should have a reference to dns records they have (record name + IP) and during deploy release all dns records that we no longer need.

That will also open possibilities in future to easily rename deployment.",4.0,104961060,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",clean up old dns records for jobs that were migrated,553935.0,"[553935, 1783496]",956238,553935,feature,2015-10-17T06:53:15Z,https://www.pivotaltracker.com/story/show/104961060
2015-10-17T06:54:54Z,2015-08-19T00:58:31Z,accepted,,,4.0,101552452,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",multiple jobs with persistent disks can be aggregated into a single job via `migrated_from` key,553935.0,[553935],956238,81882,feature,2015-10-17T06:54:54Z,https://www.pivotaltracker.com/story/show/101552452
2015-10-17T06:54:55Z,2015-08-19T00:54:32Z,accepted,,,,101552310,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",deployments can be migrated from multi-job (blah_z1 & blah_z2) to single job spanning multiple az (blah),,[],956238,81882,release,2015-10-17T06:54:55Z,https://www.pivotaltracker.com/story/show/101552310
2015-10-21T00:23:01Z,2015-10-05T22:13:26Z,accepted,,"  - bosh disks --orphaned
  - disks that were not deleted should be in this list
  - show disk cid, size, deployment, az (could be nil), instance, orphaned at
  - only admin",4.0,104951926,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",user can list orphaned disk,1813272.0,"[1813272, 1017727]",956238,81882,feature,2015-10-21T00:23:01Z,https://www.pivotaltracker.com/story/show/104951926
2015-10-21T00:23:35Z,2015-10-17T00:19:38Z,accepted,,"related code:

```
def orphan_disk(disk)
      disk.db.transaction do
        orphan_disk = Models::OrphanDisk.create(
          disk_cid: disk.disk_cid,
          size: disk.size,
          availability_zone: disk.instance.availability_zone,
          deployment_name: disk.instance.deployment.name,
          instance_name: ""#{disk.instance.job}/#{disk.instance.uuid}"",
          cloud_properties_json: disk.cloud_properties
        )

        orphan_snapshots(disk.snapshots, orphan_disk)
        @logger.info(""Orphaning disk: '#{disk.disk_cid}', "" +
            ""#{disk.active ? ""active"" : ""inactive""}"")

        disk.destroy
      end
    end
```",0.0,105895902,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",find out if we need to transact on multiple tables or does *.db hold shared connection,1813272.0,[1813272],956238,81882,feature,2015-10-21T00:23:35Z,https://www.pivotaltracker.com/story/show/105895902
2015-10-21T19:17:07Z,2015-10-20T23:24:30Z,accepted,,"current:

| Disk CID     | Deployment Name | Instance Name | Disk Size | Availability Zone | Orphaned At  |

wanted:

| Disk CID  | Size (MiB) | Deployment Name | Instance Name | AZ | Orphaned At |

*note changes in names of columns

also let's order based on most recent orphaned at comes first.",1.0,106165800,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",change order of columns for disks --orphaned,1017727.0,"[1017727, 1813272]",956238,81882,feature,2015-10-21T19:17:07Z,https://www.pivotaltracker.com/story/show/106165800
2015-10-21T21:38:33Z,2015-10-05T22:14:12Z,accepted,,"  - only allow deleting disks in the orphaned list; otherwise err
  - delete from db only after cpi
  - race is possible for delete-then-attach call
  - ignore not found error
  - not in a deployment context
  - only admin",4.0,104951978,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",user can run delete disk command,1426194.0,"[1426194, 1813272, 1017727]",956238,81882,feature,2015-10-21T21:38:34Z,https://www.pivotaltracker.com/story/show/104951978
2015-10-21T23:10:22Z,2015-10-20T21:42:48Z,accepted,,,,106157710,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Updating a deployment with static IP to switch to a dynamic network (with no 'subnets' key in manifest) should not blow up the deploy,1813272.0,"[1813272, 1017727]",956238,1813272,bug,2015-10-21T23:12:17Z,https://www.pivotaltracker.com/story/show/106157710
2015-10-22T01:15:20Z,2015-10-20T20:13:14Z,accepted,,"http://www.ubuntu.com/usn/usn-2778-1/
expected: 
linux-image-3.19.0-31-generic 3.19.0-31.36~14.04.1",1.0,106149908,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",USN-2778-1: Linux kernel (Vivid HWE) vulnerabilities,1550486.0,[1550486],956238,81882,feature,2015-10-22T01:15:20Z,https://www.pivotaltracker.com/story/show/106149908
2015-10-22T22:19:42Z,2015-10-22T21:09:18Z,accepted,,,,106344396,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","Remove bad published OS images (centos builds 35, 36, 37 and ubuntu 118, 119, 120)",1550486.0,[1550486],956238,1550486,chore,2015-10-22T22:19:42Z,https://www.pivotaltracker.com/story/show/106344396
2015-10-23T19:09:57Z,2015-10-10T00:05:43Z,accepted,,,1.0,105359266,story,"[{'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",remove swift_blobstore_client from blobstore_client which should remove fog as a dep of blobstore_client,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-10-23T19:09:57Z,https://www.pivotaltracker.com/story/show/105359266
2015-10-24T00:44:29Z,2015-10-09T18:24:29Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/950/files
dont take in changes to the blobstore_client",1.0,105334890,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Fix Openstack Keystone V3 API',1751936.0,[1751936],956238,81882,feature,2015-10-24T00:44:34Z,https://www.pivotaltracker.com/story/show/105334890
2015-10-25T18:27:44Z,2015-10-09T18:15:50Z,accepted,,,,105333928,story,[],keystone v3 openstack,,[],956238,81882,release,2015-10-25T18:27:44Z,https://www.pivotaltracker.com/story/show/105333928
2015-10-25T19:00:00Z,2015-09-14T19:12:11Z,accepted,,,,103347768,story,"[{'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",better ssh host fingerprint checking,,[],956238,81882,release,2015-10-26T18:24:01Z,https://www.pivotaltracker.com/story/show/103347768
2015-10-25T19:00:00Z,2015-08-20T22:43:47Z,accepted,,"running `bosh upload release cf-release-205.tgz` takes ~1min if release was already uploaded. 

```
$ time tar -Oxzf ~/Downloads/cf-release-215.tgz release.MF
.... commit_hash: 2a4897df
uncommitted_changes: true
name: cf
version: '215'

real	0m0.900s
user	0m0.376s
sys	0m0.519s
```

Running tar command against the same tarball and looking at the .MF files takes less than a second most of the time. Investigate and speed up no-op scenario. Shall you accept this story, OSS/OpsMgr teams will be eternally greatful.

It's ok to eliminate validations done for the tarball on the cli as long as there is an appropriate validation in the director.

Desired time: 10sec

Running below command on ubuntu produces content of the release file immediately:

```
time  tar -Oxf /var/tempest/releases/cf-release-218.tgz --occurrence ./release.MF
```

We should make sure that:
- finalize/create release produces tarballs with release.MF at the top of the tarball
- remove validation checks we do for the tarball before uploading",4.0,101726210,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should be able upload cf release in ~30s if it's already uploaded,1355110.0,"[1355110, 631325]",956238,81882,feature,2015-10-27T00:30:40Z,https://www.pivotaltracker.com/story/show/101726210
2015-10-25T19:06:13Z,2015-09-24T19:06:57Z,accepted,,introduce two properties on director job (director.default_ssh_options.gateway_host & director.default_ssh_options.gateway_user) with gateway_user defaulted to vcap. bosh cli when reading ip and host public key should also received optional gateway_host and gateway_user from the director if configured. use them if returned from the director. add --no-gateway to not use gateway provided by the director.,4.0,104125746,story,"[{'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]","user should be able to configure director to provide default gateway settings (host, user)",659629.0,"[659629, 1441196]",956238,81882,feature,2015-10-25T19:06:13Z,https://www.pivotaltracker.com/story/show/104125746
2015-10-25T19:08:10Z,2015-09-29T01:52:38Z,accepted,,"- stop asking for a password
- modify stemcell builder to have bosh_sudoers group that has passwordless sudo access
- change bosh-agent to add user to bosh_sudoers group

```
new cli - new stemcell -> dont ask password, dont enter password (backwarks compat break, potentially)
new cli - old stemcell -> dont ask password, cannot sudo, use --default_password if necessary or upgrade stemcell or downgrade cli
old cli - new stemcell -> ask password, enter password (sudo will not care, though stdin is propagated)
old cli - old stemcell -> ask password, enter password
```",2.0,104387452,story,"[{'name': 'ssh1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T04:44:09Z', 'id': 9115410, 'updated_at': '2014-10-28T18:46:51Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",allow passwordless sudo for users created for bosh-ssh,659629.0,"[659629, 1441196]",956238,81882,feature,2015-10-25T19:08:10Z,https://www.pivotaltracker.com/story/show/104387452
2015-10-26T23:15:24Z,2015-10-01T23:28:10Z,accepted,,"after refactor useful logging is gone for finding what was updated. since this is a product feature pls add tests for the logging.

wherever the other debug info goes for changes

FROM: ... TO: ... where ... is some networking hashes in the director

Bonus: show a diff as well.",,104678034,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",operator should see network changes during the update,1017727.0,"[1017727, 1813272]",956238,81882,bug,2015-10-26T23:15:24Z,https://www.pivotaltracker.com/story/show/104678034
2015-10-27T00:30:15Z,2015-10-12T21:46:37Z,accepted,,"allow absence of az key on jobs when user is not using azs (related to subnets having azs or not).

Currently fails:

```
[2015-10-12 14:43:14 #42544] [task:3] ERROR -- DirectorJobRunner: undefined method `find' for nil:NilClass
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:46:in `block in az_list_from_static_ips'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:46:in `map'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:46:in `az_list_from_static_ips'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:9:in `place_and_match_in'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:37:in `assign_zones'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:29:in `results'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:14:in `needed'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:13:in `plan_job_instances'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:36:in `block (2 levels) in bind_models'
```",,105515044,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",deploy should not fail when availabilty_zones keys is not provided on job with static ip,1813272.0,"[1813272, 553935]",956238,553935,bug,2015-10-27T00:30:15Z,https://www.pivotaltracker.com/story/show/105515044
2015-10-27T16:04:42Z,2015-10-26T17:43:10Z,accepted,,,,106671578,story,"[{'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'retro action item', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032000, 'updated_at': '2015-06-23T16:06:50Z'}]","For hotfix documentation, note that non-hotfix builds should not be run concurrently",1550486.0,[1550486],956238,1550486,chore,2015-10-27T16:04:42Z,https://www.pivotaltracker.com/story/show/106671578
2015-10-27T17:21:44Z,2015-10-22T21:06:09Z,accepted,,,2.0,106344100,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Move OS image building from Jenkins to Concourse,1550486.0,"[1550486, 344]",956238,1550486,feature,2015-10-27T17:27:08Z,https://www.pivotaltracker.com/story/show/106344100
2015-10-27T21:15:41Z,2015-10-17T03:16:33Z,accepted,,"deploy two instance with static ips in the same az (same subnet). change one of the static ips to use a different az (diff subnet).

- had 2 instances with .62 and .63
- somehow new instance is created with the same index
  - bug: should not have same index within a job => instances should never cross azs (if necessary old instance should be deleted, new instance should be created)

```
irb(main):004:0> puts Bosh::Director::Models::IpAddress.all.map(&:info); nil
tiny-dummy.db/1 - private - 10.10.0.62 (static)
tiny-dummy.db/0 - private - 10.10.0.63 (static)
tiny-dummy.db/0 - private - 10.10.64.123 (static)
```",,105902172,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",director should not reuse existing instances with static IPs if their AZ was changed,1813272.0,"[1813272, 553935]",956238,81882,bug,2015-10-27T21:15:41Z,https://www.pivotaltracker.com/story/show/105902172
2015-10-27T21:16:59Z,2015-10-19T19:06:36Z,accepted,,"this happens with static ips when renaming an az

see manifests from https://www.pivotaltracker.com/story/show/105902644",,106041308,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",director should raise an error if static ip is already taken by a different instance that is in a different az,553935.0,"[553935, 1813272]",956238,81882,bug,2015-10-27T21:17:10Z,https://www.pivotaltracker.com/story/show/106041308
2015-10-27T21:53:16Z,2015-10-16T15:20:17Z,accepted,,"When uploading a compiled release, the cli matches with packages already uploaded to the director to skip uploading them again, but the check should match on package fingerprint, dependencies and stemcell operating system and version. It was just matching on fingerprint.",,105860376,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",Uploading of compiled releases doesn't match against package dependencies and stemcell,1355110.0,"[1355110, 631325]",956238,1355110,bug,2015-10-27T21:53:16Z,https://www.pivotaltracker.com/story/show/105860376
2015-10-27T22:48:32Z,2015-10-23T22:00:29Z,accepted,,"We currently call out to configure_networks on the CPI when trying to update network settings on the VM. But configure_networks is unreliable on most CPIs. Move away from using CPI#configure_networks.

Acceptance (on global-net:)
1. Changing any property of networks will recreate VM",2.0,106522606,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Changing networks on a job should always recreate VM (instead of live reconfiguration),553935.0,"[553935, 344]",956238,1813272,feature,2015-10-27T22:48:33Z,https://www.pivotaltracker.com/story/show/106522606
2015-10-27T23:21:04Z,2015-10-01T17:22:46Z,accepted,,"Lastpass has a CLI...

```
$ brew install lastpass-cli
$ lpass login
$ lpass ls
```

A note on that from Chris Brown:

> Fueled by the discovery that the LastPass CLI is actually kind of amazing the
> Concourse team is aiming to move all secrets from our private repositories into
> LastPass. I thought I'd send out an email with some of the patterns that we're
> trying or we're thinking of using to get some feedback on them and to see if
> anyone is doing something similar.
> 
> Using private repositories to store secrets has a number of problems: they end
> up checked out on all of the workstations so that anyone who has access to the
> workstation has access to everything in those repositories. Teams share
> repositories and so access to them is more open than it needs to be. GitHub
> does not encrypt your repositories at rest on their file servers[1].
> 
> Keeping secrets in LastPass makes sure that they're never stored locally, the
> credentials can be scoped to the teams that actually need them, and they're
> kept encrypted at rest.
> 
> For example, we've moved all of the credentials we inject into our Concourse
> into LastPass. This lets us make the script that updates our Concourse public
> and remove a directory from our private repository. The script now looks like
> this (anonymous file descriptors are the best):
> 
>   <https://github.com/concourse/concourse/blob/develop/scripts/reconfigure.sh>
> 
> We can use `lpass edit` to edit the secrets in our favourite $EDITOR. The
> secrets are never stored on the machine in plaintext and the login can be told
> to timeout after a short period of time.
> 
> We're going to go through every secret in our private repositories and move
> them. Success looks like deleting the private repository altogether. We're
> still thinking about how to get access to the credentials from LastPass into CI
> jobs. A Concourse resource for LastPass is the way we're leaning but that may
> just be moving the problem.
> 
> In the short-to-mid-to-long term I'm interested in trying out Vault[2] or a
> similar secret store. I think switching from hardcoded secrets to fetching them
> from a service (LastPass in this case) is the first step towards this and
> should make it simpler to switch out the backend in the future.
> 
> I'm interested to hear what you think.
> 
> Chris
",1.0,104648522,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",move concourse secrets config ,1550486.0,"[1550486, 1813272]",956238,81882,feature,2015-10-27T23:21:04Z,https://www.pivotaltracker.com/story/show/104648522
2015-10-28T01:43:42Z,2015-09-23T20:59:15Z,accepted,,,4.0,104037014,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",specifying static ips on a job spanning multiple azs should reflect on placement of instances into azs,553935.0,"[553935, 1426194]",956238,81882,feature,2015-10-28T01:43:42Z,https://www.pivotaltracker.com/story/show/104037014
2015-10-28T01:43:50Z,2015-10-20T17:24:27Z,accepted,,"We want to preserve which VIP IPs instance used to have so that on subsequent deploy it will get same IPs. We used to rely on index and static IPs order to assign static IPs from VIP networks. Since we are no longer rely on ordering/indexes we need to explicitly save this in instance model (not VM, because VM can go away).

Static IP selection will not be affected by VIP networks. VIP networks don't belong to AZs. We should exclude vip networks from StaticIpsAZPicker. ",2.0,106132714,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",specified static ips for vip networks should be reused just like static ips for manual networks,553935.0,"[553935, 1813272]",956238,81882,feature,2015-10-28T01:43:50Z,https://www.pivotaltracker.com/story/show/106132714
2015-10-28T16:21:44Z,2015-05-28T21:32:54Z,accepted,,"For each of the tasks below:
- Use `lpass` CLI
- Add a README.md",,95665326,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",make pipelines conform to bosh concourse guidelines,1550486.0,[1550486],956238,119,chore,2015-10-28T20:44:35Z,https://www.pivotaltracker.com/story/show/95665326
2015-10-29T02:34:53Z,2015-10-05T17:41:39Z,accepted,,"https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/blob/master/jobs/cpi/templates/cpi.json.erb

problems:
- director does not work with ruby 1.9.3
- escaping problems for all properties",,104923910,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",convert director.yml.erb to use top level JSON.dump(...),1783496.0,"[1783496, 1778582]",956238,81882,bug,2015-10-29T02:34:54Z,https://www.pivotaltracker.com/story/show/104923910
2015-10-29T02:36:44Z,2015-10-09T18:20:44Z,accepted,,"supports urls in resources pools and releases.
https://github.com/cloudfoundry/bosh/pull/969",4.0,105334458,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",PR 'bosh deploy does not upload stemcells or releases that already exist' (actually adds support for url in releases/resource pool),1355110.0,"[1355110, 631325, 948679]",956238,81882,feature,2015-10-29T02:36:44Z,https://www.pivotaltracker.com/story/show/105334458
2015-10-29T13:36:09Z,2015-10-21T00:13:44Z,accepted,,,,106168274,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Fix flaky unit test: release_tarball_spec.rb:85,1355110.0,"[1355110, 631325]",956238,1017727,chore,2015-10-29T13:36:10Z,https://www.pivotaltracker.com/story/show/106168274
2015-10-29T20:31:47Z,2015-10-06T18:29:22Z,accepted,,"```
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `initialize': PG::Error: could not connect to server: Connection refused (Sequel::DatabaseConnectionError)
        Is the server running on host ""127.0.0.1"" and accepting
        TCP/IP connections on port 61506?
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `new'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `connect'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool.rb:94:in `make_new'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:164:in `make_new'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:137:in `available'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:127:in `block in acquire'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `block in sync'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `synchronize'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `sync'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:126:in `acquire'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:85:in `acquire'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:94:in `hold'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:452:in `server_version'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:1319:in `server_version'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:1279:in `select_clause_methods'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:835:in `clause_sql'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:136:in `select_sql'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/sql.rb:141:in `sql'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:299:in `valid_connection_sql'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:67:in `extended'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:107:in `extend'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/extensions/connection_validator.rb:107:in `block in <module:Sequel>'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:147:in `call'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:147:in `block in extension'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:145:in `each'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/misc.rb:145:in `extension'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/postgres.rb:280:in `extension'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/lib/bosh/director/config.rb:195:in `block in configure_db'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh_common/lib/common/retryable.rb:28:in `call'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh_common/lib/common/retryable.rb:28:in `block in retryer'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh_common/lib/common/retryable.rb:26:in `loop'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh_common/lib/common/retryable.rb:26:in `retryer'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh_common/lib/common/common.rb:119:in `retryable'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/lib/bosh/director/config.rb:194:in `configure_db'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/lib/bosh/director/config.rb:131:in `configure'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/lib/bosh/director/config.rb:440:in `configure_evil_config_singleton!'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/lib/bosh/director/app.rb:26:in `initialize'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/bin/bosh-director-worker:52:in `new'
        from /tmp/build/cb556eb3-4f06-47d8-677f-530e404526b3/bosh-src/bosh-director/bin/bosh-director-worker:52:in `<top (required)>'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `load'
        from /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `<main>'

```",,105027970,story,[],Add logs to tcp proxy in integration tests to see why it stops working (Resque failed to start workers in 300 seconds),553935.0,[553935],956238,553935,chore,2015-10-29T20:31:59Z,https://www.pivotaltracker.com/story/show/105027970
2015-10-29T21:19:17Z,2015-08-05T01:38:24Z,accepted,,,,100611910,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can find out instance processes,,[],956238,81882,release,2015-10-29T21:19:17Z,https://www.pivotaltracker.com/story/show/100611910
2015-10-29T21:32:23Z,2015-10-17T01:20:20Z,accepted,,"NOTE: This bug cannot be emulated with the dummy CPI as is (because it does not support configure_networks)

Steps to reproduce (on AWS:)
1) deploy 1 instance with private (manual) network

2) switch 1 instance to private-dyn (dynamic) network (make director fail like this: https://www.pivotaltracker.com/story/show/104949172)
  - see it fails *after* actual vm in the iaas changed its networking

3) switch 1 instance back to private network and see that it fails

director does not realize that networking changed in step 2:

```
Failed updating job db (00:00:02)
Error 450001: Action Failed get_task: Task 19ebaf74-5fc2-4add-7d21-9c0d26614b97 result: Resolving dynamic networks: Network 'private' is not found in settings
```

```
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: (0.000081s) SELECT NULL
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: (0.000087s) SELECT NULL
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: (0.000180s) SELECT * FROM ""records"" WHERE ((""name"" = 'c8a79661-b917-4889-a9ca-890fd49e0dff.db.private.tiny-dummy.bosh') AND (""type"" = 'A') AND (""content"" = '10.10.0.70')) LIMIT 1
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: Need to update instance 'db/0', changes: ""state""
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: Need to update instance 'db/1', changes: ""state""
I, [2015-10-17 01:02:51 #1732] [task:18098]  INFO -- DirectorJobRunner: Found 2 instances to update
I, [2015-10-17 01:02:51 #1732] [task:18098]  INFO -- DirectorJobRunner: Starting canary update num_canaries=0
I, [2015-10-17 01:02:51 #1732] [task:18098]  INFO -- DirectorJobRunner: Waiting for canaries to update
D, [2015-10-17 01:02:51 #1732] [task:18098] DEBUG -- DirectorJobRunner: Waiting for tasks to complete
I, [2015-10-17 01:02:51 #1732] [task:18098]  INFO -- DirectorJobRunner: Finished canary update
I, [2015-10-17 01:02:51 #1732] [task:18098]  INFO -- DirectorJobRunner: Continuing the rest of the update
```",,105899202,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",director does not realize that networking settings got changed after failed deploy,553935.0,[553935],956238,81882,bug,2015-10-29T21:32:23Z,https://www.pivotaltracker.com/story/show/105899202
2015-10-29T21:41:47Z,2015-10-23T01:02:29Z,accepted,,"When migrating from legacy manifest to new style, we drop key 'uuid' but vSphere CPI uses it for resource pool configuration.
Acceptance: 
1. 'uuid' from bosh-init legacy manifest should be migrated as director id in new manifest file
1. if uuid if not present/empty, we should generate a new one
",,106356570,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-init should properly migrate vsphere deployment manifests that uses resource pools,1355110.0,"[1355110, 948679]",956238,81882,bug,2015-10-29T21:41:47Z,https://www.pivotaltracker.com/story/show/106356570
2015-10-30T00:03:13Z,2015-09-23T20:57:40Z,accepted,,"after `bosh vm resurrection off` runs no new tasks should be enqueued. any attempts by hm to submit scan-n-fix tasks should be rejected and not enqueued.

if all instances in a deployment have resurrection=paused then do not enqueue the task.

https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/api/controllers/deployments_controller.rb#L267 is probably the area to look at.",2.0,104036894,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]","when bosh vm resurrection is off globally, no new tasks should be enqueued",1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-10-30T00:03:13Z,https://www.pivotaltracker.com/story/show/104036894
2015-10-30T18:30:36Z,2015-10-27T15:03:50Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/989
https://github.com/cloudfoundry/bosh/pull/983
https://github.com/cloudfoundry/bosh/pull/975
https://github.com/cloudfoundry/bosh/pull/974",2.0,106755040,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pull in PRs,1355110.0,[1355110],956238,81882,feature,2015-10-30T18:30:36Z,https://www.pivotaltracker.com/story/show/106755040
2015-11-01T20:00:00Z,2015-10-12T18:52:30Z,accepted,,"supports urls in resources pools and releases.
https://github.com/cloudfoundry/bosh/pull/969",2.0,105499572,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",urls for releases/stemcells should work on global-net,1550486.0,[1550486],956238,81882,feature,2015-11-02T08:00:54Z,https://www.pivotaltracker.com/story/show/105499572
2015-11-02T01:49:33Z,2015-09-29T17:12:17Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/963
this PR contains registry & stemcell building changes: for this story let's *only* pull in registry changes. im still discussing with abelhu about stemcell changes.",2.0,104454938,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",PR 'Update bosh-registry to support Azure without plugin settings',631325.0,"[631325, 1441196]",956238,81882,feature,2015-11-02T01:50:01Z,https://www.pivotaltracker.com/story/show/104454938
2015-11-02T01:56:29Z,2015-10-13T18:31:52Z,accepted,,"Tests are trying to upload release, but workers died because of redis timeout error",2.0,105595222,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",reconnect to redis (Redis::TimeoutError: Connection timed out) indefinitely,1550486.0,[1550486],956238,553935,feature,2015-11-02T01:56:39Z,https://www.pivotaltracker.com/story/show/105595222
2015-11-02T02:24:46Z,2015-09-23T21:12:22Z,accepted,,,1.0,104038562,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",remove index in a link,553935.0,[553935],956238,81882,feature,2015-11-02T02:24:46Z,https://www.pivotaltracker.com/story/show/104038562
2015-11-04T17:36:59Z,2015-10-29T22:13:09Z,accepted,,"**AFTER #104119296**

It was set to ""none"" for build 3122, so we can publish a BOSH release without new stemcells.

Needs to be restored to:

```
vsphere-esxi-ubuntu-trusty,vsphere-esxi-centos,light-aws-xen-ubuntu-trusty,aws-xen-ubuntu-trusty,light-aws-xen-centos,aws-xen-centos,light-aws-xen-hvm-ubuntu-trusty,light-aws-xen-hvm-centos,vcloud-esxi-ubuntu-trusty,openstack-kvm-ubuntu-trusty,openstack-kvm-centos
```",,106994460,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]","Restore `BOSH_PROMOTE_STEMCELLS` in Jenkins ""Promote Artifacts"" build",1550486.0,[1550486],956238,1550486,chore,2015-11-04T17:36:59Z,https://www.pivotaltracker.com/story/show/106994460
2015-11-04T22:37:04Z,2015-10-31T02:35:32Z,accepted,,this one was related to only some compilation vms would come up successfully.,,107096494,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",resolve suraci's bosh problem,553935.0,"[553935, 1813272]",956238,381857,bug,2015-11-04T22:37:04Z,https://www.pivotaltracker.com/story/show/107096494
2015-11-04T22:42:40Z,2015-11-02T19:53:29Z,accepted,,and az was specified in the manifest for a job,,107235206,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",availability_zone on a link was nil,553935.0,"[553935, 1813272]",956238,81882,bug,2015-11-04T22:42:40Z,https://www.pivotaltracker.com/story/show/107235206
2015-11-04T22:49:16Z,2015-11-02T02:29:06Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/e529d60a964c6033e19ee2a32b4b7c12601929b7/bosh-director/lib/bosh/director/disk_manager.rb#L229
look up other transaction usages.",,107163430,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",do not transact over a network call,1426194.0,[1426194],956238,81882,bug,2015-11-04T22:49:16Z,https://www.pivotaltracker.com/story/show/107163430
2015-11-04T23:04:39Z,2015-10-05T22:14:23Z,accepted,,"  - should not remove disks that are being orphaned as command runs
  - bosh cleanup without all flag should not touch disks
  - consider disknotfound a success and delete db row after that
  - delete in parallel
  - only admin",4.0,104951996,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",user can run `bosh cleanup --all` to clean up orphaned disks,1017727.0,"[1017727, 1426194, 344]",956238,81882,feature,2015-11-04T23:04:39Z,https://www.pivotaltracker.com/story/show/104951996
2015-11-05T00:07:52Z,2015-10-28T17:09:08Z,accepted,,See <https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-2.1-postgres/builds/348> for a failure example that could use improving,,106868560,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}]",Add an RSpec matcher for our CLI tables for improved diff output,1550486.0,[1550486],956238,1550486,chore,2015-11-05T00:07:52Z,https://www.pivotaltracker.com/story/show/106868560
2015-11-05T02:21:52Z,2015-10-29T17:48:48Z,accepted,,"http://www.ubuntu.com/usn/usn-2788-1/
expected:
Ubuntu 14.04 LTS:
unzip 6.0-9ubuntu1.4",1.0,106972810,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",USN-2788-1: unzip vulnerabilities,1550486.0,[1550486],956238,81882,feature,2015-11-05T02:21:52Z,https://www.pivotaltracker.com/story/show/106972810
2015-11-06T00:42:29Z,2015-09-15T18:28:40Z,accepted,,"stop exporting director logs, tasks logs and blobstore
https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/backup.rb",1.0,103435832,story,"[{'name': 'backup1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-18T16:50:10Z', 'id': 11156522, 'updated_at': '2015-03-18T16:51:22Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh backup should only export database contents,1386874.0,[1386874],956238,81882,feature,2015-11-06T00:42:29Z,https://www.pivotaltracker.com/story/show/103435832
2015-11-06T00:52:59Z,2015-08-01T01:01:12Z,accepted,,"bosh instances --failing
bosh instances --ps --failing

- if --ps is not specified
  - if instance is failing show instance row
  - if instance is running do not show instance row
- if --ps is specified
  - if instance is failing show instance row
  - if instance is running but one process is failing, show instance and process row
  - if instance is failing and one process is failing, show instance and process row

failing = not running
show instance row (that include IP) if one of the processes if failing

```

+------------------------------------+---------+---------------+--------------+
| Job/index                          | State   | Resource Pool | IPs          |
+------------------------------------+---------+---------------+--------------+
| router_z2/1                        | failing | router_z2     | 10.10.80.16  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| runner_z1/0                        | failing | runner_z1     | 10.10.17.31  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| runner_z2/3                        | failing | runner_z2     | 10.10.81.3   |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| stats_z1/0                         | failing | small_z1      | 10.10.17.21  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| uaa_z1/0                           | failing | medium_z1     | 10.10.17.22  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
| uaa_z2/0                           | failing | medium_z2     | 10.10.81.18  |
|   cloud_controller_ng                running
|   cloud_controller_worker_local_1    unmonitored
|   cloud_controller_worker_local_2    Execution failed
+------------------------------------+---------+---------------+--------------+
```
",2.0,100344958,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh instances should filter non-failing instances if --failing flag is given,1495236.0,"[1495236, 1550486]",956238,1482982,feature,2015-11-06T00:53:08Z,https://www.pivotaltracker.com/story/show/100344958
2015-11-06T01:40:51Z,2015-10-26T13:11:20Z,accepted,,while vm is not pingable over nats. see other checkpointing code. if cancelled vm should be deleted.,2.0,106641948,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",user can cancel deploy while bosh is waiting for vm to come up after create_vm cpi call,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-11-06T01:40:51Z,https://www.pivotaltracker.com/story/show/106641948
2015-11-06T01:51:35Z,2015-11-05T19:15:08Z,accepted,,"http://www.ubuntu.com/usn/usn-2798-1/
expected: linux-image-3.19.0-32-generic 3.19.0-32.37~14.04.1",1.0,107517192,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for [USN-2798-1] Linux kernel (Vivid HWE) vulnerabilities,1550486.0,"[1550486, 1751936]",956238,81882,feature,2015-11-06T01:51:35Z,https://www.pivotaltracker.com/story/show/107517192
2015-11-07T00:30:53Z,2015-10-06T16:20:16Z,accepted,,"Fails in 50% of the times:

```
I, [2015-10-28T09:18:40.883042 #24940]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20151028-24940-302iqp --user admin --password admin vm resurrection off 2>&1
I, [2015-10-28T09:19:06.468124 #24940]  INFO -- : Bosh command failed: [WARNING] cannot access director, trying 4 more times...
[WARNING] cannot access director, trying 3 more times...
[WARNING] cannot access director, trying 2 more times...
[WARNING] cannot access director, trying 1 more times...
cannot access director (Connection refused - connect(2) (https://10.85.33.50:25555))

BATs example failed  'service configuration runit when restarted after agent has been started when the agent dies restarts it' ./spec/system/service_configuration_spec.rb:254 

  1) service configuration runit when restarted after agent has been started when the agent dies restarts it
     Failure/Error: bosh('vm resurrection off')
     Bosh::Exec::Error:
       command 'bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20151028-24940-302iqp --user admin --password admin vm resurrection off 2>&1' failed with exit code 1
     # /mnt/jenkins/workspace/bat_micro_vsphere_ubuntu_trusty_go_agent_nested_esxi/bosh/bosh_common/lib/common/exec.rb:62:in `sh'
     # ./lib/bat/bosh_runner.rb:24:in `bosh'
     # ./lib/bat/bosh_helper.rb:14:in `bosh'
     # ./spec/system/service_configuration_spec.rb:26:in `instance_reboot'
     # ./spec/system/service_configuration_spec.rb:151:in `block (3 levels) in <top (required)>'   

<snip>

Failures:

  1) service configuration runit when restarted after agent has been started when the agent dies restarts it
     Failure/Error: bosh('vm resurrection off')
     Bosh::Exec::Error:
       command 'bundle exec bosh --non-interactive -P 1 --config /mnt/ci-tmp/bosh_config20151028-24940-302iqp --user admin --password admin vm resurrection off 2>&1' failed with exit code 1
     # /mnt/jenkins/workspace/bat_micro_vsphere_ubuntu_trusty_go_agent_nested_esxi/bosh/bosh_common/lib/common/exec.rb:62:in `sh'
     # ./lib/bat/bosh_runner.rb:24:in `bosh'
     # ./lib/bat/bosh_helper.rb:14:in `bosh'
     # ./spec/system/service_configuration_spec.rb:26:in `instance_reboot'
     # ./spec/system/service_configuration_spec.rb:151:in `block (3 levels) in <top (required)>'

Finished in 48 minutes 52 seconds (files took 1.15 seconds to load)
100 examples, 1 failure, 2 pending
```",,105015410,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Flaky BATs runit/monit tests on trusty,1550486.0,[1550486],956238,553935,chore,2015-11-07T00:30:53Z,https://www.pivotaltracker.com/story/show/105015410
2015-11-07T00:31:05Z,2015-10-26T23:40:29Z,accepted,,"For example, we don't want to have to retrieve the S3 bucket key names from the build output log.",,106706262,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Review hotfix workflow for more automation opportunities,1550486.0,"[1550486, 1751936]",956238,1550486,chore,2015-11-07T00:31:05Z,https://www.pivotaltracker.com/story/show/106706262
2015-11-07T00:31:23Z,2015-10-29T17:20:36Z,accepted,,http://bosh-jenkins.cf-app.com:8080/view/1.%20Flow/job/bat_micro_vsphere_ubuntu_trusty_go_agent_nested_esxi/91/consoleFull,,106969912,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Fix flaky VSphere BATs test,1550486.0,[1550486],956238,1550486,chore,2015-11-07T00:31:23Z,https://www.pivotaltracker.com/story/show/106969912
2015-11-07T23:45:11Z,2015-09-24T18:04:38Z,accepted,,"currently on a fresh deployment, it seems that apply action reload monit at the end which i believe makes monit think that it should start processes. that makes pre-start effectively run in parallel to monit starting services.

may be we should generate monit files on start and remove them on stop?",,104119296,story,"[{'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",agent (via monit) should not start scripts until start action is issued (which happens after pre-start is finished),659629.0,[659629],956238,81882,bug,2015-11-07T23:45:11Z,https://www.pivotaltracker.com/story/show/104119296
2015-11-08T00:31:17Z,2015-07-21T17:47:59Z,accepted,,,,99539364,story,"[{'name': 'pre-start', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-21T17:53:16Z', 'id': 12277312, 'updated_at': '2015-07-21T17:53:16Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pre-start script is executed for release jobs,,[],956238,81882,release,2015-11-08T00:31:17Z,https://www.pivotaltracker.com/story/show/99539364
2015-11-08T00:42:33Z,2015-11-02T17:43:22Z,accepted,,task 59 on suraci's director,,107221434,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",cannot colocate jobs that do not use links with jobs that do,553935.0,"[553935, 1813272]",956238,81882,bug,2015-11-08T00:42:33Z,https://www.pivotaltracker.com/story/show/107221434
2015-11-08T00:42:43Z,2015-11-02T17:36:43Z,accepted,,task #34 on suraci's director,,107220716,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",eval of templates should happen before any update is made,553935.0,"[553935, 1813272]",956238,81882,bug,2015-11-08T00:42:43Z,https://www.pivotaltracker.com/story/show/107220716
2015-11-08T00:53:14Z,2015-10-26T19:05:50Z,accepted,,"This helps us make bosh-init faster by not re-compiling release if provided

Acceptance:
1. Normal releases continue working
2. If compiled release is provided then let's raise an error if the compiled release is a CPI release
3. If compiled release is provided, and stemcell os/version is correct,  and it is a not a CPI release then continue on (okay if it errors later.)",2.0,106681150,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'init-cr', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-22T16:09:30Z', 'id': 13116212, 'updated_at': '2015-10-22T16:09:30Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-init raises an error if cpi release is compiled,948679.0,"[948679, 1441196]",956238,81882,feature,2015-11-08T00:53:14Z,https://www.pivotaltracker.com/story/show/106681150
2015-11-08T00:53:24Z,2015-10-22T16:23:04Z,accepted,,"This helps us make bosh-init faster by not re-compiling release if provided and matching stemcell os/version.

Acceptance:
1. Normal releases continue working
2. If compiled release is provided then let's raise error if stemcell version referenced in a resource pool is not supported. Compiled releases will be provided in exactly same way as any other release.
3. If compiled release is provided, and stemcell os/version is correct then continue on (okay if it errors later.)",2.0,106319836,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'init-cr', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-22T16:09:30Z', 'id': 13116212, 'updated_at': '2015-10-22T16:09:30Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-init raises an error if compiled release does not match given stemcell os / version,948679.0,"[948679, 1441196]",956238,81882,feature,2015-11-08T00:53:24Z,https://www.pivotaltracker.com/story/show/106319836
2015-11-08T00:53:34Z,2015-10-28T20:14:12Z,accepted,,,2.0,106886960,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'init-cr', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-22T16:09:30Z', 'id': 13116212, 'updated_at': '2015-10-22T16:09:30Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",validate compiled release packages if used with bosh-init,1441196.0,"[1441196, 948679]",956238,81882,feature,2015-11-08T00:53:34Z,https://www.pivotaltracker.com/story/show/106886960
2015-11-09T18:28:40Z,2015-10-10T01:36:25Z,accepted,,,,105362222,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",persistent disk should be properly orphaned when vm is recreated due to changes and persistent disk is no longer specified in the manifest,344.0,"[344, 553935, 1426194]",956238,81882,bug,2015-11-09T18:28:40Z,https://www.pivotaltracker.com/story/show/105362222
2015-11-09T22:13:10Z,2015-11-04T22:49:32Z,accepted,,"Orphan disk's created_at column stores the date the PersistentDisk was created. 
Instead let's keep a volume_created_at on OrphanDisks, and use created_at as the date the model was created.
Be sure that the cli output shows the desired information.",,107443014,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",change the way orphaning disks tracks orphaned at dates,,[],956238,344,chore,2015-11-09T22:13:10Z,https://www.pivotaltracker.com/story/show/107443014
2015-11-10T23:11:26Z,2015-11-10T04:32:24Z,accepted,,"http://www.ubuntu.com/usn/usn-2806-1/
expected: linux-image-3.19.0-33-generic 3.19.0-33.38~14.04.1",0.0,107786178,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for USN-2806-1: Linux kernel (Vivid HWE) vulnerability,553935.0,[553935],956238,81882,feature,2015-11-10T23:11:29Z,https://www.pivotaltracker.com/story/show/107786178
2015-11-11T21:30:40Z,2015-11-09T18:40:31Z,accepted,,"http://www.ubuntu.com/usn/usn-2788-2/
unzip 6.0-9ubuntu1.5",1.0,107744432,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Bump ubuntu for USN-2788-2: unzip regression,1550486.0,"[1550486, 1426194]",956238,81882,feature,2015-11-11T21:30:40Z,https://www.pivotaltracker.com/story/show/107744432
2015-11-11T22:24:11Z,2015-10-22T16:09:30Z,accepted,,let's not tackle cpi releases since they have to be compiled on multiple os-es potentially.,4.0,106317704,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'init-cr', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-22T16:09:30Z', 'id': 13116212, 'updated_at': '2015-10-22T16:09:30Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-init can use non-cpi compiled releases,948679.0,"[948679, 1441196, 659629]",956238,81882,feature,2015-12-10T23:11:22Z,https://www.pivotaltracker.com/story/show/106317704
2015-11-11T22:24:17Z,2015-10-22T16:25:00Z,accepted,,,,106319972,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'init-cr', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-22T16:09:30Z', 'id': 13116212, 'updated_at': '2015-10-22T16:09:30Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-init can use non-cpi compiled releases,,[],956238,81882,release,2015-11-11T22:24:17Z,https://www.pivotaltracker.com/story/show/106319972
2015-11-11T23:24:26Z,2015-11-03T22:08:00Z,accepted,,,0.0,107343744,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",update bosh-lite stemcell with new agent,81882.0,[81882],956238,81882,feature,2015-11-11T23:24:26Z,https://www.pivotaltracker.com/story/show/107343744
2015-11-11T23:31:36Z,2015-11-04T22:41:27Z,accepted,,,2.0,107442314,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",rename availability_zone(s) to az(s),344.0,"[344, 353433]",956238,553935,feature,2015-11-11T23:31:36Z,https://www.pivotaltracker.com/story/show/107442314
2015-11-11T23:43:01Z,2015-09-23T21:11:38Z,accepted,,,1.0,104038514,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user can use bosh ssh with job/id in addition to job/index,553935.0,"[553935, 1426194]",956238,81882,feature,2015-11-11T23:43:01Z,https://www.pivotaltracker.com/story/show/104038514
2015-11-11T23:53:47Z,2015-10-05T22:14:35Z,accepted,,"  - add `director.disks.max_orphaned_age_in_days` configurable as a director property in days
    - default to 5 days
    - allow 0 days to delete asap
  - add `director.disks.cleanup_interval` configurable as a director property (cron format)
    - we have other interval; let's keep them consistent
    - default to 30min",4.0,104952008,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",user sees that orphaned disks are deleted after 5 days regularly,344.0,[344],956238,81882,feature,2015-11-11T23:53:47Z,https://www.pivotaltracker.com/story/show/104952008
2015-11-12T00:09:16Z,2015-10-12T19:13:56Z,accepted,,cleanup --all should not fail if director returns 404 for disk cleanup background job enqueue,2.0,105501680,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",cleanup --all should not fail if run against an old director that doesn't know about orphan disks,1426194.0,"[1426194, 344]",956238,81882,feature,2015-11-12T00:09:16Z,https://www.pivotaltracker.com/story/show/105501680
2015-11-12T00:20:21Z,2015-10-31T02:25:55Z,accepted,,"validate that if network ref-ed by compilation has azs specified then compilation should also specify az
if job / compilation only has vip network i guess we should not blow up
if job / compilation has any network that specifies any az in any subnets then we should require job / compilation to have az",,107096308,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",job / compilation section should specify az if referenced network specifies az,553935.0,"[553935, 1426194]",956238,81882,bug,2015-11-12T00:20:21Z,https://www.pivotaltracker.com/story/show/107096308
2015-11-13T02:25:11Z,2015-11-10T06:57:42Z,accepted,,"current description:
delete artifacts
wanted:
clean up
clean up all

current result:
release(s) deleted: none; stemcell(s) deleted: none; orphaned disk(s) deleted:...
wanted:
Deleted X release(s), X stemcell(s), X orphaned disk(s)",1.0,107790594,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",improve cleanup task name & description,344.0,[344],956238,81882,feature,2015-11-13T02:25:11Z,https://www.pivotaltracker.com/story/show/107790594
2015-11-13T02:27:51Z,2015-11-02T02:38:17Z,accepted,,"- if instance does not exist it does not make sense to unmount (why would instance be nil?)
- if disk does not exist it does not make sense to unmount
- still need to orphan disks

https://github.com/cloudfoundry/bosh/blob/e529d60a964c6033e19ee2a32b4b7c12601929b7/bosh-director/lib/bosh/director/disk_manager.rb#L122",,107163734,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",orphaning of disks should still happen in several cases,344.0,[344],956238,81882,bug,2015-11-13T02:27:51Z,https://www.pivotaltracker.com/story/show/107163734
2015-11-13T18:30:00Z,2015-11-12T18:13:32Z,accepted,,"affected:
krb5-admin-server
krb5-kdc
krb5-kdc-ldap
krb5-otp
krb5-pkinit
krb5-user
libgssapi-krb5-2 <---
libgssrpc4 <---
libk5crypto3 <---
libkadm5clnt-mit9 <---
libkdb5-7 <---
libkrad0
libkrb5-3 <---
libkrb5support0 <---",1.0,108018662,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu trusty for USN-2810-1: Kerberos vulnerabilities,1550486.0,[1550486],956238,81882,feature,2015-11-13T18:30:01Z,https://www.pivotaltracker.com/story/show/108018662
2015-11-13T18:31:16Z,2015-11-06T18:25:39Z,accepted,,unnecessary check: https://github.com/cloudfoundry/bosh-agent/blob/master/platform/linux_platform.go#L871,1.0,107594924,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",always overwrite monit credentials,553935.0,"[553935, 1426194]",956238,81882,feature,2015-11-13T18:31:17Z,https://www.pivotaltracker.com/story/show/107594924
2015-11-13T21:34:28Z,2015-07-13T19:42:40Z,accepted,,"Accepting ""secure"" ICMP redirects (from those gateways listed as default gateways) has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50327r2_rule
---
F-43474r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.secure_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.secure_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.secure_redirects = 0
---
C-46084r2_chk
---
The status of the ""net.ipv4.conf.all.secure_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.secure_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.secure_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding.",1.0,98978114,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38526] [medium] The system must not accept ICMPv4 secure redirect packets on any interface.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:34:28Z,https://www.pivotaltracker.com/story/show/98978114
2015-11-13T21:34:49Z,2015-07-13T19:42:40Z,accepted,,"Accepting ICMP redirects has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50325r2_rule
---
F-43472r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.accept_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.accept_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.accept_redirects = 0
---
C-46082r2_chk
---
The status of the ""net.ipv4.conf.all.accept_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.accept_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.accept_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978116,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38524] [medium] The system must not accept ICMPv4 redirect packets on any interface.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:34:51Z,https://www.pivotaltracker.com/story/show/98978116
2015-11-13T21:35:08Z,2015-07-13T19:42:40Z,accepted,,"Accepting source-routed packets in the IPv4 protocol has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50330r2_rule
---
F-43478r1_fix
---
To set the runtime status of the ""net.ipv4.conf.default.accept_source_route"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.default.accept_source_route=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.default.accept_source_route = 0
---
C-46088r2_chk
---
The status of the ""net.ipv4.conf.default.accept_source_route"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.default.accept_source_route

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.default.accept_source_route /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978130,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38529] [medium] The system must not accept IPv4 source-routed packets by default.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:35:09Z,https://www.pivotaltracker.com/story/show/98978130
2015-11-13T21:35:26Z,2015-07-13T19:42:41Z,accepted,,"Accepting source-routed packets in the IPv4 protocol has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50324r2_rule
---
F-43471r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.accept_source_route"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.accept_source_route=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.accept_source_route = 0
---
C-46081r3_chk
---
The status of the ""net.ipv4.conf.all.accept_source_route"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.accept_source_route

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.accept_source_route /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978170,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38523] [medium] The system must not accept IPv4 source-routed packets on any interface.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:35:26Z,https://www.pivotaltracker.com/story/show/98978170
2015-11-13T21:36:07Z,2015-07-13T19:42:43Z,accepted,,"Permitting direct root login reduces auditable information about who ran privileged commands on the system and also allows direct attack attempts on root's password.
---
None
---
SV-50414r1_rule
---
F-43561r1_fix
---
The root user should never be allowed to log in to a system directly over a network. To disable root login via SSH, add or correct the following line in ""/etc/ssh/sshd_config"": 

PermitRootLogin no
---
C-46171r1_chk
---
To determine how the SSH daemon's ""PermitRootLogin"" option is set, run the following command: 

# grep -i PermitRootLogin /etc/ssh/sshd_config

If a line indicating ""no"" is returned, then the required value is set. 
If the required value is not set, this is a finding.",1.0,98978248,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38613] [medium] The system must not permit root logins using remote access programs such as ssh.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:36:09Z,https://www.pivotaltracker.com/story/show/98978248
2015-11-13T21:36:29Z,2015-07-13T19:42:40Z,accepted,,"Users need to be aware of activity that occurs regarding their account. Providing users with information regarding the date and time of their last successful login allows the user to determine if any unauthorized activity has occurred and gives them an opportunity to notify administrators.

At ssh login, a user must be presented with the last successful login date and time.
---
None
---
SV-50285r2_rule
---
F-43431r2_fix
---
Update the ""PrintLastLog"" keyword to ""yes"" in /etc/ssh/sshd_config:

PrintLastLog yes

While it is acceptable to remove the keyword entirely since the default action for the SSH daemon is to print the last logon date and time, it is preferred to have the value explicitly documented.
---
C-46041r2_chk
---
Verify the value associated with the ""PrintLastLog"" keyword in /etc/ssh/sshd_config:

# grep -i ""^PrintLastLog"" /etc/ssh/sshd_config

If the ""PrintLastLog"" keyword is not present, this is not a finding.  If the value is not set to ""yes"", this is a finding.",1.0,98978124,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]","[V-38484] [medium] The operating system, upon successful logon, must display to the user the date and time of the last logon or access via ssh.",1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-13T21:36:37Z,https://www.pivotaltracker.com/story/show/98978124
2015-11-13T21:37:48Z,2015-10-29T16:26:23Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/42,2.0,106964552,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pull in 'eliminate root paritition has to be first one',631325.0,[631325],956238,81882,feature,2015-11-13T21:37:57Z,https://www.pivotaltracker.com/story/show/106964552
2015-11-14T01:50:10Z,2015-11-05T01:58:25Z,accepted,,"/etc/network/intefaces should not contain multiple gateway and broadcast configs for each iface. should only specify it for gateway-default iface.

",,107452992,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",fix dns configuration for multiple networks on ubuntu,553935.0,"[553935, 1426194]",956238,81882,bug,2015-11-14T01:50:10Z,https://www.pivotaltracker.com/story/show/107452992
2015-11-14T01:53:03Z,2015-11-05T18:34:16Z,accepted,,,2.0,107513004,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should explicitly fail if dns and interfaces are not configured correctly,553935.0,"[553935, 1426194]",956238,553935,feature,2015-11-14T01:53:04Z,https://www.pivotaltracker.com/story/show/107513004
2015-11-15T00:21:12Z,2015-11-09T20:00:30Z,accepted,,sep repo; create file per test case; timebox 1 full day,2.0,107752754,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz networking + az without migration pt1,353433.0,"[353433, 1426194, 1750722]",956238,81882,feature,2015-11-15T00:21:12Z,https://www.pivotaltracker.com/story/show/107752754
2015-11-15T00:24:44Z,2015-10-26T19:18:45Z,accepted,,"something like this for possible permutations

```
- job configuration
  - name: [5 char, 256 char]
  - instances: [0, 1, 4 (even), 5 (odd)]
  - availability_zones: [null, [z1], [z1, z2], [z3, z1, z2]]
```",4.0,106682370,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",create an integration test suite that generates different kinds of permutation of the deployment manifest config and runs deploys continuously pt1,553935.0,"[553935, 1550486, 353433]",956238,81882,feature,2015-11-16T06:54:23Z,https://www.pivotaltracker.com/story/show/106682370
2015-11-15T01:06:16Z,2015-11-12T00:44:14Z,accepted,,,,107962646,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",deploy with dynamic network should not result in recreate,344.0,"[344, 553935, 353433]",956238,81882,bug,2015-11-15T01:06:16Z,https://www.pivotaltracker.com/story/show/107962646
2015-11-16T17:49:12Z,2015-11-16T16:11:45Z,accepted,,"sample failures: https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/unit-1.9/builds/781

```
  4) Bosh::Cli::Command::JobManagement if there are many jobs of the specified type in the deployment detaching a job behaves like a command which modifies the vm state if an index is not supplied tells the user what it is about to do
     Failure/Error: expect {
       expected Bosh::Cli::CliError with ""You should specify the job index. There is more than one instance of this job type."", got #<Bosh::Cli::CliError: You should specify the job index or id. There is more than one instance of this job type.> with backtrace:
```

related to this commit: https://github.com/cloudfoundry/bosh/commit/113beaac4da29433eaefd92fae650a18361b9f14 and story #104038162",,108239464,story,[],Adjust for new CLI message,353433.0,[353433],956238,353433,chore,2015-11-16T17:49:12Z,https://www.pivotaltracker.com/story/show/108239464
2015-11-16T19:44:40Z,2015-08-19T01:04:39Z,accepted,,,,101552790,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",first class AZs are usable,,[],956238,81882,release,2015-11-16T19:44:40Z,https://www.pivotaltracker.com/story/show/101552790
2015-11-16T22:19:56Z,2015-11-04T01:40:17Z,accepted,,"cli was trying to get vms and worker died with error:

```
-03 23:47:10 #17227] []  INFO -- DirectorWorker: Running before_fork hooks with [(Job{normal} | Bosh::Director::Jobs::VmState | [35, 7, ""full""])]
I, [2015-11-03 23:47:10 #3614] []  INFO -- DirectorWorker: Running after_fork hooks with [(Job{normal} | Bosh::Director::Jobs::VmState | [35, 7, ""full""])]
I, [2015-11-03 23:47:11 #3614] []  INFO -- DirectorWorker: (Job{normal} | Bosh::Director::Jobs::VmState | [35, 7, ""full""]) failed: #<Sequel::ValidationFailed: description presence>
I, [2015-11-03 23:47:11 #3614] []  INFO -- DirectorWorker: /opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1248:in `save'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:155:in `finish_task'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:117:in `rescue in perform_job'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:88:in `perform_job'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:31:in `block in run'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh_common/lib/common/thread_formatter.rb:49:in `with_thread_name'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:31:in `run'
/tmp/build/5d83b2bc-001d-429e-5d82-20c6000bb61e/bosh-src/bosh-director/lib/bosh/director/jobs/base_job.rb:10:in `perform'
```",,107357338,story,"[{'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Hanging CI integration test,353433.0,[353433],956238,553935,chore,2015-11-16T22:19:56Z,https://www.pivotaltracker.com/story/show/107357338
2015-11-16T23:46:03Z,2015-07-13T19:42:40Z,accepted,,"The xinetd service provides a dedicated listener service for some programs, which is no longer necessary for commonly-used network services. Disabling it ensures that these uncommon services are not running, and also prevents attacks against xinetd itself.
---
None
---
SV-50383r2_rule
---
F-43530r2_fix
---
The ""xinetd"" service can be disabled with the following commands: 

# chkconfig xinetd off
# service xinetd stop
---
C-46140r2_chk
---
If network services are using the xinetd service, this is not applicable.

To check that the ""xinetd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""xinetd"" --list

Output should indicate the ""xinetd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""xinetd"" --list
""xinetd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""xinetd"" is disabled through current runtime configuration: 

# service xinetd status

If the service is disabled the command will return the following output: 

xinetd is stopped


If the service is running, this is a finding.",1.0,98978102,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38582] [medium] The xinetd service must be disabled if no network services utilizing it are enabled.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-11-16T23:46:03Z,https://www.pivotaltracker.com/story/show/98978102
2015-11-16T23:52:05Z,2015-11-10T05:56:37Z,accepted,,"task description:
current:
scheduled ScheduledOrphanCleanup
wanted:
clean up orphan disks

task result:
current:
Cleaned up orphaned disks and orphaned snapshots older than 2015-11-05 05:30:00...
wanted:
Deleted X orphaned disk(s) older than 2015-11-05 05:30:00 UTC",1.0,107788998,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",improve orphan disk descriptions,344.0,[344],956238,81882,feature,2015-11-16T23:52:05Z,https://www.pivotaltracker.com/story/show/107788998
2015-11-16T23:52:48Z,2015-11-11T01:24:35Z,accepted,,"2015-11-11_01:16:09.62514 [clientRetryable] 2015/11/11 01:16:09 DEBUG - [requestID=0effa23e-ecc2-45a1-7c83-e107cb79428f] Request attempt failed (attempts=22), error: Post http://127.0.0.1:2822/batlight: net/http: request canceled

```
Director task 1892
  Started deleting instances
  Started deleting instances > batlight/0
  Started deleting instances > batlight/1. Done (00:00:02)
   Failed deleting instances > batlight/0: Action Failed get_task: Task a93bad77-d61e-4b1e-7572-a28298766494 result: Stopping Monitored Services: Stopping service batlight: Sending stop request to monit: Post http://127.0.0.1:2822/batlight: net/http: request canceled (00:00:22)
   Failed deleting instances (00:00:22)
```",1.0,107878748,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should disregard canceled http request to monit when waiting for monit to become available to do other things,553935.0,[553935],956238,81882,feature,2015-11-16T23:52:48Z,https://www.pivotaltracker.com/story/show/107878748
2015-11-17T17:33:59Z,2015-11-16T22:10:49Z,accepted,,"http://www.ubuntu.com/usn/usn-2812-1/
Ubuntu 14.04 LTS:
  libxml2                         2.9.1+dfsg1-3ubuntu4.5",0.0,108279644,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for USN-2812-1: libxml2 vulnerabilities,553935.0,"[553935, 353433]",956238,81882,feature,2015-11-17T17:34:00Z,https://www.pivotaltracker.com/story/show/108279644
2015-11-17T23:09:30Z,2015-11-06T18:54:29Z,accepted,,"CI is failed with error:

```
D, [2015-11-06 16:09:17 #66146] [instance_update(foobar/2)] DEBUG -- DirectorJobRunner: (0.000393s) DELETE FROM `instances_templates` WHERE (`instance_id` = 3)
D, [2015-11-06 16:09:17 #66146] [instance_update(foobar/0)] DEBUG -- DirectorJobRunner: (0.000606s) INSERT INTO `instances_templates` (`instance_id`, `template_id`) VALUES (1, 4)
E, [2015-11-06 16:09:17 #66146] [instance_update(foobar/2)] ERROR -- DirectorJobRunner: Mysql2::Error: Deadlock found when trying to get lock; try restarting transaction: INSERT INTO `instances_templates` (`instance_id`, `template_id`) VALUES (3, 4)
D, [2015-11-06 16:09:17 #66146] [instance_update(foobar/2)] DEBUG -- DirectorJobRunner: (0.000088s) ROLLBACK
E, [2015-11-06 16:09:17 #66146] [instance_update(foobar/2)] ERROR -- DirectorJobRunner: Error updating instance: #<Sequel::DatabaseError: Mysql2::Error: Deadlock found when trying to get lock; try restarting transaction>
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:87:in `query'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:87:in `block in _execute'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/logging.rb:37:in `log_yield'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:87:in `_execute'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `block in execute'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:91:in `hold'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `execute'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:72:in `execute_insert'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:817:in `execute_insert'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:343:in `insert'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/model/associations.rb:1190:in `block in def_many_to_many'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/model/associations.rb:1517:in `add_associated_object'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/model/associations.rb:1089:in `block in def_add_method'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance.rb:311:in `block (2 levels) in update_templates'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance.rb:310:in `each'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance.rb:310:in `block in update_templates'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:338:in `_transaction'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:300:in `block in transaction'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:293:in `transaction'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance.rb:308:in `update_templates'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/instance_updater.rb:148:in `apply_state'
/tmp/build/032cef26-56c8-425c-5d34-2d1582e43cdf/bosh-src/bosh-director/lib/bosh/director/instance_updater.rb:95:in `update'
```

Mysql doc suggests to retry on deadlocks. 

http://dev.mysql.com/doc/refman/5.0/en/innodb-deadlocks.html

Find if there is a global sequel configuration to do that.",2.0,107597576,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Retry db transactions on deadlock,1426194.0,[1426194],956238,553935,feature,2015-11-17T23:09:30Z,https://www.pivotaltracker.com/story/show/107597576
2015-11-18T00:27:38Z,2015-07-30T20:21:46Z,accepted,,should also work with existing --hard option for stop command. --hard deletes vms. bosh stop --hard is useful for releasing all of the compute resources without losing any persistent data.,4.0,100244046,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",bosh stop should stop the whole deployment,1783496.0,"[1783496, 1778582]",956238,81882,feature,2016-01-05T01:40:19Z,https://www.pivotaltracker.com/story/show/100244046
2015-11-18T00:27:39Z,2015-10-29T06:34:45Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/998/files
- move disable_blank_passwords into os image
- move disable_password_authentication into os image into base_ssh stage
- add bosh_enable_password_authentication for vsphere, vcloud, and warden
- collapse vsphere and vcloud stages since they are the same (also no diff based on os)",2.0,106917440,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pull in 'rebased azure support',1355110.0,"[1355110, 631325, 1550486]",956238,81882,feature,2015-11-18T00:27:53Z,https://www.pivotaltracker.com/story/show/106917440
2015-11-18T00:27:39Z,2015-10-29T16:32:46Z,accepted,,"https://github.com/cloudfoundry/bosh-agent/pull/39
do not pull in repartitioning bit.",2.0,106965166,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pull in 'azure support' for bosh-agent,631325.0,"[631325, 553935]",956238,81882,feature,2015-11-18T00:27:45Z,https://www.pivotaltracker.com/story/show/106965166
2015-11-19T00:52:23Z,2015-11-18T01:54:53Z,accepted,,,,108404874,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz should run at interval,553935.0,"[553935, 1750722]",956238,1426194,chore,2015-11-19T00:52:23Z,https://www.pivotaltracker.com/story/show/108404874
2015-11-19T19:34:11Z,2015-11-16T19:51:48Z,accepted,,"- size: 0, 100, 200
- disk type vs disk pool vs persistent_size
- persistent disks (migrated from)",4.0,108264714,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: persistence,553935.0,"[553935, 353433, 1750722]",956238,81882,feature,2015-11-19T19:34:11Z,https://www.pivotaltracker.com/story/show/108264714
2015-11-19T19:35:58Z,2015-10-02T22:32:46Z,accepted,,https://github.com/cloudfoundry/bosh/blob/master/bosh-stemcell/spec/stemcells/stig_spec.rb,1.0,104769092,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",move non-os specific stig numbers into expected_base_stig_test_cases,1386874.0,"[1386874, 1495236, 1426194]",956238,81882,feature,2015-11-19T19:37:56Z,https://www.pivotaltracker.com/story/show/104769092
2015-11-19T19:37:29Z,2015-11-17T17:31:26Z,accepted,,,0.0,108350966,story,[],bump vsphere gem to 2.2.0 for micro/director,1426194.0,[1426194],956238,81882,feature,2015-11-19T19:37:29Z,https://www.pivotaltracker.com/story/show/108350966
2015-11-19T19:47:06Z,2015-09-05T00:29:43Z,accepted,,"Wrong AZ for compilation:
 - current: Error 120002: Bosh::Director::CompilationConfigInvalidAvailabilityZone
 - wanted: same description as job using wrong az
""Job '#{job_name}' references unknown availability zone '#{name}'""

***AZ for compilation that does not have associated subnet:
 - current: ...
 - wanted: Error 150006: Compilation refers to an availability zone(s) '[""lol1""]' but 'default' has no matching subnet(s).***

Dynamic network:
 - current: Error 160008: top-level 'dns' invalid when specifying subnets
 - wanted: Error X: Specifying top-level 'dns' on network 'blah' when specifying 'subnets' is not supported

Dynamic network:
 - current: Error 160008: top-level 'cloud_properties' invalid when specifying subnets
 - wanted: Error X: Specifying top-level 'cloud_properties' on network 'blah' when specifying 'subnets' is not supported

Stemcell:
- current: Error 40001: Alias is required for top level stemcell
- wanted: Required property `alias' was not specified in object ({""os""=>""ubuntu-trusty""})

Stemcell:
- current: Error 40001: An OS or a name must be specified for a stemcell
- wanted: Required property `os' or `name' was not specified in object ({""os""=>""ubuntu-trusty"", ""alias""=>""something""})

Stemcell:
- current: Error 50005: Stemcell alias default already exists
- wanted: Duplicate stemcell alias `default'

Disks:
- current: Error 190012: Both disk_types and disk_pools are specified, only one key is allowed *Disk pools will be DEPRECATED in the future
- wanted: Error 190012: Both disk_types and disk_pools are specified, only one key is allowed. disk_pools key is will be DEPRECATED in the future.

Disks:
- current: Error 140011: Job `db' references both a persistent disk pool `small' and a persistent disk type `small'
- wanted: Error 140011:  <something similar to above>

Migration:
- current: Error 190018: Failed to migrate job 'db_z2' to 'db', deployment still contains it

Migration:
- current: Error 190018: Migrating job 'db_z2' refers to availability_zone 'z5' that is not in the list of availability_zones of 'db' job
- wanted: less underscores

Networking:
- current: Error 150002: Job 'db' declares static ip '10.10.0.103' which belongs to no subnet
- wanted: <include network name>",2.0,102788742,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",improve error messages (pair with dk),344.0,"[344, 1750722]",956238,81882,feature,2015-11-19T19:47:07Z,https://www.pivotaltracker.com/story/show/102788742
2015-11-19T19:47:24Z,2015-07-08T02:11:06Z,accepted,,"Current: 
  - Error 140012: Job 'new_one' requires links: [""dummy"", ""dummy2""] but only has following links: []
  - Error 140012: Job 'new_one' requires links: [""dummy"", ""dummy2""] but only has following links: [""dummy""]

Current:
  - Error 190016: Link 'd' references non-existent job 'b'

Current: 
  - Error 190016: Link 'a.c.d' is in invalid format

Current:
  - Error 190016: Link 'dummy' references non-existent template 'dummy2' in job 'new_one'
  - Error 190016: Link 'dummy2' is not provided by template 'dummy' in job 'new_one'

Current: 
  - Error 80014: Job 'dummy' 'provides' specifies links with duplicate name 'dummy'

Current:
  - Error 190016: Link 'name: dummy, type: dummy' references unknown deployment 'tiny-dummy2'

Current: (same type instead of same name??)
  - Error 190016: Link 'name: dummy2, type: dummy2' must reference link with the same name

Current:
  - Error 190016: Link 'db' can not be found by path 'tiny-dummy.db.dummy_with_properties.db'

Current:
  - Error 190016: Link 'dummy2' can not be found by path 'tiny-dummy.new_one.dummy.dummy2'

Current:
 - Error 140013: Link 'lol' is not required in job 'rails2'

Current:
  - Error 100: Error filling in template `lol.erb' for `rails/0' (line 1: Can't find link 'db.nodes')
  - Error 100: Error filling in file template `lol.erb' in release job `blah` for instance`rails/0' (line 1: Can't find link 'db.nodes')",2.0,98614152,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",improve links errors messages (pair with dk),344.0,"[344, 1750722]",956238,81882,feature,2015-11-19T19:47:24Z,https://www.pivotaltracker.com/story/show/98614152
2015-11-20T00:31:58Z,2015-11-19T19:16:31Z,accepted,,http://www.ubuntu.com/usn/usn-2815-1/,1.0,108561768,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for libpng vulns,1550486.0,[1550486],956238,81882,feature,2015-11-20T00:31:58Z,https://www.pivotaltracker.com/story/show/108561768
2015-11-20T20:10:07Z,2015-11-18T01:41:36Z,accepted,,InstanceUpdater is sending drain before every stop. Director drain script is looping forever right now because it fails to find pid files for workers,0.0,108404432,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Investigate if we need to drain on initial deploy,1550486.0,"[1550486, 1426194]",956238,1426194,feature,2015-11-20T20:10:07Z,https://www.pivotaltracker.com/story/show/108404432
2015-11-20T20:36:21Z,2015-11-08T02:54:13Z,accepted,,"```
+------------------+---------+---------------+-------------+
| Instance         | State   | Resource Pool | IPs         |
+------------------+---------+---------------+-------------+
| ha_proxy_z1/0    | failing | router_z1     | 10.244.0.34 |
|   haproxy_config | failing |               |             |
+------------------+---------+---------------+-------------+
+------------------+---------+---------------+-------------+
```",,107647526,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",fix formatting for --failing,1386874.0,"[1386874, 1495236]",956238,81882,bug,2015-11-20T20:36:21Z,https://www.pivotaltracker.com/story/show/107647526
2015-11-21T00:29:17Z,2015-07-13T19:42:42Z,accepted,,"An illicit ICMP redirect message could result in a man-in-the-middle attack.
---
None
---
SV-50349r3_rule
---
F-43496r1_fix
---
To set the runtime status of the ""net.ipv6.conf.default.accept_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv6.conf.default.accept_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv6.conf.default.accept_redirects = 0
---
C-46106r3_chk
---
If IPv6 is disabled, this is not applicable.

The status of the ""net.ipv6.conf.default.accept_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv6.conf.default.accept_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv6.conf.default.accept_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978206,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38548] [medium] The system must ignore ICMPv6 redirects by default.ZZ,1386874.0,"[1386874, 1495236, 1426194]",956238,81882,feature,2015-11-21T00:29:18Z,https://www.pivotaltracker.com/story/show/98978206
2015-11-21T00:34:30Z,2015-07-13T19:42:46Z,accepted,,"Accepting ""secure"" ICMP redirects (from those gateways listed as default gateways) has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50333r2_rule
---
F-43479r1_fix
---
To set the runtime status of the ""net.ipv4.conf.default.secure_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.default.secure_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.default.secure_redirects = 0
---
C-46089r2_chk
---
The status of the ""net.ipv4.conf.default.secure_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.default.secure_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.default.secure_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978348,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38532] [medium] The system must not accept ICMPv4 secure redirect packets by default.,1386874.0,"[1386874, 1495236, 1426194]",956238,81882,feature,2015-11-21T00:34:30Z,https://www.pivotaltracker.com/story/show/98978348
2015-11-21T00:39:41Z,2015-07-13T19:42:43Z,accepted,,"Sending ICMP redirects permits the system to instruct other systems to update their routing information. The ability to send ICMP redirects is only appropriate for systems acting as routers.
---
None
---
SV-50401r2_rule
---
F-43547r1_fix
---
To set the runtime status of the ""net.ipv4.conf.default.send_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.default.send_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.default.send_redirects = 0
---
C-46157r2_chk
---
The status of the ""net.ipv4.conf.default.send_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.default.send_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.default.send_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978234,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38600] [medium] The system must not send ICMPv4 redirects by default.,1386874.0,"[1386874, 1495236, 1426194]",956238,81882,feature,2015-11-21T00:39:41Z,https://www.pivotaltracker.com/story/show/98978234
2015-11-21T00:40:02Z,2015-07-13T19:42:43Z,accepted,,"Sending ICMP redirects permits the system to instruct other systems to update their routing information. The ability to send ICMP redirects is only appropriate for systems acting as routers.
---
None
---
SV-50402r2_rule
---
F-43548r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.send_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.send_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.send_redirects = 0
---
C-46159r2_chk
---
The status of the ""net.ipv4.conf.all.send_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.send_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.send_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978232,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38601] [medium] The system must not send ICMPv4 redirects from any interface.,1386874.0,"[1386874, 1495236, 1426194]",956238,81882,feature,2015-11-21T00:40:02Z,https://www.pivotaltracker.com/story/show/98978232
2015-11-21T00:40:33Z,2015-10-23T18:20:17Z,accepted,,"name = job/811da786-04f8-4019-8ece-ae670e5a0196
name = <job-name>/<id>",1.0,106505158,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",director should send name to the set_vm_metadata,344.0,[344],956238,1602644,feature,2015-11-21T00:40:33Z,https://www.pivotaltracker.com/story/show/106505158
2015-11-21T00:55:27Z,2015-11-17T20:02:00Z,accepted,,"- azs
- networking
- migration
- links
- cloud-config
- persistent disks

put into https://github.com/pivotal-cf-experimental/bosh-manual-fuzz-testing's README",1.0,108377422,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",create a list of 50 deploy combinations for manual tests,553935.0,"[553935, 344]",956238,81882,feature,2015-11-21T00:55:27Z,https://www.pivotaltracker.com/story/show/108377422
2015-11-23T00:53:47Z,2015-08-01T00:57:50Z,accepted,,"bosh instances --vitals
bosh instances --ps --vitals
bosh instances --ps --vitals --failing should work

```
+------------------------------------+---------+---------------+--------------+-----------------------+------+------+-------+--------------+------------+------------+------------+------------+
| uaa_z2/0                           | running | medium_z2     | 10.10.81.18  | 0.00, 0.01, 0.05      | 0.2% | 0.1% | 0.1%  | 17% (653.6M) | 0% (0B)    | 47%        | 0%         | 47%        |
|                                            |
| cloud_controller_ng        | running
|   status                            running
|   monitoring status                 monitored
|   pid                               30269
|   parent pid                        1
|   uptime                            1d 16h 16m
|   children                          10
|   memory kilobytes                  818996
|   memory kilobytes total            835824
|   memory percent                    10.6%
|   memory percent total              10.9%
|   cpu percent                       0.4%
|   cpu percent total                 0.4%
|   port response time                0.034s to 10.10.80.255:9022/v2/info [HTTP via TCP]
|   data collected                    Fri Jul 24 23:30:12 2015
|...
```",2.0,100344890,story,"[{'name': 'cli-ps', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-11T01:47:49Z', 'id': 12458762, 'updated_at': '2015-08-11T01:47:49Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh cli should present the full details of the vm state ,1495236.0,"[1495236, 1386874]",956238,1482982,feature,2015-11-23T00:54:22Z,https://www.pivotaltracker.com/story/show/100344890
2015-11-23T00:54:14Z,2015-11-02T08:16:20Z,accepted,,"https://github.com/cloudfoundry/bosh-agent/pull/35
ArePreconfigured?
- add similar support for centos",2.0,107178186,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]","pull in 'add network attribute ""preconfigured"" to support Softlayer network'",553935.0,[553935],956238,81882,feature,2015-11-23T00:54:14Z,https://www.pivotaltracker.com/story/show/107178186
2015-11-23T01:56:28Z,2015-11-10T06:02:11Z,accepted,,"do not want to see lots of finished empty ScheduledOrphanCleanup tasks. as time goes there is just too many of them.
",1.0,107789192,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",do not enqueue orphan cleanup task when there is nothing to cleanup,344.0,[344],956238,81882,feature,2015-11-23T01:56:28Z,https://www.pivotaltracker.com/story/show/107789192
2015-11-23T01:57:38Z,2015-08-11T04:24:15Z,accepted,,"download each blob in a release version (name/version) and verify its checksum
- blobs that do not match recorded sha1 should be deleted and correct blob should be uploaded
- blobs that are not found should be backfilled with correct blob
- include detailed logs in debug log of what actions were taken

repro:
- bosh upload release blah.tgz
- delete some blobs from the blobstore
- corrupt some blobs in the blobstore (so they dont match sha1)
- bosh deploy will fail now
- bosh upload relese blah.tzg --fix
- bosh deploy should succeed now

bosh inspect release blah/1.1 may be useful

https://github.com/cloudfoundry/bosh/issues/878",2.0,101003512,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can run upload release --fix which will verify blobstore contents and replace with correct versions,1386874.0,[1386874],956238,81882,feature,2015-11-23T01:57:46Z,https://www.pivotaltracker.com/story/show/101003512
2015-11-23T02:45:06Z,2015-11-16T20:16:04Z,accepted,,revert previous commit?,1.0,108267180,story,"[{'name': 'links', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:03Z', 'id': 11514474, 'updated_at': '2015-04-27T17:32:03Z'}]",link must include index,1426194.0,[1426194],956238,81882,feature,2015-11-23T02:45:06Z,https://www.pivotaltracker.com/story/show/108267180
2015-11-23T06:02:41Z,2015-10-05T22:15:18Z,accepted,,,,104952082,story,"[{'name': 'orph-disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-05T22:13:10Z', 'id': 12964312, 'updated_at': '2015-10-05T22:13:10Z'}]",basic orphaned disks,,[],956238,81882,release,2015-11-23T06:02:41Z,https://www.pivotaltracker.com/story/show/104952082
2015-11-23T18:20:55Z,2015-11-20T23:24:26Z,accepted,,,,108659128,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: Revert this commit https://github.com/cloudfoundry-incubator/bosh-fuzz-tests/commit/a817437e96f471e762729fbf224bd80d9998a703,344.0,[344],956238,344,chore,2015-11-26T00:26:03Z,https://www.pivotaltracker.com/story/show/108659128
2015-11-23T23:06:01Z,2015-10-29T18:57:52Z,accepted,,,,106980090,story,[],Move os_image_versions.json from bosh-dev to bosh-stemcell,1550486.0,[1550486],956238,1550486,chore,2015-11-23T23:06:01Z,https://www.pivotaltracker.com/story/show/106980090
2015-11-24T00:17:01Z,2015-11-20T02:23:24Z,accepted,,"When CPI does not return IP address for dynamic network (probably not possible in real CPI) we fail with error:

```
, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: Fetching existing instance for: #<Bosh::Director::Models::Instance @values={:id=>2, :job=>""uaC31A6UFD"", :index=>1, :deployment_id=>1, :vm_id=>3, :state=>""started"", :resurrection_paused=>false, :uuid=>""b723561f-2c02-4525-a080-3aefb0c05138"", :availability_zone=>""z1"", :cloud_properties=>""{}"", :compilation=>false, :bootstrap=>false, :dns_records=>""[\""1.uac31a6ufd.gp9rp5w.foo-deployment.bosh\"",\""b723561f-2c02-4525-a080-3aefb0c05138.uac31a6ufd.gp9rp5w.foo-deployment.bosh\""]"", :spec_json=>""{\""deployment\"":\""foo-deployment\"",\""job\"":{\""name\"":\""uaC31A6UFD\"",\""templates\"":[{\""name\"":\""simple\"",\""version\"":\""e4221f5e7c80d0f804f454fe164a64ca03753325\"",\""sha1\"":\""aa73ff6d5322f78a478cff596590a6728f7aabef\"",\""blobstore_id\"":\""2af3661f-6974-44f2-bf5c-e60de87e8230\""}],\""template\"":\""simple\"",\""version\"":\""e4221f5e7c80d0f804f454fe164a64ca03753325\"",\""sha1\"":\""aa73ff6d5322f78a478cff596590a6728f7aabef\"",\""blobstore_id\"":\""2af3661f-6974-44f2-bf5c-e60de87e8230\""},\""index\"":1,\""bootstrap\"":false,\""id\"":\""b723561f-2c02-4525-a080-3aefb0c05138\"",\""az\"":\""z1\"",\""networks\"":{\""gP9Rp5w\"":{\""type\"":\""dynamic\"",\""cloud_properties\"":{},\""dns\"":[\""8.8.8.8\""],\""dns_record_name\"":\""1.uac31a6ufd.gp9rp5w.foo-deployment.bosh\"",\""ip\"":\""\"",\""netmask\"":\""\"",\""gateway\"":\""\""}},\""vm_type\"":{\""name\"":\""default\"",\""cloud_properties\"":{}},\""stemcell\"":{\""name\"":\""ubuntu-stemcell\"",\""version\"":\""1\""},\""env\"":{},\""packages\"":{},\""properties\"":{},\""dns_domain_name\"":\""bosh\"",\""links\"":{},\""persistent_disk\"":200,\""persistent_disk_pool\"":{\""name\"":\""a5TDgeNjM9\"",\""disk_size\"":200,\""cloud_properties\"":{}},\""persistent_disk_type\"":{\""name\"":\""a5TDgeNjM9\"",\""disk_size\"":200,\""cloud_properties\"":{}},\""template_hashes\"":{\""simple\"":\""dd0b618d9684b1ff3211351883e9e6470fc228b8\""},\""rendered_templates_archive\"":{\""blobstore_id\"":\""ea860731-6b62-466f-83cd-ea8ea852d642\"",\""sha1\"":\""5ee8b86692f199e850f6b93a8a07d18058f29102\""},\""configuration_hash\"":\""dd0b618d9684b1ff3211351883e9e6470fc228b8\""}""}>
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: (0.000098s) SELECT NULL
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: (0.000294s) SELECT * FROM ""ip_addresses"" WHERE (""ip_addresses"".""instance_id"" = 2)
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: [network-configuration] Creating instance network reservations from agent state for instance 'wz1WomQyHGCppJ7QeUX5HEN0cbrlVXyJslbQMLoKkKaPzUmNq8DU19PZJWPcuvIhsuVMt2P7aJAtz4FpLmkUNBGGmBc79iU89MUf1Wed5dwhWNGhxDkRoq0opHltqAcXPSI2cfv4sAWce3MVxGzUCOYmCoyGUXsQvnBMlQNQ15i6kRCc18xMe8slV0B4/0'
D, [2015-11-19 18:07:10 #42234] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:foo-deployment
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:foo-deployment
I, [2015-11-19 18:07:10 #42234] [task:4]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-11-19 18:07:10 #42234] [task:4] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""302c0de8-5af3-48f7-8dc3-3fb272104c4f"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'foo-deployment' against Director '7c0227e3-685f-4c43-8fa3-c8f52d9669d5': #<NetAddr::ValidationError: Could not auto-detect IP version for ''.>"",""created_at"":1447985230}
E, [2015-11-19 18:07:10 #42234] [task:4] ERROR -- DirectorJobRunner: Could not auto-detect IP version for ''.
/Users/pivotal/.gem/ruby/2.1.7/gems/netaddr-1.5.0/lib/ip_math.rb:54:in `detect_ip_version'
/Users/pivotal/.gem/ruby/2.1.7/gems/netaddr-1.5.0/lib/cidr.rb:157:in `create'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/ip_util.rb:55:in `ip_to_netaddr'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/ip_util.rb:63:in `format_ip'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:64:in `add_existing'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:12:in `block in create_from_state'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:11:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:11:in `create_from_state'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_repository.rb:19:in `fetch_existing'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_plan_factory.rb:28:in `desired_existing_instance_plan'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/availability_zone_picker.rb:59:in `block in desired_existing_instance_plans'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/availability_zone_picker.rb:58:in `map'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/availability_zone_picker.rb:58:in `desired_existing_instance_plans'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/availability_zone_picker.rb:22:in `place_and_match_in'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:31:in `assign_zones'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:14:in `create_instance_plans'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:14:in `plan_job_instances'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:35:in `block in bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/planner.rb:128:in `bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/jobs/update_deployment.rb:44:in `block (2 levels) in perform'
```",,108588820,story,"[{'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",Dummy CPI should not return empty IP on dynamic network,344.0,"[344, 1426194]",956238,553935,bug,2015-11-24T00:17:01Z,https://www.pivotaltracker.com/story/show/108588820
2015-11-24T00:17:19Z,2015-10-26T18:48:01Z,accepted,,,,106678868,story,[],benchmark integration tests and find causes of slowness,1426194.0,"[1426194, 1750722, 1550486]",956238,1017727,chore,2015-11-24T00:17:19Z,https://www.pivotaltracker.com/story/show/106678868
2015-11-24T01:20:24Z,2015-07-13T21:58:30Z,accepted,,,,98991314,story,"[{'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",extra hardening of stemcell - part 3,,[],956238,81882,release,2015-11-24T01:20:24Z,https://www.pivotaltracker.com/story/show/98991314
2015-11-24T18:03:11Z,2015-11-18T19:48:23Z,accepted,,"https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/109

```
Director task 12
  Started preparing deployment > Preparing deployment. Done (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started deleting unneeded instances
  Started deleting unneeded instances > u069L/2
  Started deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/2
  Started deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/3
  Started deleting unneeded instances > u069L/0
  Started deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/1
  Started deleting unneeded instances > u069L/3
  Started deleting unneeded instances > u069L/4
  Started deleting unneeded instances > y1F7b/0
  Started deleting unneeded instances > u069L/1
  Started deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/4
  Started deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/0. Failed: Attempt to delete object did not result in a single row modification (Rows Deleted: 0, SQL: DELETE FROM ""records"" WHERE (""id"" = 157)) (00:00:00)
     Done deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/4 (00:00:01)
     Done deleting unneeded instances > u069L/2 (00:00:01)
     Done deleting unneeded instances > u069L/0 (00:00:01)
     Done deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/2 (00:00:01)
     Done deleting unneeded instances > u069L/4 (00:00:01)
     Done deleting unneeded instances > y1F7b/0 (00:00:01)
     Done deleting unneeded instances > u069L/3 (00:00:01)
     Done deleting unneeded instances > u069L/1 (00:00:01)
     Done deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/1 (00:00:01)
     Done deleting unneeded instances > l9PlXVIFqfudYWwvXfVvVe3Tmo5ipHpRb3RtbWOpDpzkati5GIfePW1iRF10FVEK6hroaWZQrtBisuEmbV0UbaG0MzGdfcrD7VWldCl9fsb6IvzZAhkHQPntprXtaYBBizVGiUblDK6ILupNuS9Ul9FzUiZzvEFRv1QtP6jkImArTsue5pBZTstBiikO/3 (00:00:01)
   Failed deleting unneeded instances (00:00:01)

Error 100: Attempt to delete object did not result in a single row modification (Rows Deleted: 0, SQL: DELETE FROM ""records"" WHERE (""id"" = 157))
```",,108476894,story,"[{'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",deleting dns record should not fail,344.0,[344],956238,553935,bug,2015-11-24T18:03:11Z,https://www.pivotaltracker.com/story/show/108476894
2015-11-24T23:15:48Z,2015-11-24T23:12:34Z,accepted,,spec/integration/deploy_spec.rb:53,,108906398,story,[],Fix flaky deploy spec on global-net,1550486.0,[1550486],956238,1550486,chore,2015-11-25T21:27:17Z,https://www.pivotaltracker.com/story/show/108906398
2015-11-25T00:58:09Z,2015-11-16T19:51:28Z,accepted,,"- networks (dynamic, manual, vip)
- use static_ips vs not?
- generate some reserved ips, static ips
- use azs in a subnet or not",4.0,108264672,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: networks,553935.0,"[553935, 1750722]",956238,81882,feature,2015-11-25T00:58:09Z,https://www.pivotaltracker.com/story/show/108264672
2015-11-25T22:41:13Z,2015-11-24T23:09:37Z,accepted,,"only allow:

```
- name: blah
  type: dynamic
  cloud_properties: {...}
  dns: [...]
```

or

```
- name: blah
  type: dynamic
  subnets:
  - cloud_properties: {...}
    azs: [z1,z2]
    dns: [...]
```",1.0,108906254,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user sees an error if they specify az/azs on a dynamic network,344.0,[344],956238,81882,feature,2015-11-25T22:41:13Z,https://www.pivotaltracker.com/story/show/108906254
2015-11-25T23:55:35Z,2015-11-19T21:55:52Z,accepted,,"on develop
",1.0,108575628,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Director drain script should not drain forever on initial deploy,1550486.0,[1550486],956238,1550486,feature,2015-11-25T23:55:35Z,https://www.pivotaltracker.com/story/show/108575628
2015-11-25T23:55:49Z,2015-10-28T21:20:33Z,accepted,,"currently domain_name configuration assumes user want to specify tld (e.g. 'bosh' or 'com'). instead we should allow domain name to be more complex domain (e.g. dc1.bosh.company.com).

""Having a full proper domain name would allow us to delegate NS records internally to BOSH.""

this change should be done on global-net branch!

today:
       bosh -> job.network.deployment.bosh
       bosh.com -> job.network.deployment.boshcom
wanted:
       dc1.bosh.company.com -> job.network.deployment.dc1.bosh.company.com

- reverse dns
- ns records...",4.0,106893036,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can configure bosh dns to complex domain name instead of just tld,631325.0,[631325],956238,81882,feature,2015-11-25T23:55:49Z,https://www.pivotaltracker.com/story/show/106893036
2015-11-26T00:10:09Z,2015-11-18T23:46:08Z,accepted,,"Right now it fails, saying that, migrated_from must specify az, because old job did not have az",1.0,108499370,story,"[{'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",It should  be possible to migrate job without az to job without az,344.0,[344],956238,553935,feature,2015-11-26T00:10:09Z,https://www.pivotaltracker.com/story/show/108499370
2015-11-26T00:12:50Z,2015-11-20T23:58:36Z,accepted,,,,108660744,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Drain scripts should not be called on initial job deploy,344.0,"[344, 553935]",956238,81882,bug,2015-11-26T00:12:50Z,https://www.pivotaltracker.com/story/show/108660744
2015-11-26T00:28:20Z,2015-11-20T21:58:54Z,accepted,,"When changing the static IP from`10.10.0.21, 10.10.0.22` to `10.10.0.22 , 10.10.0.23` it fails with the error:
```
: #<Bosh::Director::NetworkReservationAlreadyInUse: Failed to reserve IP '10.10.0.22' for 'default': already reserved>"",""crea
ted_at"":1448055465}
E, [2015-11-20 21:37:45 #6119] [task:17] ERROR -- DirectorJobRunner: Failed to reserve IP '10.10.0.22' for 'default': already reserved
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/ip_provider/in_memory_ip_repo.rb:52:in `add_ip
'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/ip_provider/in_memory_ip_repo.rb:23:in `add'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/ip_provider/ip_provider.rb:102:in `reserve_man
ual'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/ip_provider/ip_provider.rb:70:in `reserve'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job.rb:280:in `block in bind_instance_networks
'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job.rb:278:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/job.rb:278:in `bind_instance_networks'
```

See attached files for additional details",,108654620,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",Changing one static IP and order should not fail,344.0,[344],956238,1550486,bug,2015-11-26T00:28:20Z,https://www.pivotaltracker.com/story/show/108654620
2015-11-26T01:36:32Z,2015-11-18T00:48:46Z,accepted,,,0.0,108402100,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: bootstrap,344.0,[344],956238,553935,feature,2015-11-26T01:36:32Z,https://www.pivotaltracker.com/story/show/108402100
2015-11-30T18:29:07Z,2015-11-25T22:23:31Z,accepted,,,0.0,108988124,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",rename availability zone to az in job erb template and links,344.0,"[344, 659629]",956238,81882,feature,2015-11-30T18:29:07Z,https://www.pivotaltracker.com/story/show/108988124
2015-11-30T18:30:36Z,2015-11-25T22:26:23Z,accepted,,"https://github.com/cloudfoundry/bosh/blob/global-net/bosh-director/lib/bosh/director/jobs/vm_state.rb#L81
in director and cli",1.0,108988222,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",rename availability zone in bosh vms api call,344.0,[344],956238,81882,feature,2015-11-30T18:30:36Z,https://www.pivotaltracker.com/story/show/108988222
2015-11-30T18:33:36Z,2015-11-17T23:41:25Z,accepted,,"```
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.000089s) SELECT NULL
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.000843s) DELETE FROM ""deployments_stemcells"" WHERE ((""stemcell_id"" = 9) AND (""deployment_id"" = 23))
D, [2015-11-17 23:37:51 #24981] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:tiny-dummy
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:tiny-dummy
I, [2015-11-17 23:37:51 #24981] [task:20273]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""ab8363af-b47a-4c24-8213-b013979a3d91"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'tiny-dummy' against Director 'f976642f-ade0-4d68-8660-2bba6817f10f': #<NoMethodError: undefined method `credentials' for nil:NilClass>"",""created_at"":1447803471}
E, [2015-11-17 23:37:51 #24981] [task:20273] ERROR -- DirectorJobRunner: undefined method `credentials' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/agent_client.rb:30:in `with_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/disk_manager.rb:215:in `agent'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/disk_manager.rb:211:in `disks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/disk_manager.rb:178:in `unmount'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/disk_manager.rb:104:in `unmount_disk_for'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/instance_updater.rb:69:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_updater.rb:100:in `block (2 levels) in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_updater.rb:98:in `block in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_updater.rb:97:in `update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_updater.rb:91:in `block (2 levels) in update_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.008237s) SELECT NULL
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.000058s) BEGIN
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.000201s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-11-17 23:37:51.217958+0000', ""description"" = 'create deployment', ""result"" = 'undefined method `credentials'' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/20273', ""checkpoint_time"" = '2015-11-17 23:37:50.804463+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 20273)
D, [2015-11-17 23:37:51 #24981] [task:20273] DEBUG -- DirectorJobRunner: (0.000679s) COMMIT
I, [2015-11-17 23:37:51 #24981] []  INFO -- DirectorJobRunner: Task took 0.425420936 seconds to process.
```",,108398600,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",running bosh deploy after a stop results in a nil error,344.0,"[344, 1550486, 353433]",956238,81882,bug,2015-11-30T18:33:36Z,https://www.pivotaltracker.com/story/show/108398600
2015-11-30T18:37:03Z,2015-11-23T22:18:18Z,accepted,,Dynamic networks do not allow the key `az` when subnets are present. It should not allow the key azs either.,,108815764,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",dynamic networks should not allow the key `azs` on network level,344.0,[344],956238,344,bug,2015-11-30T18:37:03Z,https://www.pivotaltracker.com/story/show/108815764
2015-11-30T18:41:42Z,2015-11-24T18:37:20Z,accepted,,"my deploy using two stemcells did not record them being used. only first one was shown to be used in bosh deployments output.

i was using stemcells and vm_type.",,108884580,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bosh deploy should result in used stemcells marked as such,344.0,[344],956238,81882,bug,2015-11-30T18:41:42Z,https://www.pivotaltracker.com/story/show/108884580
2015-12-01T19:53:20Z,2015-12-01T18:42:32Z,accepted,,"the `ifg.cfapps.io` app no longer exists.  
In LA, we use Firefox with these two plugins: 
https://addons.mozilla.org/en-US/firefox/addon/tile-view/
https://addons.mozilla.org/en-US/firefox/addon/menubar-autohide-fixed/?src=api
",,109289216,story,[],fix information radiator,1550486.0,[1550486],956238,119,chore,2015-12-01T19:53:20Z,https://www.pivotaltracker.com/story/show/109289216
2015-12-01T21:47:05Z,2015-11-30T23:33:48Z,accepted,,http://www.ubuntu.com/usn/usn-2821-1/,1.0,109218492,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for USN-2821-1: GnuTLS vulnerability,344.0,[344],956238,81882,feature,2015-12-01T21:49:03Z,https://www.pivotaltracker.com/story/show/109218492
2015-12-01T21:52:38Z,2015-11-30T17:45:30Z,accepted,,"Ubuntu 14.04 LTS:
  dpkg                            1.17.5ubuntu5.5",1.0,109185276,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for [USN-2820-1] dpkg vulnerability,344.0,[344],956238,81882,feature,2015-12-01T21:52:38Z,https://www.pivotaltracker.com/story/show/109185276
2015-12-03T01:38:24Z,2015-11-16T20:03:29Z,accepted,,pivotal-cf-experimental/tmp-bosh-cf-mysql-release,2.0,108265920,story,"[{'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",finish tmp cf mysql release to use links/az/etc.,948679.0,"[948679, 659629, 1169826]",956238,81882,feature,2015-12-03T01:38:25Z,https://www.pivotaltracker.com/story/show/108265920
2015-12-03T01:38:34Z,2015-11-09T20:02:07Z,accepted,,"expect that data is still there (persistent disks are not lost)

- deploy old cf with old manifest
- make sure cats pass
- change old manifest to use new features (cloud-config, azs without links,  migrated_from)
- make sure cats pass",4.0,107752912,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",migrate old cf manifest to new manifest,353433.0,"[353433, 631325, 1233340]",956238,81882,feature,2015-12-03T01:38:34Z,https://www.pivotaltracker.com/story/show/107752912
2015-12-03T01:38:36Z,2015-11-20T21:49:10Z,accepted,,"bump stemcell version in manifest and do bosh deploy. current code for some reason doesnt recreate vms. somehow also bosh deployments doesnt show any used stemcells.

i was using stemcells section and vm_types.",,108654084,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bumping stemcell version should recreate vms,344.0,"[344, 1426194, 659629]",956238,81882,bug,2015-12-03T01:38:36Z,https://www.pivotaltracker.com/story/show/108654084
2015-12-03T01:38:47Z,2015-11-18T01:16:36Z,accepted,,"Review the results at https://github.com/pivotal-cf-experimental/bosh-manual-fuzz-testing/tree/master/successful_deployments.

Check the subdirs that are today's date. Look, it's totally boring and you don't need to review the individual files. Rest assured that we have done the work in the tradition of the fine, Old World craftsmanship which you have come to expect from us.",1.0,108403406,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: orphaned disks,353433.0,"[353433, 1017727]",956238,553935,feature,2015-12-03T01:38:47Z,https://www.pivotaltracker.com/story/show/108403406
2015-12-03T01:45:44Z,2015-12-01T01:03:11Z,accepted,,,,109223850,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",When not using cloud config director should not reserve same IP for different VMs,553935.0,"[553935, 1868036]",956238,1750722,bug,2015-12-03T01:45:44Z,https://www.pivotaltracker.com/story/show/109223850
2015-12-03T18:45:02Z,2015-11-19T22:26:36Z,accepted,,"https://github.com/cloudfoundry/bosh/pull/1028
",0.0,108577924,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh #1028: Add source into the payload of notifier,81882.0,[81882],956238,1524644,feature,2015-12-03T18:45:11Z,https://www.pivotaltracker.com/story/show/108577924
2015-12-03T18:48:53Z,2015-11-24T23:38:39Z,accepted,,https://github.com/cloudfoundry/bosh/pull/1039,2.0,108907938,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",pull in photon stemcell rename,659629.0,"[659629, 1233340]",956238,81882,feature,2015-12-03T18:48:53Z,https://www.pivotaltracker.com/story/show/108907938
2015-12-03T19:25:51Z,2015-11-25T01:25:42Z,accepted,,"- deleting jobs
- adding jobs
- reordering same jobs",2.0,108913634,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: order of jobs,553935.0,"[553935, 659629]",956238,81882,feature,2015-12-03T19:25:51Z,https://www.pivotaltracker.com/story/show/108913634
2015-12-03T19:25:56Z,2015-11-16T19:52:22Z,accepted,,- vm types vs stemcells vs resource pools,2.0,108264798,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: vms,553935.0,"[553935, 1868036]",956238,81882,feature,2015-12-03T19:25:56Z,https://www.pivotaltracker.com/story/show/108264798
2015-12-03T21:35:29Z,2015-11-09T18:53:30Z,accepted,,"e.g.:

- Concourse pipeline could listen for certain file paths / sha diffs, etc. 
- Concourse pipeline could update os_image_version.json with new S3 keys, and OS_IMAGES.md with description from Tracker story and S3 keys.
",,107745646,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Review OS image building for more automation opportunities,1550486.0,[1550486],956238,1550486,chore,2015-12-03T21:35:29Z,https://www.pivotaltracker.com/story/show/107745646
2015-12-04T00:49:13Z,2015-09-16T22:18:39Z,accepted,,"copy bosh-aws-cpi pipeline 
cross-pair with cpi team?",,103551020,story,"[{'name': 'aws', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-31T04:33:45Z', 'id': 9082176, 'updated_at': '2014-07-31T04:33:45Z'}, {'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",run aws ubuntu bats for global-net branch in ci,1017727.0,"[1017727, 1750722]",956238,81882,chore,2015-12-05T00:54:37Z,https://www.pivotaltracker.com/story/show/103551020
2015-12-04T08:13:12Z,2015-11-11T23:24:47Z,accepted,,,0.0,107958688,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",make sure resolvconf -u works on bosh-lite stemcells,81882.0,[81882],956238,81882,feature,2015-12-04T08:13:12Z,https://www.pivotaltracker.com/story/show/107958688
2015-12-04T20:09:47Z,2015-11-17T23:52:46Z,accepted,,"similar to what we did for bosh stop (https://www.pivotaltracker.com/story/show/100244046)
note that maria's pair did some minor changes before merging it in",2.0,108399300,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",bosh start/restart/recreate should allow to start/restart/recreate full deployment,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-12-04T20:09:57Z,https://www.pivotaltracker.com/story/show/108399300
2015-12-07T19:28:26Z,2015-11-24T01:16:08Z,accepted,,i suspect it has something to do with untarring on michael's system. it appears that we dont check for exit code on tar in https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/release_tarball.rb#L40-L52 (system returns bool). stacktrace attached...,,108826704,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",uploading release should not fail,948679.0,"[948679, 1169826, 631325]",956238,81882,bug,2015-12-07T19:28:26Z,https://www.pivotaltracker.com/story/show/108826704
2015-12-07T21:19:51Z,2015-07-13T19:42:41Z,accepted,,"Files from shared library directories are loaded into the address space of processes (including privileged ones) or of the kernel itself at runtime. Proper ownership is necessary to protect the integrity of the system.
---
None
---
SV-50266r1_rule
---
F-43411r1_fix
---
System-wide shared library files, which are linked to executables during process load time or run time, are stored in the following directories by default: 

/lib
/lib64
/usr/lib
/usr/lib64

If any file in these directories is found to be owned by a user other than root, correct its ownership with the following command: 

# chown root [FILE]
---
C-46021r1_chk
---
System-wide shared library files, which are linked to executables during process load time or run time, are stored in the following directories by default: 

/lib
/lib64
/usr/lib
/usr/lib64


Kernel modules, which can be added to the kernel during runtime, are stored in ""/lib/modules"". All files in these directories should not be group-writable or world-writable.  To find shared libraries that are not owned by ""root"", run the following command for each directory [DIR] which contains shared libraries: 

$ find -L [DIR] \! -user root


If any of these files are not owned by root, this is a finding.",1.0,98978144,story,"[{'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38466] [medium] Library files must be owned by root.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-12-07T21:19:52Z,https://www.pivotaltracker.com/story/show/98978144
2015-12-07T21:20:11Z,2015-07-13T19:42:42Z,accepted,,"System binaries are executed by privileged users as well as system services, and restrictive permissions are necessary to ensure that their execution of these programs cannot be co-opted.
---
None
---
SV-50272r1_rule
---
F-43417r1_fix
---
System executables are stored in the following directories by default: 

/bin
/usr/bin
/usr/local/bin
/sbin
/usr/sbin
/usr/local/sbin

If any file [FILE] in these directories is found to be owned by a user other than root, correct its ownership with the following command: 

# chown root [FILE]
---
C-46027r1_chk
---
System executables are stored in the following directories by default: 

/bin
/usr/bin
/usr/local/bin
/sbin
/usr/sbin
/usr/local/sbin

All files in these directories should not be group-writable or world-writable. To find system executables that are not owned by ""root"", run the following command for each directory [DIR] which contains system executables: 

$ find -L [DIR] \! -user root


If any system executables are found to not be owned by root, this is a finding.",1.0,98978210,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38472] [medium] All system command files must be owned by root.,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-12-07T21:20:11Z,https://www.pivotaltracker.com/story/show/98978210
2015-12-07T21:20:13Z,2015-07-13T19:42:41Z,accepted,,"Files from shared library directories are loaded into the address space of processes (including privileged ones) or of the kernel itself at runtime. Restrictive permissions are necessary to protect the integrity of the system.
---
None
---
SV-50265r3_rule
---
F-43409r2_fix
---
System-wide shared library files, which are linked to executables during process load time or run time, are stored in the following directories by default: 

/lib
/lib64
/usr/lib
/usr/lib64

If any file in these directories is found to be group-writable or world-writable, correct its permission with the following command: 

# chmod go-w [FILE]
---
C-46019r4_chk
---
System-wide shared library files, which are linked to executables during process load time or run time, are stored in the following directories by default: 

/lib
/lib64
/usr/lib
/usr/lib64


Kernel modules, which can be added to the kernel during runtime, are stored in ""/lib/modules"". All files in these directories should not be group-writable or world-writable. To find shared libraries that are group-writable or world-writable, run the following command for each directory [DIR] which contains shared libraries: 

$ find -L [DIR] -perm /022 -type f


If any of these files (excluding broken symlinks) are group-writable or world-writable, this is a finding.",1.0,98978146,story,"[{'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38465] [medium] Library files must have mode 0755 or less permissive.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-12-07T21:20:13Z,https://www.pivotaltracker.com/story/show/98978146
2015-12-07T21:20:14Z,2015-07-13T19:42:41Z,accepted,,"System binaries are executed by privileged users, as well as system services, and restrictive permissions are necessary to ensure execution of these programs cannot be co-opted.
---
None
---
SV-50269r3_rule
---
F-43414r1_fix
---
System executables are stored in the following directories by default: 

/bin
/usr/bin
/usr/local/bin
/sbin
/usr/sbin
/usr/local/sbin

If any file in these directories is found to be group-writable or world-writable, correct its permission with the following command: 

# chmod go-w [FILE]
---
C-46024r3_chk
---
System executables are stored in the following directories by default: 

/bin
/usr/bin
/usr/local/bin
/sbin
/usr/sbin
/usr/local/sbin

All files in these directories should not be group-writable or world-writable. To find system executables that are group-writable or world-writable, run the following command for each directory [DIR] which contains system executables: 

$ find -L [DIR] -perm /022 -type f

If any system executables are found to be group-writable or world-writable, this is a finding.",1.0,98978154,story,"[{'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38469] [medium] All system command files must have mode 755 or less permissive.,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-12-07T21:20:14Z,https://www.pivotaltracker.com/story/show/98978154
2015-12-07T21:35:15Z,2015-09-23T21:08:08Z,accepted,,- get '/:deployment/jobs/:job/:index/logs',2.0,104038162,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[rupa]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:59:51Z', 'id': 13197994, 'updated_at': '2015-10-30T23:59:51Z'}]",user can fetch logs of the vm when vm is referenced by id,1017727.0,"[1017727, 353433]",956238,81882,feature,2015-12-07T21:35:15Z,https://www.pivotaltracker.com/story/show/104038162
2015-12-07T21:36:12Z,2015-11-25T22:05:50Z,accepted,,"job_must_exist_in_deployment
valid_index_for",1.0,108987288,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",remove strict validation of indices,1550486.0,"[1550486, 1426194]",956238,81882,feature,2015-12-07T21:36:25Z,https://www.pivotaltracker.com/story/show/108987288
2015-12-07T22:56:59Z,2015-12-01T23:33:40Z,accepted,,audit other places where we may return n/a,0.0,109314904,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",dont return n/a from the api for orphaned disks,353433.0,"[353433, 1017727]",956238,81882,feature,2015-12-07T22:56:59Z,https://www.pivotaltracker.com/story/show/109314904
2015-12-08T00:42:08Z,2015-12-03T22:56:38Z,accepted,,"During manual fuzz testing...

- initial deployment:

```
  ---
  name: deployment
  # ...snip...
  jobs:
  - name: dummy_z1
    # ...snip...
    networks:
    - name: default
      static_ips:
      - 10.0.0.22
      - 10.0.0.23
      - 10.0.0.24
```
- following deployment:

```
  ---
  name: deployment
  # ...snip...
  jobs:
  - name: dummy_z1
    # ...snip...
    networks:
    - name: default
      static_ips:
      - 10.0.0.24
      - 10.0.0.22
      - 10.0.0.23
```
- exception:

```
  E, [2015-12-03 21:57:12 #16976] [task:44] ERROR -- DirectorJobRunner: `dummy_z1/0' asked for a static IP 10.0.0.24 but it's already reserved/in use
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/network_reservation.rb:94:in `handle_error'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/network.rb:40:in `reserve!'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/job.rb:238:in `block (2 levels) in bind_instance_networks'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/job.rb:235:in `each'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/job.rb:235:in `block in bind_instance_networks'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/job.rb:234:in `each'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/job.rb:234:in `bind_instance_networks'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/assembler.rb:276:in `each'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/assembler.rb:276:in `bind_instance_networks'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:188:in `block in run_prepare_step'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:195:in `block in track_and_log'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/event_log.rb:97:in `call'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/event_log.rb:50:in `track'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:193:in `track_and_log'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:187:in `run_prepare_step'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:96:in `bind_vms'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/deployment_plan/planner_factory.rb:39:in `planner'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/jobs/update_deployment.rb:31:in `block in perform'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/lock.rb:56:in `lock'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/jobs/update_deployment.rb:25:in `perform'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/job_runner.rb:102:in `perform_job'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/job_runner.rb:32:in `block in run'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3143.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/job_runner.rb:32:in `run'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
  /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3143.0/bin/bosh-director-worker:76:in `<top (required)>'
  /var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
  /var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```",,109488200,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Changing order of static IPs in deployment manifest should not cause deploy to fail,1017727.0,"[1017727, 1868036]",956238,1550486,bug,2015-12-08T00:42:08Z,https://www.pivotaltracker.com/story/show/109488200
2015-12-08T02:33:48Z,2015-12-04T20:06:05Z,accepted,,"http://www.ubuntu.com/usn/usn-2830-1/
http://www.ubuntu.com/usn/usn-2829-1/",1.0,109549592,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for 'USN-2829-1: Linux kernel vulnerabilities' and 'USN-2830-1: OpenSSL vulnerabilities',553935.0,[553935],956238,81882,feature,2015-12-08T02:33:48Z,https://www.pivotaltracker.com/story/show/109549592
2015-12-08T23:27:21Z,2015-11-18T00:59:04Z,accepted,,,1.0,108402562,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: resurrector & cck,353433.0,"[353433, 1426194]",956238,553935,feature,2015-12-08T23:27:21Z,https://www.pivotaltracker.com/story/show/108402562
2015-12-08T23:27:29Z,2015-11-25T01:26:43Z,accepted,,"issue referenced: https://www.pivotaltracker.com/story/show/108654084
first story with expectations",4.0,108913674,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: figure out how to try to find stemcell problem with auto fuzz,553935.0,"[553935, 659629]",956238,81882,feature,2015-12-08T23:27:29Z,https://www.pivotaltracker.com/story/show/108913674
2015-12-09T00:09:23Z,2015-11-30T20:31:33Z,accepted,,,,109203308,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'bats-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-30T20:31:33Z', 'id': 13444002, 'updated_at': '2015-11-30T20:31:33Z'}]",Changing only DNS should trigger redeploy,1750722.0,"[1750722, 1017727]",956238,1750722,bug,2015-12-09T00:09:23Z,https://www.pivotaltracker.com/story/show/109203308
2015-12-09T00:22:54Z,2015-12-03T22:47:12Z,accepted,,Updating network name should delete previous IP reservation from database,,109487604,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",No change deploys after a network name update should not cause redeploys,353433.0,"[353433, 1426194]",956238,353433,bug,2015-12-09T00:22:54Z,https://www.pivotaltracker.com/story/show/109487604
2015-12-09T00:34:45Z,2015-12-08T00:31:48Z,accepted,,,,109682706,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Merge develop into global-net,1426194.0,"[1426194, 353433]",956238,553935,chore,2015-12-09T00:34:45Z,https://www.pivotaltracker.com/story/show/109682706
2015-12-09T00:57:01Z,2015-12-08T00:59:55Z,accepted,,"Changing job from:

```
jobs:
- name: dummy_z1
  template: dummy
  instances: 2
  resource_pool: default
  networks:
  - name: default
    static_ips:
    - 10.10.0.21
    - 10.10.10.21
```

to:

```
jobs:
- name: dummy
  template: dummy
  instances: 2
  resource_pool: default
  azs: [z1, z2]
  migrated_from:
  - name: dummy_z1
    az: z1 <--------------------------------------------------------------------------
  networks:
  - name: default
    static_ips:
    - 10.10.0.21
    - 10.10.10.21
```

10.10.0.21 belongs to z1,  10.10.10.21 belongs to z2",0.0,109683808,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'discuss', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-20T02:23:23Z', 'id': 13374294, 'updated_at': '2015-11-20T02:23:23Z'}]",When using migrated_from bosh should fail validate that migration destination az has all specified static IPs,81882.0,[81882],956238,1017727,feature,2015-12-09T00:57:02Z,https://www.pivotaltracker.com/story/show/109683808
2015-12-09T01:00:01Z,2015-12-03T18:00:37Z,accepted,,This is currently causing BATS on global-net to fail,,109463978,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",resource pool on `bosh vms --details` should not be empty,1017727.0,"[1017727, 1750722, 1868036]",956238,1017727,bug,2015-12-09T01:00:01Z,https://www.pivotaltracker.com/story/show/109463978
2015-12-09T01:03:35Z,2015-12-07T20:10:07Z,accepted,,,,109660176,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Add integration test to validate that VMs that were created before cloud config are ressurected correctly in old state,1017727.0,"[1017727, 1750722]",956238,553935,chore,2015-12-09T01:03:35Z,https://www.pivotaltracker.com/story/show/109660176
2015-12-10T18:13:47Z,2015-12-10T17:41:29Z,accepted,,"global-nats's *bats-ubuntu* job runs out of space, https://main.bosh-ci.cf-app.com/pipelines/global-net-aws-ubuntu-bats/jobs/bats-ubuntu/builds/98:
```
I, [2015-12-10T02:03:58.047131 #18415]  INFO -- : /opt/rubies/ruby-2.1.2/lib/ruby/2.1.0/tmpdir.rb:85:in `mkdir': No space left on device @ dir_s_mkdir - /tmp/d20151210-21301-1hl2su1 (Errno::ENOSPC)
```

Use the integration worker to fix, e.g. add `tags: [""bosh-integration""]` to `ci/pipelines/global-net-bats/pipeline-aws-unbuntu-bats.yml`",,109903764,story,[],global-net's bat-ubuntu should not run out of disk space,353433.0,[353433],956238,353433,chore,2015-12-10T18:13:47Z,https://www.pivotaltracker.com/story/show/109903764
2015-12-10T21:41:47Z,2015-12-02T21:50:14Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/integration-2.1-mysql/builds/793,,109397160,story,[],investigate build failure 'simultaneous deploys running two errands allocates IPs correctly for simultaneous errand run',553935.0,[553935],956238,344,chore,2015-12-10T21:41:47Z,https://www.pivotaltracker.com/story/show/109397160
2015-12-10T22:48:38Z,2015-12-10T19:05:48Z,accepted,,,,109911046,story,[],Investigate concourse disk full issue,1426194.0,"[1426194, 1868036]",956238,1426194,chore,2015-12-10T22:48:38Z,https://www.pivotaltracker.com/story/show/109911046
2015-12-11T02:41:42Z,2015-12-10T02:19:58Z,accepted,,"fuzz tests are failing right now because of this
",1.0,109858836,story,[],auto fuzz test should not generate jobs with one vip network,1750722.0,"[1750722, 344]",956238,553935,feature,2015-12-11T02:41:42Z,https://www.pivotaltracker.com/story/show/109858836
2015-12-11T03:07:26Z,2015-11-24T23:38:00Z,accepted,,https://github.com/cloudfoundry/bosh/pull/1038,1.0,108907900,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in blobstore updates from cpi team,1426194.0,[1426194],956238,81882,feature,2015-12-11T03:07:26Z,https://www.pivotaltracker.com/story/show/108907900
2015-12-11T03:15:52Z,2015-12-08T01:23:12Z,accepted,,"```
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:dummy-deployment
D, [2015-12-08 01:20:35 #5411] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:dummy-deployment
I, [2015-12-08 01:20:35 #5411] [task:16]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""1b91b365-eaff-46ea-8c6a-01c269c8da43"",""severity"":3,""source"":""director"",""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'dummy-deployment' against Director '9169acb6-1621-4c31-acdc-669723ccf757': #<NoMethodError: undefined method `az_names' for nil:NilClass>"",""created_at"":1449537635}
E, [2015-12-08 01:20:35 #5411] [task:16] ERROR -- DirectorJobRunner: undefined method `az_names' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:214:in `create_network_plan_with_ip'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:126:in `block (2 levels) in create_instance_plan_based_on_existing_ips'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:115:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:115:in `block in create_instance_plan_based_on_existing_ips'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:110:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:110:in `create_instance_plan_based_on_existing_ips'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:33:in `block in place_existing_instance_plans'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:32:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:32:in `place_existing_instance_plans'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:23:in `place_and_match_in'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/plan.rb:28:in `assign_zones'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/placement_planner/plan.rb:14:in `create_instance_plans'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/instance_planner.rb:14:in `plan_job_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:35:in `block in bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:32:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:32:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/planner.rb:128:in `bind_models'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:44:in `block (2 levels) in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:41:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:99:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: (0.000687s) SELECT NULL
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: (0.000072s) BEGIN
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: (0.000343s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-12-08 01:20:35.731459+0000', ""description"" = 'create deployment', ""result"" = 'undefined method `az_names'' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/16', ""checkpoint_time"" = '2015-12-08 01:20:35.629515+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 16)
D, [2015-12-08 01:20:35 #5411] [task:16] DEBUG -- DirectorJobRunner: (0.002584s) COMMIT
I, [2015-12-08 01:20:35 #5411] []  INFO -- DirectorJobRunner: Task took 0.110735398 seconds to process.
```",,109685050,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",deployment fails on multiple networks with static IPs,1750722.0,"[1750722, 1426194]",956238,344,bug,2015-12-11T03:15:52Z,https://www.pivotaltracker.com/story/show/109685050
2015-12-11T04:00:36Z,2015-10-26T13:14:50Z,accepted,,affects both director and agent. see how run_errand action is implemented. use TerminateNicely behaviour.,4.0,106642152,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",user can cancel while drain agent action is running,1778582.0,"[1778582, 1783496]",956238,81882,feature,2015-12-11T04:00:37Z,https://www.pivotaltracker.com/story/show/106642152
2015-12-11T04:00:43Z,2015-10-26T16:50:52Z,accepted,,,,106665004,story,"[{'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",cancelling deploys,,[],956238,81882,release,2015-12-11T04:00:43Z,https://www.pivotaltracker.com/story/show/106665004
2015-12-11T04:15:59Z,2015-12-03T01:21:18Z,accepted,,"looks like agent checks for ""unrecognized disk label"" (https://github.com/cloudfoundry/bosh-agent/blob/aeecf03b878dccde6c83857ece7b813429b6ec6a/platform/linux_platform.go#L553) however, it's only checking stdout (happens on ubuntu). on centos it looks like that goes to stderr. let's check in both places. 

run through a manual check to see that agent is able to bootstrap a machine and set up ephemeral disks.

```
bash-4.2# parted -s /dev/xvdba print
Error: /dev/xvdba: unrecognised disk label
Model: Xen Virtual Block Device (xvd)
Disk /dev/xvdba: 2000GB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags:

bash-4.2# echo $?
1

bash-4.2# parted -s /dev/xvdba print 2>/dev/null
Model: Xen Virtual Block Device (xvd)
Disk /dev/xvdba: 2000GB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags:
bash-4.2#
```

to repro:
- use raw_instance_storage=true
- pick instance with instance storage
- use i2 instance types",,109409018,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh-agent should be determine unpartitioned disk on centos,659629.0,[659629],956238,81882,bug,2015-12-11T04:16:00Z,https://www.pivotaltracker.com/story/show/109409018
2015-12-11T04:59:11Z,2015-12-08T00:13:04Z,accepted,,integration test to migrate from cf1 and cf2 to just cf with multiple subnets?,4.0,109680324,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",remove network name constraint from net-ip uniqueness to allow easier networks migration,553935.0,"[553935, 1017727, 353433]",956238,81882,feature,2015-12-11T04:59:11Z,https://www.pivotaltracker.com/story/show/109680324
2015-12-11T06:16:59Z,2015-11-06T06:49:44Z,accepted,,"We tried to export a compiled release from bosh and upload this compiled release to BOSH again, bosh cli will report a ```Cannot find file 'path-to-release/release.MF'``` error.

Mac OS system does not have this issue.

The steps to reproduce this issue:
1. Upload a release to BOSH(upload a compiled release or deploy the uploaded source release)
2. use ```bosh export release xxx xxx```command to export this release as a compiled release
3. Use BOSH CLI to upload the compiled release to BOSH again",,107553924,story,"[{'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",exported compiled release can not be uploaded again under Linux systems,631325.0,"[631325, 1233340, 1169826]",956238,1386874,bug,2015-12-11T06:16:59Z,https://www.pivotaltracker.com/story/show/107553924
2015-12-11T06:17:03Z,2015-11-30T18:02:15Z,accepted,,,,109186998,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",bosh export release should work when cloud-config and vm_type/stemcell is used,344.0,"[344, 659629]",956238,81882,bug,2015-12-11T06:17:03Z,https://www.pivotaltracker.com/story/show/109186998
2015-12-11T06:46:19Z,2015-10-01T22:33:09Z,accepted,,"why do we even delete them from the instance?
do not ever delete dns records -- user should see old and new dns records
Old records should be updated to new address. This may happen when a job is migrated or renamed. the instance should retain the old domain names and the old domain names should point to the instance/vm's ip address.",2.0,104674344,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",when vm is resurrected/recreated it should have proper dns addresses with static/dynamic ips,1550486.0,"[1550486, 344]",956238,81882,feature,2015-12-11T06:46:19Z,https://www.pivotaltracker.com/story/show/104674344
2015-12-11T18:39:52Z,2015-12-03T23:58:36Z,accepted,,"Job templates should be backwards compatible with develop.
Job templates should not contain IaaS specific properties (vm_type, stemcell, persistent_disk_type, persistent_disk_pool)",,109491666,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Job templates should not contain IaaS specific properties,1017727.0,"[1017727, 1750722]",956238,1017727,bug,2015-12-11T18:39:53Z,https://www.pivotaltracker.com/story/show/109491666
2015-12-11T19:06:24Z,2015-12-11T07:10:18Z,accepted,,,,109943026,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",debug dk env for constant private-dyn network changes,553935.0,[553935],956238,81882,chore,2015-12-11T19:06:24Z,https://www.pivotaltracker.com/story/show/109943026
2015-12-11T20:32:27Z,2015-11-17T23:24:57Z,accepted,,,0.0,108395102,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: availability zones,353433.0,"[353433, 344]",956238,553935,feature,2015-12-11T20:32:27Z,https://www.pivotaltracker.com/story/show/108395102
2015-12-11T20:32:32Z,2015-11-25T01:26:09Z,accepted,,"- sometimes keep net names
- sometimes dont keep net names",2.0,108913656,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: keep net names ,553935.0,"[553935, 1750722]",956238,81882,feature,2015-12-11T20:32:32Z,https://www.pivotaltracker.com/story/show/108913656
2015-12-11T20:42:02Z,2015-12-11T19:08:13Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2196,,109984804,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Investigate fuzz test failure 'Compilation config references unknown az',553935.0,[553935],956238,553935,chore,2015-12-11T20:42:02Z,https://www.pivotaltracker.com/story/show/109984804
2015-12-11T23:48:23Z,2015-12-11T20:41:47Z,accepted,,,,109993248,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto-fuzz: compilation config must contain az from network,553935.0,"[553935, 1750722]",956238,553935,chore,2015-12-11T23:48:23Z,https://www.pivotaltracker.com/story/show/109993248
2015-12-13T21:07:53Z,2015-11-18T19:36:23Z,accepted,,"- release with packages
- number of workers
- az and network",1.0,108475366,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: compilation,553935.0,[553935],956238,553935,feature,2015-12-13T21:07:54Z,https://www.pivotaltracker.com/story/show/108475366
2015-12-13T21:10:28Z,2015-12-10T23:41:54Z,accepted,,,0.0,109930798,story,[],investigate gem dep checking in release,1550486.0,[1550486],956238,81882,feature,2015-12-13T21:10:28Z,https://www.pivotaltracker.com/story/show/109930798
2015-12-13T21:10:30Z,2015-11-17T22:56:24Z,accepted,,,0.0,108393250,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: migrating to cloud config,1868036.0,"[1868036, 344, 659629]",956238,553935,feature,2015-12-13T21:10:30Z,https://www.pivotaltracker.com/story/show/108393250
2015-12-14T19:39:04Z,2015-11-30T20:04:24Z,accepted,,,1.0,109200786,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",manual fuzz: migrating to vm_types & stemcells,659629.0,[659629],956238,344,feature,2015-12-14T19:39:04Z,https://www.pivotaltracker.com/story/show/109200786
2015-12-14T19:39:19Z,2015-11-18T01:20:05Z,accepted,,,1.0,108403550,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",manual fuzz: failed deploys,948679.0,"[948679, 1233340]",956238,553935,feature,2015-12-14T19:39:19Z,https://www.pivotaltracker.com/story/show/108403550
2015-12-14T19:40:22Z,2015-12-07T22:07:25Z,accepted,,"Steps to reproduce:
1. Deploy bosh-director from latest release on bosh.io
2. Deploy dummy deployment
3. Upgrade director to global-net
4. Redeploy dummy with no changes

We think this bug is because the network settings have changed as the director upgrades:
```
[canary_update(dummy_z1/0)] DEBUG -- DirectorJobRunner: networks_changed? changed FROM: {""default""=>{""ip""=>""10.10.0.21"", ""netmask""=>""255.255.255.0"", ""cloud_properties""=>{""subnet""=>""subnet-63196648""}, ""default""=>[""dns"", ""gateway""], ""dns""=>[""10.10.0.11""], ""gateway""=>""10.10.0.1""}} TO: {""default""=>{""ip""=>""10.10.0.21"", ""netmask""=>""255.255.255.0"", ""cloud_properties""=>{""subnet""=>""subnet-63196648""}, ""dns""=>[""10.10.0.11""], ""gateway""=>""10.10.0.1""}} on instance dummy_z1/0
```",,109669626,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Redeploy (with no manifest changes) after upgrading director from master to global-net should not cause instance to update,1868036.0,"[1868036, 344]",956238,1017727,bug,2015-12-14T19:40:22Z,https://www.pivotaltracker.com/story/show/109669626
2015-12-14T19:41:10Z,2015-12-04T00:48:55Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/47,1.0,109494128,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in bin/build change for agent,659629.0,[659629],956238,81882,feature,2015-12-14T19:41:10Z,https://www.pivotaltracker.com/story/show/109494128
2015-12-15T00:18:41Z,2015-12-14T17:53:13Z,accepted,,,,110089848,story,[],merge develop into global-net,1426194.0,[1426194],956238,344,chore,2015-12-15T00:18:42Z,https://www.pivotaltracker.com/story/show/110089848
2015-12-15T01:41:31Z,2015-12-03T22:50:29Z,accepted,,"after every successful deploy, run bosh deploy and see that no changes were discovered. raise an error if changes are found.",1.0,109487788,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",add nothing changed step to auto fuzz,553935.0,"[553935, 659629]",956238,81882,feature,2015-12-15T01:41:31Z,https://www.pivotaltracker.com/story/show/109487788
2015-12-15T19:49:21Z,2015-12-14T20:33:10Z,accepted,,"http://www.ubuntu.com/usn/usn-2834-1/
2.9.1+dfsg1-3ubuntu4.6",1.0,110105282,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",USN-2834-1: libxml2 vulnerabilities,344.0,"[344, 1868036]",956238,81882,feature,2015-12-15T19:49:25Z,https://www.pivotaltracker.com/story/show/110105282
2015-12-15T21:51:53Z,2015-11-18T00:44:54Z,accepted,,,1.0,108401884,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",manual fuzz: networking,353433.0,"[353433, 1017727]",956238,553935,feature,2015-12-15T21:51:53Z,https://www.pivotaltracker.com/story/show/108401884
2015-12-16T19:24:53Z,2015-12-14T18:56:31Z,accepted,,"BOSH rebalances instances without static IPs and persistent disks based on az with least number of instances. This sometimes results in unnecessary instances update on each deploy. We should try to preserve existing instances there they are if they already balanced correctly.

https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2481
https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2479",,110096112,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",Auto-fuzz: allow rebalancing instances after persistent disk was removed,353433.0,"[353433, 1017727]",956238,553935,chore,2015-12-16T19:24:53Z,https://www.pivotaltracker.com/story/show/110096112
2015-12-16T19:29:28Z,2015-12-10T22:08:43Z,accepted,,,1.0,109925096,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto-fuzz: update,1426194.0,"[1426194, 1750722]",956238,353433,feature,2015-12-16T19:29:28Z,https://www.pivotaltracker.com/story/show/109925096
2015-12-16T20:04:44Z,2015-12-14T21:57:42Z,accepted,,,,110112256,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto-fuzz should not generate jobs with IP addresses used by other jobs in previous deployment (static and auto),553935.0,"[553935, 1833948]",956238,553935,chore,2015-12-16T20:04:44Z,https://www.pivotaltracker.com/story/show/110112256
2015-12-16T23:28:43Z,2015-12-14T19:19:55Z,accepted,,"The following reserved range caused the ip_util class to raise, giving a 500 error at the command line.
```
- name: first-network
  type: manual
  subnets:
  - range: 10.85.41.0/24
    gateway: 10.85.41.1
    dns: [10.87.8.10]
    reserved:
      - 10.85.41.1-10.85.41.97,10.85.41.117-10.85.41.255
    azs:
    - z1
    - z2
    cloud_properties:
      name: hinterlands-1
```
",,110098668,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Invalid network range in cloud config should give a better error message,1426194.0,"[1426194, 1750722]",956238,344,bug,2015-12-16T23:28:43Z,https://www.pivotaltracker.com/story/show/110098668
2015-12-16T23:39:20Z,2015-09-23T21:06:31Z,accepted,,"recreate, start, stop?",2.0,104037808,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",show job/id while doing bosh deploy,948679.0,"[948679, 793309]",956238,81882,feature,2015-12-16T23:39:20Z,https://www.pivotaltracker.com/story/show/104037808
2015-12-17T00:43:55Z,2015-12-10T01:09:28Z,accepted,,"If deploy fails during migration it gets stuck like this:

```
[2015-12-10 01:05:18 #3582] [task:53]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-12-10 01:05:18 #3582] [task:53] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""8c5cd8f2-8558-495a-9fb2-93424e615097"",""severity"":3,""source"":""director"",""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'cf' against Director 'e932af6b-4b85-4d1c-9245-364a77ba8ceb': #<Bosh::Director::AgentJobMismatch: VM `i-8f719f3e' is out of sync: it reports itself as `api_z1/0' but according to DB it is `api/0'>"",""created_at"":1449709518}
E, [2015-12-10 01:05:18 #3582] [task:53] ERROR -- DirectorJobRunner: VM `i-8f719f3e' is out of sync: it reports itself as `api_z1/0' but according to DB it is `api/0'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/agent_state_migrator.rb:81:in `verify_state'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/agent_state_migrator.rb:15:in `get_state'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:75:in `block (4 levels) in current_states_by_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/deployment_plan/assembler.rb:73:in `block (3 levels) in current_states_by_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-12-10 01:05:18 #3582] [task:53] DEBUG -- DirectorJobRunner: (0.000188s) SELECT NULL
D, [2015-12-10 01:05:18 #3582] [task:53] DEBUG -- DirectorJobRunner: (0.000074s) BEGIN
D, [2015-12-10 01:05:18 #3582] [task:53] DEBUG -- DirectorJobRunner: (0.000399s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-12-10 01:05:18.440678+0000', ""description"" = 'create deployment', ""result"" = 'VM `i-8f719f3e'' is out of sync: it reports itself as `api_z1/0'' but according to DB it is `api/0''', ""output"" = '/var/vcap/store/director/tasks/53', ""checkpoint_time"" = '2015-12-10 01:05:13.260918+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 53)
```",1.0,109856732,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Remove out of sync check when running deploy,344.0,"[344, 1868036]",956238,553935,feature,2015-12-17T00:43:55Z,https://www.pivotaltracker.com/story/show/109856732
2015-12-17T00:50:03Z,2015-12-11T06:53:34Z,accepted,,in evaluation_context.rb,2.0,109942708,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","id, az, bootstrap should **only** be accessible via spec.X",1426194.0,"[1426194, 1868036]",956238,81882,feature,2015-12-17T00:50:03Z,https://www.pivotaltracker.com/story/show/109942708
2015-12-17T00:50:22Z,2015-05-18T23:58:13Z,accepted,,"Acceptance (on global-net:)
1. After resurrecting a VM (setup with a dynamic network) using `bosh cck`, the rendered templates for jobs on that VM should return the current IP of the VM",,94906096,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",jobs templates on resurrected VM should return the current IP of the VM,1426194.0,"[1426194, 1833948]",956238,81882,bug,2015-12-17T00:50:22Z,https://www.pivotaltracker.com/story/show/94906096
2015-12-17T00:50:34Z,2015-12-09T23:16:55Z,accepted,,,1.0,109851192,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",When vm type name changes it should not recreate vm,1868036.0,"[1868036, 344]",956238,1868036,feature,2015-12-17T00:50:34Z,https://www.pivotaltracker.com/story/show/109851192
2015-12-17T00:54:01Z,2015-12-12T01:47:44Z,accepted,,"See #109930798

System installed gems should not be considered when building the dependency graph (only those in bosh/vendor/cache).

Potential solutions/attempts:

- [x] Update `GemComponent#build_gem` to use `bundle exec`. Notes:
  - increased gem building time from 20 sec to 35 sec
  - failed to avoid updating fog-core reference in Gemfile.lock
- [ ] Execute `bundle clean` before building. This approach will likely introduce issues for development of other projects, with their own dependency graphs. Notes:
  - TODO
- [ ] Narrow the ""sources"" scope for `Bundler::Resolver` to exclude non-vendored gems. This will probably not work without some other change to the building of the BOSH gems... the Gemfile.lock is updated during the gem building, prior to resolution. Notes:
  - TODO
- [ ] Configure bundler in some other way to only consider vendor/cache. Notes:
  - TODO
- [ ] Configure `gem build` in some way to only consider vendor/cache. Notes:
  - TODO
- [ ] Change the version updating mechanism in some way that will avoid new dependency resolution. Notes:
  - TODO
- [ ] Lock down gem dependency versions. Notes:
  - TODO
- [ ] Build BOSH gems via rubygems lib (e.g., `Gem::PackageTask`, `Gem::Package`) in some way that controls source lookup. Notes:
  - TODO

WIP:

```
spec = Gem::Specification.load 'bosh-director.gemspec'
spec.version = '1.0000.0'
spec.dependencies.each { |dep| ... }
Gem::Package.build spec
```",1.0,110009284,story,[],`rake release:create_dev_release` should be more strict about the transitive gems added to the release,1550486.0,[1550486],956238,1550486,feature,2015-12-17T00:54:01Z,https://www.pivotaltracker.com/story/show/110009284
2015-12-17T01:01:21Z,2015-09-23T21:11:19Z,accepted,,- post '/:deployment/jobs/:job/:index/snapshots',1.0,104038484,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",user can take a snapshot of the vm when vm is referenced by id,1017727.0,"[1017727, 353433]",956238,81882,feature,2015-12-17T01:01:21Z,https://www.pivotaltracker.com/story/show/104038484
2015-12-17T01:08:55Z,2015-09-23T21:11:10Z,accepted,,- put '/:deployment/jobs/:job/:index/resurrection',1.0,104038478,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can set resurrection state of the vm when vm is referenced by id,948679.0,"[948679, 1233340, 793309]",956238,81882,feature,2015-12-17T01:08:55Z,https://www.pivotaltracker.com/story/show/104038478
2015-12-17T02:36:16Z,2015-12-16T17:59:46Z,accepted,,,1.0,110266492,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu for USN-2836-1: GRUB vulnerability,1868036.0,"[1868036, 1750722]",956238,81882,feature,2015-12-17T02:36:38Z,https://www.pivotaltracker.com/story/show/110266492
2015-12-17T02:37:20Z,2015-11-24T23:19:24Z,accepted,,"See attached manifests. Currently fails with error:

```
E, [2015-11-24 14:56:12 #88702] [task:16] ERROR -- DirectorJobRunner: Job 'emdmh9Q7giQpYgXVs8g28K48nulSsjAOHNaSvIriacHZD3f5BjkmLphBLCzN99QxeKldJ5p9Qh0u6B4l0jKmScNbXuVtRHR7CuHvGhrDYxiBn5d8HL10PryoxsIP9i5R0QvVlVGqZsg9tUgMlqvIBmRnu7rgng7jws7CjvAgom2uz6dCuQlsqCNS6q5T' declares static ip '192.168.47.173' which does not belong to any of the job's availability zones.
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/networks_to_static_ips.rb:58:in `block in validate_ips_are_in_desired_azs'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/networks_to_static_ips.rb:52:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/networks_to_static_ips.rb:52:in `validate_ips_are_in_desired_azs'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/static_ips_availability_zone_picker.rb:20:in `place_and_match_in'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:28:in `assign_zones'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/placement_planner/plan.rb:14:in `create_instance_plans'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:14:in `plan_job_instances'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:35:in `block in bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/planner.rb:128:in `bind_models'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/jobs/update_deployment.rb:44:in `block (2 levels) in perform'
```",,108906850,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",It should be possible to deploy job with static IP when static IP belongs to multiple azs and job specifies only subset of those azs,553935.0,"[553935, 1750722]",956238,553935,bug,2015-12-17T02:37:20Z,https://www.pivotaltracker.com/story/show/108906850
2015-12-17T02:37:21Z,2015-11-25T03:27:40Z,accepted,,"Currently it looks only at subnet and in case if subnet has more azs than job it will randomly pick az from subnet. It should filter those azs with azs specified on a job.

Reproducible with 5 instances and 2 azs on subnet and 1 az on job",,108924230,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",When selecting az from instance with static IP it should filter subnet azs that are specified on a job,553935.0,"[553935, 1750722]",956238,553935,bug,2015-12-17T02:37:21Z,https://www.pivotaltracker.com/story/show/108924230
2015-12-17T02:37:25Z,2015-12-14T19:08:20Z,accepted,,"Reservation reconciler assumes that az is always present to log the az name:

https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2475
https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2474

```
MethodError: undefined method `name' for nil:NilClass>"",""created_at"":1450099857}
E, [2015-12-14 13:30:57 #42504] [task:9] ERROR -- DirectorJobRunner: undefined method `name' for nil:NilClass
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/network_planner/reservation_reconciler.rb:20:in `block in reconcile'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:55:in `each'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_network_reservations.rb:55:in `each'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/network_planner/reservation_reconciler.rb:18:in `reconcile'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:77:in `block in reconcile_network_plans'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:75:in `each'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:75:in `reconcile_network_plans'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/instance_planner.rb:21:in `plan_job_instances'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:35:in `block in bind_models'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `each'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/assembler.rb:32:in `bind_models'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/deployment_plan/planner.rb:120:in `bind_models'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/jobs/update_deployment.rb:44:in `block (2 levels) in perform'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/event_log.rb:97:in `call'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/event_log.rb:50:in `track'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/jobs/update_deployment.rb:41:in `block in perform'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/lock_helper.rb:13:in `block in with_deployment_lock'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/lock.rb:56:in `lock'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/lock_helper.rb:13:in `with_deployment_lock'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/jobs/update_deployment.rb:34:in `perform'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:99:in `perform_job'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:32:in `block in run'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh_common/lib/common/thread_formatter.rb:49:in `with_thread_name'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/job_runner.rb:32:in `run'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/opt/rubies/ruby-2.1.7/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/tmp/build/0ffa7eb0-4c19-4560-7c8f-5b2315dbe84b/bosh-src/bosh-director/bin/bosh-director-worker:75:in `<main>'
```",,110097152,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",Failing to deploy when static IP subnet was changed from az to non-az,353433.0,"[353433, 1017727]",956238,553935,bug,2015-12-17T02:37:25Z,https://www.pivotaltracker.com/story/show/110097152
2015-12-17T03:01:23Z,2015-12-14T21:44:12Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2455,,110111258,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Director does not release IP when network changes from manual to dynamic,553935.0,"[553935, 1017727]",956238,553935,bug,2015-12-17T03:01:23Z,https://www.pivotaltracker.com/story/show/110111258
2015-12-17T03:08:13Z,2015-12-03T00:22:19Z,accepted,,"We're seeing that only the new instances are distributed across the azs, even if there is an imbalance in the existing instances.
[z1, z1, z1] - add one = [z1, z1, z1, z1]
[] add four = [z1, z2, z1, z2]
[z1, z2] add two = [z1, z2, z1, z2]",,109406708,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","Subnets with multiple azs, static ips should be balanced based on all new and existing instances.",553935.0,"[553935, 659629]",956238,344,bug,2015-12-17T03:08:13Z,https://www.pivotaltracker.com/story/show/109406708
2015-12-17T03:08:16Z,2015-11-24T00:56:43Z,accepted,,"5 vms deployed to 3 azs.  Subnet 1 has azs = [z1, z2, z3 ]. Subnet 2 has azs [ z2 ]. Subnet 3 has azs [ z3 ] Vms are spread accross all azs. All IPs are from Subnet 1.

Remove z2 and z3 from Subnet 1. Re-deploy.

All instances still have IPs from Subnet 1. (They should have IPs from their proper subnets).",,108825998,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","when removing azs from a subnet, vms should be recreated that use ips that do not fall into to that subnet anymore",553935.0,[553935],956238,344,bug,2015-12-17T03:08:16Z,https://www.pivotaltracker.com/story/show/108825998
2015-12-17T18:34:55Z,2015-12-14T20:13:52Z,accepted,,"- use logrotate to forcefully rotate (--force?)
- other option?
- should we do this in agent apply or start or somewhere else?
- centos vs ubuntu",4.0,110103634,story,"[{'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",investigate a way to capture logs from the current run of errand/other jobs,659629.0,"[659629, 948679, 1169826]",956238,81882,feature,2015-12-17T18:34:55Z,https://www.pivotaltracker.com/story/show/110103634
2015-12-17T19:32:19Z,2015-12-11T06:20:55Z,accepted,,,4.0,109942068,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",auto fuzz: generate random cloud properties,1750722.0,"[1750722, 1868036]",956238,81882,feature,2015-12-17T19:32:19Z,https://www.pivotaltracker.com/story/show/109942068
2015-12-17T19:36:44Z,2015-12-17T17:39:30Z,accepted,,,,110345052,story,[],Fix Councourse workers not picking up new jobs,1833948.0,"[1833948, 353433]",956238,553935,chore,2015-12-17T19:36:45Z,https://www.pivotaltracker.com/story/show/110345052
2015-12-17T20:20:49Z,2015-01-26T18:00:06Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/10,1.0,86932096,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Catch error from http dispatcher' in bosh-agent,659629.0,[659629],956238,81882,feature,2015-12-17T20:20:49Z,https://www.pivotaltracker.com/story/show/86932096
2015-12-17T21:32:19Z,2015-09-23T21:11:55Z,accepted,,,1.0,104038540,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]","show inactive disk label with ""job/id (index)"" in bosh cck output",948679.0,"[948679, 793309]",956238,81882,feature,2015-12-17T21:32:19Z,https://www.pivotaltracker.com/story/show/104038540
2015-12-17T21:58:46Z,2015-12-14T23:48:27Z,accepted,,"Bosh::Cli::ReleaseTarball create_from_unpacked creates a tarball that reflects changes made in @unpack_dir

https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/unit-1.9/builds/1271
https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/unit-1.9/builds/1209

Danny and Kam added more debug to related areas in a previous commit. This test has failed before.

Current resolution is to re-run and it'll probably pass the next build.",,110120770,story,[],1.9 Unit failure - tarball spec is flaky,1426194.0,"[1426194, 1833948]",956238,344,chore,2015-12-17T21:59:03Z,https://www.pivotaltracker.com/story/show/110120770
2015-12-17T23:38:03Z,2015-12-17T19:41:33Z,accepted,,,,110354622,story,[],Merge develop into global-net,1017727.0,[1017727],956238,1017727,chore,2015-12-17T23:38:03Z,https://www.pivotaltracker.com/story/show/110354622
2015-12-17T23:47:55Z,2015-12-15T21:46:43Z,accepted,,Seems that concourse does not schedule tasks across workers properly.,,110198076,story,[],Tag specific integration tests with specific workers,1833948.0,"[1833948, 353433]",956238,553935,chore,2015-12-17T23:47:55Z,https://www.pivotaltracker.com/story/show/110198076
2015-12-18T00:56:52Z,2015-12-10T23:36:46Z,accepted,,"Updating a package with ... in the name fails. 
For example, `./update-dep github.com/cloudfoundry/bosh-utils`",,109930588,story,[],Fix update-dep script on bosh-agent,553935.0,"[553935, 1868036]",956238,1017727,chore,2015-12-18T00:56:52Z,https://www.pivotaltracker.com/story/show/109930588
2015-12-18T01:40:41Z,2015-12-17T22:45:30Z,accepted,,,,110369244,story,[],Merge global-net into develop!,553935.0,"[553935, 1868036]",956238,553935,chore,2015-12-18T01:44:38Z,https://www.pivotaltracker.com/story/show/110369244
2015-12-18T01:57:33Z,2015-12-09T20:34:24Z,accepted,,"- vm_types can only be specified in cloud-config (raise error if deployment manifest contains it)
  - Deployment manifest contains 'vm_types' section, but it can only be used in cloud-config.
- azs can only be specified in cloud-config (raise error if deployment manifest contains it)
  - Deployment manifest contains 'azs' section, but it can only be used in cloud-config.
- migrated_from can only be used when cloud-config is enabled (raise error if it's not enabled)
  - migrated_from can only be used with cloud-config.",2.0,109838304,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","Add validation error when user is trying to use azs, vm types, stemcells, or migrated_from without cloud config",1426194.0,"[1426194, 1868036]",956238,553935,feature,2015-12-18T01:57:33Z,https://www.pivotaltracker.com/story/show/109838304
2015-12-18T01:57:51Z,2015-12-11T19:05:26Z,accepted,,Since we dropped network uniqueness we don't need to find IP by network to delete it.,,109984626,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Delete old static IP if network was renamed,553935.0,[553935],956238,553935,bug,2015-12-18T01:57:52Z,https://www.pivotaltracker.com/story/show/109984626
2015-12-18T02:25:20Z,2015-12-16T19:35:32Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/fuzz-tests/builds/2682,,110274586,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",When job network switched to not use static ip (auto-assigned) if IP belongs to static range it cannot be reused,553935.0,[553935],956238,553935,bug,2015-12-18T02:25:20Z,https://www.pivotaltracker.com/story/show/110274586
2015-12-18T16:51:28Z,2015-12-02T18:22:57Z,accepted,,https://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/integration-1.9-postgres/builds/437,,109378882,story,[],investigate build error 'cli: cloudcheck properly resurrects VMs with dead agents',1833948.0,"[1833948, 353433]",956238,344,chore,2015-12-18T16:51:29Z,https://www.pivotaltracker.com/story/show/109378882
2015-12-21T17:22:57Z,2015-12-17T17:56:55Z,accepted,,,,110346322,story,[],AWS key in stemcell publishing account should not be root key,553935.0,"[553935, 1868036]",956238,553935,chore,2015-12-21T17:23:03Z,https://www.pivotaltracker.com/story/show/110346322
2015-12-21T17:24:13Z,2015-12-18T00:31:16Z,accepted,,"bosh-acceptance-tests/spec/system/with_release_stemcell_failed_deployment_spec.rb:36,58 is not valid any more",,110374416,story,[],Fix failed deployment spec in bats,553935.0,[553935],956238,1426194,chore,2015-12-21T17:24:13Z,https://www.pivotaltracker.com/story/show/110374416
2015-12-21T18:50:37Z,2015-12-18T21:48:44Z,accepted,,http://bosh-jenkins.cf-app.com:8080/job/bat_micro_openstack_ubuntu_trusty_go_agent_dynamic/346,,110436802,story,[],Fix Openstack BATs,553935.0,"[553935, 1868036]",956238,553935,chore,2015-12-21T18:50:37Z,https://www.pivotaltracker.com/story/show/110436802
2015-12-21T23:01:12Z,2015-12-17T18:59:17Z,accepted,,"USN-2842-1: Linux kernel vulnerabilities
",1.0,110351808,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump ubuntu stemcells for USN-2842-1: Linux kernel vulnerabilities,1833948.0,"[1833948, 353433, 1868036]",956238,81882,feature,2015-12-21T23:01:12Z,https://www.pivotaltracker.com/story/show/110351808
2015-12-22T18:05:41Z,2015-12-15T18:58:45Z,accepted,,,2.0,110183138,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",Change start/stop/recreate/restart commands to take instances UUID,553935.0,[553935],956238,81882,feature,2015-12-22T18:05:41Z,https://www.pivotaltracker.com/story/show/110183138
2015-12-23T17:38:23Z,2015-12-22T21:43:58Z,accepted,,,,110615866,story,[],Switch BATs to use develop branch,553935.0,"[553935, 1868036]",956238,553935,chore,2015-12-23T17:39:03Z,https://www.pivotaltracker.com/story/show/110615866
2015-12-24T00:36:18Z,2015-12-23T22:37:29Z,accepted,,"might be related to sfdisk
https://bosh-azure-cpi.ci.cf-app.com/pipelines/azure-cpi/jobs/bats-ubuntu/builds/13",,110682184,story,[],investigate failure on azure in the agent,81882.0,[81882],956238,81882,bug,2015-12-24T00:36:18Z,https://www.pivotaltracker.com/story/show/110682184
2016-01-04T20:29:53Z,2015-12-21T23:01:30Z,accepted,,http://www.ubuntu.com/usn/usn-2854-1/,1.0,110535308,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",USN-2854-1: Linux kernel (Vivid HWE) vulnerabilities,553935.0,[553935],956238,81882,feature,2016-01-04T20:29:53Z,https://www.pivotaltracker.com/story/show/110535308
2016-01-04T20:30:00Z,2015-12-11T19:19:50Z,accepted,,"https://github.com/cloudfoundry/bosh-agent/pull/51/files
add tests
may be use retry strategy package",4.0,109985564,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",retry sfdisk-ing the disk in bosh-agent,1233340.0,"[1233340, 631325, 659629]",956238,81882,feature,2016-01-04T20:30:00Z,https://www.pivotaltracker.com/story/show/109985564
2016-01-04T20:30:05Z,2015-12-17T20:23:07Z,accepted,,We don't need an unbound vm handler any longer since after merging global-net vms always have instances.,1.0,110358328,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",bosh cck should no longer show an option to delete unbound VM,1017727.0,[1017727],956238,81882,feature,2016-01-04T20:30:06Z,https://www.pivotaltracker.com/story/show/110358328
2016-01-04T20:30:19Z,2015-12-04T00:39:54Z,accepted,,https://github.com/cloudfoundry/bosh-agent/pull/32/files,1.0,109493708,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in keep root password option for bosh-agent,659629.0,[659629],956238,81882,feature,2016-01-04T20:30:19Z,https://www.pivotaltracker.com/story/show/109493708
2016-01-04T20:30:47Z,2015-08-26T17:34:49Z,accepted,,currently uses debug; should use error: https://github.com/cloudfoundry/bosh/blob/ede389a2e112e1b4f2dbc4495c08977da4439483/bosh_common/lib/common/thread_pool.rb#L89-L93,1.0,102091336,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",log worker error as error,1868036.0,[1868036],956238,81882,feature,2016-01-04T20:30:47Z,https://www.pivotaltracker.com/story/show/102091336
2016-01-04T20:30:56Z,2015-05-14T18:15:06Z,accepted,,See: https://github.com/fog/fog/issues/3409,,94607678,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]","Update fog to resolve ""duplicate key"" error ",1868036.0,[1868036],956238,1054467,bug,2016-01-04T20:30:56Z,https://www.pivotaltracker.com/story/show/94607678
2016-01-05T18:36:08Z,2015-12-29T01:25:23Z,accepted,,"https://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/fuzz-tests/builds/997

Seed: 1451285840",,110782790,story,[],fuzz tests: should not reuse existing IP if IP does not belong to network subnet,353433.0,"[353433, 1732048]",956238,553935,chore,2016-01-05T18:36:08Z,https://www.pivotaltracker.com/story/show/110782790
2016-01-06T01:23:01Z,2015-09-23T21:13:44Z,accepted,,,,104038802,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",cli can show/accept instance ids,,[],956238,81882,release,2016-01-06T01:23:01Z,https://www.pivotaltracker.com/story/show/104038802
,2015-09-22T01:23:38Z,delivered,,"corrupted means:
- missing from the blobstore, or
- have mismatched sha1 (which means we need to download compiled package and verify its sha1)",2.0,103869294,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",bosh upload release --fix should eliminate corrupted compiled packages,1386874.0,[1386874],956238,81882,feature,2015-12-07T19:23:38Z,https://www.pivotaltracker.com/story/show/103869294
,2015-11-12T20:23:21Z,delivered,,"director.debug.keep_unreachable_vms (bool)
against global-net",2.0,108029536,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",add a debug flag to the director to keep unreachable vms for debug,793309.0,"[793309, 1233340]",956238,81882,feature,2015-12-22T19:32:56Z,https://www.pivotaltracker.com/story/show/108029536
,2015-11-23T21:40:15Z,delivered,,"an example:

```
$ bosh ssh
1. db3/0 (idX)
2. db3/5 (idX)
3. foo3/0 (idX)
Choose an instance:
```

indices should come from the director so that cli can show gaps.
",2.0,108812110,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh ssh (and other commands that list instances) should show list of instances as returned from the director such that it shows real indices,1426194.0,"[1426194, 1750722]",956238,81882,feature,2015-12-23T18:23:23Z,https://www.pivotaltracker.com/story/show/108812110
,2015-07-13T19:42:41Z,delivered,,"Unnecessary services should be disabled to decrease the attack surface of the system.
---
None
---
SV-50475r1_rule
---
F-43623r1_fix
---
Setting the system's runlevel to 3 will prevent automatic startup of the X server. To do so, ensure the following line in ""/etc/inittab"" features a ""3"" as shown: 

id:3:initdefault:
---
C-46234r1_chk
---
To verify the default runlevel is 3, run the following command: 

# grep initdefault /etc/inittab

The output should show the following: 

id:3:initdefault:


If it does not, this is a finding.",1.0,98978178,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38674] [medium] X Windows must not be enabled unless required.,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-12-23T18:23:25Z,https://www.pivotaltracker.com/story/show/98978178
,2015-07-13T19:42:41Z,delivered,,"The ""sudo"" command allows authorized users to run programs (including shells) as other users, system users, and root. The ""/etc/sudoers"" file is used to configure authorized ""sudo"" users as well as the programs they are allowed to run. Some configuration options in the ""/etc/sudoers"" file allow configured users to run programs without re-authenticating. Use of these configuration options makes it easier for one compromised account to be used to compromise other accounts.
---
None
---
SV-73331r1_rule
---
F-64285r1_fix
---
Update the ""/etc/sudoers"" or other sudo configuration files to remove or comment out lines utilizing the ""NOPASSWD"" and ""!authenticate"" options.

# visudo
# visudo -f [other sudo configuration file]
---
C-59747r1_chk
---
Verify neither the ""NOPASSWD"" option nor the ""!authenticate"" option is configured for use in ""/etc/sudoers"" and associated files. Note that the ""#include"" and ""#includedir"" directives may be used to include configuration data from locations other than the defaults enumerated here.

# egrep '^[^#]*NOPASSWD' /etc/sudoers /etc/sudoers.d/*
# egrep '^[^#]*!authenticate' /etc/sudoers /etc/sudoers.d/*

If the ""NOPASSWD"" or ""!authenticate"" options are configured for use in ""/etc/sudoers"" or associated files, this is a finding.",1.0,98978184,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-58901] [medium] The sudo command must require authentication.,1495236.0,"[1495236, 1386874]",956238,81882,feature,2015-12-23T18:23:27Z,https://www.pivotaltracker.com/story/show/98978184
,2015-11-30T14:11:38Z,delivered,,"- cli currently works
- api doesnt work without specifying manifest
- use stopped for example",1.0,109161914,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",user should be able restart/start/stop deployment without havinv to send in a full manifest,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-12-23T18:23:30Z,https://www.pivotaltracker.com/story/show/109161914
,2015-12-22T00:22:55Z,delivered,,"Jobs
batlight
  ± networks: 
    - {""name""=>""static"", ""default""=>[""dns"", ""gateway""], ""static_ips""=>[""10.85.33.151""]}
    + {""name""=>""static"", ""default""=>[""dns"", ""gateway""], ""static_ips""=>[""10.85.33.150""]}
    + {""name""=>""second"", ""static_ips""=>[""10.85.33.160""]}


Director task 7
  Started preparing deployment > Preparing deployment. Failed: Failed to reserve IP '10.85.33.160' for network 'static': IP belongs to reserved range (00:00:01)

Error 130013: Failed to reserve IP '10.85.33.160' for network 'static': IP belongs to reserved range",,110539128,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",static ips should not be looked up against other nets reserved range,553935.0,"[553935, 1868036]",956238,81882,bug,2015-12-23T18:25:15Z,https://www.pivotaltracker.com/story/show/110539128
,2015-12-14T23:16:05Z,delivered,,"The existing link 404s: 
- http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm

New link: 
- http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-2.1511.el7.centos.2.10.x86_64.rpm

stemcell_builder/stages/base_centos/apply.sh",1.0,110118628,story,"[{'name': 'centos', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034576, 'updated_at': '2013-11-14T20:34:18Z'}]",update centos os image building to use the new base image url,1868036.0,"[1868036, 344]",956238,81882,feature,2015-12-23T18:25:19Z,https://www.pivotaltracker.com/story/show/110118628
,2015-09-23T21:08:30Z,delivered,,-  get '/:deployment/jobs/:job/:index/snapshots' do,1.0,104038244,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can list snapshots of the vm when vm is referenced by id,1017727.0,[1017727],956238,81882,feature,2015-12-23T18:25:25Z,https://www.pivotaltracker.com/story/show/104038244
,2015-12-21T23:18:28Z,delivered,,"value should be index based e.g. 0.blah.blah.blah... (not id-based since releases may be comparing it to self) 
e.g. 0.network-name.dummy-job.dep-name.bosh.cppforlife.com",,110536080,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",dns_record_name should be available to the erb templates,1017727.0,[1017727],956238,81882,bug,2015-12-23T18:25:50Z,https://www.pivotaltracker.com/story/show/110536080
,2015-12-18T23:11:01Z,delivered,,Right now it fails with undefined az_names on nil,,110440702,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",When user specifies same static IP in manifest it should fail nicely,553935.0,"[553935, 1868036]",956238,553935,bug,2015-12-23T18:25:57Z,https://www.pivotaltracker.com/story/show/110440702
,2015-12-22T00:16:27Z,delivered,,"after recreate

```
{""name"":""db3"",""index"":2,""properties"":""#<OpenStruct service=#<OpenStruct pre_start=#<OpenStruct stdout=""stdout"", stderr=""stderr"", delay_secs=0, exit_code=0>, drain=#<OpenStruct stderr=""whatsgoingon2"", delay_secs=3, echoed_status=0, exit_code=0>, start=#<OpenStruct delay_secs=0, success=true>, stop=#<OpenStruct delay_secs=0, success=true>>>"",""raw_properties"":{""service"":{""pre_start"":{""stdout"":""stdout"",""stderr"":""stderr"",""delay_secs"":0,""exit_code"":0},""drain"":{""stderr"":""whatsgoingon2"",""delay_secs"":3,""echoed_status"":0,""exit_code"":0},""start"":{""delay_secs"":0,""success"":true},""stop"":{""delay_secs"":0,""success"":true}}},""spec"":""#<OpenStruct deployment=""tiny-dummy"", job=#<OpenStruct name=""db3"", templates=[#<OpenStruct name=""service"", version=""548fca64d65bcdbc2f6b0e8074eaee6e8ca7e23c"", sha1=""6ddd26c68ca1cd5a0997e3d7bc7d4b8c33120ec5"", blobstore_id=""18a3516f-8819-4e6c-b288-d51c71ea0ffa"">], template=""service"", version=""548fca64d65bcdbc2f6b0e8074eaee6e8ca7e23c"", sha1=""6ddd26c68ca1cd5a0997e3d7bc7d4b8c33120ec5"", blobstore_id=""18a3516f-8819-4e6c-b288-d51c71ea0ffa"">, index=2, bootstrap=false, id=""87156334-96c7-453b-96f2-503f916958bb"", az=""z3"", networks=#<OpenStruct vip=#<OpenStruct type=""vip"", ip=""xxx.39"", cloud_properties=#<OpenStruct>>, private-dyn=#<OpenStruct type=""dynamic"", cloud_properties=#<OpenStruct subnet=""subnet-eb8bd3ad"">, dns=[""10.10.0.36""], default=[""dns"", ""gateway""], ip=""10.10.64.254"", netmask=""255.255.255.0"", gateway=""10.10.64.1"">>, packages=#<OpenStruct>, properties=#<OpenStruct service=#<OpenStruct pre_start=#<OpenStruct stdout=""stdout"", stderr=""stderr"", delay_secs=0, exit_code=0>, drain=#<OpenStruct stderr=""whatsgoingon2"", delay_secs=3, echoed_status=0, exit_code=0>, start=#<OpenStruct delay_secs=0, success=true>, stop=#<OpenStruct delay_secs=0, success=true>>>, dns_domain_name=""bosh.cppforlife.com"", links=#<OpenStruct>, persistent_disk=0, template_hashes=#<OpenStruct service=""152b3ec1baf6986f878df54705318901494801ff"">, resource_pool=""small"">""}
{""name"":null,""index"":2,""networks"":""#<OpenStruct vip=#<OpenStruct type=""vip"", ip=""xxx.39"", cloud_properties=#<OpenStruct>>, private-dyn=#<OpenStruct type=""dynamic"", cloud_properties=#<OpenStruct subnet=""subnet-eb8bd3ad"">, dns=[""10.10.0.36""], default=[""dns"", ""gateway""], ip=""10.10.64.254"", netmask=""255.255.255.0"", gateway=""10.10.64.1"">>""}
```

after deploy

```
{""name"":""db3"",""index"":2,""properties"":""#<OpenStruct service=#<OpenStruct pre_start=#<OpenStruct stdout=""stdout"", stderr=""stderr"", delay_secs=0, exit_code=0>, drain=#<OpenStruct stderr=""whatsgoingon2"", delay_secs=3, echoed_status=0, exit_code=0>, start=#<OpenStruct delay_secs=0, success=true>, stop=#<OpenStruct delay_secs=0, success=true>>>"",""raw_properties"":{""service"":{""pre_start"":{""stdout"":""stdout"",""stderr"":""stderr"",""delay_secs"":0,""exit_code"":0},""drain"":{""stderr"":""whatsgoingon2"",""delay_secs"":3,""echoed_status"":0,""exit_code"":0},""start"":{""delay_secs"":0,""success"":true},""stop"":{""delay_secs"":0,""success"":true}}},""spec"":""#<OpenStruct deployment=""tiny-dummy"", job=#<OpenStruct name=""db3"", templates=[#<OpenStruct name=""service"", version=""548fca64d65bcdbc2f6b0e8074eaee6e8ca7e23c"", sha1=""6ddd26c68ca1cd5a0997e3d7bc7d4b8c33120ec5"", blobstore_id=""18a3516f-8819-4e6c-b288-d51c71ea0ffa"">], template=""service"", version=""548fca64d65bcdbc2f6b0e8074eaee6e8ca7e23c"", sha1=""6ddd26c68ca1cd5a0997e3d7bc7d4b8c33120ec5"", blobstore_id=""18a3516f-8819-4e6c-b288-d51c71ea0ffa"">, index=2, bootstrap=false, id=""87156334-96c7-453b-96f2-503f916958bb"", az=""z3"", networks=#<OpenStruct vip=#<OpenStruct type=""vip"", ip=""xxx.39"", cloud_properties=#<OpenStruct>>, private-dyn=#<OpenStruct type=""dynamic"", cloud_properties=#<OpenStruct subnet=""subnet-eb8bd3ad"">, dns=[""10.10.0.36""], default=[""dns"", ""gateway""], ip=""10.10.64.254"", netmask=""255.255.255.0"", gateway=""10.10.64.1"">>, packages=#<OpenStruct>, properties=#<OpenStruct service=#<OpenStruct pre_start=#<OpenStruct stdout=""stdout"", stderr=""stderr"", delay_secs=0, exit_code=0>, drain=#<OpenStruct stderr=""whatsgoingon2"", delay_secs=3, echoed_status=0, exit_code=0>, start=#<OpenStruct delay_secs=0, success=true>, stop=#<OpenStruct delay_secs=0, success=true>>>, dns_domain_name=""bosh.cppforlife.com"", links=#<OpenStruct>, persistent_disk=0, resource_pool=""small"">""}
{""name"":null,""index"":2,""networks"":""#<OpenStruct vip=#<OpenStruct type=""vip"", ip=""xxx.39"", cloud_properties=#<OpenStruct>>, private-dyn=#<OpenStruct type=""dynamic"", cloud_properties=#<OpenStruct subnet=""subnet-eb8bd3ad"">, dns=[""10.10.0.36""], default=[""dns"", ""gateway""], ip=""10.10.64.254"", netmask=""255.255.255.0"", gateway=""10.10.64.1"">>""}
```

template_hashed are only present after recreate.",,110538912,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",expected that deploy after recreate does not update instance,553935.0,"[553935, 1868036]",956238,81882,bug,2015-12-23T18:26:10Z,https://www.pivotaltracker.com/story/show/110538912
,2015-11-03T01:21:42Z,delivered,,,2.0,107262134,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can run upload release --fix which will verify that blobstore contents from compiled release and replace with correct versions,1386874.0,[1386874],956238,81882,feature,2015-12-23T18:27:16Z,https://www.pivotaltracker.com/story/show/107262134
,2015-08-14T17:44:32Z,delivered,,"If I'm just trying to colocate packages I don't need any templates. Right now I have to create a file and list it under `templates` in the job:

```
templates:
  bosh: pls
```

Saying either `templates: {}` or not specifying it all (though I can see why you may want to validate that) should work. Currently we get this error:

```

  Started updating job cell_z1 > cell_z1/0 (canary). Failed: Action Failed get_task: Task 0377f115-2cc9-44ca-5fe5-927b99e98427 result: Applying: Applying job rootfses: Preparing job: Installing job bundle: Settting permissions on source directory: chmod /var/vcap/data/tmp/bosh-agent-applier-jobs-RenderedJobApplier-Apply162939844/rootfses: no such file or directory (00:00:04)

Error 450001: Action Failed get_task: Task 0377f115-2cc9-44ca-5fe5-927b99e98427 result: Applying: Applying job rootfses: Preparing job: Installing job bundle: Settting permissions on source directory: chmod /var/vcap/data/tmp/bosh-agent-applier-jobs-RenderedJobApplier-Apply162939844/rootfses: no such file or directory
```",2.0,101301276,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",I should be able to configure a job with an empty set of templates,1778582.0,"[1778582, 1783496]",956238,381857,feature,2015-12-23T18:28:11Z,https://www.pivotaltracker.com/story/show/101301276
,2015-11-25T22:00:06Z,delivered,,"we currently wait indefinitely for pid to die which will not happen in some cases. typical pattern in issue kill then wait 10 secs (return early if pid is gone) and then issue kill -9.

do so for director_ctl, scheduler_ctl, worker_ctl

https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/worker_ctl.erb#L65-L71
",1.0,108987020,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]","director, scheduler and workers should die after 10 secs",1778582.0,"[1778582, 1783496]",956238,81882,feature,2015-12-23T18:28:11Z,https://www.pivotaltracker.com/story/show/108987020
,2015-12-23T01:19:58Z,delivered,,"Put back when #110640170 is done
",0.0,110640316,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",Remove name and id from vm metadata,553935.0,"[553935, 1868036]",956238,553935,feature,2015-12-23T20:32:29Z,https://www.pivotaltracker.com/story/show/110640316
,2015-09-23T21:07:17Z,delivered,,"  - start/stop?
  - put '/:deployment/jobs/:job/:index'
  - convert index to id
  - if id is not found, raise an error",2.0,104037948,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}]",user can change state of the vm when vm is referenced by id,344.0,"[344, 1868036, 1017727]",956238,81882,feature,2015-12-30T21:42:43Z,https://www.pivotaltracker.com/story/show/104037948
,2015-12-17T22:35:49Z,delivered,,"allow to set vm_type on compilation instead of specifying cloud_properties. do not allow to set both. still allow setting cloud_properties for backwards compat. validate that vm_type exists in valid vm_types, otherwise raise an error.

```
compilation:
  workers: 5
  az: z1
  network: private
  vm_type: large
```",2.0,110368764,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",vm type on compilation,1868036.0,"[1868036, 1017727]",956238,81882,feature,2015-12-30T21:42:48Z,https://www.pivotaltracker.com/story/show/110368764
,2015-12-18T01:14:04Z,delivered,,"- execute after successfully starting all processes on a machine
- fail if post-start fails (use run_scripts agent rpc call)
- pretty much should be same as pre-start
- call during resurrection??? (nope for now)",2.0,110375758,story,[],add post-start hook,344.0,"[344, 1017727]",956238,81882,feature,2015-12-30T21:42:51Z,https://www.pivotaltracker.com/story/show/110375758
,2015-11-12T01:14:45Z,delivered,,"it happens already, but unintentionally. let's have a test for this.",2.0,107963970,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",send in previously assigned ips for dynamic networks,1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-30T21:42:55Z,https://www.pivotaltracker.com/story/show/107963970
,2015-12-15T20:29:25Z,delivered,,"current

```
$ bosh task 157
Acting as user 'admin' on 'micro'

Director task 157
  Started scanning 1 vms
  Started scanning 1 vms > Checking VM states. Done (00:00:24)
  Started scanning 1 vms > 0 OK, 1 unresponsive, 0 missing, 0 unbound, 0 out of sync. Done (00:00:00)
     Done scanning 1 vms (00:00:24)

  Started applying problem resolutions > unresponsive_agent 32: Recreate VM. Done (00:04:04)

Task 157 done

Started		2015-12-15 20:20:28 UTC
Finished	2015-12-15 20:24:56 UTC
Duration	 00:04:28
```

wanted

```
$ bosh task 157
Acting as user 'admin' on 'micro'

Director task 157
  Started scanning 1 vms
  Started scanning 1 vms > Checking VM states. Done (00:00:24)
  Started scanning 1 vms > 0 OK, 1 unresponsive, 0 missing, 0 unbound, 0 out of sync. Done (00:00:00)
     Done scanning 1 vms (00:00:24)

  Started applying problem resolutions > unresponsive_agent 32: Recreate VM for 'job/id'. Done (00:04:04)

Task 157 done

Started		2015-12-15 20:20:28 UTC
Finished	2015-12-15 20:24:56 UTC
Duration	 00:04:28
```
",2.0,110190940,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",scan and fix task should show instance name it recreated,1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-30T21:42:57Z,https://www.pivotaltracker.com/story/show/110190940
,2015-10-31T02:35:01Z,delivered,,Cannot reproduce problem; we closed this without changing any code.,,107096482,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",show error message for yaml parsing when cloud config is not valid yaml,353433.0,"[353433, 1868036]",956238,1633158,bug,2015-12-30T21:43:00Z,https://www.pivotaltracker.com/story/show/107096482
,2015-12-18T02:13:37Z,delivered,,"- canaries and max_in_flight should not overstep into the next az (sort azs the same way as for display in bosh instances (alpha num))
- how about max_in_flight to update all nodes across all azs?",2.0,110377630,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",updating machines of a job with multiple azs should not affect multiple azs at once,1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-30T21:43:02Z,https://www.pivotaltracker.com/story/show/110377630
,2015-10-20T15:52:31Z,delivered,,"ssh attempts are already being forwarded to rsyslog, which records them in auth.log.

```
cat /etc/rsyslog.d/*
...
#
# First some standard log files.  Log by facility.
#
auth,authpriv.*			/var/log/auth.log
...
```

We verified successful logins are logged:

```
$ tail -f /var/log/auth.log
Dec 29 21:51:20 ip-10-149-5-119 sshd[7336]: Accepted publickey for bosh_tn71sg2r7 from 10.10.0.11 port 45523 ssh2: RSA f5:93:fd:5c:15:a7:1a:f3:d9:08:a6:4d:31:ee:cf:40
Dec 29 21:51:20 ip-10-149-5-119 sshd[7336]: pam_unix(sshd:session): session opened for user bosh_tn71sg2r7 by (uid=0)
```

We checked by ssh'ing in as *danny-and-brian*:
```
tail -f /var/log/auth.log &
ssh danny-and-brian@localhost
# failed ssh attempt
Dec 29 21:57:49 ip-10-149-5-119 sshd[7412]: Invalid user danny-and-brian from ::1
Dec 29 21:57:49 ip-10-149-5-119 sshd[7412]: input_userauth_request: invalid user danny-and-brian [preauth]
Dec 29 21:57:49 ip-10-149-5-119 sshd[7412]: Connection closed by ::1 [preauth]
```",0.0,106121600,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",user can see ssh attempts forwarded to syslog,1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-30T21:43:07Z,https://www.pivotaltracker.com/story/show/106121600
,2015-10-02T23:15:28Z,delivered,,"this may break multi vm bosh configuration, but they can update it themselves.",1.0,104771792,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",redis and postgres should listen on 127.0.0.1 by default,1868036.0,"[1868036, 353433]",956238,81882,feature,2016-01-04T21:43:49Z,https://www.pivotaltracker.com/story/show/104771792
,2015-12-10T19:40:09Z,delivered,,"present diff to the user and confirm they want to proceed
be backwards compatible -- if api returns 404 show today's client side diff (do not augment it with newer content)",4.0,109914094,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'diff', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T18:21:13Z', 'id': 13692916, 'updated_at': '2016-01-05T18:21:13Z'}]",show detailed diff including cloud-config and deployment manifest with all manifest changes,553935.0,[553935],956238,81882,feature,2016-01-05T22:07:37Z,https://www.pivotaltracker.com/story/show/109914094
,2015-11-04T19:28:09Z,delivered,,"let's chat more when you guys had a chance to take a look around

- cli should show file content from downloaded log bundle
  - verify that log bundle's sha1 is same as expected sha1 (pretty much same as https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/release/export_release.rb#L30-L49)
- bosh-agent should log errand's output to a file
  - similar to what we do with run_script: https://github.com/cloudfoundry/bosh-agent/blob/master/agent/script/generic_script.go
    - can we use GenericScript to run an errand?
  - for backwards compatibility it also needs to return truncated data over nats in stdout/stderr
   - we do something similar here: https://github.com/cloudfoundry/bosh-agent/blob/master/agent/cmdrunner/file_logging_cmd_runner.go for when we compile pkgs",4.0,107424238,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",send errand logs via a tarball to avoid 1mb limit over nats,948679.0,"[948679, 659629, 1169826]",956238,81882,feature,2016-01-06T00:34:45Z,https://www.pivotaltracker.com/story/show/107424238
,2015-12-17T21:59:08Z,delivered,,let's also grep code base for #{@instance} and see if there are any other places like this.,1.0,110365872,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",make sure error 'Error 400010: `db3/2' has invalid disks: agent reports `vol-840e7179' while director record shows `'' shows job/id (index),659629.0,"[659629, 631325]",956238,81882,feature,2016-01-06T00:34:47Z,https://www.pivotaltracker.com/story/show/110365872
,2015-08-11T04:29:45Z,unstarted,,,,101003788,story,"[{'name': 'fix-assets', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T07:47:10Z', 'id': 9116242, 'updated_at': '2015-08-20T19:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can recover corrupted releases/stemcells,,[],956238,81882,release,2015-11-20T00:40:29Z,https://www.pivotaltracker.com/story/show/101003788
,2015-11-08T03:36:52Z,finished,,"investigate how db can be restored:
bosh restore backup tarball.tgz does what inside?",8.0,107648464,story,"[{'name': 'backup1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-18T16:50:10Z', 'id': 11156522, 'updated_at': '2015-03-18T16:51:22Z'}, {'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",user can restore director db from a backup,1386874.0,"[1386874, 1750722]",956238,81882,feature,2016-01-05T23:26:28Z,https://www.pivotaltracker.com/story/show/107648464
,2015-09-15T19:19:32Z,unstarted,,,,103440490,story,"[{'name': 'backup1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-18T16:50:10Z', 'id': 11156522, 'updated_at': '2015-03-18T16:51:22Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",backup and restore of the director database,,[],956238,81882,release,2015-12-03T18:53:00Z,https://www.pivotaltracker.com/story/show/103440490
,2015-11-10T07:27:00Z,finished,,"- reconfigure ntpd with specified ntp servers
- make sure drift amount is included instead of ntpdate drift amount by the agent in get_state
- install ntpd package on a stemcell? is it already there?
- how to avoid taking up port by (ntp takes port 123)
- what time drift conf is recommended
  - this what we do for one of the brokers: server 0.uk.pool.ntp.org iburst prefer minpoll 4 maxpoll 4",2.0,107791384,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked on pm', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-07T21:21:46Z', 'id': 9637086, 'updated_at': '2014-10-07T21:21:46Z'}, {'name': 'ntp', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-10T07:27:46Z', 'id': 13280472, 'updated_at': '2015-11-10T07:27:46Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",switch to using ntpd instead of ntpdate,1386874.0,"[1386874, 1495236]",956238,81882,feature,2015-12-21T19:44:53Z,https://www.pivotaltracker.com/story/show/107791384
,2015-11-10T07:27:33Z,unstarted,,,,107791404,story,"[{'name': 'ntp', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-10T07:27:46Z', 'id': 13280472, 'updated_at': '2015-11-10T07:27:46Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",ntpd is investigated,,[],956238,81882,release,2015-11-25T02:46:51Z,https://www.pivotaltracker.com/story/show/107791404
,2015-10-29T16:23:57Z,finished,,"we should use db instead of redis for all of our bookkeeping. locks, tasks state, etc.",8.0,106964214,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': 'no-redis-no-cry', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T00:31:57Z', 'id': 13697618, 'updated_at': '2016-01-06T00:31:57Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]","bosh uses db instead of redis to keep locks, task state, etc.",1778582.0,"[1778582, 1783496, 1426194]",956238,81882,feature,2016-01-06T00:32:55Z,https://www.pivotaltracker.com/story/show/106964214
,2015-12-11T03:01:41Z,finished,,https://github.com/cloudfoundry/bosh/pull/1072,1.0,109937840,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'merge', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-22T00:32:04Z', 'id': 12828478, 'updated_at': '2015-09-22T00:32:04Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",keep last X tasks *per task type* instead of total,1783496.0,"[1783496, 1778582]",956238,81882,feature,2016-01-06T00:02:43Z,https://www.pivotaltracker.com/story/show/109937840
,2015-11-09T20:01:41Z,started,,,4.0,107752876,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",finish migrating tmp cf release to azs for new cf deployment,81882.0,[81882],956238,81882,feature,2015-12-17T22:00:18Z,https://www.pivotaltracker.com/story/show/107752876
,2015-07-30T00:29:46Z,unstarted,,,,100170490,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'dp', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-18T17:27:15Z', 'id': 12807422, 'updated_at': '2015-09-18T17:27:15Z'}]",--- dynamic provisioning mvp ---,,[],956238,81882,release,2015-12-22T00:16:30Z,https://www.pivotaltracker.com/story/show/100170490
,2014-01-06T22:36:13Z,started,,useful for debugging run away vms. escpecially useful on vsphere. add `created_at` when calling set_vm_metadata. format should be 2015-13-30T20:11:00Z (utc time).,1.0,63339192,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",add creation time to vm via metadata,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-12-23T17:40:17Z,https://www.pivotaltracker.com/story/show/63339192
,2015-12-03T21:48:08Z,finished,,"keep mysql
similar to https://github.com/cloudfoundry/cf-mysql-release/tree/develop/packages/mysqlclient
example https://github.com/cloudfoundry/cf-mysql-release/blob/develop/packages/cf-mysql-broker/packaging#L21",2.0,109483880,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",switch having director/workers/etc. to talk to mysql db via mariadb connector,1550486.0,"[1550486, 1868036, 553935]",956238,81882,feature,2015-12-30T18:53:02Z,https://www.pivotaltracker.com/story/show/109483880
,2015-07-13T19:42:39Z,started,,"Preventing reuse of previous passwords helps ensure that a compromised password is not reused by a user.
---
None
---
SV-50459r1_rule
---
F-43608r1_fix
---
Do not allow users to reuse recent passwords. This can be accomplished by using the ""remember"" option for the ""pam_unix"" PAM module. In the file ""/etc/pam.d/system-auth"", append ""remember=24"" to the line which refers to the ""pam_unix.so"" module, as shown: 

password sufficient pam_unix.so [existing_options] remember=24

The DoD requirement is 24 passwords.
---
C-46219r1_chk
---
To verify the password reuse setting is compliant, run the following command: 

$ grep remember /etc/pam.d/system-auth

The output should show the following at the end of the line: 

remember=24


If it does not, this is a finding.",1.0,98978100,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",[V-38658] [medium] The system must prohibit the reuse of passwords within twenty-four iterations.,1386874.0,"[1386874, 1750722]",956238,81882,feature,2016-01-04T08:48:52Z,https://www.pivotaltracker.com/story/show/98978100
,2015-12-14T21:19:05Z,delivered,,"previously allows:

```
requires:
- data-node
```

only allow full format:

```
requires:
- {name: data-node, type: data-node}
```

requires & provides declarations",2.0,110108938,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",allow **only** full link declaration (consumes & provides) in release job specs,793309.0,"[793309, 1233340]",956238,81882,feature,2016-01-04T20:31:06Z,https://www.pivotaltracker.com/story/show/110108938
,2015-12-14T21:32:15Z,delivered,,"use network thats marked a default-for-gateway as the network for links.

for example:

```
jobs:
- name: node
  templates:
  - name: node
    release: cf-mysql
    links: {nats: {path: nats.nats}}
```


```
jobs:
- name: nats
  templates:
  - name: node
    release: cf-mysql
    networks:
    - name: private
      default: [gateway] <-------------should use this network
   - name: vip
     default: [dns]
```",2.0,110110180,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user does not have to enter network name for any link,793309.0,[793309],956238,81882,feature,2016-01-04T20:31:04Z,https://www.pivotaltracker.com/story/show/110110180
,2015-12-14T22:26:10Z,delivered,,,2.0,110114888,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",rename requires to consumes,659629.0,"[659629, 631325, 1169826]",956238,81882,feature,2016-01-04T19:40:36Z,https://www.pivotaltracker.com/story/show/110114888
,2015-12-22T01:33:12Z,delivered,,"in the deployment manifest

```
jobs
- name: proxy
  templates:
  - name: proxy 						
    release: cf-mysql 			
    consumes: # <---- used to be links
       data-node:            
           from: my-data-node 	
```",1.0,110541840,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",rename links to consumes section in deployment manifest,793309.0,"[793309, 1233340]",956238,81882,feature,2016-01-04T19:40:26Z,https://www.pivotaltracker.com/story/show/110541840
,2015-12-22T01:37:16Z,started,,"instead of our current fully qualified link name user can now only use given link name

```yaml
jobs:
- name: proxy
  templates:
  - name: proxy 						
    release: cf-mysql 			
  	consumes:              
  	  data-node:           
  	    from: my-data-node # <----- uses ""from""
```",2.0,110541930,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can fulfill consumes via link name,793309.0,"[793309, 1233340]",956238,81882,feature,2015-12-24T16:18:49Z,https://www.pivotaltracker.com/story/show/110541930
,2015-12-22T01:42:05Z,started,,"from: other-dep.link-name.
allow specifying shared: true/false in provides
raise an error stating link is not found or not shared
see https://github.com/cloudfoundry/bosh-notes/blob/master/links.md#deployment-manifest-schema for where shared: true goes",4.0,110542084,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can fulfill consumes across deployments ,793309.0,[793309],956238,81882,feature,2016-01-04T16:38:21Z,https://www.pivotaltracker.com/story/show/110542084
,2015-09-22T04:56:57Z,started,,"```
+--------------------------------------------------------+--------------------+----+---------------+--------------+
| Instance                                               | State              | AZ | Resource Pool | IPs          |
+--------------------------------------------------------+--------------------+----+---------------+--------------+
| unknown/63f8d619-062e-490f-9a59-237a21e1f097 (unknown) | unresponsive agent | z1 |               |              |
| db/7deac7fa-20a6-47af-b749-872949a7a941 (1)            | running            | z2 | z1            | 10.10.64.55  |
| db/3cfc9984-2ef2-4e6e-87ae-dca23f5fc6b5 (3)            | running            | z2 | z1            | 10.10.64.245 |
| db/dd1883d2-4e22-4e10-a584-1e028f9f6a28 (5)            | running            | z2 | z1            | 10.10.64.180 |
| db/21e702ae-a0aa-4393-ae1b-3c4a9506714e (7)            | running            | z2 | z1            | 10.10.64.7   |
| db/76384ec0-bd9f-461d-b547-b46e71c95006 (8)            | running            | z2 | z1            | 10.10.64.197 |
| db/cdf56246-a6d9-4fa9-b552-bcc61211d61d (9)            | running            | z2 | z1            | 10.10.64.123 |
| db/e490d01c-4a1a-4e94-9de3-5c85fd08cde6 (10)           | running            | z2 | z1            | 10.10.64.109 |
+--------------------------------------------------------+--------------------+----+---------------+--------------+
```

should not happen:

```
{""vm_cid"":""i-592149f9"",""disk_cid"":null,""ips"":[],""dns"":[],""agent_id"":""0ecb1767-17e0-4364-b77e-69fd5b9498f2"",""job_name"":null,""index"":null,""job_state"":""unresponsive agent"",""resource_pool"":null,""vitals"":null,""resurrection_paused"":false,""availability_zone"":""z2"",""instance_id"":""76384ec0-bd9f-461d-b547-b46e71c95006"",""is_bootstrap"":false}
```",1.0,103875276,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",always show job/id in bosh vms and instances,1783496.0,"[1783496, 1778582]",956238,81882,feature,2015-12-28T13:37:51Z,https://www.pivotaltracker.com/story/show/103875276
,2015-08-18T22:10:18Z,finished,,"currently deployment name is not saved by the agent, hence it's not sent in the heartbeat hence it's not picked up the hm hearts.",,101543812,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",hm plugins should send correct deployment name to downstream providers,1426194.0,[1426194],956238,81882,bug,2016-01-04T21:37:30Z,https://www.pivotaltracker.com/story/show/101543812
,2015-09-23T21:11:47Z,delivered,,"  - cli command
  - director support",1.0,104038532,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",remove rename job functionality (which removes index reconcilation),1426194.0,[1426194],956238,81882,feature,2016-01-06T00:34:19Z,https://www.pivotaltracker.com/story/show/104038532
,2015-12-28T23:08:12Z,delivered,,"https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/problem_handlers/missing_disk.rb#L70

When user deletes disk in IAAS there is no way to recover. Since vsphere CPI throws DiskNotFound instead of DiskIsNotAttached in 3112",1.0,110779166,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",cck missing disk delete reference should ignore error when disk is not found in IAAS,1426194.0,[1426194],956238,553935,feature,2016-01-06T00:34:29Z,https://www.pivotaltracker.com/story/show/110779166
,2015-02-18T00:47:37Z,finished,,"https://github.com/cloudfoundry/bosh/pull/743. 
- make sure that license did not change between 5.2.5 and 5.2.4.
- make sure included files have correct checksum",1.0,88572424,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",PR 'Update monit to 5.2.5',1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-31T19:20:57Z,https://www.pivotaltracker.com/story/show/88572424
,2015-09-18T17:00:12Z,finished,,"can be seen before agent starts up
run vsphere stemcell on virtualbox to see?
can hostname be empty before agent comes up? (otherwise, ""bosh"")",2.0,103688828,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",reset hostname at the end of the stemcell builder,1868036.0,"[1868036, 353433]",956238,81882,feature,2015-12-31T02:18:07Z,https://www.pivotaltracker.com/story/show/103688828
,2015-07-24T00:25:35Z,started,,"This line outputs several times in the Mbus  test suite:

`WARNING: yagnats.NewClient() and fakeyagnats.New() are deprecated. You should use yagnats.Connect() and fakeyagnats.Connect() instead` ",,99751052,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Fix deprecation warning in go-agent,,[],956238,344,chore,2016-01-05T18:37:37Z,https://www.pivotaltracker.com/story/show/99751052
,2015-09-06T00:51:03Z,started,,`bosh vms` may sometimes show vm without association to an instance. that should never happen. currently it happens because fkey is on the instances table instead of on the vms table.,,102805692,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",vm should never exist without being associated to an instance,1017727.0,[1017727],956238,81882,chore,2016-01-05T02:10:04Z,https://www.pivotaltracker.com/story/show/102805692
,2015-12-01T01:17:05Z,started,,"check out few stigs in icebox for a bit more context
what is a recommended configuration for log keeping?",2.0,109224480,story,"[{'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",investigate auditd for a stemcell,1750722.0,"[1750722, 1386874]",956238,81882,feature,2016-01-04T03:00:19Z,https://www.pivotaltracker.com/story/show/109224480
,2015-12-03T20:33:47Z,finished,,"```
root@a43e4cd5-d4ad-4b3f-aa23-bc1378821f8a:/home/vcap# lsof -i|grep 4222
bosh-agen   747   root    5u  IPv4   12406      0t0  TCP 10.10.0.36:60440->10.10.0.6:4222 (CLOSE_WAIT)  #<-------------
bosh-agen   747   root    8u  IPv4 5527428      0t0  TCP 10.10.0.36:39350->10.10.0.6:4222 (SYN_SENT)  #<-------------
ruby      15516   vcap   10u  IPv4 4431295      0t0  TCP *:4222 (LISTEN)
ruby      15516   vcap   12u  IPv4 5048511      0t0  TCP 10.10.0.36:4222->10.10.64.123:34056 (ESTABLISHED)
ruby      15516   vcap   13u  IPv4 4432196      0t0  TCP 10.10.0.36:4222->10.10.0.36:47515 (ESTABLISHED)
ruby      15772   vcap   11u  IPv4 4432195      0t0  TCP 10.10.0.36:47515->10.10.0.36:4222 (ESTABLISHED)
root@a43e4cd5-d4ad-4b3f-aa23-bc1378821f8a:/home/vcap#
```
---
```
2015-12-03_20:32:24.26350 [Received SIGSEGV] 2015/12/03 20:32:24 ERROR - Dumping goroutines...
2015-12-03_20:32:24.26395 [Received SIGSEGV] 2015/12/03 20:32:24 ERROR - goroutine 6 [running]:
2015-12-03_20:32:24.27617 github.com/cloudfoundry/bosh-agent/infrastructure/agentlogger.NewSignalableLogger.func1(0xc8200165a0, 0x7f3f85086908, 0xc82000f580, 0xc8200ea1c0)
2015-12-03_20:32:24.27618 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/infrastructure/agentlogger/signalable_logger.go:17 +0xe1
2015-12-03_20:32:24.27619 created by github.com/cloudfoundry/bosh-agent/infrastructure/agentlogger.NewSignalableLogger
2015-12-03_20:32:24.27619 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/infrastructure/agentlogger/signalable_logger.go:21 +0x81
2015-12-03_20:32:24.27619
2015-12-03_20:32:24.27619 goroutine 1 [chan receive, 31528 minutes]:
2015-12-03_20:32:24.27620 github.com/cloudfoundry/bosh-agent/agent.Agent.Run(0x7f3f85086908, 0xc82000f580, 0x7f3f8508d528, 0xc8202105a0, 0x7f3f85087248, 0xc8200dfd40, 0x7f3f8508e278, 0xc82022c460, 0xdf8475800, 0x7f3f8508d758, ...)
2015-12-03_20:32:24.27620 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:90 +0x1e2
2015-12-03_20:32:24.27621 github.com/cloudfoundry/bosh-agent/app.(*app).Run(0xc820090500, 0x0, 0x0)
2015-12-03_20:32:24.27621 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/app/app.go:220 +0x5a
2015-12-03_20:32:24.27621 main.main()
2015-12-03_20:32:24.27621 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/main/agent.go:34 +0x7e3
2015-12-03_20:32:24.27622
2015-12-03_20:32:24.27622 goroutine 17 [syscall, 31528 minutes, locked to thread]:
2015-12-03_20:32:24.27622 runtime.goexit()
2015-12-03_20:32:24.27622 	/usr/local/go/src/runtime/asm_amd64.s:1696 +0x1
2015-12-03_20:32:24.27622
2015-12-03_20:32:24.27623 goroutine 5 [syscall]:
2015-12-03_20:32:24.27623 os/signal.loop()
2015-12-03_20:32:24.27623 	/usr/local/go/src/os/signal/signal_unix.go:22 +0x18
2015-12-03_20:32:24.27623 created by os/signal.init.1
2015-12-03_20:32:24.27623 	/usr/local/go/src/os/signal/signal_unix.go:28 +0x37
2015-12-03_20:32:24.27624
2015-12-03_20:32:24.27624 goroutine 7 [chan receive]:
2015-12-03_20:32:24.27624 github.com/cloudfoundry/bosh-agent/sigar.(*sigarStatsCollector).StartCollecting(0xc82000af50, 0x2540be400, 0x0)
2015-12-03_20:32:24.27624 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/sigar/sigar_stats_collector.go:28 +0x93
2015-12-03_20:32:24.27625 created by github.com/cloudfoundry/bosh-agent/platform.NewProvider
2015-12-03_20:32:24.27625 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/platform/provider.go:59 +0x87f
2015-12-03_20:32:24.27625
2015-12-03_20:32:24.27625 goroutine 223 [IO wait, 4449 minutes]:
2015-12-03_20:32:24.27626 net.runtime_pollWait(0x7f3f85088b48, 0x72, 0xc820012100)
2015-12-03_20:32:24.27626 	/usr/local/go/src/runtime/netpoll.go:157 +0x60
2015-12-03_20:32:24.27626 net.(*pollDesc).Wait(0xc8202188b0, 0x72, 0x0, 0x0)
2015-12-03_20:32:24.27626 	/usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a
2015-12-03_20:32:24.27627 net.(*pollDesc).WaitRead(0xc8202188b0, 0x0, 0x0)
2015-12-03_20:32:24.27627 	/usr/local/go/src/net/fd_poll_runtime.go:78 +0x36
2015-12-03_20:32:24.27627 net.(*netFD).accept(0xc820218850, 0x0, 0x7f3f8508e310, 0xc8205149e0)
2015-12-03_20:32:24.27627 	/usr/local/go/src/net/fd_unix.go:408 +0x27c
2015-12-03_20:32:24.27627 net.(*TCPListener).AcceptTCP(0xc8200253d8, 0xc82004d9d8, 0x0, 0x0)
2015-12-03_20:32:24.27628 	/usr/local/go/src/net/tcpsock_posix.go:254 +0x4d
2015-12-03_20:32:24.27628 net.(*TCPListener).Accept(0xc8200253d8, 0x0, 0x0, 0x0, 0x0)
2015-12-03_20:32:24.27628 	/usr/local/go/src/net/tcpsock_posix.go:264 +0x3d
2015-12-03_20:32:24.27628 github.com/cloudfoundry/bosh-agent/vendor/github.com/pivotal/go-smtpd/smtpd.(*Server).Serve(0xc82022c820, 0x7f3f8508e2d8, 0xc8200253d8, 0x0, 0x0)
2015-12-03_20:32:24.27629 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/pivotal/go-smtpd/smtpd/smtpd.go:120 +0x9d
2015-12-03_20:32:24.27629 github.com/cloudfoundry/bosh-agent/vendor/github.com/pivotal/go-smtpd/smtpd.(*Server).ListenAndServe(0xc82022c820, 0x0, 0x0)
2015-12-03_20:32:24.27630 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/pivotal/go-smtpd/smtpd/smtpd.go:114 +0xdb
2015-12-03_20:32:24.27630 github.com/cloudfoundry/bosh-agent/jobsupervisor.monitJobSupervisor.MonitorJobFailures(0x7f3f85086968, 0xc82000dbc0, 0x7f3f85086d58, 0xc8200f0ae0, 0x7f3f8508d710, 0xc820205200, 0x7f3f85086908, 0xc82000f580, 0xa45900, 0x9, ...)
2015-12-03_20:32:24.27630 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/jobsupervisor/monit_job_supervisor.go:265 +0x1f7
2015-12-03_20:32:24.27630 github.com/cloudfoundry/bosh-agent/jobsupervisor.(*monitJobSupervisor).MonitorJobFailures(0xc820218230, 0xc820238d60, 0x0, 0x0)
2015-12-03_20:32:24.27631 	<autogenerated>:23 +0xc8
2015-12-03_20:32:24.27631 github.com/cloudfoundry/bosh-agent/agent.Agent.Run.func1(0xc8200ef340, 0xc820210ae0)
2015-12-03_20:32:24.27631 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:76 +0x7b
2015-12-03_20:32:24.27632 created by github.com/cloudfoundry/bosh-agent/agent.Agent.Run
2015-12-03_20:32:24.27632 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:80 +0x166
2015-12-03_20:32:24.27632
2015-12-03_20:32:24.27632 goroutine 220 [chan receive, 31528 minutes]:
2015-12-03_20:32:24.27632 github.com/cloudfoundry/bosh-agent/agent/task.(*concreteManager).processFsFuncs(0xc8201f1cc0)
2015-12-03_20:32:24.27633 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/concrete_manager.go:103 +0xa2
2015-12-03_20:32:24.27633 created by github.com/cloudfoundry/bosh-agent/agent/task.NewManager
2015-12-03_20:32:24.27633 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/concrete_manager.go:46 +0x159
2015-12-03_20:32:24.27633
2015-12-03_20:32:24.27634 goroutine 11 [select]:
2015-12-03_20:32:24.27634 github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/gosigar.(*ConcreteSigar).CollectCpuStats.func1(0xc820087540, 0x2540be400, 0xc820016960)
2015-12-03_20:32:24.27634 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/gosigar/concrete_sigar.go:26 +0x4f1
2015-12-03_20:32:24.27635 created by github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/gosigar.(*ConcreteSigar).CollectCpuStats
2015-12-03_20:32:24.27635 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/gosigar/concrete_sigar.go:42 +0x8e
2015-12-03_20:32:24.27635
2015-12-03_20:32:24.27635 goroutine 222 [chan receive, 1486 minutes]:
2015-12-03_20:32:24.27635 github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/yagnats.(*Client).Publish(0xc8201cfd10, 0xc8200fbe80, 0x37, 0xc820406240, 0x175, 0x22a, 0x0, 0x0)
2015-12-03_20:32:24.27636 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/yagnats/client.go:100 +0x59
2015-12-03_20:32:24.27636 github.com/cloudfoundry/bosh-agent/mbus.(*natsHandler).Send(0xc8202105a0, 0xa3ff20, 0x2, 0xa4fce0, 0x9, 0x9ce440, 0xc820443080, 0x0, 0x0)
2015-12-03_20:32:24.27636 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/mbus/nats_handler.go:127 +0x8f2
2015-12-03_20:32:24.27637 github.com/cloudfoundry/bosh-agent/agent.Agent.sendHeartbeat(0x7f3f85086908, 0xc82000f580, 0x7f3f8508d528, 0xc8202105a0, 0x7f3f85087248, 0xc8200dfd40, 0x7f3f8508e278, 0xc82022c460, 0xdf8475800, 0x7f3f8508d758, ...)
2015-12-03_20:32:24.27637 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:131 +0x550
2015-12-03_20:32:24.27637 github.com/cloudfoundry/bosh-agent/agent.Agent.generateHeartbeats(0x7f3f85086908, 0xc82000f580, 0x7f3f8508d528, 0xc8202105a0, 0x7f3f85087248, 0xc8200dfd40, 0x7f3f8508e278, 0xc82022c460, 0xdf8475800, 0x7f3f8508d758, ...)
2015-12-03_20:32:24.27638 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:118 +0x17a
2015-12-03_20:32:24.27638 created by github.com/cloudfoundry/bosh-agent/agent.Agent.Run
2015-12-03_20:32:24.27638 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:73 +0x134
2015-12-03_20:32:24.27639
2015-12-03_20:32:24.27639 goroutine 221 [chan receive]:
2015-12-03_20:32:24.27639 github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/yagnats.(*Client).Disconnect(0xc8201cfd10)
2015-12-03_20:32:24.27639 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/vendor/github.com/cloudfoundry/yagnats/client.go:93 +0x55
2015-12-03_20:32:24.27640 github.com/cloudfoundry/bosh-agent/mbus.(*natsHandler).runUntilInterrupted(0xc8202105a0)
2015-12-03_20:32:24.27640 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/mbus/nats_handler.go:168 +0x1f4
2015-12-03_20:32:24.27640 github.com/cloudfoundry/bosh-agent/mbus.(*natsHandler).Run(0xc8202105a0, 0xc82021af20, 0x0, 0x0)
2015-12-03_20:32:24.27640 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/mbus/nats_handler.go:66 +0x458
2015-12-03_20:32:24.27641 github.com/cloudfoundry/bosh-agent/agent.Agent.subscribeActionDispatcher(0x7f3f85086908, 0xc82000f580, 0x7f3f8508d528, 0xc8202105a0, 0x7f3f85087248, 0xc8200dfd40, 0x7f3f8508e278, 0xc82022c460, 0xdf8475800, 0x7f3f8508d758, ...)
2015-12-03_20:32:24.27641 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:98 +0x126
2015-12-03_20:32:24.27641 created by github.com/cloudfoundry/bosh-agent/agent.Agent.Run
2015-12-03_20:32:24.27642 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/agent.go:71 +0xfa
2015-12-03_20:32:24.27642
2015-12-03_20:32:24.27642 goroutine 219 [chan receive, 4449 minutes]:
2015-12-03_20:32:24.27642 github.com/cloudfoundry/bosh-agent/agent/task.asyncTaskService.processSemFuncs(0x7f3f8508d570, 0xd599e8, 0x7f3f85086908, 0xc82000f580, 0xc820207e30, 0xc820210900, 0xc820210960)
2015-12-03_20:32:24.27643 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/async_task_service.go:92 +0x92
2015-12-03_20:32:24.27643 created by github.com/cloudfoundry/bosh-agent/agent/task.NewAsyncTaskService
2015-12-03_20:32:24.27643 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/async_task_service.go:30 +0x1cc
2015-12-03_20:32:24.27643
2015-12-03_20:32:24.27643 goroutine 218 [chan receive, 4449 minutes]:
2015-12-03_20:32:24.27644 github.com/cloudfoundry/bosh-agent/agent/task.asyncTaskService.processTasks(0x7f3f8508d570, 0xd599e8, 0x7f3f85086908, 0xc82000f580, 0xc820207e30, 0xc820210900, 0xc820210960)
2015-12-03_20:32:24.27644 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/async_task_service.go:101 +0xd0
2015-12-03_20:32:24.27644 created by github.com/cloudfoundry/bosh-agent/agent/task.NewAsyncTaskService
2015-12-03_20:32:24.27645 	/mnt/stemcells/aws/xen/ubuntu/build/build/stages/bosh_go_agent/assets/go/src/github.com/cloudfoundry/bosh-agent/agent/task/async_task_service.go:29 +0x168
2015-12-03_20:32:24.27645
2015-12-03_20:32:24.27645 goroutine 224 [IO wait, 31527 minutes]:
2015-12-03_20:32:24.27646 net.runtime_pollWait(0x7f3f85088cc8, 0x72, 0xc820012100)
2015-12-03_20:32:24.27646 	/usr/local/go/src/runtime/netpoll.go:157 +0x60
2015-12-03_20:32:24.27646 net.(*pollDesc).Wait(0xc8202183e0, 0x72, 0x0, 0x0)
2015-12-03_20:32:24.27646 	/usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a
2015-12-03_20:32:24.27647 net.(*pollDesc).
```",,109478174,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh-agent should be more aggressive about reconnecting,344.0,"[344, 1868036]",956238,81882,bug,2016-01-04T21:37:41Z,https://www.pivotaltracker.com/story/show/109478174
,2015-12-22T01:40:21Z,started,,"self-referential example:

```yaml
jobs:
- name: proxy
  templates:
  - name: proxy 				
    release: cf-mysql 	
  	consumes:             
  	  data-node:          
  	    from: my-data-node  # <--- uses alt-name
  	provides:             
  	  data-node:          
  	    as: my-data-node    # <--- sets alt-name
```

when as is used, original link name cannot be used.",4.0,110542034,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can fulfill consumes via link alt-name specified with `as` key,946329.0,"[946329, 948679, 659629]",956238,81882,feature,2016-01-05T21:09:26Z,https://www.pivotaltracker.com/story/show/110542034
,2015-12-14T21:31:55Z,started,,"change format for link to only include *single* address:

```
{
    ""nodes"": [
        # For each one of the deployment job instances
        {
            ""name"": ""data-node"",
            ""id"": ""4c213d80-05c1-429f-996f-8911854991a0"",
            ""index"": 0,
            ""az"": ""z1"",
            ""address"": ""10.0.0.44"" || ""0.private.data-node.deployment"" || ""IPv6"" <--------- single key
        },
        {
            ""name"": ""data-node"",
            ""id"": ""76009006-20c7-4cc0-b17d-134cf61226d2"",
            ""index"": 1,
            ""az"": ""z1"",
            ""address"": ""10.0.0.45"" || ""1.private.data-node.deployment"" || ""IPv6""
        }
    ]
}
```

definition of a link consumed over specific network:

see examples in https://github.com/cloudfoundry/bosh-notes/blob/master/links.md#address-resolution

- raise an error if network name specified is not one of the networks on the link",4.0,110110150,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user can specify which network to use for address resolution,631325.0,"[631325, 1169826]",956238,81882,feature,2016-01-04T20:43:38Z,https://www.pivotaltracker.com/story/show/110110150
,2016-01-04T18:09:45Z,finished,,https://github.com/cloudfoundry/bosh/pull/1062/files,2.0,110990098,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Fixing problem with skip_drain option for changing job(deployment) state',1868036.0,"[1868036, 344]",956238,81882,feature,2016-01-05T01:40:40Z,https://www.pivotaltracker.com/story/show/110990098
,2016-01-04T18:12:00Z,finished,,https://github.com/cloudfoundry/bosh/pull/1035/files,1.0,110990384,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Consul event forwarder ttl registrations',1868036.0,"[1868036, 344]",956238,81882,feature,2016-01-05T00:14:52Z,https://www.pivotaltracker.com/story/show/110990384
,2016-01-04T18:11:27Z,started,,https://github.com/cloudfoundry/bosh/pull/1041,2.0,110990348,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Add code to building stemcell for Softlayer based on Ubuntu trusty',1868036.0,"[1868036, 344]",956238,81882,feature,2016-01-05T00:14:59Z,https://www.pivotaltracker.com/story/show/110990348
,2016-01-04T18:10:35Z,finished,,https://github.com/cloudfoundry/bosh/pull/1027/files,2.0,110990282,story,"[{'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Changes for building a ppc64le stemcell for Power8 Ubuntu LE',1868036.0,[1868036],956238,81882,feature,2016-01-05T01:03:57Z,https://www.pivotaltracker.com/story/show/110990282
,2016-01-05T00:48:20Z,started,,,0.0,111023548,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",produce pre-az stemcell version with updated os image,553935.0,"[553935, 1732048]",956238,81882,feature,2016-01-05T01:17:19Z,https://www.pivotaltracker.com/story/show/111023548
,2016-01-05T01:37:20Z,started,,,1.0,111026042,story,"[{'name': 'compiler-cleanup', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T01:38:04Z', 'id': 13685024, 'updated_at': '2016-01-05T01:38:04Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",investigate which compilers/dev binaries should be removed,1386874.0,"[1386874, 1750722]",956238,81882,feature,2016-01-05T09:24:14Z,https://www.pivotaltracker.com/story/show/111026042
,2016-01-05T17:30:40Z,started,,,,111075644,story,[],"Update concourse director, add azs, dog food",344.0,[344],956238,344,chore,2016-01-05T17:30:56Z,https://www.pivotaltracker.com/story/show/111075644
,2016-01-05T18:19:37Z,started,,use same mecahnism as normal deploy,2.0,111079610,story,"[{'name': 'diff', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T18:21:13Z', 'id': 13692916, 'updated_at': '2016-01-05T18:21:13Z'}]","Director should resolve latest for stemcells, releases when displaying diff",553935.0,"[553935, 1794602]",956238,553935,feature,2016-01-06T01:08:52Z,https://www.pivotaltracker.com/story/show/111079610
,2016-01-05T22:32:35Z,started,,http://www.ubuntu.com/usn/usn-2857-1/,1.0,111103500,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bump kernel for ubuntu trusty,553935.0,[553935],956238,81882,feature,2016-01-06T01:04:36Z,https://www.pivotaltracker.com/story/show/111103500
,2016-01-05T23:55:16Z,started,,https://github.com/cloudfoundry/bosh/blob/develop/stemcell_builder/stages/base_ubuntu_packages/apply.sh,,111109032,story,[],rsyslog-mmjsonparse should be installed on non-powerpc arch,353433.0,"[353433, 1732048]",956238,553935,bug,2016-01-06T01:08:37Z,https://www.pivotaltracker.com/story/show/111109032
,2016-01-05T23:43:16Z,started,,https://github.com/cloudfoundry/bosh/blob/d2159dca24cacbf2731ecd6b4d0d5545b16c678e/bosh-director/lib/bosh/director/deployment_plan/instance_plan_factory.rb,,111108272,story,[],Director should reserve IP addresses for obsolete instances,344.0,[344],956238,353433,bug,2016-01-06T00:10:44Z,https://www.pivotaltracker.com/story/show/111108272
,2016-01-06T00:31:58Z,started,,,2.0,111111028,story,"[{'name': 'no-redis-no-cry', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T00:31:57Z', 'id': 13697618, 'updated_at': '2016-01-06T00:31:57Z'}]",worker should fail delayed job if its task state is not queued,1426194.0,"[1426194, 1868036]",956238,1426194,feature,2016-01-06T01:08:28Z,https://www.pivotaltracker.com/story/show/111111028
,2016-01-05T23:42:18Z,unstarted,,,,111108202,story,[],migrated_from should allow to use job name if it is defined on the same job,,[],956238,353433,feature,2016-01-05T23:42:18Z,https://www.pivotaltracker.com/story/show/111108202
,2015-09-23T21:11:31Z,unstarted,,- put '/:deployment/scan_and_fix',1.0,104038502,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'id-index', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:44:41Z', 'id': 13197944, 'updated_at': '2015-10-30T23:44:41Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",resurrector/hm resolves problems by referencing ids,948679.0,"[948679, 793309, 1017727]",956238,81882,feature,2016-01-05T21:09:15Z,https://www.pivotaltracker.com/story/show/104038502
,2016-01-05T18:41:07Z,unstarted,,,,111081488,story,"[{'name': 'diff', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T18:21:13Z', 'id': 13692916, 'updated_at': '2016-01-05T18:21:13Z'}]",user should not see changes when release/stemcell version is used as an integer vs string during diff,,[],956238,81882,feature,2016-01-05T18:41:14Z,https://www.pivotaltracker.com/story/show/111081488
,2015-12-22T01:51:58Z,unstarted,,"e.g. given a deployment with one etcd (provides etcd), nats (provides nats), nats2 (provides nats), if i have a job (web) that uses etcd and nats, i only have to explicitly specify nats under web to point one of the nats-es. etcd should be filled in automatically because there is only one thing that provides etcd in that deployment.",4.0,110542444,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",user should not have to fulfill consumes explicitly if there is only one provides of that type in the deployment,659629.0,"[659629, 1457658]",956238,81882,feature,2016-01-05T15:15:00Z,https://www.pivotaltracker.com/story/show/110542444
,2016-01-04T21:27:13Z,unstarted,,https://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/fuzz-tests/builds/1109,,111009072,story,[],Investigate fuzz tests failure,,[],956238,353433,chore,2016-01-04T21:27:13Z,https://www.pivotaltracker.com/story/show/111009072
,2015-12-31T10:01:32Z,unstarted,,"make sure we dont leak files.
keep the cli using ruby version.",,110879584,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",switch to using s3cli binary in bosh-release in the director ,,[],956238,81882,feature,2016-01-04T07:32:10Z,https://www.pivotaltracker.com/story/show/110879584
,2015-08-05T01:06:54Z,unstarted,,,,100610872,story,[],misc track above,,[],956238,81882,release,2015-11-24T18:04:51Z,https://www.pivotaltracker.com/story/show/100610872
,2016-01-06T01:20:31Z,unstarted,,"  * same as cloud-config
  * sync http response",4.0,111113432,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",user can successfully save/get runtime config via 'bosh update runtime-config .blah.yml' and 'bosh runtime-config',,[],956238,81882,feature,2016-01-06T01:23:15Z,https://www.pivotaltracker.com/story/show/111113432
,2016-01-06T01:20:53Z,unstarted,,,2.0,111113444,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",raise an error if latest version is specified (only support explicit values),,[],956238,81882,feature,2016-01-06T01:23:17Z,https://www.pivotaltracker.com/story/show/111113444
,2016-01-06T01:21:10Z,unstarted,,,2.0,111113456,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",raise an error if releases section does not have necessary release used as an addon,,[],956238,81882,feature,2016-01-06T01:23:19Z,https://www.pivotaltracker.com/story/show/111113456
,2016-01-06T01:21:30Z,unstarted,,"  * use release job that doesnt have any links or properties for now
  * addon jobs should be compiled for that stemcell properly",4.0,111113472,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",user can see that each vm in each deployment have all addons collocated,,[],956238,81882,feature,2016-01-06T01:23:21Z,https://www.pivotaltracker.com/story/show/111113472
,2016-01-06T01:21:43Z,unstarted,,  * only support properties on the addon (no global properties section),2.0,111113482,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",user can see that addon properties are evaluated,,[],956238,81882,feature,2016-01-06T01:23:23Z,https://www.pivotaltracker.com/story/show/111113482
,2016-01-06T01:21:58Z,unstarted,,,4.0,111113494,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",user can see that addon links are resolved and validated during bosh deploy,,[],956238,81882,feature,2016-01-06T01:23:25Z,https://www.pivotaltracker.com/story/show/111113494
,2016-01-06T01:22:12Z,unstarted,,"example should work

```
jobs:
- name: blah
  networks: [...]
  templates: [] #<--------- empty
```",1.0,111113502,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",user can deploy deployment manifest with 0 templates,,[],956238,81882,feature,2016-01-06T01:25:10Z,https://www.pivotaltracker.com/story/show/111113502
,2016-01-06T01:22:48Z,unstarted,,,,111113518,story,"[{'name': 'addons', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-06T01:08:48Z', 'id': 13697954, 'updated_at': '2016-01-06T01:08:48Z'}]",addons v1,,[],956238,81882,release,2016-01-06T01:22:56Z,https://www.pivotaltracker.com/story/show/111113518
,2015-12-14T22:55:54Z,unstarted,,,,110117296,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",links should use migrated job names (add integration test),,[],956238,553935,chore,2015-12-22T01:28:23Z,https://www.pivotaltracker.com/story/show/110117296
,2015-12-09T00:41:29Z,unstarted,,"```
$ bosh export release test/0+dev.30 ubuntu-trusty/3148
Director task 20620
Error 140014: Link path was not provided for required link 'http_endpoint' in job 'dummy-job-for-compilation'
```

in this story let's actually figure out how to ask the compiler to compile things without spinning up a virtual job in memory.",4.0,109771344,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",bosh export release should work for releases that use links,,[],956238,81882,feature,2015-12-22T01:28:17Z,https://www.pivotaltracker.com/story/show/109771344
,2015-11-17T23:53:17Z,unstarted,,,,108399324,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",better networking with links,,[],956238,81882,release,2015-12-14T21:55:01Z,https://www.pivotaltracker.com/story/show/108399324
,2015-07-02T00:48:39Z,unstarted,,,,98242738,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}]",[PLACEHOLDER] for more linking stories...,,[],956238,81882,feature,2015-12-14T21:55:09Z,https://www.pivotaltracker.com/story/show/98242738
,2015-06-29T16:59:03Z,unstarted,,,,98027980,story,"[{'name': 'links2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-12-14T21:19:22Z', 'id': 13565894, 'updated_at': '2015-12-14T21:19:22Z'}]",job links can be used to share property information,,[],956238,81882,release,2015-12-14T21:20:54Z,https://www.pivotaltracker.com/story/show/98027980
,2015-08-17T19:05:03Z,unstarted,,,1.0,101435040,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",provide a list of plugins for hm that should take an instance id and forward it somewhere,,[],956238,81882,feature,2016-01-04T17:49:19Z,https://www.pivotaltracker.com/story/show/101435040
,2015-08-17T19:06:23Z,unstarted,,,,101435148,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",[PLACEHOLDER] include instance id in healthmonitor plugin data,,[],956238,81882,feature,2015-10-12T06:47:08Z,https://www.pivotaltracker.com/story/show/101435148
,2015-09-22T04:57:15Z,unstarted,,,,103875404,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",better azs,,[],956238,81882,release,2015-09-22T04:57:35Z,https://www.pivotaltracker.com/story/show/103875404
,2015-12-16T19:24:40Z,unstarted,,"currently it does it on the second try
undo change to fuzz test to disable this check",4.0,110273634,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",rebalance vms after removal of persistent disk,,[],956238,81882,feature,2016-01-04T20:15:29Z,https://www.pivotaltracker.com/story/show/110273634
,2015-11-23T05:21:08Z,unstarted,,,,108728926,story,"[{'name': 'dp', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-18T17:27:15Z', 'id': 12807422, 'updated_at': '2015-09-18T17:27:15Z'}]",--- dynamic provisioning v1 ---,,[],956238,81882,release,2015-12-20T02:05:31Z,https://www.pivotaltracker.com/story/show/108728926
,2015-08-21T17:14:01Z,unstarted,,"ARP entries are not gc-ed if connection is kept around. Should we use Close() on a connection to terminate it after a timeout? Timeout after a minute.

```
vcap@0f5228d7-3dd5-48b3-94d1-83724e17ece7:~$ sudo lsof |grep 10.10.0.6
[sudo] password for vcap:
bosh-agen   663         root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663         root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   743   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   743   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   744   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   744   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   761   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   761   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   763   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   763   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   765   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   765   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663   767   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663   767   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663  1167   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663  1167   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663  1179   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663  1179   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663  1371   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663  1371   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
bosh-agen   663  1394   root    3u     IPv4              12216      0t0        TCP 10.10.0.36:36171->10.10.0.6:4222 (CLOSE_WAIT)
bosh-agen   663  1394   root    7u     IPv4             563643      0t0        TCP 10.10.0.36:36506->10.10.0.6:4222 (SYN_SENT)
ruby       4239         vcap   11u     IPv4              14498      0t0        TCP 10.10.0.36:4222->10.10.0.63:51723 (ESTABLISHED)
ruby-time  4239  4245   vcap   11u     IPv4              14498      0t0        TCP 10.10.0.36:4222->10.10.0.63:51723 (ESTABLISHED)
```

add integration test for it. iptables?",,101785624,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': '[rus]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-12T06:39:14Z', 'id': 13012714, 'updated_at': '2015-10-12T06:39:14Z'}]",agent should disconnect from nats after specific timeout,,[],956238,81882,bug,2015-12-24T20:18:23Z,https://www.pivotaltracker.com/story/show/101785624
,2015-11-07T01:07:00Z,unstarted,,,,107622198,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",switch to using track on a stage,,[],956238,81882,chore,2015-11-08T03:07:02Z,https://www.pivotaltracker.com/story/show/107622198
,2015-10-05T22:13:43Z,unstarted,,"  - stop, drain, attach cpi, mount, pre-start, start flow for chosen instance
  - unorphan attached disk
  - allow to attach disk that is in the IaaS but is not in the orphaned list
  - in a deployment context
  - only admin",8.0,104951944,story,"[{'name': 'orph-disks2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-08T03:24:43Z', 'id': 13263482, 'updated_at': '2015-11-08T03:24:43Z'}]",user can run attach disk to an existing instance for instance that does not have an attached disk,,[],956238,81882,feature,2015-12-29T21:33:59Z,https://www.pivotaltracker.com/story/show/104951944
,2015-10-05T22:13:59Z,unstarted,,"  - stop, drain, umount, detach, attach, mount, pre-start, start flow for chosen instance
  - orphan previous attached disk
  - unorphan attached disk
  - allow to attach disk that is in the IaaS but is not in the orphaned list
  - in a deployment context
  - only admin",4.0,104951964,story,"[{'name': 'orph-disks2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-08T03:24:43Z', 'id': 13263482, 'updated_at': '2015-11-08T03:24:43Z'}]",user can run attach disk to attach a disk to an existing instance for instance that already have a disk attached,,[],956238,81882,feature,2015-11-08T03:25:12Z,https://www.pivotaltracker.com/story/show/104951964
,2015-10-05T22:21:22Z,unstarted,,,,104952654,story,"[{'name': 'orph-disks2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-08T03:24:43Z', 'id': 13263482, 'updated_at': '2015-11-08T03:24:43Z'}]",recoverable orphaned disks,,[],956238,81882,release,2015-11-08T03:25:22Z,https://www.pivotaltracker.com/story/show/104952654
,2015-10-20T15:51:58Z,unstarted,,i think we already drop vcap.* syslog messages from going to disk (/var/log/). make sure that syslog does not write any additional logs to /var/log director. tag with `vcap.***I*`,2.0,106121532,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",user can see director's api calls forwarded to syslog,,[],956238,81882,feature,2016-01-04T20:20:57Z,https://www.pivotaltracker.com/story/show/106121532
,2015-10-20T15:52:11Z,unstarted,,"cpi: create_vm (username)
cpi: delete_disk (system)",2.0,106121560,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",user can see cpi calls forwarded to syslog,,[],956238,81882,feature,2016-01-04T20:25:35Z,https://www.pivotaltracker.com/story/show/106121560
,2015-10-20T15:53:03Z,unstarted,,,,106121650,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",improved director auditing support,,[],956238,81882,release,2015-10-20T15:53:45Z,https://www.pivotaltracker.com/story/show/106121650
,2015-10-20T15:54:25Z,unstarted,,,,106121800,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",user can see metrics and events from all agents forwarded to syslog,,[],956238,81882,feature,2015-10-20T15:55:03Z,https://www.pivotaltracker.com/story/show/106121800
,2015-10-20T15:54:45Z,unstarted,,,,106121832,story,"[{'name': 'audit1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-20T15:51:22Z', 'id': 13092972, 'updated_at': '2015-10-20T15:51:22Z'}]",improved audit support with aux info,,[],956238,81882,release,2015-10-20T15:54:47Z,https://www.pivotaltracker.com/story/show/106121832
,2016-01-05T01:36:09Z,unstarted,,"- change director to automatically add bosh.remove_compilers=true (next to bosh.password config) to env for each vm created
- change director to automatically add bosh.remove_compilers=false for compilation vms
- change agent to remove appropriate compiler binaries from the vm when remove_compilers is set to true
  - do cleanup in bootstrap (idempotent)
- which binaries to remove?
   - consider collecting this in the stemcell builder into a file?

example manifest:

```
jobs:
- name: blah
  ...
  env: 
    bosh:
      remove_compilers: false # <--- keeps compilers
```",4.0,111025994,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'compiler-cleanup', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T01:38:04Z', 'id': 13685024, 'updated_at': '2016-01-05T01:38:04Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': '[cn]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-02T01:07:47Z', 'id': 11832168, 'updated_at': '2015-06-08T22:55:32Z'}]",director automatically removes compilers from the stemcell by default and keeps them on compilation vms,,[],956238,81882,feature,2016-01-05T01:41:07Z,https://www.pivotaltracker.com/story/show/111025994
,2016-01-05T01:40:45Z,unstarted,,,,111026156,story,"[{'name': 'compiler-cleanup', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T01:38:04Z', 'id': 13685024, 'updated_at': '2016-01-05T01:38:04Z'}]",compilers are not on stemcells,,[],956238,81882,release,2016-01-05T01:40:48Z,https://www.pivotaltracker.com/story/show/111026156
,2015-11-04T18:15:31Z,unstarted,,See #89460604,,107417762,story,[],Submit pull request to FakeFS for `unhijack!` fix.,,[],956238,1550486,chore,2015-11-04T20:29:05Z,https://www.pivotaltracker.com/story/show/107417762
,2015-10-06T17:10:54Z,unstarted,,,,105020812,story,[],Create a document how to test director on bosh lite with manifest and everything,,[],956238,553935,chore,2015-11-04T20:29:20Z,https://www.pivotaltracker.com/story/show/105020812
,2015-10-15T20:55:18Z,unstarted,,,,105800400,story,"[{'name': 'tech forum', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-02T21:43:15Z', 'id': 12666536, 'updated_at': '2015-09-02T21:43:15Z'}, {'name': '[rupa]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T23:59:51Z', 'id': 13197994, 'updated_at': '2015-10-30T23:59:51Z'}]",Spike on creating an integration test suite for the director only,,[],956238,1017727,chore,2015-11-04T20:29:27Z,https://www.pivotaltracker.com/story/show/105800400
,2015-09-30T22:09:24Z,unstarted,,"- deploy attached bsoh with uaa manifest
- configure it to have:
```
          scope: openid
          authorities: uaa.none,bosh.admin
```
- bosh login and see that you cannot run bosh vms
- reconfigure bsoh with
```
          scope: openid,bosh.admin
          authorities: uaa.none
```
- bosh login (*wihtout logout*) and see that you still cannot run bosh vms
- bosh logout
- bosh login and see that it work

why?????",,104581488,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",failing to login and get correct access permissions after uaa redeploy,,[],956238,81882,bug,2015-10-09T18:18:45Z,https://www.pivotaltracker.com/story/show/104581488
,2015-08-24T18:24:04Z,unstarted,,,1.0,101914406,story,[],Split BOSH unit & integration suites such that only bosh-cli is tested on Ruby 1.9,,[],956238,1550486,feature,2015-10-09T18:18:45Z,https://www.pivotaltracker.com/story/show/101914406
,2015-10-02T18:19:30Z,unstarted,,update vagrant file.,,104747574,story,[],rebuild stemcell bulder vm packer boxes,,[],956238,81882,chore,2015-10-05T07:04:54Z,https://www.pivotaltracker.com/story/show/104747574
,2015-12-23T23:45:44Z,unstarted,,https://github.com/cloudfoundry/bosh-agent/pull/56,1.0,110684306,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",pull in bosh-agent partition type,1868036.0,"[1868036, 353433]",956238,81882,feature,2016-01-05T18:41:25Z,https://www.pivotaltracker.com/story/show/110684306
,2015-09-22T00:57:53Z,unstarted,,"Apparently we have functional tests, we should probably run them in our pipeline",,103868516,story,[],Add (blobstore_client?) functional tests to concourse pipeline,,[],956238,553935,chore,2015-09-24T22:03:01Z,https://www.pivotaltracker.com/story/show/103868516
,2015-09-09T00:51:36Z,unstarted,,,1.0,102964458,story,[],bosh CLI should raise an exception if CLI_RUBY_VERSION is specified and does not match RUBY_VERSION,,[],956238,1550486,feature,2015-09-24T22:02:14Z,https://www.pivotaltracker.com/story/show/102964458
,2015-09-02T21:44:58Z,unstarted,,"ideas:
- look for already compiled versions at a path that includes the sha1 the tarball
- think about adding a cached version to the docker file",,102624846,story,"[{'name': 'tech forum', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-02T21:43:15Z', 'id': 12666536, 'updated_at': '2015-09-02T21:43:15Z'}]",Try to cache the nginx compilation for the integration tests,,[],956238,687691,chore,2015-09-10T07:06:16Z,https://www.pivotaltracker.com/story/show/102624846
,2015-08-17T22:39:16Z,unstarted,,,,101455128,story,[],spike on bosh build/dev improvements/speed-ups to present at a tech forum                            ,,[],956238,1550486,chore,2015-09-10T07:06:06Z,https://www.pivotaltracker.com/story/show/101455128
,2015-08-14T21:46:54Z,unstarted,,- put that in bosh-concourse-ci/docs,,101318108,story,[],document concourse update path,,[],956238,553935,chore,2015-08-28T22:10:42Z,https://www.pivotaltracker.com/story/show/101318108
,2015-09-10T21:50:40Z,unstarted,,"There is an issue where VMs can run out of tcp resources if a large number of connections are made to it in a short amount of time (>65k in <2 min). This affects many things, but one example is if there is a lot of traffic to cf apps, the ha proxy, routers, and DEAs can be affected.  It causes requests to fail.

See #102698920 for the specific kernel properties that need to be tuned.",1.0,103144524,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Update stemcell so it frees tcp resources from finished tcp connections more quickly,,[],956238,1120042,feature,2015-09-22T04:16:42Z,https://www.pivotaltracker.com/story/show/103144524
,2013-12-11T23:24:18Z,unstarted,,"This is a bug #62152150 in the ruby agent. I haven't verified that Go agent has this bug but it's very likely.

Monit's STOP http endpoint returns 200 even before all the jobs are actually stopped. This can lead to a race between ""apply"" creating the monitrc and monit executing the _ctl scripts.",2.0,62354622,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}]","Agent should poll monit process state on ""STOP"" messages",,[],956238,1338772,feature,2015-12-15T14:14:09Z,https://www.pivotaltracker.com/story/show/62354622
,2015-03-16T22:07:40Z,unstarted,,"agent seems to get stuck on the second call to metadata service when it uses default http client which in turn caches persistent connections

seen behavior: agent would get stuck on calling the metadata.",,90459670,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",agent should not try to reuse keep alive connections (while contacting metadata service),,[],956238,81882,bug,2015-07-15T23:24:14Z,https://www.pivotaltracker.com/story/show/90459670
,2015-06-18T16:38:00Z,unstarted,,- remove director api for create/delete users,1.0,97312442,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}, {'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",remove users database so that director can only be configured with local static users or uaa,,[],956238,81882,feature,2015-12-20T02:07:32Z,https://www.pivotaltracker.com/story/show/97312442
,2015-02-09T23:24:03Z,unstarted,,,1.0,87998350,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}, {'name': 'uaa', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-02-09T20:06:23Z', 'id': 10772822, 'updated_at': '2015-02-09T20:06:23Z'}]",remove create/delete user cli commands,,[],956238,81882,feature,2015-06-29T23:37:03Z,https://www.pivotaltracker.com/story/show/87998350
,2015-05-13T18:42:42Z,unstarted,,"Get call at [1] does not pass in sha1 to be verified. related to https://github.com/cloudfoundry/bosh/commit/742daeb3334791805503758e3ad8a14d7f730453

[1] https://github.com/cloudfoundry/bosh-agent/blob/555a36ad3e5822022da49c9971b18c50c2f8b462/agent/compiler/concrete_compiler.go#L149",,94524232,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",agent should verify source pkg sha1 when trying to compile the package,,[],956238,1550486,bug,2015-05-13T18:43:17Z,https://www.pivotaltracker.com/story/show/94524232
,2015-02-18T00:44:28Z,unstarted,,"currently when db connection is lost, operator has to ssh into the vm and monit restart director (which also restarts workers). goal of this story is for director/workers to properly reconnect when db connection is lost. 

there are 2 possibilities to reconnection:
- try best effort for short period of time (~1min?) and if successful continue with the worker test
- fail if connection cannot be reestablished and have monit keep on restarting director/workers until db connection is successful

see PR for possible solution: https://github.com/cloudfoundry/bosh/pull/748
- lower timeout to 150?
- what does cloud controller do?",2.0,88572280,story,"[{'name': 'blocked on pm', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-07T21:21:46Z', 'id': 9637086, 'updated_at': '2014-10-07T21:21:46Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",bosh director and workers should be able to recover from database connection loss (PR 'reconnect on broken db connection'),,[],956238,81882,feature,2015-06-11T17:59:19Z,https://www.pivotaltracker.com/story/show/88572280
,2015-04-22T22:05:02Z,unstarted,,"@dk said it was ok to no longer support this feature.

should no longer have tests like: /Users/pivotal/workspace/bosh/spec/integration/deprecating_template_keyword_spec.rb

",1.0,93060872,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",Remove support for deprecated 'template' (singular) key,,[],956238,687691,feature,2015-07-13T19:02:15Z,https://www.pivotaltracker.com/story/show/93060872
,2015-02-27T23:49:12Z,unstarted,,"talk more with @dk

notes:
- jobs fingerprint spec file
- pkgs do not fingerprint spec file",,89328254,story,[],user should be able to import packages w/ different names from different releases even if their fingerprints match,,[],956238,81882,bug,2015-05-15T18:35:18Z,https://www.pivotaltracker.com/story/show/89328254
,2015-06-08T21:24:02Z,unstarted,,"Currently the BOSH agent integration tests only run on the Ubuntu-based bosh-lite Vagrant box, which means they only cover the Ubuntu-specific impls of all the agent behaviours.

We should find a way to run these tests on CentOS as well.

Once CentOS is supported, update_settings_test.go will need to be updated to assert things about files certificate in different locations. See cert_manager for the correct CentOS and RHEL file locations.

- add concourse build",,96496218,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent integration tests for CentOS,,[],956238,1541728,feature,2015-07-01T00:26:49Z,https://www.pivotaltracker.com/story/show/96496218
,2015-02-20T00:54:07Z,unstarted,,"BOSH failed on disk partioning; GoCD log output
```
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/agent_client-1.2690.4.0/lib/agent_client/base.rb:21:in `method_missing': {""message""=>""Action Failed get_task: Task e6770df0-f308-4196-6ad0-2a96a5bd06e5 result: Mounting persistent disk: Partitioning disk: Shelling out to sfdisk: Running command: 'sfdisk -uM /dev/sdc', stdout: '', stderr: 'Checking that no-one is using this disk right now ...\nBLKRRPART: Device or resource busy\n\nThis disk is currently in use - repartitioning is probably a bad idea.\nUmount all file systems, and swapoff all swap partitions on this disk.\nUse the --no-reread flag to suppress this check.\nUse the --force flag to overrule all checks.\n': exit status 1""} (Bosh::Agent::HandlerError)
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/agent_client-1.2690.4.0/lib/agent_client/base.rb:12:in `run_task'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:254:in `block in mount_disk'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:253:in `mount_disk'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:313:in `attach_disk'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:349:in `update_persistent_disk'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:137:in `block in create'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:136:in `create'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli_plugin_micro-1.2690.4.0/lib/bosh/cli/commands/micro.rb:179:in `perform'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli-1.2690.4.0/lib/cli/command_handler.rb:57:in `run'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli-1.2690.4.0/lib/cli/runner.rb:56:in `run'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli-1.2690.4.0/lib/cli/runner.rb:16:in `run'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/gems/bosh_cli-1.2690.4.0/bin/bosh:7:in `<top (required)>'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/bin/bosh:23:in `load'
	from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.1.0/bin/bosh:23:in `<main>'
{""type"": ""step_finished"", ""id"": ""microbosh.deploying""}
Exited with 1.
```
We subsequently logged into the BOSH and re-ran the command and it succeed:
```
oot@bm-892b8696-3807-4e8a-a236-f9adca3a6d81:~# sfdisk -uM /dev/sdc
Checking that no-one is using this disk right now ...
OK

Disk /dev/sdc: 2610 cylinders, 255 heads, 63 sectors/track

sfdisk: ERROR: sector 0 does not have an msdos signature
 /dev/sdc: unrecognized partition table type
Old situation:
No partitions found
Input in the following format; absent fields get a default value.
<start> <size> <type [E,S,L,X,hex]> <bootable [-,*]> <c,h,s> <c,h,s>
Usually you only need to specify <start> and <size> (and perhaps <type>).

/dev/sdc1 :
```
This problem appears to be an intermittent race condition: https://github.com/coreos/bugs/issues/152

The problem occurred on vSphere, on the pizza-box environment *hound*.

- confirm centos shows same output
- make sure it works on ubuntu/centos
- use retry strategy: 1sec, 10 times",4.0,88754896,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",BOSH fails on disk partitioning,,[],956238,353433,feature,2015-02-23T18:40:26Z,https://www.pivotaltracker.com/story/show/88754896
,2015-02-19T00:13:12Z,unstarted,,"1. bosh deploy - timed out sending get_task after 45 seconds - VM is unreachable in AWS
2. stop VM through AWS console
3. start VM through AWS console
4. bosh deploy - `Error 450001: Action Failed get_task: Task 42645eb3-85aa-4446-5fe9-0c1f3faeff5b result: Applying: Applying job doppler: Enabling job: failed to enable: symlink /var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad /var/vcap/jobs/doppler: file exists`

Step 4 continues occurring on successive tries.

log from the agent:

```
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00449 [renderedJobApplier] 2015/02/18 23:49:45 DEBUG - Preparing job {doppler 3b52db93b7a71973799b1abf2419ce927a6e94b0 {426cb62e144faef5db203dd693a901ae51952aad 4acab646-2b2a-4aa0-a4c7-179744e4f129 doppler} [{syslog_drain_binder 66fb3363ee3abb60a636acab4333133f7704e3f2.1 {d04c8ef6247d3d5e821f59c9beed715afe1fed6f dc16b4f6-dcd7-46cd-b428-379522bce843 }} {appfirst 2420667158c4c529be1306753bcbc6821621f9c3.1 {1466e371abeede80e276d3787575a4403d1af7ae ca75b687-b937-4546-5919-931d418ab094 }} {common 43595236d1ce5f9a6120198108c226c07ab17012.1 {7ada32130c749162a13bc5994a5aabc90dd060aa 6ee2a4c3-9c45-44ad-a79f-fce3513f3a2a }} {doppler a3fe881848a73e755e14e09f12de933059edf7d1.1 {37f5f46add8eb86125c580873f9c7b9a4d4ae2ca 46f5a5f9-2886-4bd5-a46a-6558b01b594c }} {metron_agent f5edd87fb69f22206f1afbc17fb53f8f120aba28.1 {e7ebea576af06cf0aa90c91f19256ab2bff04c8e 862cea5c-f091-4d59-9bec-faea803af25b }}]}
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00451 [File System] 2015/02/18 23:49:45 DEBUG - Checking if file exists /var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00452 [FileBundle] 2015/02/18 23:49:45 DEBUG - Enabling {/var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad /var/vcap/jobs/doppler {{0 0xc20801a320 0xc20801a370} File System} {0 0xc20801a320 0xc20801a370}}
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00453 [File System] 2015/02/18 23:49:45 DEBUG - Checking if file exists /var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00454 [File System] 2015/02/18 23:49:45 DEBUG - Making dir /var/vcap/jobs with perm 493
/var/vcap/bosh/log/current:2015-02-18_23:49:45.00455 [File System] 2015/02/18 23:49:45 DEBUG - Symlinking oldPath /var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad with newPath /var/vcap/jobs/doppler
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00455 [File System] 2015/02/18 23:49:45 DEBUG - Checking if file exists /var/vcap/jobs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ERROR HERE:
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00456 [Task Service] 2015/02/18 23:49:45 ERROR - Failed processing task #42645eb3-85aa-4446-5fe9-0c1f3faeff5b got: Applying: Applying job doppler: Enabling job: failed to enable: symlink 

/var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad /var/vcap/jobs/doppler: file exists
/var/vcap/bosh/log/current-2015-02-18_23:49:45.00541 [MBus Handler] 2015/02/18 23:49:45 INFO - Received request with action get_task
```

director after failure:

```
root@2812a717-8a8d-4600-8dfd-a288c292996c:/var/vcap/bosh/log# ls -lah /var/vcap/jobs/
total 24K
drwxr-xr-x  2 root root 4.0K Feb 18 19:59 .
drwxr-xr-x 11 root root 4.0K Feb 18 23:53 ..
lrwxrwxrwx  1 root root  110 Feb 18 19:59 appfirst -> /var/vcap/data/jobs/appfirst/067e02f07962a2548f7fb859c3d7cfc7f8631930-6e31e74fec034658a32e956be3825c1c7fe6cf9d
lrwxrwxrwx  1 root root  109 Feb 18 19:59 doppler -> /var/vcap/data/jobs/doppler/3b52db93b7a71973799b1abf2419ce927a6e94b0-6e31e74fec034658a32e956be3825c1c7fe6cf9d
lrwxrwxrwx  1 root root  114 Feb 18 19:59 metron_agent -> /var/vcap/data/jobs/metron_agent/4cf0a43aa50c72ea4dd4538f7289aa97f68de3cd-6e31e74fec034658a32e956be3825c1c7fe6cf9d
lrwxrwxrwx  1 root root  121 Feb 18 19:59 syslog_drain_binder -> /var/vcap/data/jobs/syslog_drain_binder/d1cef58efbc1504e9b6f5b6aeb5a1f387ac82c40-6e31e74fec034658a32e956be3825c1c7fe6cf9d
```

insides of the jobs directory:

```
root@2812a717-8a8d-4600-8dfd-a288c292996c:/var/vcap/bosh/log# ls -lah /var/vcap/data/jobs/**
/var/vcap/data/jobs/appfirst:
drwxr-xr-x 6 root root 4.0K Feb 18 22:54 067e02f07962a2548f7fb859c3d7cfc7f8631930-426cb62e144faef5db203dd693a901ae51952aad

/var/vcap/data/jobs/doppler:
drwxr-xr-x 4 root root 4.0K Feb 18 22:54 3b52db93b7a71973799b1abf2419ce927a6e94b0-426cb62e144faef5db203dd693a901ae51952aad

/var/vcap/data/jobs/metron_agent:
drwxr-xr-x 4 root root 4.0K Feb 18 22:54 4cf0a43aa50c72ea4dd4538f7289aa97f68de3cd-426cb62e144faef5db203dd693a901ae51952aad

/var/vcap/data/jobs/syslog_drain_binder:
drwxr-xr-x 4 root root 4.0K Feb 18 22:54 d1cef58efbc1504e9b6f5b6aeb5a1f387ac82c40-426cb62e144faef5db203dd693a901ae51952aad
```",,88665966,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",agent fails to symlink the job back to what it was,,[],956238,1123776,bug,2015-06-24T22:56:59Z,https://www.pivotaltracker.com/story/show/88665966
,2015-02-18T00:50:29Z,unstarted,,"https://github.com/cloudfoundry/bosh-agent/pull/14
- maria reminds about the integration tests",4.0,88572990,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",PR 'Multi-homed hosts handle default gateway properly',,[],956238,81882,feature,2015-06-24T22:56:42Z,https://www.pivotaltracker.com/story/show/88572990
,2014-11-05T22:55:26Z,unstarted,,this should forcefully runs a deploy and restart jobs even if there were no changes to them.,,82169558,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'req', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-11-05T22:56:07Z', 'id': 9917370, 'updated_at': '2014-11-05T22:56:07Z'}]",user should be able to restart all jobs in the cluster by running `bosh deploy --restart`,,[],956238,81882,feature,2015-06-24T22:56:29Z,https://www.pivotaltracker.com/story/show/82169558
,2015-04-29T21:56:34Z,unstarted,,,,93579190,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",add a deprecation warning when cloud-config is not used,,[],956238,81882,feature,2015-05-06T16:25:53Z,https://www.pivotaltracker.com/story/show/93579190
,2014-08-12T17:52:35Z,unstarted,,"use get_disks CPI method.

following options should be presented:
- Reattach disk


- call umount agent - old disk
- call attach_disk cpi
- call mount agent
- (...start jobs...??)
",4.0,76811168,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'pd1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-01T21:56:50Z', 'id': 9098728, 'updated_at': '2014-08-01T21:56:50Z'}, {'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",bosh cck should provide options to fix disk attachment if disk got somehow detached from the vm,,[],956238,81882,feature,2014-10-28T18:47:39Z,https://www.pivotaltracker.com/story/show/76811168
,2014-09-03T17:37:13Z,unstarted,,"- it should retry and return an error after 5 times (waiting 10 secs in between) if it cannot send the msg after which agent should restart 
  - (new method PublishWithMaxRetries(..., 5) for yagnats?)",2.0,78134068,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should retry sending back nats response from ActionDispatcher,,[],956238,81882,feature,2014-12-25T01:13:26Z,https://www.pivotaltracker.com/story/show/78134068
,2014-06-16T13:13:15Z,unstarted,,"Seen on BOSH Lite with stemcells 58 and 60 (go_agent)

Steps to reproduce:

* Create a job with a drain script that logs the output of `env` somewhere
* Run `bosh restart [job_name]` on that job, you'll see that `BOSH_JOB_NEXT_STATE` is set in the drain script environment
* Run `bosh recreate [job_name]` on the same job. You'll probably want to `tail -f` the logfile so that you can see it live before the VM goes away.

Expected result:

* `BOSH_JOB_NEXT_STATE` is set with the relevant values for the VM hosting the job

Actual result:

* `BOSH_JOB_NEXT_STATE` is not set :(",,73299316,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'drain', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623538, 'updated_at': '2014-06-06T18:53:38Z'}, {'name': 'drain1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-05T08:09:20Z', 'id': 9116420, 'updated_at': '2014-08-05T08:09:20Z'}]",BOSH_JOB_NEXT_STATE in drain not populated when recreating job,,[],956238,1139720,bug,2014-08-05T08:09:22Z,https://www.pivotaltracker.com/story/show/73299316
,2014-04-11T22:05:54Z,unstarted,,"BLOCKED: waiting for new bosh-micro cli to be released and recommended

The reasons for pulling it are:
- we strongly discourage release developers from any reliance on any binaries that ship with stemcells.
- ruby binary takes a long time to compile and consumes a lot of resources to compile.  performance gains in removing it
",,69380568,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",remove old bosh agent ruby from the stemcell,,[],956238,1210852,feature,2015-09-25T23:52:59Z,https://www.pivotaltracker.com/story/show/69380568
,2014-09-09T22:54:24Z,unstarted,,"We got a usage error from create release:

```
+ bosh --non-interactive create release --name push-console-release-dev --force --with-tarball
+ tee create_release_output.txt
Syncing blobs...

Building DEV release
--------------------

Building packages
-----------------
Building cf...
  Final version:   FOUND LOCAL

Building console...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Pre-packaging...
Usage: create release [<manifest_file>] [--force] [--final] [--with-tarball] [--dry-run] [--name NAME] [--version VERSION]
```

When we added a rescue into the pre_packaging steps, we saw that the output from the pre_packaging script could not be split on newlines:

```
+ bosh --non-interactive create release --name push-console-release-dev --force --with-tarball
+ tee create_release_output.txt
Syncing blobs...

Building DEV release
--------------------

Building packages
-----------------
Building cf...
  Final version:   FOUND LOCAL

Building console...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Pre-packaging...
invalid byte sequence in US-ASCII: /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:199:in `split'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:199:in `block in pre_package'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:314:in `block in in_build_dir'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:314:in `chdir'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:314:in `in_build_dir'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:194:in `pre_package'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/package_builder.rb:172:in `copy_files'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/packaging_helper.rb:126:in `generate_tarball'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/packaging_helper.rb:46:in `block in build'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/core_ext.rb:14:in `with_indent'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/packaging_helper.rb:45:in `build'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/commands/release/create_release.rb:146:in `block in build_packages'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/commands/release/create_release.rb:144:in `each'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/commands/release/create_release.rb:144:in `build_packages'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/commands/release/create_release.rb:88:in `create_from_spec'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/commands/release/create_release.rb:35:in `create'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/command_handler.rb:57:in `run'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/runner.rb:56:in `run'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/lib/cli/runner.rb:16:in `run'
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2710.0/bin/bosh:7:in `<top (required)>'
/usr/local/bin/bosh:23:in `load'
/usr/local/bin/bosh:23:in `<main>'
```",,78509228,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]","invalid byte sequence in pre_packaging error output causes ""usage"" message",,[],956238,687691,bug,2015-07-31T01:15:50Z,https://www.pivotaltracker.com/story/show/78509228
,2014-06-23T19:57:31Z,unstarted,,"If compilation fails the director will store compilation logs in the blobstore (see #72740894). The CLI should make those available so that they don't need to be printed out on screen if they are too large.

* add a director API call to download logs from blobstore
* need to augment CLI  -  maybe syntax = bosh logs --compilation --package ruby --dir /tmp/",2.0,73744892,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'logs1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T19:10:56Z', 'id': 8097590, 'updated_at': '2014-08-05T08:10:47Z'}]",Add `fetch logs` command to CLI,,[],956238,1210852,feature,2015-06-24T22:36:07Z,https://www.pivotaltracker.com/story/show/73744892
,2014-06-23T19:34:05Z,unstarted,,"Agent should be saving logs to a file (#73742560), director needs to fetch those log files on failure, then CLI needs to fetch them from the director on failure.

- director sees that compilation failed
- director issues fetch_logs call to the agent
- cli sees that failed event log task includes log blob
- cli provides the command with the blobstore_id to the user (so they can fetch it manually)",2.0,73742758,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'logs1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T19:10:56Z', 'id': 8097590, 'updated_at': '2014-08-05T08:10:47Z'}]",bosh-cli should show command for getting full logs when packaging fails,,[],956238,1210852,feature,2015-06-24T22:36:17Z,https://www.pivotaltracker.com/story/show/73742758
,2014-06-30T18:57:06Z,unstarted,,,,74164236,story,"[{'name': 'logs1', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T19:10:56Z', 'id': 8097590, 'updated_at': '2014-08-05T08:10:47Z'}]",investigate how to consolidate compilation/drain/errand log behavior,,[],956238,1210852,chore,2014-08-05T08:10:47Z,https://www.pivotaltracker.com/story/show/74164236
,2014-07-11T20:53:00Z,unstarted,,,1.0,74884814,story,"[{'name': 'idempotent-upload', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-30T01:49:44Z', 'id': 9070428, 'updated_at': '2014-07-30T01:49:44Z'}]",make --skip-if-exists flag default for upload stemcell and upload release commands,,[],956238,81882,feature,2015-04-09T18:37:37Z,https://www.pivotaltracker.com/story/show/74884814
,2014-06-23T17:04:03Z,unstarted,,"when task goes to 'timeout' state, no need to continue tracking it, but CLI is tracking it.",,73729466,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",cli should not keep tracking task in timeout state,,[],956238,553935,bug,2015-06-24T22:36:27Z,https://www.pivotaltracker.com/story/show/73729466
,2014-06-18T16:33:33Z,unstarted,,bosh-director should say that stemcell was already present in the event log. bosh-cli should exit with 0.,1.0,73481446,story,"[{'name': 'idempotent-upload', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-30T01:49:44Z', 'id': 9070428, 'updated_at': '2014-07-30T01:49:44Z'}]",bosh upload stemcell <remote_url> should exit with 0 if director already has that stemcell,,[],956238,381857,feature,2014-08-11T18:25:30Z,https://www.pivotaltracker.com/story/show/73481446
,2014-03-25T22:19:01Z,unstarted,,"`bosh generate job` currently creates a spec file with nil `templates` and `packages` keys:

```
---
name: test
templates:

packages:
```

These cause an error when trying to upload a release. The skeleton spec file should have empty arrays:

```
---
name: test

templates: {}

packages: []
```",1.0,68236324,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",update `bosh generate job [name]` to generate up-to-date skeleton spec file,,[],956238,5637,feature,2015-06-24T22:48:25Z,https://www.pivotaltracker.com/story/show/68236324
,2014-03-04T07:56:38Z,unstarted,,"should be mounted with 'noexec,nosuid' ",2.0,66839276,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",bosh agent should secure /tmp and /var/vcap/data/tmp directories,,[],956238,1015645,feature,2015-05-19T00:23:52Z,https://www.pivotaltracker.com/story/show/66839276
,2013-12-26T23:29:26Z,unstarted,,"The implementation for #62807592 appears to garbage collect more recent tasks rather than oldest tasks after max_tasks number is reached.  This seems wrong, and that it should garbage collect the oldest tasks first.",1.0,62996422,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Bosh should garbage collect oldest tasks first when it is over its max task number,,[],956238,1068489,feature,2015-07-31T01:13:39Z,https://www.pivotaltracker.com/story/show/62996422
,2014-03-20T18:07:46Z,unstarted,,"certain VMs should not be resurrectable.  eg-hadoop master nodes.  This would be for a long-running service & for stateful vm.

question - what to do when master node has problem:
-  vm should triggers out an alert to hadoop admin team.
- or could failover to 2ndary master node & then kill/resurrect

The ability to disable/enable resurrector on a per VM basis exists today.  However, we need the ability to specify that a VM is created and *always* lives in a non-resurrectable state. Thus the functionality needs to be in the director rather than CLI and it should be configurable in the manifest for a specific job",,67942286,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'hadoop', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-20T18:07:46Z', 'id': 8004562, 'updated_at': '2014-03-20T18:07:46Z'}]",user should be able specify job resurrection state in the manifest,,[],956238,1210852,feature,2015-09-07T22:32:05Z,https://www.pivotaltracker.com/story/show/67942286
,2014-08-08T18:11:31Z,unstarted,,,1.0,76619400,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",bosh cli should try using DEL /tasks/id instead of /task/id,,[],956238,81882,feature,2015-09-07T22:31:52Z,https://www.pivotaltracker.com/story/show/76619400
,2015-06-18T17:01:02Z,unstarted,,https://github.com/cloudfoundry/bosh/commit/ad4f885,,97314790,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",do not default client_id and client_secret in the templates once micro cli is deprecated,,[],956238,81882,feature,2015-06-23T16:06:52Z,https://www.pivotaltracker.com/story/show/97314790
,2015-07-13T19:42:39Z,unstarted,,"Setting the screensaver mode to blank-only conceals the contents of the display from passersby.
---
None
---
SV-50440r3_rule
---
F-43588r2_fix
---
Run the following command to set the screensaver mode in the GNOME desktop to a blank screen: 

# gconftool-2 \
--direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type string \
--set /apps/gnome-screensaver/mode blank-only
---
C-46199r4_chk
---
If the GConf2 package is not installed, this is not applicable. 

To ensure the screensaver is configured to be blank, run the following command: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gnome-screensaver/mode

If properly configured, the output should be ""blank-only"". 
If it is not, this is a finding.",1.0,98978068,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38639] [low] The system must display a publicly-viewable pattern during a graphical desktop environment session lock.,,[],956238,81882,feature,2015-10-29T21:02:06Z,https://www.pivotaltracker.com/story/show/98978068
,2015-12-29T19:49:04Z,unscheduled,,"See this page: https://docs.cloudfoundry.org/deploying/aws/

Then click ""BOSH on AWS"" link, which points to `https://http//bosh.io/docs/init-aws.html`",,110815246,story,[],Bad link to docs on docs.cloudfoundry.org,,[],956238,1814454,bug,2015-12-29T19:49:04Z,https://www.pivotaltracker.com/story/show/110815246
,2016-01-05T18:20:09Z,unscheduled,,,,111079664,story,"[{'name': 'diff', 'project_id': 956238, 'kind': 'label', 'created_at': '2016-01-05T18:21:13Z', 'id': 13692916, 'updated_at': '2016-01-05T18:21:13Z'}]","Director should deploy manifest with stemcell, release and cloud config versions shown in diff",,[],956238,553935,feature,2016-01-05T18:38:26Z,https://www.pivotaltracker.com/story/show/111079664
,2015-12-29T02:33:31Z,unscheduled,,"[Github Pull Request 12](https://github.com/cloudfoundry/bosh-acceptance-tests/pull/12):

>[#107648464](https://www.pivotaltracker.com/story/show/107648464)

Filed by [guoger](http://github.com/guoger)",1.0,110783898,story,"[{'name': 'new-activity', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-09T17:32:55Z', 'id': 8907244, 'updated_at': '2014-07-09T17:32:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",cloudfoundry/bosh-acceptance-tests #12: BAT test for story: user can restore director db from a backup.,,[],956238,1134058,feature,2015-12-29T02:33:31Z,https://www.pivotaltracker.com/story/show/110783898
,2015-12-21T22:35:55Z,unscheduled,,"In one of the deploys in our pipeline, pre-start scripts failed:

```
Failed updating job cflinuxfs2-1.21.0 > cflinuxfs2-1.21.0/0 (canary): Action Failed get_task: Task 8f181500-5c3c-48b1-4d61-bff596989778 result: 1 of 1 pre-start scripts failed. Failed Jobs: rootfs-target. (00:13:47)
```

The pipeline correctly went red. The next deploy was green, even though the instance where the pre-start script failed was in a broken state. The instance was not fully set up. BOSH reported the job as running even though no monit jobs were configured.

We would expect BOSH to:

1) know that one of its instances is broken
2) keep trying to converge into the desired state",,110533846,story,[],As a BOSH user I would like to know when pre-start scripts have failed.,,[],956238,756869,feature,2015-12-21T22:35:55Z,https://www.pivotaltracker.com/story/show/110533846
,2015-12-17T02:59:46Z,unscheduled,,even if agent is accessible we should just delete the vm,,110301418,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",cck should have delete vm option before delete vm reference,,[],956238,81882,feature,2015-12-17T03:00:23Z,https://www.pivotaltracker.com/story/show/110301418
,2015-12-17T03:00:09Z,unscheduled,,,,110301422,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",bosh recreate should allow forceful recreate even if it cannot talk to the agent,,[],956238,81882,feature,2015-12-17T03:00:21Z,https://www.pivotaltracker.com/story/show/110301422
,2015-12-15T21:40:38Z,unscheduled,,"BOSH has 181 branches, and I think a lot of them can be cleaned up...

These PR branches have been merged and should be very safe to close...

 * [wip-agent-infrastructure](https://github.com/cloudfoundry/bosh/tree/wip-agent-infrastructure) in [#747](https://github.com/cloudfoundry/bosh/pull/747)
 * [max_upload_size_10gb](https://github.com/cloudfoundry/bosh/tree/max_upload_size_10gb) in [#777](https://github.com/cloudfoundry/bosh/pull/777)
 * [cck_auto](https://github.com/cloudfoundry/bosh/tree/cck_auto) in [#789](https://github.com/cloudfoundry/bosh/pull/789)
 * [wip-107783878-v2-aws-sdk-blobstore-client](https://github.com/cloudfoundry/bosh/tree/wip-107783878-v2-aws-sdk-blobstore-client) in [#1038](https://github.com/cloudfoundry/bosh/pull/1038)
 * [s3cli-bump-0.0.13](https://github.com/cloudfoundry/bosh/tree/s3cli-bump-0.0.13) in [#1033](https://github.com/cloudfoundry/bosh/pull/1033)
 * [s3cli-bump](https://github.com/cloudfoundry/bosh/tree/s3cli-bump) in [#1032](https://github.com/cloudfoundry/bosh/pull/1032)
 * [update-s3cli](https://github.com/cloudfoundry/bosh/tree/update-s3cli) in [#1022](https://github.com/cloudfoundry/bosh/pull/1022)


This PR branch is still open after 2 years...

 * [warden-cpi-resource-limits](https://github.com/cloudfoundry/bosh/tree/warden-cpi-resource-limits) in [#759](https://github.com/cloudfoundry/bosh/pull/579)


These branches have no unmerged commits and should be very safe to delete...

 * [version_refac](https://github.com/cloudfoundry/bosh/tree/version_refac) - updated a year ago by Jenkins User
 * [fog](https://github.com/cloudfoundry/bosh/tree/fog) - updated a year ago by Andrea Law
 * [2719.1](https://github.com/cloudfoundry/bosh/tree/2719.1) - updated a year ago by krishicks
 * [2690.1](https://github.com/cloudfoundry/bosh/tree/2690.1) - updated a year ago by krishicks
 * [2690.2](https://github.com/cloudfoundry/bosh/tree/2690.2) - updated a year ago by krishicks
 * [2690.3](https://github.com/cloudfoundry/bosh/tree/2690.3) - updated a year ago by krishicks
 * [2719.2](https://github.com/cloudfoundry/bosh/tree/2719.2) - updated a year ago by mariash
 * [2690.4](https://github.com/cloudfoundry/bosh/tree/2690.4) - updated a year ago by mariash
 * [no_diff](https://github.com/cloudfoundry/bosh/tree/no_diff) - updated a year ago by krumts
 * [johnmcteague-feature/openstack_storage_az](https://github.com/cloudfoundry/bosh/tree/johnmcteague-feature/openstack_storage_az) - updated 11 months ago by Tyler Schultz
 * [fix-vsphere-cpi-lifecycle](https://github.com/cloudfoundry/bosh/tree/fix-vsphere-cpi-lifecycle) - updated 9 months ago by Kihyeon Kim
 * [wip-uaa](https://github.com/cloudfoundry/bosh/tree/wip-uaa) - updated 9 months ago by mariash
 * [concourse-integration-testing](https://github.com/cloudfoundry/bosh/tree/concourse-integration-testing) - updated 8 months ago by aaronshurley
 * [fix_3026](https://github.com/cloudfoundry/bosh/tree/fix_3026) - updated 4 months ago by mariash
 * [hofix-101510068](https://github.com/cloudfoundry/bosh/tree/hofix-101510068) - updated 4 months ago by mariash


These branches haven’t been touched in more than a year...

 * [metrics_migration](https://github.com/cloudfoundry/bosh/tree/metrics_migration) - updated 2 years ago by Casey McTaggart
 * [wip-s3-blobstore](https://github.com/cloudfoundry/bosh/tree/wip-s3-blobstore) - updated 2 years ago by dam5s
 * [travis_ruby_484](https://github.com/cloudfoundry/bosh/tree/travis_ruby_484) - updated 2 years ago by calebamiles
 * [dk-mjs-vlease](https://github.com/cloudfoundry/bosh/tree/dk-mjs-vlease) - updated 2 years ago by mjstallard
 * [openstack_centos](https://github.com/cloudfoundry/bosh/tree/openstack_centos) - updated 2 years ago by mjstallard
 * [wip-drain](https://github.com/cloudfoundry/bosh/tree/wip-drain) - updated 2 years ago by dam5s
 * [drain_fixes](https://github.com/cloudfoundry/bosh/tree/drain_fixes) - updated 2 years ago by mjstallard
 * [wip_director_attributes](https://github.com/cloudfoundry/bosh/tree/wip_director_attributes) - updated 2 years ago by monkeyherder
 * [wip_mb_yt_vsphere_cpi](https://github.com/cloudfoundry/bosh/tree/wip_mb_yt_vsphere_cpi) - updated 2 years ago by ytolsk
 * [travis_mysql_test](https://github.com/cloudfoundry/bosh/tree/travis_mysql_test) - updated 2 years ago by mjstallard
 * [wip-jz-mb-vsphere-cpi](https://github.com/cloudfoundry/bosh/tree/wip-jz-mb-vsphere-cpi) - updated 2 years ago by mmb
 * [wip-go-agent-cdrom-62330260](https://github.com/cloudfoundry/bosh/tree/wip-go-agent-cdrom-62330260) - updated 2 years ago by Jeffrey Peckham
 * [remove_mirror](https://github.com/cloudfoundry/bosh/tree/remove_mirror) - updated 2 years ago by maximilien
 * [wip-adam-jz-agent-vsphere-disk](https://github.com/cloudfoundry/bosh/tree/wip-adam-jz-agent-vsphere-disk) - updated 2 years ago by Adam Stegman
 * [wip-jz-mb-go-agent](https://github.com/cloudfoundry/bosh/tree/wip-jz-mb-go-agent) - updated 2 years ago by mmb
 * [jmp_dk_run_errand](https://github.com/cloudfoundry/bosh/tree/jmp_dk_run_errand) - updated 2 years ago by johannespetzold
 * [dk_external_cpi](https://github.com/cloudfoundry/bosh/tree/dk_external_cpi) - updated 2 years ago by dmitriy kalinin
 * [ci-workspace](https://github.com/cloudfoundry/bosh/tree/ci-workspace) - updated 2 years ago by mmb
 * [test_travis](https://github.com/cloudfoundry/bosh/tree/test_travis) - updated 2 years ago by monkeyherder
 * [rdr_help_message](https://github.com/cloudfoundry/bosh/tree/rdr_help_message) - updated 2 years ago by KartikTalwar
 * [wip-fingerprint-versions](https://github.com/cloudfoundry/bosh/tree/wip-fingerprint-versions) - updated 2 years ago by Jenkins User
 * [WIP-travis-networking](https://github.com/cloudfoundry/bosh/tree/WIP-travis-networking) - updated 2 years ago by monkeyherder
 * [wip-new-version-same-fingerprint#](https://github.com/cloudfoundry/bosh/tree/wip-new-version-same-fingerprint#) - updated 2 years ago by krishicks
 * [datastore-clusters](https://github.com/cloudfoundry/bosh/tree/datastore-clusters) - updated 2 years ago by mariash
 * [resque-bump](https://github.com/cloudfoundry/bosh/tree/resque-bump) - updated 2 years ago by mariash
 * [spot-request-lifecycle-spec](https://github.com/cloudfoundry/bosh/tree/spot-request-lifecycle-spec) - updated 2 years ago by jfoley
 * [ruby2-release](https://github.com/cloudfoundry/bosh/tree/ruby2-release) - updated a year ago by mariash
 * [travis-failure](https://github.com/cloudfoundry/bosh/tree/travis-failure) - updated a year ago by Andrea Law
 * [travis_debug](https://github.com/cloudfoundry/bosh/tree/travis_debug) - updated a year ago by karlkfi
 * [director-routing-changes](https://github.com/cloudfoundry/bosh/tree/director-routing-changes) - updated a year ago by Phan Le
 * [nats-logging](https://github.com/cloudfoundry/bosh/tree/nats-logging) - updated a year ago by xingzhou
 * [disk-pools-wip](https://github.com/cloudfoundry/bosh/tree/disk-pools-wip) - updated a year ago by karlkfi
 * [disk-cloud-properties](https://github.com/cloudfoundry/bosh/tree/disk-cloud-properties) - updated a year ago by karlkfi
 * [root-partition](https://github.com/cloudfoundry/bosh/tree/root-partition) - updated a year ago by Adam Stegman
 * [ephemeral-partition](https://github.com/cloudfoundry/bosh/tree/ephemeral-partition) - updated a year ago by Adam Stegman
 * [ruby2-health-monitor](https://github.com/cloudfoundry/bosh/tree/ruby2-health-monitor) - updated a year ago by zaksoup
 * [lock-down-os-image-version](https://github.com/cloudfoundry/bosh/tree/lock-down-os-image-version) - updated a year ago by krishicks
 * [test-updated-bosh-agent](https://github.com/cloudfoundry/bosh/tree/test-updated-bosh-agent) - updated a year ago by krishicks
 * [remove-drs-cleaner](https://github.com/cloudfoundry/bosh/tree/remove-drs-cleaner) - updated a year ago by mariash
 * [disk-pool-size](https://github.com/cloudfoundry/bosh/tree/disk-pool-size) - updated a year ago by Andrei Dinin
 * [centos-stemcell](https://github.com/cloudfoundry/bosh/tree/centos-stemcell) - updated a year ago by krishicks
 * [2719.3](https://github.com/cloudfoundry/bosh/tree/2719.3) - updated a year ago by mariash
 * [test-nebula](https://github.com/cloudfoundry/bosh/tree/test-nebula) - updated a year ago by mariash
 * [as-huawei-wip](https://github.com/cloudfoundry/bosh/tree/as-huawei-wip) - updated a year ago by zaksoup
 * [as-nats-em-wip](https://github.com/cloudfoundry/bosh/tree/as-nats-em-wip) - updated a year ago by zaksoup
 * [artifact_promotion_tests](https://github.com/cloudfoundry/bosh/tree/artifact_promotion_tests) - updated a year ago by zaksoup
 * [wip_logging](https://github.com/cloudfoundry/bosh/tree/wip_logging) - updated a year ago by karlkfi
 * [vsan](https://github.com/cloudfoundry/bosh/tree/vsan) - updated a year ago by Phan Le
 * [turn_on_kernel_logging_trusty](https://github.com/cloudfoundry/bosh/tree/turn_on_kernel_logging_trusty) - updated a year ago by krishicks
 * [promotion_spec_work](https://github.com/cloudfoundry/bosh/tree/promotion_spec_work) - updated a year ago by Phan Le
 * [email_spec_flake](https://github.com/cloudfoundry/bosh/tree/email_spec_flake) - updated a year ago by st3v
 * [vcloud_bats](https://github.com/cloudfoundry/bosh/tree/vcloud_bats) - updated a year ago by zhang-hua
 * [vcloud_bats_final](https://github.com/cloudfoundry/bosh/tree/vcloud_bats_final) - updated a year ago by zhang-hua
 * [abylaw-resolve-localhost](https://github.com/cloudfoundry/bosh/tree/abylaw-resolve-localhost) - updated a year ago by zaksoup


These branches are referenced by stories, but I have difficulty groking whether they’re closed; I suspect most of these can be deleted...

 * [WIP_62152150](https://github.com/cloudfoundry/bosh/tree/WIP_62152150) - updated 2 years ago by ytolsk in [#62152150](https://www.pivotaltracker.com/story/show/62152150)
 * [WIP-62152150](https://github.com/cloudfoundry/bosh/tree/WIP-62152150) - updated 2 years ago by mjstallard in [#62152150](https://www.pivotaltracker.com/story/show/62152150)
 * [WIP-61831052](https://github.com/cloudfoundry/bosh/tree/WIP-61831052) - updated 2 years ago by calebamiles in [#61831052](https://www.pivotaltracker.com/story/show/61831052)
 * [WIP-60722772](https://github.com/cloudfoundry/bosh/tree/WIP-60722772) - updated 2 years ago by mjstallard in [#60722772](https://www.pivotaltracker.com/story/show/60722772)
 * [WIP-62749248](https://github.com/cloudfoundry/bosh/tree/WIP-62749248) - updated 2 years ago by calebamiles in [#62749248](https://www.pivotaltracker.com/story/show/62749248)
 * [wip-max-mmb-vsphere-cpi](https://github.com/cloudfoundry/bosh/tree/wip-max-mmb-vsphere-cpi) - updated 2 years ago by maximilien in [#62927424](https://www.pivotaltracker.com/story/show/62927424)
 * [warden-cpi](https://github.com/cloudfoundry/bosh/tree/warden-cpi) - updated 2 years ago by Kaixiang in [#69554908](https://www.pivotaltracker.com/story/show/69554908)
 * [openstack_config_drive](https://github.com/cloudfoundry/bosh/tree/openstack_config_drive) - updated 2 years ago by monkeyherder in [#68141956](https://www.pivotaltracker.com/story/show/68141956)
 * [wip-ki-release-versioning](https://github.com/cloudfoundry/bosh/tree/wip-ki-release-versioning) - updated a year ago by karlkfi in [#72987828](https://www.pivotaltracker.com/story/show/72987828)
 * [version_refactor](https://github.com/cloudfoundry/bosh/tree/version_refactor) - updated a year ago by karlkfi in [#75451362](https://www.pivotaltracker.com/story/show/75451362)
 * [stemcell-building-vm-bump](https://github.com/cloudfoundry/bosh/tree/stemcell-building-vm-bump) - updated a year ago by Andrea Law in [#74994436](https://www.pivotaltracker.com/story/show/74994436)
 * [agent-extraction](https://github.com/cloudfoundry/bosh/tree/agent-extraction) - updated a year ago by xingzhou in [#72296090](https://www.pivotaltracker.com/story/show/72296090)
 * [wip-72120872](https://github.com/cloudfoundry/bosh/tree/wip-72120872) - updated a year ago by krishicks in [#72120872](https://www.pivotaltracker.com/story/show/72120872)
 * [travis-killed-75854450](https://github.com/cloudfoundry/bosh/tree/travis-killed-75854450) - updated a year ago by maximilien in [#75854450](https://www.pivotaltracker.com/story/show/75854450)
 * [travis_integration_test](https://github.com/cloudfoundry/bosh/tree/travis_integration_test) - updated a year ago by karlkfi in [#76564098](https://www.pivotaltracker.com/story/show/76564098)
 * [2682.2](https://github.com/cloudfoundry/bosh/tree/2682.2) - updated a year ago by mariash in [#77894442](https://www.pivotaltracker.com/story/show/77894442)
 * [HuaweiTech-hw-issue-31](https://github.com/cloudfoundry/bosh/tree/HuaweiTech-hw-issue-31) - updated a year ago by zaksoup in [#74744712](https://www.pivotaltracker.com/story/show/74744712)
 * [53110543-resizing-persistent-disk](https://github.com/cloudfoundry/bosh/tree/53110543-resizing-persistent-disk) - updated a year ago by krishicks in [#53110543](https://www.pivotaltracker.com/story/show/53110543)
 * [HuaweiTech-hw-issue-31-rebase](https://github.com/cloudfoundry/bosh/tree/HuaweiTech-hw-issue-31-rebase) - updated a year ago by karlkfi in [#74744712](https://www.pivotaltracker.com/story/show/74744712)
 * [voelzmo-add_no_proxy_for_download](https://github.com/cloudfoundry/bosh/tree/voelzmo-add_no_proxy_for_download) - updated a year ago by Andrea Law in [#79805910](https://www.pivotaltracker.com/story/show/79805910)
 * [2690.5](https://github.com/cloudfoundry/bosh/tree/2690.5) - updated a year ago by dmitriy kalinin in [#80567156](https://www.pivotaltracker.com/story/show/80567156)
 * [2690.6](https://github.com/cloudfoundry/bosh/tree/2690.6) - updated a year ago by karlkfi in [#80843398](https://www.pivotaltracker.com/story/show/80843398)
 * [wip_aws_lifecycle](https://github.com/cloudfoundry/bosh/tree/wip_aws_lifecycle) - updated a year ago by karlkfi in [#79810782](https://www.pivotaltracker.com/story/show/79810782)
 * [delete_vm_refactor](https://github.com/cloudfoundry/bosh/tree/delete_vm_refactor) - updated a year ago by krishicks in [#80340124](https://www.pivotaltracker.com/story/show/80340124)
 * [pivotalservices](https://github.com/cloudfoundry/bosh/tree/pivotalservices) - updated a year ago by krishicks in [#82678498](https://www.pivotaltracker.com/story/show/82678498)
",,110197458,story,[],Cleanup repo branches,,[],956238,1868036,chore,2015-12-15T21:40:38Z,https://www.pivotaltracker.com/story/show/110197458
,2015-12-11T23:04:10Z,unscheduled,,"https://main.bosh-ci.cf-app.com/pipelines/bosh-agent/jobs/test-unit/builds/96

`bin/env: line 18: exec: golint: not found`

should probably fail the build if this happens as well",,110003618,story,[],Agent should run golint in CI,,[],956238,1615052,chore,2015-12-11T23:04:42Z,https://www.pivotaltracker.com/story/show/110003618
,2015-12-10T16:44:13Z,unscheduled,,"cidisplay15 loses Checkman's status (Chrome displays ""Lost connection"") after 48 hours or so. I notice there are an excessive number of sockets in `CLOSE_WAIT` (489 according to `netstat -an | grep 1234 | grep CLOSE_WAIT | wc -l`). This *may* to be caused by the Checkman server not closing the connection after receiving a FIN from the Chrome client (reason: the foreign IP address is also 127.0.0.1, and Chrome is the only app I know pinging Checkman. But also it could be something weird that `ifg` is doing.

```
netstat -an | grep 1234
...
tcp4       0      0  127.0.0.1.1234         127.0.0.1.51132        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.51131        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.63613        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.63612        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.63611        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.63610        CLOSE_WAIT
tcp4       0      0  127.0.0.1.1234         127.0.0.1.63609        CLOSE_WAIT
tcp4       0      0  *.1234                 *.*                    LISTEN
```",,109897640,story,[],cidisplay15's Chrome Browser sometimes loses Checkman status,,[],956238,353433,chore,2015-12-10T16:44:14Z,https://www.pivotaltracker.com/story/show/109897640
,2015-11-20T17:50:05Z,unscheduled,,Similar to the way the PV stemcells are published in both light and non-light,,108636956,story,[],As a BOSH user I would like to use a non-light AWS HVM stemcell,,[],956238,756869,feature,2015-11-20T17:50:05Z,https://www.pivotaltracker.com/story/show/108636956
,2015-11-18T16:45:27Z,unscheduled,,,,108455134,story,[],As a BOSH user I would like to be able to set the MTU for network interfaces in my deployment,,[],956238,756869,feature,2015-11-18T16:45:27Z,https://www.pivotaltracker.com/story/show/108455134
,2015-11-13T18:35:46Z,unscheduled,,,,108110442,story,[],BOSH sandbox logging should be augmented to provide greater insight,,[],956238,1550486,chore,2015-11-13T18:35:46Z,https://www.pivotaltracker.com/story/show/108110442
,2015-11-12T18:53:15Z,unscheduled,,"Should not receive the following DB error.

```
  Started updating job vh7bg5SiKgVIXyAlvLIpAOs-RV42314BpLmP39snskdXtEK-zvj1xOri6NRdE9CQKPcFIlOQ2NMjh80F5bDjRPtdY_ycZKhyJn_bNB9h6Y1_WkP-gObd5x2_QnVIwDoBLK31o714YGpyJS8apSAR8ih8CXpX6II3-k6oh_q-aoohO4kHV7-gPuVWp2yxA1sDWa0Vkia1KVjUOh63uWdpaoSdVnCtL9RXXg0ki4KI1DSeh6imV9TCipD9jeTLVhwG
  Started updating job vh7bg5SiKgVIXyAlvLIpAOs-RV42314BpLmP39snskdXtEK-zvj1xOri6NRdE9CQKPcFIlOQ2NMjh80F5bDjRPtdY_ycZKhyJn_bNB9h6Y1_WkP-gObd5x2_QnVIwDoBLK31o714YGpyJS8apSAR8ih8CXpX6II3-k6oh_q-aoohO4kHV7-gPuVWp2yxA1sDWa0Vkia1KVjUOh63uWdpaoSdVnCtL9RXXg0ki4KI1DSeh6imV9TCipD9jeTLVhwG > vh7bg5SiKgVIXyAlvLIpAOs-RV42314BpLmP39snskdXtEK-zvj1xOri6NRdE9CQKPcFIlOQ2NMjh80F5bDjRPtdY_ycZKhyJn_bNB9h6Y1_WkP-gObd5x2_QnVIwDoBLK31o714YGpyJS8apSAR8ih8CXpX6II3-k6oh_q-aoohO4kHV7-gPuVWp2yxA1sDWa0Vkia1KVjUOh63uWdpaoSdVnCtL9RXXg0ki4KI1DSeh6imV9TCipD9jeTLVhwG/0 (canary). Failed: PG::Error: ERROR:  value too long for type character varying(255)
 (00:00:00)

Error 100: PG::Error: ERROR:  value too long for type character varying(255)


Task 3 error

For a more detailed error report, run: bosh task 3 --debug
', stderr: 'Acting as user 'admin' on deployment 'foo-deployment' on 'Test Director'
': exit status 1
```",,108022100,story,"[{'name': 'fuzz-result', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-11-12T18:53:15Z', 'id': 13308934, 'updated_at': '2015-11-12T18:53:15Z'}]",bosh should disallow jobs with names that result in domain name longer than 253 chars,,[],956238,553935,feature,2015-11-12T19:19:15Z,https://www.pivotaltracker.com/story/show/108022100
,2015-11-12T01:25:20Z,unscheduled,,  Started applying problem resolutions > missing_vm 213: Recreate VM. Failed: Action Failed get_task: Task 260ce04b-b815-4e3d-4598-a3dc43f9e83a result: Applying: Applying job service: Preparing job: Correcting file permissions: lstat /var/vcap/data/tmp/bosh-agent-applier-jobs-RenderedJobApplier-Apply011588039/service: no such file or directory (00:03:11),,107964544,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent fails to apply rendered templates,,[],956238,81882,bug,2015-11-12T01:25:26Z,https://www.pivotaltracker.com/story/show/107964544
,2015-10-20T22:03:54Z,unscheduled,,"**bosh task event log...**

```
I, [2015-10-20T14:46:30.884637 #6576]  INFO : Running ... bosh -n cck --auto
I, [2015-10-20T14:46:46.907242 #6576]  INFO : Exit code is 0
I, [2015-10-20T14:46:46.907368 #6576]  INFO : Command took 16.022534 seconds
output -- Acting as user 'test' on deployment 'first' on 'Test Director'
Performing cloud check...

Director task 6
  Started scanning 1 vms
  Started scanning 1 vms > Checking VM states. Done (00:00:10)
  Started scanning 1 vms > 0 OK, 0 unresponsive, 1 missing, 0 unbound, 0 out of sync. Done (00:00:00)
     Done scanning 1 vms (00:00:10)

  Started scanning 0 persistent disks
  Started scanning 0 persistent disks > Looking for inactive disks. Done (00:00:00)
  Started scanning 0 persistent disks > 0 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)
     Done scanning 0 persistent disks (00:00:00)

Task 6 done

Started		2015-10-20 21:46:33 UTC
Finished	2015-10-20 21:46:43 UTC
Duration	00:00:10

Scan is complete, checking if any problems found...

Found 1 problem

Problem 1 of 1: VM with cloud ID `6918' missing.

Applying resolutions...

Director task 7
  Started applying problem resolutions > missing_vm 3: Recreate VM. Failed: undefined method `name' for nil:NilClass (00:00:01)

Task 7 done

Started		2015-10-20 21:46:44 UTC
Finished	2015-10-20 21:46:45 UTC
Duration	00:00:01
Cloudcheck is finished
```

**bosh task debug log...**

```
D, [2015-10-20 14:46:44 #6960] [task:7] DEBUG -- DirectorJobRunner: Recreating Vm: #<Bosh::Director::Models::Vm @values={:id=>3, :agent_id=>""8426cd00-7096-4d71-b731-59b6e0c20206"", :cid=>""6918"", :deployment_id=>1, :apply_spec_json=>""{\""deployment\"":\""first\"",\""job\"":{\""name\"":\""first_job\"",\""templates\"":[{\""name\"":\""foobar\"",\""version\"":\""b36b4b5fb7d916706c5a3723172932fe7ab03f3a\"",\""sha1\"":\""54aee725ba06f88738f3e77aef9b3400374f6eee\"",\""blobstore_id\"":\""5c770b44-8ea2-4a45-be8d-3c394163bcc5\""}],\""template\"":\""foobar\"",\""version\"":\""b36b4b5fb7d916706c5a3723172932fe7ab03f3a\"",\""sha1\"":\""54aee725ba06f88738f3e77aef9b3400374...skipping...
E, [2015-10-20 14:46:45 #6960] [task:7] ERROR -- DirectorJobRunner: Error resolving problem `1': undefined method `name' for nil:NilClass
E, [2015-10-20 14:46:45 #6960] [task:7] ERROR -- DirectorJobRunner: /Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/job.rb:167:in `spec'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance.rb:271:in `apply_spec'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/deployment_plan/instance.rb:150:in `apply_vm_state'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/cloudcheck_helper.rb:154:in `recreate_vm'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_handlers/missing_vm.rb:22:in `block (2 levels) in <class:MissingVM>'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:59:in `block in apply_resolution'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/event_log.rb:97:in `call'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/event_log.rb:50:in `track'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:21:in `track_and_log'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:58:in `apply_resolution'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:42:in `block in apply_resolutions'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `block in each'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:525:in `block (2 levels) in fetch_rows'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:720:in `block in yield_hash_rows'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:714:in `times'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:714:in `yield_hash_rows'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:525:in `block in fetch_rows'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:134:in `execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:413:in `_execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block (2 levels) in execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:425:in `check_database_errors'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block in execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:801:in `execute'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:525:in `fetch_rows'
/Users/pivotal/.gem/ruby/2.1.7/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `each'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/problem_resolver.rb:33:in `apply_resolutions'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:38:in `block in perform'
```

",,106159658,story,"[{'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",`bosh cck` should not return exit code 0 (success) when the task code blows up,1550486.0,[1550486],956238,1550486,bug,2015-11-12T17:47:34Z,https://www.pivotaltracker.com/story/show/106159658
,2015-11-09T18:48:27Z,unscheduled,,"Can this rake task and associated code be deleted?

``` rake migrations:bosh_cli_plugin_aws:new[name]      # Generate a new AWS migration with NAME```
",,107745148,story,[],Remove old aws migration code,,[],956238,344,chore,2015-11-09T19:30:34Z,https://www.pivotaltracker.com/story/show/107745148
,2015-11-06T00:47:02Z,unscheduled,,"/Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/verifiers/secure.rb:50:in `process_cache_miss': fingerprint 27:24:13:92:47:53:a8:3e:47:31:45:44:a6:55:f7:d3 does not match for ""54.236.230.20"" (Net::SSH::HostKeyMismatch)

currently shows:

Task 25833 done

Cleaning up ssh artifacts

Director task 25834

Task 25834 queued
/Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/verifiers/secure.rb:50:in `process_cache_miss': fingerprint 27:24:13:92:47:53:a8:3e:47:31:45:44:a6:55:f7:d3 does not match for ""54.236.230.20"" (Net::SSH::HostKeyMismatch)
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/verifiers/secure.rb:35:in `verify'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/verifiers/strict.rb:16:in `verify'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/verifiers/lenient.rb:15:in `verify'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/kex/diffie_hellman_group1_sha1.rb:173:in `verify_server_key'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/kex/diffie_hellman_group1_sha1.rb:68:in `exchange_keys'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/algorithms.rb:364:in `exchange_keys'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/algorithms.rb:200:in `proceed!'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/algorithms.rb:191:in `send_kexinit'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/algorithms.rb:146:in `accept_kexinit'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:200:in `block in poll_message'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:178:in `loop'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:178:in `poll_message'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:215:in `block in wait'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:213:in `loop'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:213:in `wait'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh/transport/session.rb:87:in `initialize'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh.rb:207:in `new'
	from /Users/pivotal/.gem/ruby/2.2.3/gems/net-ssh-2.9.2/lib/net/ssh.rb:207:in `start'
",,107540774,story,"[{'name': 'ssh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T15:14:05Z', 'id': 8094772, 'updated_at': '2014-04-01T15:14:05Z'}, {'name': 'ssh2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T00:28:40Z', 'id': 13187676, 'updated_at': '2015-10-30T00:28:40Z'}]",show better host fingerprint error for gateway,,[],956238,81882,feature,2015-11-06T00:51:03Z,https://www.pivotaltracker.com/story/show/107540774
,2015-11-04T18:28:19Z,unscheduled,,,,107418854,story,[],Search and destroy: replace backticks in log/event output with single quotes.,,[],956238,1550486,chore,2015-11-04T18:28:20Z,https://www.pivotaltracker.com/story/show/107418854
,2015-11-04T01:32:30Z,unscheduled,,,,107357058,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",bosh create release --final should properly record if changes are uncommited,,[],956238,81882,bug,2015-11-04T02:00:02Z,https://www.pivotaltracker.com/story/show/107357058
,2015-11-03T00:05:44Z,unscheduled,,"https://main.bosh-ci.cf-app.com/builds/12698

Failures:

  1) simultaneous deploys running two errands allocates IPs correctly for simultaneous errand runs
     Failure/Error: expect(first_errand_succeeded).to be(true)
       
       expected true
            got false
       
       Test directory: /tmp/build/5d79afa7-701f-4f1b-7969-6499074e2fe8/bosh-src/tmp/integration-tests-workspace/pid-6698/spec-20151102-6698-1wsyxgz
       Sandbox directory: /tmp/build/5d79afa7-701f-4f1b-7969-6499074e2fe8/bosh-src/tmp/integration-tests-workspace/pid-6698
     # ./spec/integration/global_networking/simultaneous_deploys_spec.rb:131:in `block (3 levels) in <top (required)>'


(There is also another failed test that is believed to be a legitimate error)",,107257382,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'flaky', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-23T16:42:23Z', 'id': 9011640, 'updated_at': '2015-10-08T17:41:47Z'}]",Investigate & Fix Flaky integration test,,[],956238,1426194,chore,2015-11-09T18:54:35Z,https://www.pivotaltracker.com/story/show/107257382
,2015-11-02T23:59:34Z,unscheduled,,,,107257026,story,[],cck should drain the instance when recreating vm,,[],956238,553935,feature,2015-11-02T23:59:34Z,https://www.pivotaltracker.com/story/show/107257026
,2015-10-30T00:13:56Z,unscheduled,,,,107001864,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ssh2', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-30T00:28:40Z', 'id': 13187676, 'updated_at': '2015-10-30T00:28:40Z'}]",Remove dependency on manifest in bosh ssh,,[],956238,553935,feature,2015-10-30T00:28:48Z,https://www.pivotaltracker.com/story/show/107001864
,2015-10-28T18:30:20Z,unscheduled,,"```
cat /Users/pivotal/workspace/bosh/tmp/integration-tests-workspace/pid-4477/sandbox/logs/36239dc09c314c9d885ac3e5c99449bd.tcp_proxy.out-24b5f970
Starting to proxy connections from localhost:61006 -> 127.0.0.1:5432
Exception raised, exiting: #<Errno::EADDRINUSE: Address already in use - bind(2) for nil port 61006>
```",,106876854,story,[],Sandbox startup should fail fast if trying to start TCP proxy when it's already running,,[],956238,1550486,chore,2015-10-28T18:33:07Z,https://www.pivotaltracker.com/story/show/106876854
,2015-10-28T18:32:53Z,unscheduled,,,,106877112,story,[],Sandbox should handle SIGINT (etc.?) to shut down related services,,[],956238,1550486,chore,2015-10-28T18:33:04Z,https://www.pivotaltracker.com/story/show/106877112
,2015-10-28T18:00:54Z,unscheduled,,"
```
$ bosh vms cf-superman-routing
Deployment `cf-superman-routing'

Director task 1457

Task 1457 done

+---------------------------+---------+----------------+-------------+
| Job/index                 | State   | Resource Pool  | IPs         |
+---------------------------+---------+----------------+-------------+
| router_acceptance_tests/0 | running | small_errand   | 10.0.112.13 |
| tcp_emitter_z1/0          | running | tcp_emitter_z1 | 10.0.112.11 |
| tcp_emitter_z2/0          | running | tcp_emitter_z2 | 10.0.128.11 |
| tcp_router_z1/0           | failing | tcp_router_z1  | 10.0.112.10 |
| tcp_router_z2/0           | failing | tcp_router_z2  | 10.0.128.10 |
+---------------------------+---------+----------------+-------------+

VMs total: 5

$ bosh -d cf-superman-routing.yml ssh tcp_emitter_z1 --gateway_host bosh.superman.cf-app.com --gateway_user vcap

Processing deployment manifest
------------------------------

Processing deployment manifest
------------------------------
You should specify the job index. There is more than one instance of this job type.
```

`tcp_emitter_z1` and ` tcp_emitter_z2` are uniquely named jobs with one instance each.",,106874008,story,[],"bosh ssh fails because index was not provided, though i have one instance of each job",,[],956238,58676,bug,2015-10-28T18:00:54Z,https://www.pivotaltracker.com/story/show/106874008
,2015-10-28T16:36:24Z,unscheduled,,"We occasionally see the build fail with
```
panic: Running command: 'bundle exec bosh -n -c /tmp/bosh-config179871363 task 18 --debug', stdout: '
', stderr: 'Unknown command: task 18 --debug': exit status 1
```
See <https://main.bosh-ci.cf-app.com/pipelines/bosh-global-net/jobs/load-tests/builds/107>
",,106864768,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Fix occasional Panic in global-net load tests,,[],956238,1550486,chore,2015-10-28T17:09:24Z,https://www.pivotaltracker.com/story/show/106864768
,2015-10-28T01:12:01Z,unscheduled,,"should not try to drain if apply never succeeded

```
    Done updating job mysql-partition-f66e71400deebe4e57fa > mysql-partition-f66e71400deebe4e57fa/0 (canary) (00:02:36)
  Started updating job cloud_controller-partition-f66e71400deebe4e57fa > cloud_controller-partition-f66e71400deebe4e57fa/0 (canary). Failed: Action Failed get_task: Task f4ee8972-66c9-48c1-50dc-ceae896a1639 result: Applying: Applying job cloud_controller_ng: Applying package buildpack_go for job cloud_controller_ng: Fetching package blob: Getting blob from inner blobstore: SHA1 mismatch. Expected 4d6a5765bf2f8ceb0023036687f20ec6a58108f8, got 90e2348239b743c24cdd7443d3928016ab8846ad for blob /var/vcap/data/tmp/bosh-blobstore-externalBlobstore-Get102371899 (00:03:19)

Error 450001: Action Failed get_task: Task f4ee8972-66c9-48c1-50dc-ceae896a1639 result: Applying: Applying job cloud_controller_ng: Applying package buildpack_go for job cloud_controller_ng: Fetching package blob: Getting blob from inner blobstore: SHA1 mismatch. Expected 4d6a5765bf2f8ceb0023036687f20ec6a58108f8, got 90e2348239b743c24cdd7443d3928016ab8846ad for blob /var/vcap/data/tmp/bosh-blobstore-externalBlobstore-Get102371899

Task 12 error

For a more detailed error report, run: bosh task 12 --debug
2015-10-28 00:33:54 UTC Running ""bundle exec bosh -n deploy""
Acting as user 'director' on deployment 'cf-1fc35ce59ace4cf2b331' on 'p-bosh-f3583eb0cd14a7106d7c'
Getting deployment properties from director...

Deploying
---------

Director task 13
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)

  Started preparing deployment
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:02)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:01)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started preparing configuration > Binding configuration. Done (00:00:04)

  Started updating job cloud_controller-partition-f66e71400deebe4e57fa > cloud_controller-partition-f66e71400deebe4e57fa/0 (canary). Failed: Action Failed get_task: Task c86f57c9-26c1-4720-6ab7-da86cd40df80 result: 1 of 1 drain scripts failed. Failed Jobs: cloud_controller_ng. (00:01:52)

Error 450001: Action Failed get_task: Task c86f57c9-26c1-4720-6ab7-da86cd40df80 result: 1 of 1 drain scripts failed. Failed Jobs: cloud_controller_ng.

Task 13 error
```",,106808746,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",when apply on the vm fails we should not try to drain,,[],956238,81882,bug,2015-10-28T01:12:05Z,https://www.pivotaltracker.com/story/show/106808746
,2015-10-26T16:27:08Z,unscheduled,,,,106662202,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Determine why (and fix) the bosh integration suites (develop and global-net) get into endless loops,,[],956238,1550486,chore,2015-10-26T16:27:08Z,https://www.pivotaltracker.com/story/show/106662202
,2015-10-24T00:52:02Z,unscheduled,,,,106530652,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Move OS image and Stemcell building to execute within (a shared) Docker image instead of AWS vagrant machines,,[],956238,1550486,chore,2015-10-24T00:52:03Z,https://www.pivotaltracker.com/story/show/106530652
,2015-10-24T00:51:01Z,unscheduled,,"Alternatively, rework the relevant builds to no longer need this Vagrant VM.",,106530630,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Create a Concourse pipeline for rebuilding the 'bosh-stemcell' vagrant box to get latest packages (security and improvement in launch time),,[],956238,1550486,chore,2015-10-26T16:24:13Z,https://www.pivotaltracker.com/story/show/106530630
,2015-10-23T19:13:42Z,unscheduled,,,,106510240,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Add a test to ensure that the version of `bosh-agent` submodule in `bosh` is on `master`,,[],956238,1550486,chore,2015-10-24T00:52:27Z,https://www.pivotaltracker.com/story/show/106510240
,2015-10-22T14:06:49Z,unscheduled,,"```
Acting as user 'director' on deployment 'prism-jarvice-az1' on 'microbosh-b501320b5cb34b42b0ad'
You are about to rename `imds' to `networkimds'
Are you sure? (type 'yes' to continue): yes

Director task 3659
  Started unknown
  Started unknown > Binding deployment. Done (00:00:00)

  Started preparing deployment
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Failed: deployment_id and job and index unique (00:00:00)

Error 100: deployment_id and job and index unique

Task 3659 error
```

Debug output
```
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(85daec40-5bb0-4eaf-a5ce-519f52ac8c22)] DEBUG -- DirectorJobRunner: (0.004001s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(eea3c610-4249-43b6-8c78-9e92da0cc6f4)] DEBUG -- DirectorJobRunner: (0.003962s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(0ade9657-c23d-4455-8a2e-7f28efda8c47)] DEBUG -- DirectorJobRunner: (0.000501s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(85daec40-5bb0-4eaf-a5ce-519f52ac8c22)] DEBUG -- DirectorJobRunner: (0.000619s) SELECT * FROM `instances` WHERE (`instances`.`vm_id` = 1213) LIMIT 1
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(85daec40-5bb0-4eaf-a5ce-519f52ac8c22)] DEBUG -- DirectorJobRunner: Verified VM state
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(eea3c610-4249-43b6-8c78-9e92da0cc6f4)] DEBUG -- DirectorJobRunner: (0.001118s) SELECT COUNT(*) AS `count` FROM `instances` WHERE ((`deployment_id` = 21) AND (`job` = 'networkimds') AND (`index` = 1) AND (`id` != 224)) LIMIT 1
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(85daec40-5bb0-4eaf-a5ce-519f52ac8c22)] DEBUG -- DirectorJobRunner: (0.000669s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(0ade9657-c23d-4455-8a2e-7f28efda8c47)] DEBUG -- DirectorJobRunner: (0.001498s) SELECT * FROM `instances` WHERE (`instances`.`vm_id` = 1214) LIMIT 1
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(0ade9657-c23d-4455-8a2e-7f28efda8c47)] DEBUG -- DirectorJobRunner: Verified VM state
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(eea3c610-4249-43b6-8c78-9e92da0cc6f4)] DEBUG -- DirectorJobRunner: (0.000720s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(85daec40-5bb0-4eaf-a5ce-519f52ac8c22)] DEBUG -- DirectorJobRunner: (0.000716s) SELECT * FROM `persistent_disks` WHERE (`persistent_disks`.`instance_id` = 244)
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(0ade9657-c23d-4455-8a2e-7f28efda8c47)] DEBUG -- DirectorJobRunner: (0.000520s) SELECT NULL
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(eea3c610-4249-43b6-8c78-9e92da0cc6f4)] DEBUG -- DirectorJobRunner: (0.000551s) SELECT COUNT(*) AS `count` FROM `instances` WHERE ((`vm_id` = 1210) AND (`id` != 224)) LIMIT 1
D, [2015-10-22 14:01:18 #30529] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: deployment_id and job and index unique - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1248:in `save'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1265:in `save_changes'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1843:in `update_restricted'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1399:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:117:in `bind_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:54:in `block in bind_existing_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:48:in `synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:48:in `bind_existing_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:36:in `block (4 levels) in bind_existing_deployment'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3026.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3026.0/lib/bosh/director/deployment_plan/assembler.rb:35:in `block (3 levels) in bind_existing_deployment'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3026.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3026.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3026.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3026.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(df8a170d-5c7b-43c6-a66d-bd3b237ff354)] DEBUG -- DirectorJobRunner: Processing VM network reservations
D, [2015-10-22 14:01:18 #30529] [bind_existing_deployment(df8a170d-5c7b-43c6-a66d-bd3b237ff354)] DEBUG -- DirectorJobRunner: Binding instance VM

```",,106305404,story,[],Unable to rename job,,[],956238,5663,bug,2015-10-22T14:06:49Z,https://www.pivotaltracker.com/story/show/106305404
,2015-10-21T18:54:41Z,unscheduled,,"cf-mysql recently added a property to its plans so that the `broker-registrar` errand will skip auto-publishing certain plans (#103699052).

In the story, I specified that we should be able to express this property in the Spec file, so as not to rely on sample stubs. The spec file is supposed to be the official place, is auto-documented on bosh.io, blah blah blah.

But we weren't able to do that. BOSH spec files are not capable of representing arrays of arrays. (@lylef to expand on this.) So we went the stub route.

Over-reliance on stubs is pretty bad. Even as a new PM, I didn't discover BOSH spec files, nor understand what they were, for quite a while. Everything magically seemed to work by modifying stubs.

Plans, in particular, are central to Service Developers. Not having the ability to properly document plans in Spec files seems not good.

/cc @benlaplanche
",,106240024,story,[],"BOSH Specs aren't able to properly represent service plans, which is important for Service Developers",,[],956238,1622234,feature,2015-12-22T15:39:12Z,https://www.pivotaltracker.com/story/show/106240024
,2015-10-20T18:53:47Z,unscheduled,,"IaaS interaction is costly and takes forever, so don't recreate a VM when you don't need to.

background: When removing powerDNS from the director, it re-created *all* of the VMs in the next deploy cycle, although nothing had changed in the deployment: https://github.com/cloudfoundry/bosh/issues/859

Out of scope: The main problem of the linked item is that the user didn't see what bosh was about to do. Let's ignore this and just fix the re-creation part.",,106142298,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Director should not recreate VMs when changing DNS,,[],956238,1602644,feature,2015-10-20T19:01:49Z,https://www.pivotaltracker.com/story/show/106142298
,2015-10-14T00:40:36Z,unscheduled,,"i asked to log into `cell_z1 1`, but bosh logged me into `cell_z1 0`. very confusing; i already logged in there. i would expect an error. 
```
$ bosh ssh cell_z1 1 --gateway_host bosh.superman.cf-app.com --gateway_user vcap

Processing deployment manifest
------------------------------

Processing deployment manifest
------------------------------
Enter password (use it to sudo on remote host): *

Processing deployment manifest
------------------------------
Target deployment is `cf-superman-diego'

Setting up ssh artifacts

Director task 908

Task 908 done
Starting interactive shell on job cell_z1/0
The authenticity of host '[localhost]:65533 ([127.0.0.1]:65533)' can't be established.
RSA key fingerprint is 53:16:60:a2:a6:5f:85:47:2f:b1:dd:37:86:a4:a5:e1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:65533' (RSA) to the list of known hosts.
Ubuntu 14.04.3 LTS \n \l

Welcome to Ubuntu 14.04.3 LTS (GNU/Linux 3.19.0-30-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user ""root""), use ""sudo <command>"".
See ""man sudo_root"" for details.

bosh_w605p31qo@50163995-c66c-4c74-aa58-56b0c11314d6:~$
```

i asked to download logs for `cell_z1 1`, but bosh downloaded logs for `cell_z1 0`. waste of time; i already had those. i would expect an error.
```
$ bosh logs cell_z1 1

Processing deployment manifest
------------------------------

Processing deployment manifest
------------------------------

Director task 904
  Started fetching logs for cell_z1/0 > Finding and packing log files. Done (00:00:01)

Task 904 done

Started		2015-10-14 00:12:31 UTC
Finished	2015-10-14 00:12:32 UTC
Duration	00:00:01
Downloading log bundle (7df46617-b9f8-4796-5caa-05489654ce3c)...

Logs saved in `/Users/scoen/workspace/deployments-routing/superman/cell_z1.0.2015-10-13-17-13-33.tgz'
```",,105622944,story,[],"when using cli to download logs or ssh into a job, and specified index does not exist in deployment, unexpected behavior results",,[],956238,58676,bug,2015-10-14T00:41:28Z,https://www.pivotaltracker.com/story/show/105622944
,2015-10-08T22:04:16Z,unscheduled,,"Seen here:

https://github.com/cloudfoundry/bosh-init/blob/4f26e738f80a9a03e708ee12f71ebe6638a53743/deployment/manifest/parser_test.go#L345

""it does not expand the path..."" should be ""it expands the path...""",,105257098,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init parser test should have correct description for expanding relative paths,,[],956238,1550486,chore,2015-10-12T18:21:33Z,https://www.pivotaltracker.com/story/show/105257098
,2015-10-08T18:15:36Z,unscheduled,,,,105232536,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Configure Concourse deployment to be a rolling update,,[],956238,1550486,chore,2015-10-08T18:15:36Z,https://www.pivotaltracker.com/story/show/105232536
,2015-10-08T17:39:44Z,unscheduled,,It's annoying.,,105228730,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}]","Stop writing (or ignore or clean up) `go/bin/...`, `go/src/golang.org/...` etc.",,[],956238,1550486,chore,2015-10-08T17:39:51Z,https://www.pivotaltracker.com/story/show/105228730
,2015-10-08T17:23:56Z,unscheduled,,"Currently, we output the following if a user does `bosh target XXX --ca-cacert <path>`
`Updating certificate file path to `<path>'`

* perfectly fine like this if you just update the certificate to something new or set it for the first time
* If you *remove* the cert from the config, something like `removing certificate for target XXX` is less confusing than an empty path",,105226862,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'starter-items', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-10-08T17:23:56Z', 'id': 12994228, 'updated_at': '2015-10-08T17:23:56Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Improve message about updated certificate path in bosh_config when removing certificate,,[],956238,81882,feature,2015-10-08T17:23:57Z,https://www.pivotaltracker.com/story/show/105226862
,2015-10-08T00:20:34Z,unscheduled,,,,105158808,story,"[{'name': 'retro action item', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032000, 'updated_at': '2015-06-23T16:06:50Z'}]","Catalog test flakiness, with ""why"" (and attempt to fix)",,[],956238,1550486,chore,2015-10-08T00:20:34Z,https://www.pivotaltracker.com/story/show/105158808
,2015-10-01T19:28:57Z,unscheduled,,should only be an option on AWS,,104660462,story,"[{'name': 'root-disk1', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:15:09Z', 'id': 12776752, 'updated_at': '2015-09-15T21:15:09Z'}, {'name': '[tor]', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-08-21T17:07:11Z', 'id': 12559832, 'updated_at': '2015-08-21T17:07:11Z'}]",create a new BATS test to cover the cpi and agent changes for growing the root disk,,[],956238,1355110,chore,2015-10-22T01:33:26Z,https://www.pivotaltracker.com/story/show/104660462
,2015-10-01T17:01:18Z,unscheduled,,,,104646566,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Update Concourse pipeline for **concourse deployment** to include deployment of custom workers,,[],956238,1550486,chore,2015-10-08T17:40:47Z,https://www.pivotaltracker.com/story/show/104646566
,2015-09-30T18:30:12Z,unscheduled,,,,104559324,story,[],`net-ssh` and `fog-google` gems within `bosh_cli` should be locked down to continue to support Ruby 1.9.3,,[],956238,1550486,feature,2015-09-30T18:30:12Z,https://www.pivotaltracker.com/story/show/104559324
,2015-09-29T17:51:39Z,unscheduled,,"These key/value pairs would not be used by the tests, but would help when needing to figure out which AWS account is used for a particular pipeline.",,104458892,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Add `BOSH_AWS_ACCOUNT` key and AWS account name (value) to pipelines.,,[],956238,1550486,chore,2015-10-08T17:40:47Z,https://www.pivotaltracker.com/story/show/104458892
,2015-09-24T17:55:51Z,unscheduled,,"Today, `--recreate` means ""do a deploy of the manifest, recreating all VMs along the way."" I would like a flag that means ""do a deploy of the manifest, and for any VMs that are affected by it, recreate them rather than updating in-place.""

My use case: I have a job colocated on a few VMs, and I want to take it off and put it back on (we're testing upgrade paths). When the job is removed, its data sticks around, and I want it to go away. Today the easiest way to do that is to deploy, and either manually recreate all instances (which can take a long time + some scripting) or manually go on all VMs and blow away the data (which is risky).

I've wanted this before for other use cases, e.g. when a job is going haywire and has cluttered up some state, and I have a fix to deploy but still need the original VMs to be cleaned up.",,104118498,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",As an operator I would like to do a deploy that recreates only the instances that the deploy affects,,[],956238,381857,feature,2015-09-24T21:44:48Z,https://www.pivotaltracker.com/story/show/104118498
,2015-09-17T23:16:38Z,unscheduled,,"`bosh upload blobs` gives unclear description of error for not having private.yml

```
/Users/pivotal/.gem/ruby/1.9.3/gems/blobstore_client-1.2865.0/lib/blobstore_client/s3_blobstore_client.rb:66:in `create_file': unsupported action (Bosh::Blobstore::BlobstoreError)
	from /Users/pivotal/.gem/ruby/1.9.3/gems/blobstore_client-1.2865.0/lib/blobstore_client/base.rb:27:in `create'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/blob_manager.rb:269:in `upload_blob'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/commands/blob_management.rb:41:in `block in upload'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/commands/blob_management.rb:38:in `each'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/commands/blob_management.rb:38:in `upload'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.2865.0/bin/bosh:16:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/1.9.3/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/1.9.3/bin/bosh:23:in `<main>'
```",,103640350,story,[],bosh upload blobs unclear description,,[],956238,1550668,bug,2015-09-17T23:16:38Z,https://www.pivotaltracker.com/story/show/103640350
,2015-09-15T21:10:10Z,unscheduled,,"When attempt to deploy 2000 instances to AWS with the settings above, we receive the following error:

**Too many open files - /var/vcap/jobs/powerdns/bin/powerdns_ctl**

```
D, [2015-09-14 21:35:04 #20948] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: Too many open files - /var/vcap/jobs/powerdns/bin/powerdns_ctl - /var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in `spawn'
/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in `popen_run'
/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:in `popen3'
/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in `capture3'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/dns_helper.rb:207:in `flush_dns_cache'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:232:in `update_dns'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:60:in `block in update_steps'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:88:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:88:in `block in update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:87:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:87:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:94:in `block (2 levels) in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:92:in `block in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:91:in `update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:85:in `block (2 levels) in update_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
```
",,103450344,story,"[{'name': 'sc', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:13:56Z', 'id': 12776734, 'updated_at': '2015-09-15T21:13:56Z'}]",BOSH Director should be able to handle `max_in_flight` = 500 and `max_threads` = 32,,[],956238,1550486,feature,2015-09-15T21:14:03Z,https://www.pivotaltracker.com/story/show/103450344
,2015-09-15T21:02:19Z,unscheduled,,"Likely will need to update CPIs to catch IaaS-specific exceptions, and raise a custom BOSH exception for the Director to catch, and retry. An example stacktrace from AWS:

```
D, [2015-09-15 20:51:03 #18086] [task:63] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""84a116fb-e939-4ef1-821f-2fe6dd8ab687"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for 'dummy' against Director '0331cac4-5acd-4264-b448-de755da31e1c': #<AWS::EC2::Errors::RequestLimitExceeded: Request limit exceeded.>"",""created_at"":1442350263}
E, [2015-09-15 20:51:03 #18086] [task:63] ERROR -- DirectorJobRunner: Request limit exceeded.
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:375:in `return_or_raise'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `describe_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/resource.rb:72:in `describe_call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/instance.rb:787:in `get_resource'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/resource.rb:235:in `block (2 levels) in define_attribute_getter'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/cacheable.rb:63:in `retrieve_attribute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/resource.rb:66:in `retrieve_attribute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/resource.rb:235:in `block in define_attribute_getter'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.3069.0/lib/cloud/aws/cloud.rb:440:in `compare_private_ip_addresses'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.3069.0/lib/cloud/aws/cloud.rb:402:in `block in configure_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.3069.0/lib/cloud/aws/cloud.rb:393:in `configure_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3069.0/lib/cloud/internal_cpi.rb:22:in `invoke_cpi_method'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3069.0/lib/cloud/internal_cpi.rb:10:in `method_missing'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater/network_updater.rb:28:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:285:in `update_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:59:in `block in update_steps'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:88:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:88:in `block in update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:87:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/instance_updater.rb:87:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:94:in `block (2 levels) in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:92:in `block in update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:91:in `update_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3069.0/lib/bosh/director/job_updater.rb:85:in `block (2 levels) in update_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3069.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-09-15 20:51:03 #18086] [task:63] DEBUG -- DirectorJobRunner: (0.000592s) SELECT NULL
D, [2015-09-15 20:51:03 #18086] [task:63] DEBUG -- DirectorJobRunner: (0.000572s) BEGIN
D, [2015-09-15 20:51:03 #18086] [task:63] DEBUG -- DirectorJobRunner: (0.000785s) UPDATE `tasks` SET `state` = 'error', `timestamp` = '2015-09-15 20:51:03', `description` = 'create deployment', `result` = 'Request limit exceeded.', `output` = '/var/vcap/store/director/tasks/63', `checkpoint_time` = '2015-09-15 20:50:35', `type` = 'update_deployment', `username` = 'admin' WHERE (`id` = 63) LIMIT 1
```",,103449734,story,"[{'name': 'sc', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-09-15T21:13:56Z', 'id': 12776734, 'updated_at': '2015-09-15T21:13:56Z'}]",BOSH Director should (be able to) retry for rate limit exceeded,,[],956238,1550486,feature,2015-09-15T22:46:55Z,https://www.pivotaltracker.com/story/show/103449734
,2015-09-11T17:46:43Z,unscheduled,,"Steps that exposed the problem:
we created a bosh release with a broken postgres_ctl script and deployed. the database did not come up, and therefore director did not come up.
we fixed the postgres_ctl script and attempted to re-deploy with the new bosh release.
Bosh-init attempted to unmount the persistent disk, but was unable to. bosh-init hung while waiting for the disk to unmount, but never succeeded. 
The bosh-init logs showed that it was polling director for the task and not getting a success response.

",,103216164,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should be able to redeploy a vm that has persistent disk and director is in a bad state,,[],956238,344,feature,2015-09-14T06:11:36Z,https://www.pivotaltracker.com/story/show/103216164
,2015-09-10T19:03:23Z,unscheduled,,,,103128184,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",user can send in bosh-init manifest via stdin,,[],956238,81882,feature,2015-09-14T17:07:47Z,https://www.pivotaltracker.com/story/show/103128184
,2015-09-10T19:09:07Z,unscheduled,,"perhaps using something like:

```
lsof -i TCP | grep "":610"" | awk '{print $2}' | uniq | xargs kill
```
",,103128996,story,[],"integration sandbox, on startup, should reap any/all orphaned processes from previous runs of sandbox",,[],956238,1550486,chore,2015-09-10T19:14:10Z,https://www.pivotaltracker.com/story/show/103128996
,2015-09-10T08:11:20Z,unscheduled,,"SSH key for ketchup BOSH deployment is only key on ssh-agent keychain. No `[localhost]:NNNNN` entries in `~/.ssh/known_hosts`.

```
# with bosh 1.3058.0
$ bosh -t micro.ketchup.cf-app.com -d ~/deployments/ketchup/cf-ketchup-diego.yml ssh --gateway_user vcap --gateway_host micro.ketchup.cf-app.com access_z1 0
Acting as user 'ci' on deployment 'cf-ketchup-diego' on 'micro.ketchup.cf-app.com'
Enter password (use it to sudo on remote host): *
Target deployment is `cf-ketchup-diego'

Setting up ssh artifacts

Director task 32125

Task 32125 done
Starting interactive shell on job access_z1/0
Ubuntu 14.04.3 LTS \n \l

Welcome to Ubuntu 14.04.3 LTS (GNU/Linux 3.19.0-26-generic x86_64)
# ...fine, log out, clear out [localhost]:65535 known host entry

# with bosh 1.3062.0
$ bosh -t micro.ketchup.cf-app.com -d ~/deployments/ketchup/cf-ketchup-diego.yml ssh --gateway_user vcap --gateway_host micro.ketchup.cf-app.com access_z1 0
Acting as user 'ci' on deployment 'cf-ketchup-diego' on 'micro.ketchup.cf-app.com'
Enter password (use it to sudo on remote host): *
Target deployment is `cf-ketchup-diego'

Setting up ssh artifacts

Director task 32129

Task 32129 done
Starting interactive shell on job access_z1/0
No RSA host key is known for [localhost]:65535 and you have requested strict checking.
Host key verification failed.

Cleaning up ssh artifacts

Director task 32130

Task 32130 done
```

Non-gateway `bosh ssh` also works fine with 1.3062.0: 

```
$ bosh -t lite -d ~/deployments/bosh-lite/diego.yml ssh access_z1 0
Acting as user 'admin' on deployment 'cf-warden-diego' on 'lite'
Enter password (use it to sudo on remote host): *
Target deployment is `cf-warden-diego'

Setting up ssh artifacts

Director task 9

Task 9 done
Starting interactive shell on job access_z1/0
Welcome to Ubuntu 14.04 LTS (GNU/Linux 3.13.0-29-generic x86_64)
...
```",,103067148,story,[],gateway options on `bosh ssh` no longer work as of bosh_cli 1.3062.0,,[],956238,1158132,bug,2015-09-10T08:44:46Z,https://www.pivotaltracker.com/story/show/103067148
,2015-09-08T17:35:06Z,unscheduled,,"Acceptance criteria: Given a desired stemcell version, it should be possible to more or less automatically infer the other 3 versions in CI.

Initially, we'll probably write a test that asserts against the versions of the other 3 given a single stemcell version but hard code all 4 in our code.",,102928348,story,[],"__As an__ Ops Manager developer __I'd like__ an automated way to identify a compatible set of version for the bosh release, cpi release, stemcell & bosh-init cli __so that__ I can quickly & reliably bump to newer versions of these in the Ops Manager codebase",,[],956238,5637,feature,2015-09-08T17:36:18Z,https://www.pivotaltracker.com/story/show/102928348
,2015-09-07T07:53:04Z,unscheduled,,"Steps to reproduce:
1. Generate a keypair `bosh-kp`  in openstack dashboard and download the private key, save as `bosh-kp.pem`
2. Use  `bosh-kp` as default key name and `bosh-kp.pem` file as private key of ssh tunnel in your bosh-init deployment manifest `bosh-init-openstack.yml`
3. Deploy your manifest `bosh-init deploy bosh-init-openstack.yml`

it will report an error like: 
```
...
Started deploying
  Creating VM for instance 'bosh/0' from stemcell '606bf575-ef71-42ed-b725-d77bd106e5de'... Finished (00:00:28)
  Waiting for the agent on VM '9457771a-f7d9-4179-b622-db655441d3b1' to be ready... Failed (00:00:00)
Failed deploying (00:00:28)

Stopping registry... Finished (00:00:00)
Cleaning up rendered CPI jobs... Finished (00:00:00)

Command 'deploy' failed:
  Deploying:
    Creating instance 'bosh/0':
      Waiting until instance is ready:
        Starting SSH tunnel:
          Parsing private key file '/Users/zhanghua/my-bosh/bosh-kp.pem':
            asn1: structure error: superfluous leading zeros in length
```",,102833600,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",bosh-init can not recognize Openstack generated keypair,,[],956238,1495236,bug,2015-09-25T23:57:51Z,https://www.pivotaltracker.com/story/show/102833600
,2015-08-31T19:09:36Z,unscheduled,,"When using bosh-init, if one deploys a job with a missing properties section a `convert nil into Hash (TypeError)` is thrown by the inline ruby code. Adding an empty has for job properties fixes the issue.

To reproduce:
- Use bosh-init manifest template from bosh.io
- Use dummy release with job template with no properties for `jobs` section
- Remove `properties` section under `jobs`
- Deploy
- Add empty hash for job properties
- Deploy

```
bEvaluationContext] 2015/08/26 14:24:52 DEBUG - Marshalling context templatescompiler.RootContext{Index:0, JobContext:templatescompiler.jobContext{Na      me:""baremetal_cpi""}, Deployment:""dummy"", NetworkContexts:map[string]templatescompiler.networkContext{""default"":templatescompiler.networkContext{IP:"""", N      etmask:"""", Gateway:""""}}, GlobalProperties:property.Map{}, ClusterProperties:property.Map(nil), DefaultProperties:property.Map{""baremetal_cpi.port"":22, ""      baremetal_cpi.private_key"":property.Property(nil), ""baremetal_cpi.agent.blobstore.options"":property.Map{}, ""baremetal_cpi.agent.mbus"":property.Property(      nil), ""baremetal_cpi.user"":""root"", ""baremetal_cpi.apiserver"":property.Property(nil), ""baremetal_cpi.agent.ntp"":property.List{}, ""baremetal_cpi.agent.blo      bstore.provider"":""dav"", ""baremetal_cpi.stemcells_dir"":""/var/vcap/store/cpi/stemcells"", ""baremetal_cpi.machines"":property.Property(nil)}}

16210 [File System] 2015/08/26 14:24:52 DEBUG - Writing /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-      context.json

16211 [File System] 2015/08/26 14:24:52 DEBUG - Making dir /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511 w      ith perm 511

16212 [File System] 2015/08/26 14:24:52 DEBUG - Write content

16213 ********************

16214 {""index"":0,""job"":{""name"":""baremetal_cpi""},""deployment"":""dummy"",""networks"":{""default"":{""ip"":"""",""netmask"":"""",""gateway"":""""}},""global_properties"":{},""cluste      r_properties"":null,""default_properties"":{""baremetal_cpi.agent.blobstore.options"":{},""baremetal_cpi.agent.blobstore.provider"":""dav"",""baremetal_cpi.agent.      mbus"":null,""baremetal_cpi.agent.ntp"":[],""baremetal_cpi.apiserver"":null,""baremetal_cpi.machines"":null,""baremetal_cpi.port"":22,""baremetal_cpi.private_key""      :null,""baremetal_cpi.stemcells_dir"":""/var/vcap/store/cpi/stemcells"",""baremetal_cpi.user"":""root""}}

16215 ********************

16216 [Cmd Runner] 2015/08/26 14:24:52 DEBUG - Running command: ruby /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer0      25304511/erb-render.rb /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-context.json /home/emc/.bos      h_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/bosh-init-release170484665/extracted_jobs/baremetal_cpi/templates/cpi.erb /home/emc/.bosh_      init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/rendered-jobs217641440/bin/cpi

16217 [Cmd Runner] 2015/08/26 14:24:52 DEBUG - Stdout:

16218 [Cmd Runner] 2015/08/26 14:24:52 DEBUG - Stderr: /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-r      ender.rb:11:in `merge!': can't convert nil into Hash (TypeError)

16219         from /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-render.rb:11:in `recursive_merge!'

16220         from /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-render.rb:31:in `initialize'

16221         from /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-render.rb:188:in `new'

16222         from /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511/erb-render.rb:188:in `<main>'

16223 [Cmd Runner] 2015/08/26 14:24:52 DEBUG - Successful: false (1)

16224 [File System] 2015/08/26 14:24:52 DEBUG - Remove all /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/erb-renderer025304511

16225 [File System] 2015/08/26 14:24:52 DEBUG - Remove all /home/emc/.bosh_init/installations/3055fb1a-45e9-48aa-4333-c9f5d2cdd3e8/tmp/rendered-jobs217641440

16226  Failed (00:00:00)

16227 Failed deploying (00:09:10)
```",,102404050,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init does not allow for a job with empty properties,,[],956238,1406536,bug,2015-08-31T23:00:21Z,https://www.pivotaltracker.com/story/show/102404050
,2015-08-28T22:36:58Z,unscheduled,,"This happens in diego-release for file_server process periodically when they update deployment.

In monit logs it shows that file_server was stopped and then started but ctl script was not called. In the end the process is marked as unmonitored.",,102292294,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'monit', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-01T23:24:05Z', 'id': 12109048, 'updated_at': '2015-07-01T23:24:05Z'}]",Monit is not running start command for one of the processes,,[],956238,806897,bug,2015-09-25T23:58:13Z,https://www.pivotaltracker.com/story/show/102292294
,2015-08-27T01:22:52Z,unscheduled,,"```
Release repacked (new size is 1.3M)
Uploading release (will be rebased)
release-repac: 100% |oooooooooooooooooooooooooooooo|   1.3MB   6.3MB/s Time: 00:00:00

Director task 3
  Started extracting release > Extracting release. Done (00:00:00)

  Started verifying manifest > Verifying manifest. Done (00:00:00)

Error 100: Failed to Rebase - Invalid Version: #<Bosh::Common::Version::ReleaseVersion:0x007f21c09a77f0 @version=#<SemiSemantic::Version:0x007f21c09a78b8 @release=#<SemiSemantic::VersionSegment:0x007f21c09a7930 @components=[2]>, @pre_release=nil, @post_release=nil, @segments=[#<SemiSemantic::VersionSegment:0x007f21c09a7930 @components=[2]>]>>

Task 3 error
```",,102124218,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",bosh upload release —rebase fails if there are no existing release,,[],956238,668075,bug,2015-08-27T01:27:59Z,https://www.pivotaltracker.com/story/show/102124218
,2015-08-24T22:31:31Z,unscheduled,,"Since it lives in the CF org.

This involves:
- Add a LICENSE file
- Updating the license mentioned in metadata.rb",,101935668,story,[],sprout-bosh should use Apache 2.0 license,,[],956238,5637,feature,2015-08-24T22:31:53Z,https://www.pivotaltracker.com/story/show/101935668
,2015-08-22T00:49:48Z,unscheduled,,"It would be a very helpful third party integration option for operators if bosh was able to emit event notifications when updating job instances.

For instance, operators using a tool like datadog to monitor their deployment would benefit from being able to superimpose the boundaries of each instance's update over various other information streams.",,101817636,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",bosh should be able to emit event notifications to third party monitoring services when it starts and finishes updating each job instance,,[],956238,1196628,feature,2015-08-24T22:05:13Z,https://www.pivotaltracker.com/story/show/101817636
,2015-08-22T00:47:05Z,unscheduled,,"Right now, the default task logs, (which are output from the bosh cli during a deployment, among other things) reports timestamps only at the end of tasks. For operators trying to use task logs to correlate system or app behavior observed by other means back to bosh/deployment events, this is very inconvenient. Such an operator presently has to either use `--raw` and convert the unix timestamps themselves or use `--debug` and filter out lots and lots of other information.

If instead, timestamps were displayed by default (or available as an option), this sort of investigation would be much easier. --timestamps would be fine, if made an option - though I might prefer --no-timestamps, so that you get this output by default (and thus have it available to immediately scroll-back to, if you find you need it) when performing deployment from the command line, without having to pass extra flags to bosh deploy.

Desired output might look something like this:
```
...
  2015-08-21T17:39:24.17-0700  Started preparing configuration > Binding configuration. Done (00:00:07)

  2015-08-21T17:39:24.17-0700  Started updating job nats-partition-4bee2da20d967f09cf50 > nats-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:40)
  2015-08-21T17:39:24.17-0700  Started updating job nats-partition-23c4d26bad29345e697c > nats-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:39)
  2015-08-21T17:39:24.17-0700  Started updating job etcd_server-partition-4bee2da20d967f09cf50 > etcd_server-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:56)
  2015-08-21T17:39:24.17-0700  Started updating job nfs_server-partition-4bee2da20d967f09cf50 > nfs_server-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:57)
  2015-08-21T17:39:24.17-0700  Started updating job ccdb-partition-4bee2da20d967f09cf50 > ccdb-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:57)
  2015-08-21T17:39:24.17-0700  Started updating job uaadb-partition-4bee2da20d967f09cf50 > uaadb-partition-4bee2da20d967f09cf50/0 (canary). Done (00:01:08)
  2015-08-21T17:39:24.17-0700  Started updating job consoledb-partition-4bee2da20d967f09cf50 > consoledb-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:57)
  2015-08-21T17:39:24.17-0700  Started updating job cloud_controller-partition-4bee2da20d967f09cf50 > cloud_controller-partition-4bee2da20d967f09cf50/0 (canary). Done (00:05:54)
  2015-08-21T17:39:24.17-0700  Started updating job cloud_controller-partition-23c4d26bad29345e697c > cloud_controller-partition-23c4d26bad29345e697c/0 (canary). Done (00:06:17)
  2015-08-21T17:39:24.17-0700  Started updating job ha_proxy-partition-4bee2da20d967f09cf50 > ha_proxy-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:40)
  2015-08-21T17:39:24.17-0700  Started updating job ha_proxy-partition-23c4d26bad29345e697c > ha_proxy-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:38)
  2015-08-21T17:39:24.17-0700  Started updating job router-partition-4bee2da20d967f09cf50 > router-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:39)
  2015-08-21T17:39:24.17-0700  Started updating job router-partition-23c4d26bad29345e697c > router-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:40)
  2015-08-21T17:39:24.17-0700  Started updating job health_manager-partition-4bee2da20d967f09cf50 > health_manager-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:37)
  2015-08-21T17:39:24.17-0700  Started updating job health_manager-partition-23c4d26bad29345e697c > health_manager-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:39)
  2015-08-21T17:39:24.17-0700  Started updating job clock_global-partition-4bee2da20d967f09cf50 > clock_global-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:50)
  2015-08-21T17:39:24.17-0700  Started updating job cloud_controller_worker-partition-4bee2da20d967f09cf50 > cloud_controller_worker-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:49)
  2015-08-21T17:39:24.17-0700  Started updating job cloud_controller_worker-partition-23c4d26bad29345e697c > cloud_controller_worker-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:50)
  2015-08-21T17:39:24.17-0700  Started updating job uaa-partition-4bee2da20d967f09cf50 > uaa-partition-4bee2da20d967f09cf50/0 (canary). Done (00:01:20)
  2015-08-21T17:39:24.17-0700  Started updating job uaa-partition-23c4d26bad29345e697c > uaa-partition-23c4d26bad29345e697c/0 (canary). Done (00:01:19)
  2015-08-21T17:39:24.17-0700  Started updating job login-partition-4bee2da20d967f09cf50 > login-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:51)
  2015-08-21T17:39:24.17-0700  Started updating job mysql_proxy-partition-4bee2da20d967f09cf50 > mysql_proxy-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:36)
  2015-08-21T17:39:24.17-0700  Started updating job mysql-partition-4bee2da20d967f09cf50
  2015-08-21T17:39:24.17-0700  Started updating job mysql-partition-4bee2da20d967f09cf50 > mysql-partition-4bee2da20d967f09cf50/0 (canary). Done (00:01:59)
  2015-08-21T17:39:24.17-0700  Started updating job mysql-partition-4bee2da20d967f09cf50 > mysql-partition-4bee2da20d967f09cf50/1. Done (00:02:05)
  2015-08-21T17:39:24.17-0700     Done updating job mysql-partition-4bee2da20d967f09cf50 (00:04:04)
  2015-08-21T17:39:24.17-0700  Started updating job mysql-partition-23c4d26bad29345e697c > mysql-partition-23c4d26bad29345e697c/0 (canary). Done (00:02:05)
  2015-08-21T17:39:24.17-0700  Started updating job dea-partition-4bee2da20d967f09cf50 > dea-partition-4bee2da20d967f09cf50/0 (canary). Done (00:01:53)
  2015-08-21T17:39:24.17-0700  Started updating job dea-partition-23c4d26bad29345e697c > dea-partition-23c4d26bad29345e697c/0 (canary). Done (00:01:44)
  2015-08-21T17:39:24.17-0700  Started updating job doppler-partition-4bee2da20d967f09cf50 > doppler-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:41)
  2015-08-21T17:39:24.17-0700  Started updating job doppler-partition-23c4d26bad29345e697c > doppler-partition-23c4d26bad29345e697c/0 (canary). Done (00:01:09)
  2015-08-21T17:39:24.17-0700  Started updating job loggregator_trafficcontroller-partition-4bee2da20d967f09cf50 > loggregator_trafficcontroller-partition-4bee2da20d967f09cf50/0 (canary). Done (00:00:39)
  2015-08-21T17:39:24.17-0700  Started updating job loggregator_trafficcontroller-partition-23c4d26bad29345e697c > loggregator_trafficcontroller-partition-23c4d26bad29345e697c/0 (canary). Done (00:00:39)


Task 7 done

  Started		2015-08-18 17:10:17 UTC
  Finished	2015-08-18 18:46:29 UTC
  Duration	01:36:12
```

Things I do not care about in the above example: the use of local time in the timestamp, which is an artifact of copying it from the CF CLI's log streaming format. Also, the fact that all the timestamps are the same, which is just me being lazy.

This format puts the timestamps in a nicely readable block along the left, which is fairly standard, and also leaves a distinct space for the reporting of duration, which is also valuable.",,101817596,story,[],bosh should have an option - or possibly a default - to add timestamps to deployment output,,[],956238,1196628,feature,2015-08-22T00:47:05Z,https://www.pivotaltracker.com/story/show/101817596
,2015-08-21T18:27:00Z,unscheduled,,"i.e. possible = same stemcell, net, etc.

We have seen BOSH create bunch of vms, fail on one, and then delete all the successfully created VMs since it cannot reuse them.

```
Started deleting unneeded vms
 Started deleting unneeded vms > 75b72caf-668a-4f51-a71d-32858073610b
 Started deleting unneeded vms > 4db5a31d-e05e-4d48-b5a9-05e3000d98b7
 Started deleting unneeded vms > c71c27d1-6d40-4729-937c-a2535967ee2f
 Started deleting unneeded vms > 03a278ce-3219-4996-94ea-cfe6ee09e9d4
 Started deleting unneeded vms > 2e5d42ad-90f7-4c8b-819b-33a73aac9769
 Started deleting unneeded vms > ec3fbe76-f340-4dda-bae1-a9f4687350e3
 Started deleting unneeded vms > afca2c66-d88c-4982-b6d7-d19cf0e11ce4
 Started deleting unneeded vms > 5f3e0e77-39b3-49f5-b73e-53b2d2dfa9d4
 Started deleting unneeded vms > 9ff720b3-30fd-4e8b-92e9-cb8bc5ca8359
 Started deleting unneeded vms > e55d4fd7-0146-493d-8ab8-66949c61a63e
 Started deleting unneeded vms > 161d24bd-50e5-4489-9e1a-2dea6e44e28f
 Started deleting unneeded vms > f211acc7-13f8-4018-aa13-6495d75fa989. Done (00:02:38)
    Done deleting unneeded vms > c71c27d1-6d40-4729-937c-a2535967ee2f (00:02:38)
    Done deleting unneeded vms > e55d4fd7-0146-493d-8ab8-66949c61a63e (00:02:38)
    Done deleting unneeded vms > 2e5d42ad-90f7-4c8b-819b-33a73aac9769 (00:02:40)
    Done deleting unneeded vms > 161d24bd-50e5-4489-9e1a-2dea6e44e28f (00:02:59)
    Done deleting unneeded vms > ec3fbe76-f340-4dda-bae1-a9f4687350e3 (00:03:00)
    Done deleting unneeded vms > 5f3e0e77-39b3-49f5-b73e-53b2d2dfa9d4 (00:03:10)
    Done deleting unneeded vms > 03a278ce-3219-4996-94ea-cfe6ee09e9d4 (00:03:15)
    Done deleting unneeded vms > afca2c66-d88c-4982-b6d7-d19cf0e11ce4 (00:03:15)
    Done deleting unneeded vms > 4db5a31d-e05e-4d48-b5a9-05e3000d98b7 (00:03:20)
    Done deleting unneeded vms > 75b72caf-668a-4f51-a71d-32858073610b (00:03:29)
    Done deleting unneeded vms > 9ff720b3-30fd-4e8b-92e9-cb8bc5ca8359 (00:03:57)
    Done deleting unneeded vms (00:03:57)

Started creating bound missing vms
 Started creating bound missing vms > nats-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > nats-partition-f00eeb0c999d6f1c2d79/0
 Started creating bound missing vms > consul_server-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > etcd_server-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > nfs_server-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > ccdb-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > uaadb-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > consoledb-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > ha_proxy-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > router-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > router-partition-f00eeb0c999d6f1c2d79/0
 Started creating bound missing vms > collector-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > mysql_proxy-partition-f83270581d7bf9ef476f/0
 Started creating bound missing vms > mysql-partition-f00eeb0c999d6f1c2d79/0
 Started creating bound missing vms > doppler-partition-f00eeb0c999d6f1c2d79/0
 Started creating bound missing vms > loggregator_trafficcontroller-partition-f00eeb0c999d6f1c2d79/0
```",,101791274,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",previously created vms for specific instances should be reused if possible,,[],956238,81882,feature,2015-12-14T21:21:57Z,https://www.pivotaltracker.com/story/show/101791274
,2015-08-20T21:41:59Z,unscheduled,,"Sometimes monit gets into broken state where it reports job as not running, although job is running. Running `monit reload` fixes the problem.

@dk is concerned that reloading monit during queued commands will clear the queue.",,101722260,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'monit', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-01T23:24:05Z', 'id': 12109048, 'updated_at': '2015-07-01T23:24:05Z'}]",Checking for job status should reload monit since it incorrecly thinks that job is not running,,[],956238,553935,feature,2015-09-07T22:30:57Z,https://www.pivotaltracker.com/story/show/101722260
,2015-08-18T17:25:47Z,unscheduled,,"In the task event log, each event currently has the total number of events. The CLI relies on this total to know when it's done with a stage and event rendering is weird if it's wrong. See below for an event log and broken task output.

(broken out as pre-existing from #100700270",,101519246,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",Director events should contain the correct 'total' in each event,,[],956238,687691,bug,2015-08-18T17:26:55Z,https://www.pivotaltracker.com/story/show/101519246
,2015-08-17T21:12:36Z,unscheduled,,,,101448536,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]","`bosh recreate unknown` should recreate VMs with ""unresponsive agent""",,[],956238,1550486,feature,2015-08-17T23:47:28Z,https://www.pivotaltracker.com/story/show/101448536
,2015-08-14T18:56:01Z,unscheduled,,,,101306774,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}, {'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",remove access to persistent_disk_pool and resource_pool in templates,,[],956238,81882,feature,2015-08-14T22:30:20Z,https://www.pivotaltracker.com/story/show/101306774
,2015-08-13T04:54:05Z,unscheduled,,mimics real vm better - hourly logrotate would work,0.0,101179464,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",run cron on bosh-lite stemcells,,[],956238,81882,feature,2015-11-11T23:24:36Z,https://www.pivotaltracker.com/story/show/101179464
,2015-08-10T17:30:56Z,unscheduled,,,,100957510,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",user running bosh instances while the deploy is running before any vms were created should have azs,,[],956238,81882,feature,2015-12-14T21:21:49Z,https://www.pivotaltracker.com/story/show/100957510
,2015-08-04T03:25:34Z,unscheduled,,"current: Error 150006: Job 'bosh' refers to an availability zone(s) '[""z1""]' but 'default' has no matching subnet(s).
wanted: Error 150006: Job 'bosh' refers to an availability zone(s) 'z1, z2' but network 'default' has no matching subnet(s).

current: Error 160007: Network 'default' refers to an unknown availability zone 'z3'
wanted: ???

current: Error 160008: top-level 'dns' invalid when specifying subnets
wanted: ???

current: Error 160008: top-level 'cloud_properties' invalid when specifying subnets
wanted: ???

current: Error 160007: Network 'private-dyn' refers to an unknown availability zone '' (no az keys on subnets)
wanted: ???",,100482020,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",improve az error messages,,[],956238,81882,feature,2015-08-21T16:39:07Z,https://www.pivotaltracker.com/story/show/100482020
,2015-08-05T19:33:30Z,unscheduled,,"When the CLI loses connection with director, we see:

```
prompt$ bosh deploy

[...]
Director task 40
[...]

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages
  Started compiling packages > postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce[WARNING] cannot access director, trying 4 more times...
[WARNING] cannot access director, trying 3 more times...
[WARNING] cannot access director, trying 2 more times...
[WARNING] cannot access director, trying 1 more times...
cannot access director (system lib)
prompt$
```

Usually when this happens, the task is still running and it is possible to continue monitoring it with `bosh task 40`. It may help CLI users recover from this situation if the error message mentioned the fact that the job is probably still running and which command can be used to check.",,100681390,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]","""cannot access director"" message could offer more helpful output ",,[],956238,1541728,feature,2015-08-10T00:05:21Z,https://www.pivotaltracker.com/story/show/100681390
,2015-08-04T03:32:10Z,unscheduled,,"Error 160001: Network `default' has overlapping subnets

```
networks:
- name: default
  type: manual
  subnets:
  - range: 10.10.0.0/24
    gateway: 10.10.0.1
    availability_zone: z1
    static:
    - 10.10.0.7
    - 10.10.0.8
    reserved:
    - 10.10.0.2 - 10.10.0.6
    - 10.10.0.20 - 10.10.0.254
    dns:
    - 10.10.0.6
    cloud_properties:
      subnet: subnet-f2744a86
  - range: 10.10.0.0/24
    gateway: 10.10.0.1
    availability_zone: z2
    reserved:
    - 10.10.0.2 - 10.10.0.20
    - 10.10.0.40 - 10.10.0.254
    dns:
    - 10.10.0.6
    cloud_properties:
      subnet: subnet-f2744a86
```",,100482168,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]",allow overlapping subnets in network definitions,,[],956238,81882,feature,2015-08-04T03:32:23Z,https://www.pivotaltracker.com/story/show/100482168
,2015-07-30T23:06:04Z,unscheduled,,"When doing a ""bosh create release"": Before downloading final versions of a package, check to see if a dev package already exists and is the same.",,100256162,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]","before downloading final versions of a package, check to see if a dev package already exists and is the same.",,[],956238,81882,feature,2015-07-30T23:06:05Z,https://www.pivotaltracker.com/story/show/100256162
,2015-07-23T21:49:44Z,unscheduled,,It takes too long!,2.0,99743356,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",`bosh create release` should not need to un-tar when copying license,,[],956238,1550486,feature,2015-07-30T20:02:46Z,https://www.pivotaltracker.com/story/show/99743356
,2015-06-23T17:57:45Z,unscheduled,,"Passing --help to a BOSH CLI command produces an incorrect Usage line but a correct list of option flags for that command:

```
% bosh vms --help
Usage: bosh [options]
        --details                    Return detailed VM information
        --dns                        Return VM DNS A records
        --vitals                     Return VM vitals information
```

Compare this without output of bosh help for the same command:

```
% bosh help vms
vms [<deployment_name>] [--details] [--dns] [--vitals]
    List all VMs in a deployment
    --details           Return detailed VM information
    --dns               Return VM DNS A records
    --vitals            Return VM vitals information
```",1.0,97656564,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",the output of bosh <cmd> --help should match bosh help <cmd>,,[],956238,1541728,feature,2015-07-30T20:02:52Z,https://www.pivotaltracker.com/story/show/97656564
,2015-05-07T14:07:06Z,unscheduled,,"We just did #93650844 which prints a deprecation warning when you use the `--all` flag with `bosh logs`. But it feels funny that the usage summary for `bosh logs` still shows the `--all` option:

```
$ bundle exec bosh logs --invalid
invalid option: --invalid
Usage: bosh        logs <job> [<index>] [--agent] [--job] [--only
       filter1,filter2,...] [--dir destination_directory] [--all]
       --agent                                                 fetch agent logs
       --job                                                   fetch job logs
       --only filter1,filter2,...                              only fetch logs that satisfy given filters (defined in job spec)
       --dir destination_directory                             download directory
       --all                                                   deprecated
```

We think it would be pretty easy to modify the code so it hides options whose description is 'deprecated'.

https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/command_handler.rb#L95 and https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/command_handler.rb#L76",1.0,94092144,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",hide deprecated options in CLI help output,,[],956238,1541728,feature,2015-07-30T20:02:54Z,https://www.pivotaltracker.com/story/show/94092144
,2015-04-29T21:48:45Z,unscheduled,,,1.0,93578618,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",remove release vs releases key in the deployment manifest,,[],956238,81882,feature,2015-07-30T20:03:00Z,https://www.pivotaltracker.com/story/show/93578618
,2014-10-20T22:47:24Z,unscheduled,,"We would like to run a single command and see if a stemcell is present in the blobstore.

Not parse the output of BOSH stemcells please.

bosh stemcells --name blah --version 666
returns the sha if present.",2.0,81063878,story,"[{'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",Should be able to check a stemcell name and version,,[],956238,548715,feature,2015-07-30T20:03:05Z,https://www.pivotaltracker.com/story/show/81063878
,2014-10-20T22:44:04Z,unscheduled,,"We would like to run a single command and see if a release is present in the blobstore.

Not parse the output of BOSH releases please.

bosh releases --name blah --version 666
returns the sha if present.",2.0,81063662,story,"[{'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",Should be able to check a release name and version,,[],956238,548715,feature,2015-07-30T20:03:06Z,https://www.pivotaltracker.com/story/show/81063662
,2015-06-29T21:34:16Z,unscheduled,,"Pending test in ip_reservation_spec (grep for story id)

This should work using legacy configuration and cloud config for:
 - deleting the original job
 - scaling down the original job
 - moving the original job to a new network",2.0,98057020,story,"[{'name': 'az', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:32:08Z', 'id': 11514476, 'updated_at': '2015-04-27T17:32:08Z'}]","If a single deploy removes a job and adds a new job which also uses all available IPs, the second job should successfully deploy (make sure we correctly release IPs before reallocating)",,[],956238,687691,feature,2015-12-14T21:22:02Z,https://www.pivotaltracker.com/story/show/98057020
,2015-04-15T23:30:04Z,unscheduled,,"The deployment SHA1 is calculated by re-reading the manifest at the end of the deployment, which means it will be a match for the next attempt, rather that the first.",,92575514,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'interview', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-16T22:14:16Z', 'id': 10275020, 'updated_at': '2014-12-16T22:14:16Z'}]",Changing manifest in the middle of the deployment should not skip a subsequent deployment,,[],956238,553935,bug,2015-07-30T20:03:22Z,https://www.pivotaltracker.com/story/show/92575514
,2015-07-23T17:23:45Z,unscheduled,,"the index parameter in the help shows as optional. We expect all instances when not specified, but instead it rejects the command.",,99719108,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",'bosh recreate <job>' with no index should recreate all the instances,,[],956238,687691,feature,2015-07-31T01:20:20Z,https://www.pivotaltracker.com/story/show/99719108
,2015-07-21T18:33:49Z,unscheduled,,"was trying to deploy cf and diego at the same time, looks like maybe it was a locking issue over a shared resource or something?  the error was not helpful.   it worked after trying a few more times.

```
ruby 2.1.4p265 wlan-10-81-130-194 in ~/workspace/diego-release
± uu+zr |develop ✓| → bosh -d ~/deployments/bosh-lite/diego.yml -n deploy

Processing deployment manifest
------------------------------
Getting deployment properties from director...
Unable to get properties list from director, trying without it...
Compiling deployment manifest...
Cannot get current deployment information from director, possibly a new deployment

Deploying
---------
Deployment name: `diego.yml'
Director name: `Bosh Lite Director'

Director task 4
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Failed: undefined method `split' for nil:NilClass (00:00:10)

Error 100: undefined method `split' for nil:NilClass

Task 4 error

For a more detailed error report, run: bosh task 4 --debug

ruby 2.1.4p265 wlan-10-81-130-194 in ~/workspace/diego-release
± uu+zr |develop ✓| → mate ~/deployments/bosh-lite/diego.yml

ruby 2.1.4p265 wlan-10-81-130-194 in ~/workspace/diego-release
± uu+zr |develop ✓| → bosh releases

+-------+----------------+-------------+
| Name  | Versions       | Commit Hash |
+-------+----------------+-------------+
| cf    | 213+dev.16*    | 5491f87e    |
| diego | 0.1386.0+dev.1 | d98dafe6    |
+-------+----------------+-------------+
(*) Currently deployed

Releases total: 2

ruby 2.1.4p265 wlan-10-81-130-194 in ~/workspace/diego-release
± uu+zr |develop ✓| → bosh task 4 --debug

Director task 4
I, [2015-07-21T18:26:06.410390 #1345] [0x3ff5b600f320]  INFO -- TaskHelper: Director Version: 1.2922.0
I, [2015-07-21T18:26:06.410524 #1345] [0x3ff5b600f320]  INFO -- TaskHelper: Enqueuing task: 4
I, [2015-07-21 18:26:09 #6443] []  INFO -- DirectorJobRunner: Looking for task with task id 4
D, [2015-07-21 18:26:09 #6443] [] DEBUG -- DirectorJobRunner: (0.000844s) SELECT * FROM ""tasks"" WHERE ""id"" = 4
I, [2015-07-21 18:26:09 #6443] []  INFO -- DirectorJobRunner: Starting task: 4
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Creating job
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Reading deployment manifest
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Manifest:
---

............... redacted manifest for length ...............

D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000656s) SELECT * FROM ""cloud_configs"" WHERE (""id"" IS NULL) LIMIT 1
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Cloud Config:
nil
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Creating deployment plan
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Deployment plan options: {}

I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Created deployment plan
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000440s) SELECT * FROM ""tasks"" WHERE ""id"" = 4
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Performing task: 4
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000258s) BEGIN
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000386s) UPDATE ""tasks"" SET ""state"" = 'processing', ""timestamp"" = '2015-07-21 18:26:09.912508+0000', ""description"" = 'create deployment', ""result"" = NULL, ""output"" = '/var/vcap/store/director/tasks/4', ""checkpoint_time"" = '2015-07-21 18:26:09.912586+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 4)
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.003050s) COMMIT
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Acquiring deployment lock on cf-warden-diego
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Acquiring lock: lock:deployment:cf-warden-diego
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Acquired lock: lock:deployment:cf-warden-diego
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Preparing deployment
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: sending update deployment start event
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""6d3e4f32-4256-4136-9349-0649d01064d4"",""severity"":4,""title"":""director - begin update deployment"",""summary"":""Begin update deployment for cf-warden-diego against Director 737bd8ee-f4f9-4aec-9b82-80f3d0be26f1"",""created_at"":1437503169}
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Preparing deployment
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Binding deployment
D, [2015-07-21 18:26:09 #6443] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf-warden-diego
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.020423s) BEGIN
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000936s) SELECT * FROM ""deployments"" WHERE (""name"" = 'cf-warden-diego') LIMIT 1
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000241s) SELECT * FROM ""deployments""
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000388s) SELECT COUNT(*) AS ""count"" FROM ""deployments"" WHERE (""name"" = 'cf-warden-diego') LIMIT 1
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.007345s) INSERT INTO ""deployments"" (""name"") VALUES ('cf-warden-diego') RETURNING *
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.013648s) COMMIT
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Binding releases
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Acquiring release lock: cf
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Acquiring release lock: diego
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Acquiring lock: lock:release:cf
D, [2015-07-21 18:26:09 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:09 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989 expires in 8.434521198272705 seconds
D, [2015-07-21 18:26:10 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:10 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989 expires in 7.933710336685181 seconds
D, [2015-07-21 18:26:10 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:10 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989 expires in 7.43293309211731 seconds
D, [2015-07-21 18:26:11 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:11 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989 expires in 6.9319071769714355 seconds
D, [2015-07-21 18:26:11 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:11 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503178.4015937:24181a3f-2d7e-486e-acae-3983db41e989 expires in 6.430916786193848 seconds
D, [2015-07-21 18:26:12 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:12 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 10.931627988815308 seconds
D, [2015-07-21 18:26:12 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:12 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 10.43046236038208 seconds
D, [2015-07-21 18:26:13 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:13 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 9.928990602493286 seconds
D, [2015-07-21 18:26:13 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:13 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 9.428175210952759 seconds
D, [2015-07-21 18:26:14 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:14 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 8.927094459533691 seconds
D, [2015-07-21 18:26:14 #6443] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf-warden-diego
D, [2015-07-21 18:26:14 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:14 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 8.426350831985474 seconds
D, [2015-07-21 18:26:15 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:15 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 7.925273656845093 seconds
D, [2015-07-21 18:26:15 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:15 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 7.424369812011719 seconds
D, [2015-07-21 18:26:16 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:16 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 6.923417091369629 seconds
D, [2015-07-21 18:26:16 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:16 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503183.4032571:24181a3f-2d7e-486e-acae-3983db41e989 expires in 6.422495603561401 seconds
D, [2015-07-21 18:26:17 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:17 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 10.922900915145874 seconds
D, [2015-07-21 18:26:17 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:17 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 10.422120094299316 seconds
D, [2015-07-21 18:26:18 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:18 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 9.911277294158936 seconds
D, [2015-07-21 18:26:18 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:18 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 9.410297632217407 seconds
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:19 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 8.909305572509766 seconds
D, [2015-07-21 18:26:19 #6443] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf-warden-diego
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: Lock lock:release:cf is already locked by someone else: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989
I, [2015-07-21 18:26:19 #6443] [task:4]  INFO -- DirectorJobRunner: Lock: 1437503188.4045334:24181a3f-2d7e-486e-acae-3983db41e989 expires in 8.408352136611938 seconds
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: Deleting lock: lock:release:diego
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:cf-warden-diego
D, [2015-07-21 18:26:19 #6443] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:cf-warden-diego
I, [2015-07-21 18:26:19 #6443] [task:4]  INFO -- DirectorJobRunner: sending update deployment error event
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {""id"":""600ca143-4e46-4de0-a321-63409ea69e07"",""severity"":3,""title"":""director - error during update deployment"",""summary"":""Error during update deployment for cf-warden-diego against Director 737bd8ee-f4f9-4aec-9b82-80f3d0be26f1: #<NoMethodError: undefined method `split' for nil:NilClass>"",""created_at"":1437503179}
E, [2015-07-21 18:26:19 #6443] [task:4] ERROR -- DirectorJobRunner: undefined method `split' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock.rb:113:in `delete'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock.rb:68:in `release'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:72:in `block in with_release_locks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:72:in `reverse_each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:72:in `ensure in with_release_locks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:72:in `with_release_locks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/deployment_plan/assembler.rb:26:in `bind_releases'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/deployment_plan/preparer.rb:17:in `block in prepare'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/base_job.rb:48:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/base_job.rb:46:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/deployment_plan/preparer.rb:16:in `prepare'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/update_deployment.rb:52:in `prepare'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/update_deployment.rb:85:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock.rb:56:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/update_deployment.rb:82:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2922.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2922.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-07-21 18:26:19 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000264s) BEGIN
D, [2015-07-21 18:26:20 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000460s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-07-21 18:26:19.999396+0000', ""description"" = 'create deployment', ""result"" = 'undefined method `split'' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/4', ""checkpoint_time"" = '2015-07-21 18:26:09.912586+0000', ""type"" = 'update_deployment', ""username"" = 'admin' WHERE (""id"" = 4)
D, [2015-07-21 18:26:20 #6443] [task:4] DEBUG -- DirectorJobRunner: (0.000779s) COMMIT
I, [2015-07-21 18:26:20 #6443] []  INFO -- DirectorJobRunner: Task took 10.205815874 seconds to process.

Task 4 error
```",,99544760,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",unhelpful error message,,[],956238,1139722,bug,2015-07-22T17:46:02Z,https://www.pivotaltracker.com/story/show/99544760
,2015-07-21T00:44:06Z,unscheduled,,"```
I, [2015-07-21T00:13:28.723575 #161]  INFO -- : Requirement bosh-openstack-kvm-ubuntu-trusty-go_agent-3016
at depth 0 - 18: self signed certificate
I, [2015-07-21T00:13:29.081465 #161]  INFO -- : stemcell not uploaded
I, [2015-07-21T00:13:29.081596 #161]  INFO -- : Running bosh command --> bundle exec bosh --non-interactive -P 1 --config /tmp/bosh_config20150721-161-nsb391 --user SOMETHING --password SOMETHING upload stemcell /tmp/build/837124a5-3673-4403-603e-9fc5097e0d7c/stemcell/stemcell.tgz 2>&1
I, [2015-07-21T00:30:27.935453 #161]  INFO -- :
Verifying stemcell...
File exists and readable                                     OK
Verifying tarball...
Read tarball                                                 OK
Manifest exists                                              OK
Stemcell image file                                          OK
Stemcell properties                                          OK

Stemcell info
-------------
Name:    bosh-openstack-kvm-ubuntu-trusty-go_agent
Version: 3016

Checking if stemcell already exists...
No

Uploading stemcell...

[WARNING] cannot access director, trying 4 more times... 3.0MB/s ETA:  00:00:19
stemcell.tgz:  100% |oooooooooooooooooooooooo| 533.8MB 539.2KB/s Time: 00:16:53

Director task 1
  Started update stemcell
  Started update stemcell > Extracting stemcell archive. Failed: Extracting stemcell archive failed. Check task debug log for details. (00:00:00)

Error 50000: Extracting stemcell archive failed. Check task debug log for details.

Task 1 error

For a more detailed error report, run: bosh task 1 --debug

I, [2015-07-21T00:30:27.937139 #161]  INFO -- : Starting cleanup bosh-openstack-kvm-ubuntu-trusty-go_agent-3016
at depth 0 - 18: self signed certificate
I, [2015-07-21T00:30:28.427162 #161]  INFO -- : Cleaned up bosh-openstack-kvm-ubuntu-trusty-go_agent-3016
I, [2015-07-21T00:30:28.427490 #161]  INFO -- : Starting cleanup bat-2
I, [2015-07-21T00:30:28.496999 #161]  INFO -- : Cleaned up bat-2


Finished in 17 minutes 4 seconds (files took 0.23428 seconds to load)
0 examples, 0 failures
/home/vcap/.gem/ruby/2.1.2/gems/rspec-expectations-3.0.4/lib/rspec/expectations/fail_with.rb:30:in `fail_with': expected command to exit with 0 but was 1. output was (RSpec::Expectations::ExpectationNotMetError)

Verifying stemcell...
File exists and readable                                     OK
Verifying tarball...
Read tarball                                                 OK
Manifest exists                                              OK
Stemcell image file                                          OK
Stemcell properties                                          OK

Stemcell info
-------------
Name:    bosh-openstack-kvm-ubuntu-trusty-go_agent
Version: 3016

Checking if stemcell already exists...
No

Uploading stemcell...

[WARNING] cannot access director, trying 4 more times... 3.0MB/s ETA:  00:00:19
stemcell.tgz:  100% |oooooooooooooooooooooooo| 533.8MB 539.2KB/s Time: 00:16:53

Director task 1
  Started update stemcell
  Started update stemcell > Extracting stemcell archive. Failed: Extracting stemcell archive failed. Check task debug log for details. (00:00:00)

Error 50000: Extracting stemcell archive failed. Check task debug log for details.

Task 1 error

For a more detailed error report, run: bosh task 1 --debug
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-expectations-3.0.4/lib/rspec/expectations/handler.rb:37:in `handle_failure'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-expectations-3.0.4/lib/rspec/expectations/handler.rb:48:in `handle_matcher'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-expectations-3.0.4/lib/rspec/expectations/expectation_target.rb:54:in `to'
	from /tmp/build/837124a5-3673-4403-603e-9fc5097e0d7c/bats/lib/bat/requirements.rb:78:in `require_stemcell'
	from /tmp/build/837124a5-3673-4403-603e-9fc5097e0d7c/bats/lib/bat/requirements.rb:29:in `requirement'
	from /tmp/build/837124a5-3673-4403-603e-9fc5097e0d7c/bats/spec/system/spec_helper.rb:67:in `block (2 levels) in <top (required)>'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/example.rb:294:in `instance_exec'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/example.rb:294:in `instance_exec'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/hooks.rb:349:in `run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/hooks.rb:410:in `block in run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/hooks.rb:410:in `each'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/hooks.rb:410:in `run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/hooks.rb:485:in `run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/runner.rb:111:in `block in run_specs'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/reporter.rb:54:in `report'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/runner.rb:108:in `run_specs'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/runner.rb:86:in `run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/runner.rb:70:in `run'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/lib/rspec/core/runner.rb:38:in `invoke'
	from /home/vcap/.gem/ruby/2.1.2/gems/rspec-core-3.0.4/exe/rspec:4:in `<top (required)>'
	from /home/vcap/.gem/ruby/2.1.2/bin/rspec:23:in `load'
	from /home/vcap/.gem/ruby/2.1.2/bin/rspec:23:in `<main>'
failed


Director task 1
I, [2015-07-21T00:30:25.322145 #25488] [0x3f94e8b41330]  INFO -- TaskHelper: Director Version: 1.3016.0
I, [2015-07-21T00:30:25.322234 #25488] [0x3f94e8b41330]  INFO -- TaskHelper: Enqueuing task: 1
I, [2015-07-21 00:30:26 #25776] []  INFO -- DirectorJobRunner: Looking for task with task id 1
D, [2015-07-21 00:30:26 #25776] [] DEBUG -- DirectorJobRunner: (0.000824s) SELECT NULL
D, [2015-07-21 00:30:26 #25776] [] DEBUG -- DirectorJobRunner: (0.001024s) SELECT * FROM ""tasks"" WHERE ""id"" = 1
I, [2015-07-21 00:30:26 #25776] []  INFO -- DirectorJobRunner: Starting task: 1
I, [2015-07-21 00:30:26 #25776] [task:1]  INFO -- DirectorJobRunner: Creating job
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000147s) SELECT NULL
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000213s) SELECT * FROM ""tasks"" WHERE ""id"" = 1
I, [2015-07-21 00:30:26 #25776] [task:1]  INFO -- DirectorJobRunner: Performing task: 1
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000119s) SELECT NULL
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000062s) BEGIN
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000404s) UPDATE ""tasks"" SET ""state"" = 'processing', ""timestamp"" = '2015-07-21 00:30:26.350229+0000', ""description"" = 'create stemcell', ""result"" = NULL, ""output"" = '/var/vcap/store/director/tasks/1', ""checkpoint_time"" = '2015-07-21 00:30:26.350366+0000', ""type"" = 'update_stemcell', ""username"" = 'SOMETHING' WHERE (""id"" = 1)
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.002159s) COMMIT
I, [2015-07-21 00:30:26 #25776] [task:1]  INFO -- DirectorJobRunner: Processing update stemcell
I, [2015-07-21 00:30:26 #25776] [task:1]  INFO -- DirectorJobRunner: Extracting stemcell archive
E, [2015-07-21 00:30:26 #25776] [task:1] ERROR -- DirectorJobRunner: Extracting stemcell archive failed in dir /var/vcap/data/tmp/director/stemcell20150721-25776-1m1m709, tar returned 2, output:
gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now

E, [2015-07-21 00:30:26 #25776] [task:1] ERROR -- DirectorJobRunner: Extracting stemcell archive failed. Check task debug log for details.
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/jobs/update_stemcell.rb:48:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/jobs/base_job.rb:48:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/jobs/base_job.rb:46:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/jobs/update_stemcell.rb:42:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3016.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3016.0/bin/bosh-director-worker:75:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000296s) SELECT NULL
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000075s) BEGIN
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.000352s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-07-21 00:30:26.372826+0000', ""description"" = 'create stemcell', ""result"" = 'Extracting stemcell archive failed. Check task debug log for details.', ""output"" = '/var/vcap/store/director/tasks/1', ""checkpoint_time"" = '2015-07-21 00:30:26.350366+0000', ""type"" = 'update_stemcell', ""username"" = 'SOMETHING' WHERE (""id"" = 1)
D, [2015-07-21 00:30:26 #25776] [task:1] DEBUG -- DirectorJobRunner: (0.002180s) COMMIT
I, [2015-07-21 00:30:26 #25776] []  INFO -- DirectorJobRunner: Task took 0.029113342 seconds to process.

Task 1 error
```",,99477154,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",the bosh CLI should not report 100% completion if a connection error occurs,,[],956238,1406536,bug,2015-07-21T00:50:53Z,https://www.pivotaltracker.com/story/show/99477154
,2015-07-20T19:11:13Z,unscheduled,,https://gist.github.com/calebamiles/6a8453a7d21eb8497a22,,99449176,story,[],building the aws cli plugin can segfault,,[],956238,1406536,bug,2015-07-20T19:11:13Z,https://www.pivotaltracker.com/story/show/99449176
,2015-07-18T00:32:19Z,unscheduled,,"When deploying using  manifest + cloud config, if the version of the stemcells in the cloud config is `latest` it throws error:

Error 50003: Stemcell `bosh-aws-xen-hvm-ubuntu-trusty-go_agent/latest' doesn't exist

This behavior disappears when the version of the stemcells in the cloudconfig is changed to the number of a stemcell",,99346750,story,[],stemcell version does not resolve 'version: latest' when using cloud-config,,[],956238,1687782,bug,2015-07-18T00:33:43Z,https://www.pivotaltracker.com/story/show/99346750
,2015-07-16T20:30:29Z,unscheduled,,"To reproduce:

1. get latest bosh-lite
1. deploy cf-release (but maybe anything)
1. delete deployment
1. roughly 1 in 3 times, one of the “VMs” hangs forever during delete

@dk thinks it might be because BOSH does not call stop before destroying the warden containers, or older kernel.

/cc @dwendorf @jpalermo",,99258814,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]","deleting deployment on bosh lite occasionally hangs on deleting a “VM""",,[],956238,668075,bug,2015-07-16T22:14:33Z,https://www.pivotaltracker.com/story/show/99258814
,2015-07-13T13:33:44Z,unscheduled,,,,98936680,story,"[{'name': 'compiled-releases', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-27T17:17:25Z', 'id': 11514344, 'updated_at': '2015-04-27T17:17:25Z'}]",Export a release that is not referenced by the targeted deployment,,[],956238,948679,feature,2015-07-13T13:33:44Z,https://www.pivotaltracker.com/story/show/98936680
,2015-07-07T00:54:23Z,unscheduled,,http_proxy in bundle exec rake stemcell:build[..] as per bosh-stemcell readme command causes problems,,98516608,story,[],remove http_proxy from stemcell:build,,[],956238,1569704,chore,2015-07-07T00:55:10Z,https://www.pivotaltracker.com/story/show/98516608
,2015-07-01T23:24:05Z,unscheduled,,"```
[UTC Jul  1 22:42:25] info     : 'postgres' trying to restart
[UTC Jul  1 22:42:25] info     : 'postgres' start: /var/vcap/jobs/postgres/bin/postgres_ctl
[UTC Jul  1 22:42:39] info     : 'blobstore_nginx' start: /var/vcap/jobs/blobstore/bin/nginx_ctl
[UTC Jul  1 22:42:40] info     : 'blobstore_nginx' start action done
[UTC Jul  1 22:42:40] info     : 'director' start: /var/vcap/jobs/director/bin/director_ctl
[UTC Jul  1 22:43:10] error    : 'director' failed to start                                        # <-------------------------------------------------------------------------
[UTC Jul  1 22:43:10] info     : 'worker_1' start: /var/vcap/jobs/director/bin/worker_ctl
[UTC Jul  1 22:43:13] info     : 'worker_2' start: /var/vcap/jobs/director/bin/worker_ctl
[UTC Jul  1 22:43:14] info     : 'worker_3' start: /var/vcap/jobs/director/bin/worker_ctl
[UTC Jul  1 22:43:15] info     : 'director' start action done                                   # <-------------------------------------------------------------------------
[UTC Jul  1 22:43:15] info     : 'worker_1' start action done
[UTC Jul  1 22:43:15] info     : 'worker_2' start action done
[UTC Jul  1 22:43:15] info     : 'worker_3' start action done
[UTC Jul  1 22:43:15] info     : 'director_scheduler' start: /var/vcap/jobs/director/bin/scheduler_ctl
[UTC Jul  1 22:43:16] info     : 'director_scheduler' start action done
[UTC Jul  1 22:43:16] info     : 'director_nginx' start: /var/vcap/jobs/director/bin/nginx_ctl
[UTC Jul  1 22:43:17] info     : 'director_nginx' start action done
[UTC Jul  1 22:43:17] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl
[UTC Jul  1 22:43:18] info     : 'health_monitor' start action done
[UTC Jul  1 22:43:18] info     : Awakened by User defined signal 1
[UTC Jul  1 22:43:18] info     : 'nats' start action done
[UTC Jul  1 22:43:18] info     : 'redis' start action done
[UTC Jul  1 22:43:18] info     : 'postgres' start action done
[UTC Jul  1 22:43:18] info     : 'nats' process is running with pid 24732
[UTC Jul  1 22:43:18] info     : 'redis' process is running with pid 24740
[UTC Jul  1 22:43:18] info     : 'postgres' process is running with pid 24834
```",,98238996,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'monit', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-01T23:24:05Z', 'id': 12109048, 'updated_at': '2015-07-01T23:24:05Z'}]",monit thought that process failed to start even though it was happily running,,[],956238,81882,bug,2015-07-01T23:24:07Z,https://www.pivotaltracker.com/story/show/98238996
,2015-07-01T21:17:30Z,unscheduled,,"right now, if extraction fails, we try to clean up the temp dir, and if that fails we ignore the error
",,98231122,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Do we care about failing to remove the temp directory after extracting a stemcell in bosh-init,,[],956238,687691,feature,2015-07-01T21:24:06Z,https://www.pivotaltracker.com/story/show/98231122
,2015-07-01T21:14:43Z,unscheduled,,"right now we ignore any errors inside of the deferred compressor.CleanUp
",,98230950,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Do we care about failing to clean up tarballs after rendering jobs in bosh-init?,,[],956238,687691,feature,2015-07-01T21:24:06Z,https://www.pivotaltracker.com/story/show/98230950
,2015-07-01T21:11:42Z,unscheduled,,right now we're ignoring any errors on the deferred File.close,,98230718,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Do we care about failing to close a file after calculating its sha1 in bosh-init,,[],956238,687691,feature,2015-07-01T21:24:06Z,https://www.pivotaltracker.com/story/show/98230718
,2015-06-26T15:22:14Z,unscheduled,,"I was unable to run an errand because the VM couldn't be started up due to not enough resources.  But the error I get from the server is ""undefined method cid for nil"", which happens during the attempt to delete the errand VM (that never existed).

Stack:
```
E, [2015-06-26 15:12:40 #6923] [create_vm(c989fb86-6df3-4a5c-82ab-fd46683b24c5, ...)] ERROR -- DirectorJobRunner: Server `125a008e-563a-4cc4-97a5-cc4891dadce4' state is error, expected active
W, [2015-06-26 15:12:40 #6923] [create_vm(c989fb86-6df3-4a5c-82ab-fd46683b24c5, ...)]  WARN -- DirectorJobRunner: Failed to create server: Server `125a008e-563a-4cc4-97a5-cc4891dadce4' state is error, expected active
D, [2015-06-26 15:12:40 #6923] [create_vm(c989fb86-6df3-4a5c-82ab-fd46683b24c5, ...)] DEBUG -- DirectorJobRunner: excon.request {:chunk_size=>1048576, :ciphers=>""HIGH:!SSLv2:!aNULL:!eNULL:!3DES"", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{""User-Agent""=>""fog/1.27.0 fog-core/1.27.4"", ""Content-Type""=>""application/json"", ""Accept""=>""application/json"", ""X-Auth-Token""=>""566b1f975f944aa2aed183a001e16a5c"", ""Host""=>""198.11.195.2:8774""}, :idempotent=>false, :instrumentor_name=>""excon"", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>true, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>""excon/0.43.0 (x86_64-linux) ruby/2.1.4"", :write_timeout=>60, :host=>""198.11.195.2"", :path=>""/v2/24c44f79033a4fadb720ea6f6d9dbe0b/servers/125a008e-563a-4cc4-97a5-cc4891dadce4"", :port=>8774, :query=>nil, :scheme=>""http"", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor, :expects=>204, :method=>""DELETE"", :retries_remaining=>4, :connection=>#<Excon::Connection:7f841ac77888 @data={:chunk_size=>1048576, :ciphers=>""HIGH:!SSLv2:!aNULL:!eNULL:!3DES"", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{""User-Agent""=>""fog/1.27.0 fog-core/1.27.4""}, :idempotent=>false, :instrumentor_name=>""excon"", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>true, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>""excon/0.43.0 (x86_64-linux) ruby/2.1.4"", :write_timeout=>60, :host=>""198.11.195.2"", :path=>"""", :port=>8774, :query=>nil, :scheme=>""http"", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor} @socket_key=""http://198.11.195.2:8774"">, :stack=>#<Excon::Middleware::ResponseParser:0x007f8417e585b0 @stack=#<Excon::Middleware::Expects:0x007f8417e589e8 @stack=#<Excon::Middleware::Idempotent:0x007f8417e58f38 @stack=#<Excon::Middleware::Instrumentor:0x007f8417e59280 @stack=#<Excon::Middleware::Mock:0x007f8417e59398 @stack=#<Excon::Connection:7f841ac77888 @data={:chunk_size=>1048576, :ciphers=>""HIGH:!SSLv2:!aNULL:!eNULL:!3DES"", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{""User-Agent""=>""fog/1.27.0 fog-core/1.27.4""}, :idempotent=>false, :instrumentor_name=>""excon"", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>true, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>""excon/0.43.0 (x86_64-linux) ruby/2.1.4"", :write_timeout=>60, :host=>""198.11.195.2"", :path=>"""", :port=>8774, :query=>nil, :scheme=>""http"", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor} @socket_key=""http://198.11.195.2:8774"">>>>>>}
D, [2015-06-26 15:12:40 #6923] [create_vm(c989fb86-6df3-4a5c-82ab-fd46683b24c5, ...)] DEBUG -- DirectorJobRunner: excon.response {:body=>"""", :headers=>{""Content-Length""=>""0"", ""Content-Type""=>""application/json"", ""X-Compute-Request-Id""=>""req-f92ed6f6-be4b-4c16-9bc6-6c03b340969c"", ""Date""=>""Fri, 26 Jun 2015 15:12:40 GMT""}, :status=>204, :reason_phrase=>""No Content"", :remote_ip=>""198.11.195.2"", :local_port=>45453, :local_address=>""192.168.111.10""}
E, [2015-06-26 15:12:40 #6923] [create_missing_vm(push-console, 0/1)] ERROR -- DirectorJobRunner: failed to create VM, retrying (5)
E, [2015-06-26 15:12:40 #6923] [create_missing_vm(push-console, 0/1)] ERROR -- DirectorJobRunner: error creating vm: Server `125a008e-563a-4cc4-97a5-cc4891dadce4' state is error, expected active
I, [2015-06-26 15:12:40 #6923] [create_missing_vm(push-console, 0/1)]  INFO -- DirectorJobRunner: Cleaning up the created VM due to an error: Server `125a008e-563a-4cc4-97a5-cc4891dadce4' state is error, expected active
D, [2015-06-26 15:12:40 #6923] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: Server `125a008e-563a-4cc4-97a5-cc4891dadce4' state is error, expected active - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2980.0/lib/cloud/openstack/cloud.rb:308:in `rescue in block in create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2980.0/lib/cloud/openstack/cloud.rb:298:in `block in create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_openstack_cpi-1.2980.0/lib/cloud/openstack/cloud.rb:217:in `create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/vm_creator.rb:41:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/resource_pool_updater.rb:51:in `create_missing_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/resource_pool_updater.rb:34:in `block (4 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/resource_pool_updater.rb:32:in `block (3 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/resource_pool_updater.rb:31:in `block (2 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
D, [2015-06-26 15:12:40 #6923] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up
D, [2015-06-26 15:12:40 #6923] [task:124] DEBUG -- DirectorJobRunner: Shutting down pool
I, [2015-06-26 15:12:40 #6923] [task:124]  INFO -- DirectorJobRunner: Deleting instances
I, [2015-06-26 15:12:40 #6923] [task:124]  INFO -- DirectorJobRunner: Starting to delete job instances
I, [2015-06-26 15:12:40 #6923] [task:124]  INFO -- DirectorJobRunner: Deleting errand instances
D, [2015-06-26 15:12:40 #6923] [task:124] DEBUG -- DirectorJobRunner: Creating new thread
D, [2015-06-26 15:12:40 #6923] [task:124] DEBUG -- DirectorJobRunner: Waiting for tasks to complete
D, [2015-06-26 15:12:40 #6923] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: undefined method `cid' for nil:NilClass - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/instance_deleter.rb:31:in `delete_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2980.0/lib/bosh/director/instance_deleter.rb:21:in `block (3 levels) in delete_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2980.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
```",,97901160,story,[],Openstack CPI gives a terrible error when an errand fails to run,,[],956238,1408486,bug,2015-06-30T18:04:56Z,https://www.pivotaltracker.com/story/show/97901160
,2015-06-29T17:39:45Z,unscheduled,,"In the sentence ""Started applying problem resolutions > missing_vm 128: Recreate VM using last known apply spec. Done (00:00:10)"", only ""missing_vm 128: Recreate VM using last known apply spec"" is green. The period at the end of that sentence is white.

Acceptance criteria:
The period is green",,98032234,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",cli output should color problem resolution sentences with one color,,[],956238,951283,bug,2015-06-29T17:39:48Z,https://www.pivotaltracker.com/story/show/98032234
,2015-06-24T18:24:52Z,unscheduled,,"related: https://www.pivotaltracker.com/story/show/88754896

```
  Started deploy micro bosh
  Started deploy micro bosh > Unpacking stemcell. Done (00:00:14)
  Started deploy micro bosh > Uploading stemcell. Done (00:01:36)
  Started deploy micro bosh > Creating VM from sc-6d61b3b9-60b6-417c-833a-fc5986d3e230. Done (00:02:01)
at depth 0 - 20: unable to get local issuer certificate
  Started deploy micro bosh > Waiting for the agent. Done (00:00:31)
at depth 0 - 20: unable to get local issuer certificate
  Started deploy micro bosh > Updating persistent disk
at depth 0 - 20: unable to get local issuer certificate
  Started deploy micro bosh > Create disk. Done (00:00:18)
at depth 0 - 20: unable to get local issuer certificate
  Started deploy micro bosh > Mount diskat depth 0 - 20: unable to get local issuer certificate
at depth 0 - 20: unable to get local issuer certificate
at depth 0 - 20: unable to get local issuer certificate
at depth 0 - 20: unable to get local issuer certificate
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/agent_client-1.2989.0/lib/agent_client/base.rb:21:in `method_missing': {""message""=>""Action Failed get_task: Task ae07e0ee-0592-4f57-5a77-36c727bbcdef result: Mounting persistent disk: Partitioning disk: Shelling out to sfdisk: Running command: 'sfdisk -uM /dev/sdc', stdout: '', stderr: 'Checking that no-one is using this disk right now ...\nBLKRRPART: Device or resource busy\n\nThis disk is currently in use - repartitioning is probably a bad idea.\nUmount all file systems, and swapoff all swap partitions on this disk.\nUse the --no-reread flag to suppress this check.\nUse the --force flag to overrule all checks.\n': exit status 1""} (Bosh::Agent::HandlerError)
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/agent_client-1.2989.0/lib/agent_client/base.rb:12:in `run_task'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:254:in `block in mount_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:253:in `mount_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:317:in `attach_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:353:in `update_persistent_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:137:in `block in create'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:136:in `create'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2989.0/lib/bosh/cli/commands/micro.rb:179:in `perform'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2989.0/lib/cli/command_handler.rb:57:in `run'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2989.0/lib/cli/runner.rb:56:in `run'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2989.0/bin/bosh:16:in `<top (required)>'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/bin/bosh:23:in `load'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/bin/bosh:23:in `<main>'
{""type"": ""step_finished"", ""id"": ""microbosh.deploying""}
Exited with 1.
```",,97753210,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should retry disk partitioning when sfdisk is complaining,,[],956238,694729,feature,2015-06-24T19:23:50Z,https://www.pivotaltracker.com/story/show/97753210
,2015-06-24T17:09:56Z,unscheduled,,"If a deployment has a manifest but that manifest doesn't define the errand, the user gets a nice error.

But if the deployment has no manifest in the database (the deploy failed, or the user is targeting a deploy that is still running) running the errand blows up really confusingly before it gets far enough to give the nice error:

```
I, [2015-06-24T10:11:51.626819 #60437]  INFO :         ******************** start Director task 5 ********************
        Acting as user 'test' on 'Test Director'

Director task 5
I, [2015-06-24T10:11:46.878494 #60715] [0x3ff8c9465be8]  INFO -- TaskHelper: Director Version: 1.2999.0
I, [2015-06-24T10:11:46.878516 #60715] [0x3ff8c9465be8]  INFO -- TaskHelper: Enqueuing task: 5
I, [2015-06-24 10:11:48 #61485] []  INFO -- DirectorJobRunner: Looking for task with task id 5
D, [2015-06-24 10:11:48 #61485] [] DEBUG -- DirectorJobRunner: (0.000372s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [] DEBUG -- DirectorJobRunner: (0.000572s) SELECT * FROM ""tasks"" WHERE ""id"" = 5
I, [2015-06-24 10:11:48 #61485] []  INFO -- DirectorJobRunner: Starting task: 5
I, [2015-06-24 10:11:48 #61485] [task:5]  INFO -- DirectorJobRunner: Creating job
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000295s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000295s) SELECT * FROM ""tasks"" WHERE ""id"" = 5
I, [2015-06-24 10:11:48 #61485] [task:5]  INFO -- DirectorJobRunner: Performing task: 5
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000344s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000204s) BEGIN
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000646s) UPDATE ""tasks"" SET ""state"" = 'processing', ""timestamp"" = '2015-06-24 10:11:48.897346-0700', ""description"" = 'run errand errand from deployment second', ""result"" = NULL, ""output"" = '/Users/pivotal/workspace/bosh/tmp/integration-tests-workspace/pid-60437/sandbox/boshdir/tasks/5', ""checkpoint_time"" = '2015-06-24 10:11:48.897477-0700', ""type"" = 'run_errand', ""username"" = 'test' WHERE (""id"" = 5)
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.005789s) COMMIT
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000268s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000135s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000383s) SELECT * FROM ""deployments"" WHERE (""name"" = 'second') LIMIT 1
E, [2015-06-24 10:11:48 #61485] [task:5] ERROR -- DirectorJobRunner: no implicit conversion of nil into String
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:370:in `parse'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:370:in `parse_stream'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:318:in `parse'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/2.1.0/psych.rb:245:in `load'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/jobs/run_errand.rb:26:in `perform'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/job_runner.rb:108:in `perform_job'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/job_runner.rb:31:in `block in run'
/Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_formatter.rb:49:in `with_thread_name'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/job_runner.rb:31:in `run'
/Users/pivotal/workspace/bosh/bosh-director/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/Users/pivotal/workspace/bosh/bosh-director/bin/bosh-director-worker:75:in `<top (required)>'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `load'
/Users/pivotal/.rbenv/versions/2.1.5/lib/ruby/gems/2.1.0/bin/bosh-director-worker:23:in `<main>'
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000224s) SELECT NULL
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000128s) BEGIN
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.000297s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2015-06-24 10:11:48.910028-0700', ""description"" = 'run errand errand from deployment second', ""result"" = 'no implicit conversion of nil into String', ""output"" = '/Users/pivotal/workspace/bosh/tmp/integration-tests-workspace/pid-60437/sandbox/boshdir/tasks/5', ""checkpoint_time"" = '2015-06-24 10:11:48.897477-0700', ""type"" = 'run_errand', ""username"" = 'test' WHERE (""id"" = 5)
D, [2015-06-24 10:11:48 #61485] [task:5] DEBUG -- DirectorJobRunner: (0.064531s) COMMIT
I, [2015-06-24 10:11:48 #61485] []  INFO -- DirectorJobRunner: Task took 0.081413 seconds to process.

Task 5 error

        ******************** end Director task 5 ********************

I, [2015-06-24T10:11:55.219447 #60437]  INFO : Exit code is 0
I, [2015-06-24T10:11:55.219534 #60437]  INFO : Command took 9.915963 seconds

RuntimeError: ERROR: bosh -n -c /Users/pivotal/workspace/bosh/tmp/integration-tests-workspace/pid-60437/client-sandbox/bosh_config.yml run errand errand failed with output:

Processing deployment manifest
------------------------------
Acting as user 'test' on deployment 'second' on 'Test Director'

Director task 5
Error 100: no implicit conversion of nil into String

Task 5 error

Errand `errand' did not complete

For a more detailed error report, run: bosh task 5 --debug

```",,97746340,story,[],User should see a better error when attempting to run an errand not defined in a deployment without a manifest saved,,[],956238,687691,bug,2015-06-24T17:12:52Z,https://www.pivotaltracker.com/story/show/97746340
,2015-06-18T21:39:38Z,unscheduled,,"Will need to install go in the Vagrantfile and mount/rsync the bosh-init directory
",,97339912,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Try to run the bosh-init acceptance tests from inside of the bosh-lite VM instead of SSHing to the VM to run commands,,[],956238,687691,chore,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/97339912
,2015-06-10T22:22:23Z,unscheduled,,,,96716266,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Review integration tests in bosh-init and move them to acceptance/unit,,[],956238,553935,chore,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/96716266
,2015-05-29T23:05:14Z,unscheduled,,"- in progress downloads should be in the installation specific dir ~/.bosh_init/installations/XXX/tmp
- only move into shared downloads folder when download finished and matches sha1",1.0,95759474,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",clean up temp files when assets are being downloaded,,[],956238,81882,feature,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/95759474
,2015-06-10T22:22:36Z,unscheduled,,,,96716290,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Optimize bosh-init acceptance tests,,[],956238,553935,chore,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/96716290
,2015-05-01T23:45:22Z,unscheduled,,,,93733768,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should show help when entering one arg eg. `bosh-init ~/manifest.yml`,,[],956238,687691,feature,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/93733768
,2015-02-23T17:56:22Z,unscheduled,,"currently /var/vcap/bosh/micro is used as blobstore; however, it's on the root partition so it may not have enough space for uploading blobs.

- cannot break old micro cli
- should create blobstore directory if does not exist
- point /blobs to that directory
- local blobstore configuration?",,88923362,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",agent should be able to compile releases that are too big to fit on the root partition,,[],956238,81882,feature,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/88923362
,2015-04-27T01:34:12Z,unscheduled,,"- verify - at this point should work.
- create tmp directory per installation, remove all files at the end and the beginning of all commands?",0.0,93295812,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",downloading files should work if tmp dir is on a different mount point,,[],956238,81882,feature,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/93295812
,2015-05-04T18:46:08Z,unscheduled,,,,93832800,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should show proper name when deleting instance,,[],956238,81882,bug,2015-06-23T22:07:28Z,https://www.pivotaltracker.com/story/show/93832800
,2015-06-16T00:37:28Z,unscheduled,,moved from https://www.pivotaltracker.com/story/show/96105964,,97067932,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Consider using Go oracle to automatically detect unused packages/code,,[],956238,1550486,chore,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/97067932
,2015-06-15T23:28:15Z,unscheduled,,,,97064718,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}, {'name': 'retro action item', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032000, 'updated_at': '2015-06-23T16:06:50Z'}]",Tech talk topic: JRHB would like to talk about stemcell building and knowing when it's safe to push,,[],956238,344,chore,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/97064718
,2015-06-16T00:35:50Z,unscheduled,,moved from https://www.pivotaltracker.com/story/show/96105964,,97067860,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Consider moving bosh-init webdav blobstore into bosh-utils,,[],956238,1550486,chore,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/97067860
,2015-06-16T00:33:56Z,unscheduled,,"... by way of removing redundant patterns of saving blobs / archiving files / moving to tmp dirs  -- only to then get blobs / unarchive files / move from tmp dir in a later stage.

moved from https://www.pivotaltracker.com/story/show/96105964",,97067802,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Consider simplifying job rendering,,[],956238,1550486,chore,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/97067802
,2015-06-13T00:31:03Z,unscheduled,,"cloud > plugin is not set in case of external CPIs https://github.com/cloudfoundry/bosh/blob/ad8ce462b12d8dcd53aefea8519d8939fb5900fe/bosh-director/bin/bosh-director-migrate#L49 

Migrations fail and director does not come up",,96930126,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init deployed director fails to run migrations,,[],956238,668075,bug,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/96930126
,2015-06-11T21:30:32Z,unscheduled,,This is to provide a faster way to iterate on `bosh-init deploy`s. Especially for things like property-only changes (no change in releases),,96828818,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should not re-compile packages if it has already done so,,[],956238,1367598,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/96828818
,2015-06-08T22:09:22Z,unscheduled,,"When user is switching regions in manifest, bosh-init will fail to delete unused stemcell, because it only looks in current region",,96500322,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should save the region where stemcell was created,,[],956238,553935,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/96500322
,2015-04-20T23:10:21Z,unscheduled,,"Validating release `bosh-openstack-cpi`... Done (00:00:01)
Validating release `bosh`... Done (00:00:01)
Validating stemcell `...`... Done (00:00:01)",,92879804,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",print which releases and stemcells are validated,,[],956238,81882,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/92879804
,2015-04-20T19:09:11Z,unscheduled,,,,92858368,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",parallel download of releases and stemcells?,,[],956238,81882,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/92858368
,2015-01-28T01:48:27Z,unscheduled,,"- with defaults
- without default",,87063668,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",lock down property overrides with acceptance tests against dummy release,,[],956238,81882,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/87063668
,2014-12-30T17:48:54Z,unscheduled,,"Release.delete() should remove the record and clear the current_release_id.

Current deployment.json after a delete:

```
cat tmp/deployment.json
{
    ""director_id"": ""c6e9def2-04dd-4a50-4bce-169ba58bdc5e"",
    ""deployment_id"": ""190c6685-3b0f-4d03-7b1a-3648da1552de"",
    ""current_vm_cid"": """",
    ""current_stemcell_id"": """",
    ""current_disk_id"": """",
    ""current_release_id"": ""398c4bc1-1114-4f6e-5c65-c942301decfd"",
    ""current_manifest_sha1"": ""43b81dfa0e63accb23f730cbc7fcde7d7301ba0d"",
    ""disks"": [],
    ""stemcells"": [],
    ""releases"": [
        {
            ""id"": ""398c4bc1-1114-4f6e-5c65-c942301decfd"",
            ""name"": ""bosh-aws-cpi"",
            ""version"": ""0+dev.5""
        }
    ]
}
```",,85277998,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",current release should be deleted from deployment.json after a successful bosh-micro delete,,[],956238,1266616,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/85277998
,2014-11-19T20:36:23Z,unscheduled,,Disk size 0,,83070900,story,"[{'name': 'generic', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032002, 'updated_at': '2015-06-23T16:06:50Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",cli should delete existing disk if deployment does not need disk anymore,,[],956238,553935,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/83070900
,2014-11-08T00:25:46Z,unscheduled,,Currently the error is at packaging step which is misleading,,82330810,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Deploy should fail nicely if cli fails to parse deployment.json,,[],956238,553935,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/82330810
,2014-11-07T23:53:57Z,unscheduled,,,,82329552,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",user should see nicely formatted error message if stage fails before getting to any steps,,[],956238,81882,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/82329552
,2014-10-29T01:49:33Z,unscheduled,,is still assumes that err is magically printed. see ssh/monit_adapter files.,,81643538,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",fix logging for ssh in agent,,[],956238,81882,chore,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/81643538
,2014-10-28T22:50:51Z,unscheduled,,Disable SSLv3 on the client: talk to Karl/Zak about TLS Config in Go,,81635256,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Disable SSLv3 in the agent client,,[],956238,553935,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/81635256
,2014-10-10T18:07:58Z,unscheduled,,"It looks like we leave files from previous compilation in compiled package folder

If you update the gem source and use BOSH_INSTALL_TARGET as gem home it will not update the gem since it was already installed there.

The proposal is to use a temporary directory as the BOSH_INSTALL_TARGET, and if compilation succeeds, move to the real install target.",,80474502,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",compiling packages should clean up previously compiled package directory,,[],956238,553935,bug,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/80474502
,2014-03-10T07:31:17Z,unscheduled,,Bosh deployer is unable to discover the IP address of the microBOSH vm when using AWS VPC with only private IP addresses (no elactic IP nor public IP),,67189752,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Deployer is unable to discover microBosh IP on AWS when using VPC without public IP addresses,,[],956238,1015645,bug,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/67189752
,2014-06-26T23:30:06Z,unscheduled,,"It would be ideal if microbosh could perform an upgrade when the VM has gone away (for example it was manually terminated) but the persisten disk is still around.

This does not work currently; the missing VM crashes microbosh as it has some assumptions in the code that the VM exists when it goes to destroy it.

Note that this may be in the Openstack CPI as we saw it on openstack.

```
Stemcell info
-------------
Name:    bosh-openstack-kvm-ubuntu
Version: 2366
 
  Started deploy micro bosh
  Started deploy micro bosh > Unpacking stemcell. Done
  Started deploy micro bosh > Uploading stemcell. Done
  Started deploy micro bosh > Creating VM from 59bd7ce9-5f08-4d88-abb0-5ba93eb166calog writing failed. can't be called from trap context
/usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_openstack_cpi-1.2479.0/lib/cloud/openstack/cloud.rb:253:in `rescue in block in create_vm': Bosh::Clouds::VMCreationFailed (Bosh::Clouds::VMCreationFailed)
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_openstack_cpi-1.2479.0/lib/cloud/openstack/cloud.rb:246:in `block in create_vm'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_common-1.2479.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_openstack_cpi-1.2479.0/lib/cloud/openstack/cloud.rb:184:in `create_vm'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:243:in `create_vm'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:123:in `block in create'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:122:in `create'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:98:in `block in create_deployment'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/deployer/instance_manager.rb:98:in `create_deployment'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli_plugin_micro-1.2479.0/lib/bosh/cli/commands/micro.rb:179:in `perform'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli-1.2479.0/lib/cli/command_handler.rb:57:in `run'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli-1.2479.0/lib/cli/runner.rb:56:in `run'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli-1.2479.0/lib/cli/runner.rb:16:in `run'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/gems/bosh_cli-1.2479.0/bin/bosh:7:in `<top (required)>'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/bin/bosh:23:in `load'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/bin/bosh:23:in `<main>'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/bin/ruby_executable_hooks:15:in `eval'
        from /usr/local/rvm/gems/ruby-2.0.0-p451/bin/ruby_executable_hooks:15:in `<main>'
```",,74010586,story,"[{'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",Cannot execute microbosh upgrade when persistent disk exists but VM does not,,[],956238,1092360,bug,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/74010586
,2014-09-02T23:25:23Z,unscheduled,,"When creating an external cpi release, a monit file is no longer required.

",1.0,78076744,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",remove requirement for a monit file to be a valid job,,[],956238,514635,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/78076744
,2015-02-18T18:05:06Z,unscheduled,,,,88630002,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]","Rename ""jobs"" → ""clusters"" and ""templates"" → ""jobs"" in manifest",,[],956238,1348450,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/88630002
,2014-11-03T20:32:07Z,unscheduled,,related: https://www.pivotaltracker.com/story/show/81643554,,81980762,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",agent should not load whole blob into memory when serving from the local blobstore,,[],956238,81882,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/81980762
,2015-05-27T00:28:17Z,unscheduled,,"maybe just the first time?

it took 10 minutes (without any feedback) for us to find out that the ip we wanted was in use

```
Started deploying
  Creating VM for instance 'bosh/0' from stemcell 'ami-c2d9d4aa light'... Failed (00:10:01)
Failed deploying (00:10:01)

Stopping registry... Finished (00:00:00)

Command 'deploy' failed:
  Deploying:
    Creating instance 'bosh/0':
      Creating VM:
        Creating vm with stemcell cid 'ami-c2d9d4aa light':
          CPI 'create_vm' method responded with error: CmdError{""type"":""Unknown"",""message"":""Address 10.10.0.6 is in use."",""ok_to_retry"":false}
```",,95487568,story,"[{'name': 'bosh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458336, 'updated_at': '2014-01-13T17:49:44Z'}, {'name': 'init', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T18:31:01Z', 'id': 12034084, 'updated_at': '2015-06-23T18:31:01Z'}]",bosh-init should show errors while retrying,,[],956238,687691,feature,2015-06-23T18:34:02Z,https://www.pivotaltracker.com/story/show/95487568
,2015-06-20T01:02:21Z,unscheduled,,,,97441690,story,[],remove agent.blobstore.* defaults since we fall back to blobstore properties,,[],956238,81882,feature,2015-06-20T01:02:24Z,https://www.pivotaltracker.com/story/show/97441690
,2015-06-19T19:36:07Z,unscheduled,,"```
Director task 120
  Started update stemcell
  Started update stemcell > Extracting stemcell archive. Done (00:00:11)
  Started update stemcell > Verifying stemcell manifest. Done (00:00:00)
  Started update stemcell > Checking if this stemcell already exists. Done (00:00:00)
  Started update stemcell > Uploading stemcell bosh-aws-xen-ubuntu-trusty-go_agent/0000 to the cloud. Failed: Unable to copy stemcell root image: command 'sudo -n /var/vcap/packages/bosh_aws_cpi/gem_home/ruby/1.9.1/gems/bosh_aws_cpi-1.2859.0/scripts/stemcell-copy.sh /var/vcap/data/tmp/director/stemcell20150619-25997-3n55ei/image /dev/xvdg 2>&1' failed with exit code 1 (00:00:40)

Error 100: Unable to copy stemcell root image: command 'sudo -n /var/vcap/packages/bosh_aws_cpi/gem_home/ruby/1.9.1/gems/bosh_aws_cpi-1.2859.0/scripts/stemcell-copy.sh /var/vcap/data/tmp/director/stemcell20150619-25997-3n55ei/image /dev/xvdg 2>&1' failed with exit code 1

Task 120 error
```

Adding the following to /etc/sudoers on the director (using visudo of course!) fixes the issue:

```
vcap ALL=NOPASSWD: /var/vcap/packages/bosh_aws_cpi/gem_home/ruby/1.9.1/gems/bosh_aws_cpi-1.2859.0/scripts/stemcell-copy.sh
```",,97423494,story,[],bosh upload stemcell fails for non-lite stemcells on aws,,[],956238,1541728,bug,2015-06-19T19:36:07Z,https://www.pivotaltracker.com/story/show/97423494
,2015-06-18T21:45:16Z,unscheduled,,"https://main.bosh-ci.cf-app.com/pipelines/main/jobs/integration-1.9-mysql/builds/10

```

Failures:

  1) release lifecycle verifies a sample valid release
     Failure/Error: out = bosh_runner.run(""verify release #{release_filename}"")
     RuntimeError:
       ERROR: bosh -n -c /tmp/build/2734fba0-dc9f-4510-6e3f-0f87672a4845/bosh-src/tmp/integration-tests-workspace/pid-9163/client-sandbox/bosh_config.yml verify release /tmp/build/2734fba0-dc9f-4510-6e3f-0f87672a4845/bosh-src/spec/assets/valid_release.tgz failed with output:
       
       Test directory: /tmp/build/2734fba0-dc9f-4510-6e3f-0f87672a4845/bosh-src/tmp/integration-tests-workspace/pid-9163/spec-20150618-9163-1rq2dbm
       Sandbox directory: /tmp/build/2734fba0-dc9f-4510-6e3f-0f87672a4845/bosh-src/tmp/integration-tests-workspace/pid-9163
     # ./spec/support/bosh_runner.rb:56:in `run_in_current_dir'
     # ./spec/support/bosh_runner.rb:13:in `block in run'
     # ./spec/support/bosh_runner.rb:13:in `chdir'
     # ./spec/support/bosh_runner.rb:13:in `run'
     # ./spec/integration/release/release_lifecycle_spec.rb:142:in `block (2 levels) in <top (required)>'

Finished in 17 minutes 5 seconds (files took 5.88 seconds to load)
23 examples, 1 failure

Failed examples:
```",,97340280,story,[],"Flaky integration test ""release lifecycle verifies a sample valid release""",,[],956238,553935,chore,2015-06-18T21:45:16Z,https://www.pivotaltracker.com/story/show/97340280
,2015-06-16T16:57:01Z,unscheduled,,"From Chris Brown on the concourse team:

Hi all,

The number of Concourse CI servers are going up (yay) but with this comes the penalty of trying to find another team's server. I've made a list of the servers I can find (there are some missing) here: http://flight-tracker.cfapps.io (username: cf, password: planesarecool). As you can see there's a wide variety of conventions for domains, subdomains, ports, and SSL certificates all happening at once.

We'd like to simplify this as much as possible. We're proposing moving teams to a https://<team-name>.ci.cf-app.com convention to help with discovery and simplify things. This also has the advantage of allowing us to get a single wildcard certificate which can be loaded into your ELBs to make it really simple to encrypt the sensitive traffic. Below are the instructions for setting up with the new subdomain and certificate:

1. Find the ELB in AWS that you want to use for the new domain. If you want to keep your existing domain (temporarily or permanently) then you'll want to create a new ELB that's configured identically to the existing Concourse one.
2. Find the ""DNS Name"" of your ELB in the AWS console. It should appear in the bottom pane when you select it. Copy this and save it for later.
3. Log into the shared DNS account (details can be found in LastPass).
4. In Route53, open up the cf.cf-app.com hosted zone.
5. Create a new record set with name <team>.ci.cf-app.com, type set to CNAME, and value set to the ""DNS Name"" you noted down earlier.
6. Save all this and go play some table tennis while the DNS propagates. You should now be able to go to <team>.ci.cf-app.com and reach your CI server.
7. Log back into your AWS account and load in the *.ci.cf-app.com certificate (which can be found in LastPass) into your ELB. AWS is really picky about the names and formats of the keys. Make sure there is no trailing whitespace in the keys or certificates. Avoid using emoji for the certificate name.
8. Once this is done, assuming you have an SSL listener set up for your ELB, you'll be able to visit https://<team>.ci.cf-app.com, the little lock in your browser will be green, and your credentials won't be in plaintext.
9. (Optional, but strongly recommended) Remove the non-SSL listener from your ELB entirely.
10. (Optional) If you don't want your old ELB anymore. You can remove it and just use the new domain.

P.S. Remember to add the new ELB to your BOSH manifests (in the cloud_properties of your resource pool) so that BOSH can keep the backend instances up to date.",,97124318,story,[],move public concourse to https://bosh.ci.cf-app.com,,[],956238,878715,chore,2015-06-16T17:07:36Z,https://www.pivotaltracker.com/story/show/97124318
,2015-06-10T22:35:27Z,unscheduled,,It appears that the `coverage/index.html` is being overwritten for each unit test sub-suite. so we may be posting actual coverage only on the last sub-suite that happens to run,,96717368,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Verify that `COVERAGE rake spec:unit` is capturing coverage for all bosh subprojects,,[],956238,1550486,bug,2015-10-08T17:41:05Z,https://www.pivotaltracker.com/story/show/96717368
,2015-06-10T17:02:44Z,unscheduled,,"the go tasks is a dependency of spec:unit so why not declare it as such.

instead of `be rake go spec:unit` we should just be able to run `be rake spec:unit`",,96684460,story,[],`spec:unit` task should depend on the `go` task,,[],956238,1550486,chore,2015-06-10T17:02:45Z,https://www.pivotaltracker.com/story/show/96684460
,2015-06-08T19:59:47Z,unscheduled,,"The Go client for bosh-agent prints this message when the async action `apply` completes successfully:

```
WARN - Unable to parse get_task response value: ""applied""
```

This also affects the new `update-settings` action which similarly responds with the string ""updated"".

After fixing this, also update the `UpdateSettings` and `Apply` methods to pass on the returned string from sendAsyncTaskMessage.",,96486890,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",sendAsyncTaskMessage emits misleading WARN message and fails to return actual return value,,[],956238,1541728,feature,2015-06-08T20:32:10Z,https://www.pivotaltracker.com/story/show/96486890
,2015-06-01T19:00:05Z,unscheduled,,,,95896350,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",Deprecate security group param for AWS cpi to take name,,[],956238,81882,feature,2015-06-02T00:33:13Z,https://www.pivotaltracker.com/story/show/95896350
,2015-06-09T17:58:22Z,unscheduled,,,,96583658,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Workstations should have ~/bin created and added to $PATH,,[],956238,1550486,chore,2015-06-09T22:30:16Z,https://www.pivotaltracker.com/story/show/96583658
,2015-05-29T16:35:34Z,unscheduled,,Ted's email should somehow get added to the sprout instructions/repo list,,95727022,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",add sprout support for checking out the init-workspace repo and agent-workspace repo,,[],956238,344,chore,2015-05-29T16:35:34Z,https://www.pivotaltracker.com/story/show/95727022
,2015-05-29T16:33:59Z,unscheduled,,"We were unable to run soloist and checkout the private repos, despite having added ssh keys

Some of the repos we may not want anymore.",,95726718,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",re-evaluate the repo list,,[],956238,344,chore,2015-05-29T16:34:06Z,https://www.pivotaltracker.com/story/show/95726718
,2015-05-29T16:32:44Z,unscheduled,,The url to the blob is currently 404ing,,95726608,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Update the terminal font issue in sprout-wrap,,[],956238,344,chore,2015-05-29T16:32:54Z,https://www.pivotaltracker.com/story/show/95726608
,2015-05-26T23:32:32Z,unscheduled,,,,95484670,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",Downloading blobs percentage should not exceed 100%,,[],956238,553935,bug,2015-05-27T00:30:57Z,https://www.pivotaltracker.com/story/show/95484670
,2015-05-21T21:00:25Z,unscheduled,,,,95196158,story,[],Add OS X launchd script to reset git duet at start of day,,[],956238,1550486,chore,2015-07-30T20:37:30Z,https://www.pivotaltracker.com/story/show/95196158
,2015-05-21T21:58:25Z,unscheduled,,"@aakash and @abhi ran into this when trying to deploy Ops Manager to a Developer Cloud account on Mirantis. This is a multi-tenant Juno cluster, our account is only 1 tenant - 

```
{""type"": ""step_started"", ""id"": ""delete.logging_into_director.dummy-a917972d7b8232dd019b""}
Running ""bundle exec bosh -n target 10.0.237.241""
Target already set to `microbosh-0e91ead3ea2767cd5c3b'
Running ""bundle exec bosh -n login director a780779004b0fdca10fd""
Logged in as `director'
{""type"": ""step_finished"", ""id"": ""delete.logging_into_director.dummy-a917972d7b8232dd019b""}
{""type"": ""step_started"", ""id"": ""delete.deleting.dummy-a917972d7b8232dd019b""}
Running ""bundle exec bosh -n deployments""

+----------------------------+-------------------------------+----------------------------------------------------+--------------+
| Name                       | Release(s)                    | Stemcell(s)                                        | Cloud Config |
+----------------------------+-------------------------------+----------------------------------------------------+--------------+
| dummy-a917972d7b8232dd019b | dummy-errands-release/0+dev.1 | bosh-openstack-kvm-ubuntu-trusty-go_agent-raw/2975 | none         |
|                            | dummy/0+dev.1                 |                                                    |              |
+----------------------------+-------------------------------+----------------------------------------------------+--------------+

Deployments total: 1
Running ""bundle exec bosh -n delete deployment dummy-a917972d7b8232dd019b --force""

You are going to delete deployment `dummy-a917972d7b8232dd019b'.

THIS IS A VERY DESTRUCTIVE OPERATION AND IT CANNOT BE UNDONE!

Director task 11
  Started deleting instances
  Started deleting instances > singleton-one-partition-null-az/0
  Started deleting instances > all-alone-partition-null-az/0
  Started deleting instances > dummy-partition-null-az/0
  Started deleting instances > dummy-partition-null-az/1
     Done deleting instances > singleton-one-partition-null-az/0 (00:00:17)
     Done deleting instances > all-alone-partition-null-az/0 (00:00:22)
     Done deleting instances > dummy-partition-null-az/0 (00:00:43)
     Done deleting instances > dummy-partition-null-az/1 (00:00:43)
     Done deleting instances (00:00:43)

  Started removing deployment artifacts
  Started removing deployment artifacts > Detach stemcells. Done (00:00:00)
  Started removing deployment artifacts > Detaching releases. Done (00:00:00)

  Started deleting properties
  Started deleting properties > Delete DNS records. Done (00:00:00)
  Started deleting properties > Destroy deployment. Done (00:00:00)

Task 11 done

Started2015-05-21 19:19:23 UTC
Finished2015-05-21 19:20:06 UTC
Duration00:00:43

Deleted deployment `dummy-a917972d7b8232dd019b'
{""type"": ""step_finished"", ""id"": ""delete.deleting.dummy-a917972d7b8232dd019b""}
{""type"": ""step_started"", ""id"": ""clean_up_bosh.logging_into_director""}
{""type"": ""step_finished"", ""id"": ""clean_up_bosh.logging_into_director""}
{""type"": ""step_started"", ""id"": ""clean_up_bosh.cleaning_up""}
Running ""bundle exec bosh -n cleanup --all""

Cleanup command will attempt to delete old unused
release versions and stemcells from your currently
targeted director at microbosh-0e91ead3ea2767cd5c3b.

Only 0 latest versions of each release
and 0 latest versions of each stemcell will be kept.

Releases and stemcells that are in use will not be affected.


Deleting old stemcells
  bosh-openstack-kvm-ubuntu-trusty-go_agent-raw/2975 IN PROGRESS...
  bosh-openstack-kvm-ubuntu-trusty-go_agent-raw/2975DELETED

Deleting old release versions
  dummy/0+dev.1                            IN PROGRESS...
  dummy/0+dev.1                                     DELETED
  dummy-errands-release/0+dev.1            IN PROGRESS...
  dummy-errands-release/0+dev.1                     DELETED

Cleanup complete
{""type"": ""step_finished"", ""id"": ""clean_up_bosh.cleaning_up""}
{""type"": ""step_started"", ""id"": ""microbosh.setting_manifest""}
Running ""bundle exec bosh -n micro deployment micro/""
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/fog-1.27.0/lib/fog/openstack/volume.rb:191: warning: duplicated key at line 196 ignored: :openstack_region
Deployment set to '/var/tempest/workspaces/default/deployments/micro/micro_bosh.yml'
{""type"": ""step_finished"", ""id"": ""microbosh.setting_manifest""}
{""type"": ""step_started"", ""id"": ""microbosh.deleting""}
Running ""bundle exec bosh -n micro delete""

You are going to delete micro BOSH deployment `microbosh-0e91ead3ea2767cd5c3b'.

THIS IS A VERY DESTRUCTIVE OPERATION AND IT CANNOT BE UNDONE!
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/fog-1.27.0/lib/fog/openstack/volume.rb:191: warning: duplicated key at line 196 ignored: :openstack_region
  Started delete micro bosh
  Started delete micro bosh > Stopping agent services. Done (00:00:03)
  Started delete micro bosh > Deleting persistent disk `d0545ea2-2ebc-48f3-8447-0c295ac767e6'
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/fog-1.27.0/lib/fog/openstack/volume.rb:191: warning: duplicated key at line 196 ignored: :openstack_region
  Started delete micro bosh > Unmount disk. Done (00:00:19)
  Started delete micro bosh > Detach disk. Done (00:00:01)
  Started delete micro bosh > Delete disklog writing failed. can't be called from trap context
/home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_openstack_cpi-1.2975.0/lib/cloud/openstack/helpers.rb:20:in `cloud_error': Cannot delete volume `d0545ea2-2ebc-48f3-8447-0c295ac767e6', state is in-use (Bosh::Clouds::CloudError)
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_openstack_cpi-1.2975.0/lib/cloud/openstack/cloud.rb:482:in `block in delete_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_common-1.2975.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_openstack_cpi-1.2975.0/lib/cloud/openstack/cloud.rb:476:in `delete_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:305:in `block in delete_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:304:in `delete_disk'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:167:in `block in destroy'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:85:in `step'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:166:in `destroy'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:106:in `block in delete_deployment'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:92:in `with_lifecycle'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/deployer/instance_manager.rb:106:in `delete_deployment'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli_plugin_micro-1.2975.0/lib/bosh/cli/commands/micro.rb:215:in `delete'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2975.0/lib/cli/command_handler.rb:57:in `run'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2975.0/lib/cli/runner.rb:56:in `run'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/gems/bosh_cli-1.2975.0/bin/bosh:16:in `<top (required)>'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/bin/bosh:23:in `load'
from /home/tempest-web/tempest/web/vendor/bundle/ruby/2.2.0/bin/bosh:23:in `<main>'
{""type"": ""step_finished"", ""id"": ""microbosh.deleting""}
Exited with 1.
```",,95201142,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}]",BOSH did not notice that volume wasn't actually detached and then failed while deleting a microbosh instance,,[],956238,5637,bug,2015-05-26T18:32:50Z,https://www.pivotaltracker.com/story/show/95201142
,2015-05-19T18:03:02Z,unscheduled,,"bosh-agent uses its own built-in logging system which is not an interface.

@tyoungpi had suggested we log more and do ""log driven testing"" which sounds like a great idea, but would require us to substitute a logger implementation that we can query in Expect() assertions in the tests.

Maybe there's now a popular logging framework for Go that already supports such testability, and we could switch to that. At the very least, we could change the built-in logger.Logger struct to be an interface.",,94978804,story,[],fix logging in bosh-agent so it happens through a (fakeable) interface,,[],956238,1541728,chore,2015-05-19T18:03:11Z,https://www.pivotaltracker.com/story/show/94978804
,2015-05-15T22:30:27Z,unscheduled,,there was some talk that we could setup a repo to have the pre-commit hook installed by default so all commits would run the hooks without developer needing to 'install' them upon `git clone`,,94713302,story,"[{'name': 'retro action item', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-23T16:06:50Z', 'id': 12032000, 'updated_at': '2015-06-23T16:06:50Z'}]",add pre-commit hooks to repos,,[],956238,1541728,chore,2015-10-08T00:21:20Z,https://www.pivotaltracker.com/story/show/94713302
,2015-05-14T21:29:18Z,unscheduled,,"It appears a debian repo was down for the following:

- http://bosh-jenkins.cf-app.com:8080/job/stemcell_aws_centos_go_agent/683/
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_aws_ubuntu_trusty_go_agent/347/console
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_openstack_centos_go_agent/
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_openstack_ubuntu_trusty_go_agent/
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_vcloud_ubuntu_trusty_go_agent/
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_vsphere_centos_go_agent/
- http://bosh-jenkins.cf-app.com:8080/job/stemcell_vsphere_ubuntu_trusty_go_agent/

It may be that we can use the following to better ensure success (although the manpage for `apt-get` is unclear on that):
```
apt-get update --fix-missing
apt-get update
```",,94624650,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}]",Fix CI pipeline apt update flakiness in stemcell building,,[],956238,1550486,chore,2015-10-08T17:41:05Z,https://www.pivotaltracker.com/story/show/94624650
,2015-05-14T19:00:11Z,unscheduled,,"```
** Execute spec:integration:install_dependencies
Syncing blobs...
postgres/postgres-9.0.3-1.amd64.tar.gz 
downloading 15.5Mdownloading 15.5M (0%)downloading 15.5M (0%)downloading 15.5M (32%)downloading 15.5M (32%)downloading 15.5M (50%)downloading 15.5M (55%)downloading 15.5M (60%)downloading 15.5M (78%)downloading 15.5M (100%)downloadedpostgres/postgres-9.0.3-1.i386.tar.gz 
downloading 14.4Mdownloading 14.4M (0%)downloading 14.4M (7%)downloading 14.4M (14%)downloading 14.4M (78%)downloading 14.4M (100%)downloadedpostgres/postgresql-9.0.3.tar.gz 
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/base.rb:63:in `rescue in get': Failed to fetch object, underlying error: #<HTTPClient::ReceiveTimeoutError: execution expired> /opt/rubies/ruby-1.9.3-p547/lib/ruby/1.9.1/openssl/buffering.rb:121:in `sysread' (Bosh::Blobstore::BlobstoreError)
/opt/rubies/ruby-1.9.3-p547/lib/ruby/1.9.1/openssl/buffering.rb:121:in `readpartial'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:364:in `readpartial'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:957:in `block in read_body_length'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:955:in `read_body_length'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient/session.rb:703:in `get_body'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:1092:in `do_get_block'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:891:in `block in do_request'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:985:in `protect_keep_alive_disconnected'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:890:in `do_request'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:778:in `request'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/httpclient-2.4.0/lib/httpclient.rb:677:in `get'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/simple_blobstore_client.rb:39:in `get_file'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb:87:in `get_file'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/base.rb:50:in `get'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/sha1_verifiable_blobstore_client.rb:19:in `get'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:19:in `block in get'
/tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:28:in `call'
/tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:28:in `block in retryer'
/tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:26:in `loop'
/tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:26:in `retryer'
/tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:18:in `get'
/tmp/build/src/bosh-src/bosh_cli/lib/cli/blob_manager.rb:316:in `download_blob'
/tmp/build/src/bosh-src/bosh_cli/lib/cli/blob_manager.rb:242:in `block (3 levels) in process_index'
/tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:77:in `call'
/tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
	from /tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/base.rb:49:in `get'
	from /tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/sha1_verifiable_blobstore_client.rb:19:in `get'
	from /tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:19:in `block in get'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:28:in `call'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:28:in `block in retryer'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:26:in `loop'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/retryable.rb:26:in `retryer'
	from /tmp/build/src/bosh-src/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:18:in `get'
	from /tmp/build/src/bosh-src/bosh_cli/lib/cli/blob_manager.rb:316:in `download_blob'
	from /tmp/build/src/bosh-src/bosh_cli/lib/cli/blob_manager.rb:242:in `block (3 levels) in process_index'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:77:in `call'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:63:in `loop'
	from /tmp/build/src/bosh-src/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
	from /opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
	from /opt/rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
```",,94611492,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]","Fix CI pipeline  integration job flakiness: ""bosh sync blobs""",,[],956238,1550486,chore,2015-10-08T17:40:47Z,https://www.pivotaltracker.com/story/show/94611492
,2015-05-08T16:41:25Z,unscheduled,,Would be nice to not run as privileged,,94189816,story,[],Mysql integration tests in concourse are not running as privileged user,,[],956238,1550486,chore,2015-05-13T22:37:19Z,https://www.pivotaltracker.com/story/show/94189816
,2015-05-06T20:42:01Z,unscheduled,,"Since the Apps Manager team was being renamed, they decided to rename their blobstore bucket from dev-console-blobs to apps-manager-blobs. 

The index.yml file still pointed to blobs in the old bucket and as a result bosh wasn't able to find the blobs. 

When doing a create-release, bosh threw the error
 ```
/usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/archive_repository.rb:52:in `rescue in lookup': undefined local variable or method `name' for #<Bosh::Cli::ArchiveRepository:0x007f938cf92640> (NameError)
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/archive_repository.rb:22:in `lookup'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/archive_builder.rb:61:in `locate_artifact'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/archive_builder.rb:16:in `block in build'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/core_ext.rb:14:in `with_indent'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/archive_builder.rb:15:in `build'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/commands/release/create_release.rb:144:in `block in build_packages'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/commands/release/create_release.rb:142:in `map'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/commands/release/create_release.rb:142:in `build_packages'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/commands/release/create_release.rb:90:in `create_from_spec'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/commands/release/create_release.rb:35:in `create'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/command_handler.rb:57:in `run'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/runner.rb:56:in `run'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/lib/cli/runner.rb:16:in `run'
	from /usr/local/lib/ruby/gems/2.0.0/gems/bosh_cli-1.2847.0/bin/bosh:7:in `<top (required)>'
	from /usr/local/bin/bosh:23:in `load'
	from /usr/local/bin/bosh:23:in `<main>'
  Generated version '22313729ae3b8fb4d3adf63161dfa97b179da93c'
```

A more useful error might have been something like 
`blob version referenced in index.yml not found in blobstore`",,94030702,story,[],Misleading error when attempting to use new blobstore bucket,,[],956238,1196628,bug,2015-05-06T20:42:01Z,https://www.pivotaltracker.com/story/show/94030702
,2015-05-06T19:09:04Z,unscheduled,,"- rm -rf .dev_builds .final_builds

```
±  |master ✗| → bosh create release --final --force --version 0.0.0
Syncing blobs...
Are you sure you want to generate final version? yes

Building FINAL release
-----------------------------------
Release artifact cache: /Users/pivotal/.bosh/cache

Building license
----------------
Building license...
  Warning: Missing LICENSE or NOTICE in /Users/pivotal/workspace/abic-dummy


Building packages
-----------------
Building bad_package...
  No artifact found for bad_package
  Generating...
  Generated version 'e05b55beba82daac75cf5a941429374a7b60c815'
  Uploading final version 'e05b55beba82daac75cf5a941429374a7b60c815'...
Blobstore error: Failed to create object, S3 response error: AWS::S3::Errors::Forbidden
```

Unexpected: record is added without blobstore_id?

```
±  |master ✗| → cat .final_builds/packages/bad_package/index.yml
---
builds:
  e05b55beba82daac75cf5a941429374a7b60c815:
    version: e05b55beba82daac75cf5a941429374a7b60c815
    sha1: bd13b5dfc8aaba188152083e10db374842d574c6
format-version: '2'
```",,94022474,story,[],investigate final release building sometimes does not include blobstore_id when adding file to an index,,[],956238,81882,bug,2015-05-06T19:09:19Z,https://www.pivotaltracker.com/story/show/94022474
,2015-05-06T17:54:02Z,unscheduled,,"- yaml does not parse
- keys are not correct format [1]

[1]
```
Building jobs
-------------
Building dummy...
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/resources/job.rb:142:in `templates': undefined method `keys' for ""ctl.erb bin/dummy_ctl"":String (NoMethodError)
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/resources/job.rb:150:in `templates_files'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/resources/job.rb:48:in `files'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/build_artifact.rb:67:in `make_fingerprint'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/archive_repository.rb:22:in `lookup'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/archive_builder.rb:64:in `locate_artifact'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/archive_builder.rb:16:in `block in build'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/core_ext.rb:14:in `with_indent'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/archive_builder.rb:15:in `build'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/commands/release/create_release.rb:195:in `block in build_jobs'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/commands/release/create_release.rb:193:in `map'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/commands/release/create_release.rb:193:in `build_jobs'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/commands/release/create_release.rb:96:in `create_from_spec'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/commands/release/create_release.rb:37:in `create'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2962.0/bin/bosh:16:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
```",,94013600,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",improve error messaging when job spec file is invalid,,[],956238,81882,bug,2015-05-14T17:03:33Z,https://www.pivotaltracker.com/story/show/94013600
,2015-05-01T19:43:04Z,unscheduled,,"While working on the update to rsyslog 8, we discovered that BATs will pass even when rsyslogd doesn't start up on the stemcells.

BATs should fail if rsyslogd isn't running.
",,93721058,story,"[{'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",ensure rsyslogd is actually running during BATs,,[],956238,1541728,feature,2015-05-02T00:04:56Z,https://www.pivotaltracker.com/story/show/93721058
,2015-04-30T21:09:51Z,unscheduled,,it would be nice if we could guarantee that agent testing and stemcell building use the exact same version of go.  Even better would be to have the stemcell building use the agent binary tested in the agent tests. ,,93660496,story,[],Ensure concourse agent and stemcell building pipelines uses go 1.4.2,,[],956238,119,feature,2015-04-30T21:13:23Z,https://www.pivotaltracker.com/story/show/93660496
,2015-04-28T16:42:39Z,unscheduled,,"Seen by @zacksoup
To reproduce:
1. Run cck with options against target with missing VM:

```
bosh cck -t a1.... -d a1
```

2. Pick Recreate VM or Delete VM reference.

Expected: the VM to be deleted or recreated
Actual: BOSH is saying that it ignores the problem. Done.

If you run directly by targeting deployment, setting manifest, etc. It all works.",,93456558,story,[],Running bosh cck with -t -d options doesn't work,,[],956238,119,bug,2015-04-28T16:42:39Z,https://www.pivotaltracker.com/story/show/93456558
,2015-04-22T18:54:46Z,unscheduled,,"We are currently mounting and un-mounting per spec, which is very slow.",,93044504,story,[],mount loopback device **once** for all stemcell specs,,[],956238,1406536,chore,2015-04-22T18:54:46Z,https://www.pivotaltracker.com/story/show/93044504
,2015-04-20T21:18:50Z,unscheduled,,"With big cf-release it takes a long time for cli to extract it. The last message that is shown is ""copying license"" so users are wondering why copying license takes such a long time.",,92870816,story,[],"CLI should print ""Extracting release"" during upload release",,[],956238,553935,feature,2015-04-20T21:18:50Z,https://www.pivotaltracker.com/story/show/92870816
,2015-04-13T18:44:30Z,unscheduled,,agent should write gateway in ifcfg or /etc/network/interfaces only if it was specified,,92371508,story,[],agent should not produce invalid config for networking when gateway is not specified,,[],956238,81882,bug,2015-04-13T22:29:17Z,https://www.pivotaltracker.com/story/show/92371508
,2015-04-09T18:25:26Z,unscheduled,,,,92157630,story,[],Figure out how to get UAA running (under JDK 1.7?) on OS X for development,,[],956238,1550486,chore,2015-04-15T17:28:05Z,https://www.pivotaltracker.com/story/show/92157630
,2015-04-06T14:23:40Z,unscheduled,,"```
root@3ddc49ab-7fde-47c5-b6cb-dddb679b0a85:/var/vcap/data/gocd-agent/pipelines/deploy_staging_aws_microbosh# bosh logout
You are no longer logged in to `https://micro.stage.xxxx.com:25555'
root@3ddc49ab-7fde-47c5-b6cb-dddb679b0a85:/var/vcap/data/gocd-agent/pipelines/deploy_staging_aws_microbosh/deployments-toolsmiths/aws/scripts# bosh target
Current target is https://micro.stage.xxxx.com:25555 (micro-stage)

root@3ddc49ab-7fde-47c5-b6cb-dddb679b0a85:/var/vcap/data/gocd-agent/pipelines/deploy_staging_aws_microbosh# bosh cleanup

Cleanup command will attempt to delete old unused
release versions and stemcells from your currently
targeted director at micro-stage.

Only 2 latest versions of each release
and 2 latest versions of each stemcell will be kept.

Releases and stemcells that are in use will not be affected.

Are you sure? (type 'yes' to continue): yes

Deleting old stemcells
  bosh-aws-xen-ubuntu-trusty-go_agent/2549DELETED
  bosh-aws-xen-ubuntu-trusty-go_agent/2611DELETED
  bosh-aws-xen-ubuntu-trusty-go_agent/2657DELETED
```",,91853310,story,[],User can run `bosh cleanup` when user is no longer logged in,,[],956238,927771,bug,2015-04-06T14:23:41Z,https://www.pivotaltracker.com/story/show/91853310
,2015-04-04T00:14:56Z,unscheduled,,"https://github.com/cloudfoundry/gosigar/blob/c36220e40949db17f92b2d7eaf0355eada43993a/sigar_unix.go

gosigar calculates disk usage using the number of ""free"" blocks instead of the number of ""available"" blocks (like df).  ext4 filesystems reserve 5% of raw disk for root operations, causing ""free"" and ""available"" to differ substantially.

Perhaps this should use the more conservative calculation like df. Most BOSH jobs run as a non-root user (vcap) and would not be able to use disk reserved for root.

Some examples from our docker registry servers...

```
statfs(""/var/vcap/store"", {f_type=""EXT2_SUPER_MAGIC"", f_bsize=4096, f_blocks=252062865, f_bfree=24920612, f_bavail=12110804, f_files=64028672, f_ffree=63908810, f_fsid={-1540929786, 814416549}, f_namelen=255, f_frsize=4096}) = 0

df 95%
bosh 90%

total = (252062865 * 8 / 2)
free = (24920612 * 8 / 2)
avail = (12110804 * 8 / 2)

bosh = ( total - free ) / total
df  = ( total - avail ) / total


===============

GoCD Prod

irb(main):001:0> total = (252062865 * 8 / 2).to_f
=> 1008251460.0
irb(main):002:0> free = (24920612 * 8 / 2).to_f
=> 99682448.0
irb(main):003:0> avail = (12110804 * 8 / 2).to_f
=> 48443216.0
irb(main):004:0> bosh = total - free
=> 908569012.0
irb(main):005:0> bosh = (total - free) / total
=> 0.901133346238844
irb(main):006:0> df  = ( total - avail ) / total
=> 0.9519532399189384

=================

Gocd Vsphere

irb(main):001:0> total = ( 128980956 * 8 / 2 ).to_f
=> 515923824.0
irb(main):002:0> free = ( 66038228 * 8 / 2 ).to_f
=> 264152912.0
irb(main):003:0> avail = ( 59480617 * 8 /2 ).to_f
=> 237922468.0
irb(main):004:0> bosh = ( total - free ) / total
=> 0.48800016647418865
irb(main):005:0> df  = ( total - avail ) / total
=> 0.5388418659263156
```",,91801984,story,[],agent vitals don't take reservered blocks into account when reporting disk usage,,[],956238,756869,feature,2015-04-06T05:10:10Z,https://www.pivotaltracker.com/story/show/91801984
,2015-03-24T12:16:59Z,unscheduled,,"**Issue:** Deployment fails with a cryptic / generic error message. See below
**Root Cause:** Disk on the bosh micro was full, blob store had a lot of stuff!

`cryptic error message`
```
Failed compiling packages > dse/84b86ed9a2825c2666b0c64e5c41178a30e11ac6: Action Failed get_task: Task a9163353-de97-4eee-6ae7-3d8a8fad1c92 result: Compiling package dse: Uploading compiled package: Creating blob in inner blobstore: Making put command: Shelling out to bosh-blobstore-dav cli: Running command: 'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json put /var/vcap/data/tmp/bosh-platform-disk-TarballCompressor-CompressFilesInDir996032161 6c188073-a53d-47f8-5cb7-9b29f20ea5d1', stdout: 'Error running app - Putting dav blob 6c188073-a53d-47f8-5cb7-9b29f20ea5d1: Wrong response code: 500; body: 


500 Internal Server Error


nginx



', stderr: '': exit status 1 (00:02:44)
   Failed compiling packages (00:04:39)

Error 450001: Action Failed get_task: Task a9163353-de97-4eee-6ae7-3d8a8fad1c92 result: Compiling package dse: Uploading compiled package: Creating blob in inner blobstore: Making put command: Shelling out to bosh-blobstore-dav cli: Running command: 'bosh-blobstore-dav -c /var/vcap/bosh/etc/blobstore-dav.json put /var/vcap/data/tmp/bosh-platform-disk-TarballCompressor-CompressFilesInDir996032161 6c188073-a53d-47f8-5cb7-9b29f20ea5d1', stdout: 'Error running app - Putting dav blob 6c188073-a53d-47f8-5cb7-9b29f20ea5d1: Wrong response code: 500; body: 


500 Internal Server Error


nginx



', stderr: '': exit status 1

Task 736 error
```",,90997628,story,[],"As an Operator, I'd like an actionable error message when the blob store on the bosh micro is full, so that i can resolve my failed deployment",,[],956238,1417370,feature,2015-04-03T19:19:38Z,https://www.pivotaltracker.com/story/show/90997628
,2015-04-01T20:47:03Z,unscheduled,,"Most jobs log to /var/vcap/sys/log/jobname

Postgres logs to /var/vcap/sys/log/monit.",,91641380,story,[],postgres job does not log to expected location.,,[],956238,756869,feature,2015-04-01T20:47:03Z,https://www.pivotaltracker.com/story/show/91641380
,2013-10-25T21:20:53Z,unscheduled,,"There are two very similar but subtly different cases that can cause rebase to fail when it shouldn't.

bosh create release
bosh upload release (set #1)
# add new package
bosh create release
bosh upload release --rebase (set #2)
# remove added package
bosh create release
bosh upload release --rebase (set #1) errors on no new packages or jobs, but is the same as a previous release version

bosh create release
bosh upload release (set #1)
# add 2 new packages
bosh create release
bosh upload release --rebase (set #2)
# remove only 1 new package
bosh create release
bosh upload release --rebase (set #3) errors on no new packages or jobs, but is a new release version since it's a new set",2.0,59598956,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh upload release --rebase with no new packages or jobs should not fail,,[],956238,81882,feature,2015-03-24T22:25:49Z,https://www.pivotaltracker.com/story/show/59598956
,2015-03-24T21:01:00Z,unscheduled,,"```
Command 'deploy' failed:
  Deploying Microbosh:
    Building state for instance 'bosh/0':
      Compiling job package dependencies for instance 'bosh/0':
        Compiling job package dependencies:
          Remotely compiling package 'ruby_aws_cpi' with the agent:
            Sending 'compile_package' to the agent:
              Sending 'get_task' to the agent:
                Agent responded with error: Action Failed get_task: Task 0145bc66-40c3-495c-4793-fb22f1b21cf2 result: Compiling package ruby_aws_cpi: Running packaging script: Command exited with 1; Truncated stdout: compiling complex.c
compiling ossl_x509revoked.c
compiling ossl_ns_spki.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/tk'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/tk'
installing default readline libraries
compiling nkf.c
compiling stringio.c
installing default parser libraries
compiling init.c
installing default complex libraries
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/syslog'
installing default pty libraries
compiling strscan.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/win32ole'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/win32ole'
compiling ossl_x509name.c
compiling syslog.c
installing default stringio libraries
compiling ossl_pkcs7.c
installing default sdbm libraries
installing default strscan libraries
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/racc/cparse'
compiling yaml_tree.c
installing default syslog libraries
compiling ossl_ssl.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/zlib'
compiling readline.c
compiling to_ruby.c
compiling ossl_pkey.c
compiling zlib.c
installing default pathname libraries
compiling ossl_pkey_dh.c
installing default cparse libraries
compiling parser.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/socket'
generating constant definitions
compiling emitter.c
compiling ossl_x509attr.c
installing default zlib libraries
compiling ossl_x509crl.c
generating constant definitions
installing default libraries
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/json'
compiling ossl_x509ext.c
compiling cparse.c
compiling ossl_pkey_ec.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/ripper'
installing default nkf libraries
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/syck'
compiling ossl_x509store.c
compiling ossl_ssl_session.c
compiling ripper.c
linking shared-object continuation.so
installing default socket libraries
compiling token.c
compiling ossl_bio.c
make[2]: Entering directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/tk/tkutil'
...
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/psych'
linking shared-object readline.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/sdbm'
linking shared-object json/ext/parser.so
linking shared-object stringio.so
linking shared-object pathname.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/objspace'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/strscan'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/json/parser'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/stringio'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/readline'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/pathname'
linking shared-object json/ext/generator.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/json/generator'
linking shared-object syck.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/syck'
linking shared-object dl.so
linking shared-object curses.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/dl'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/curses'
linking shared-object openssl.so
linking shared-object zlib.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/zlib'
linking shared-object socket.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/openssl'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/socket'
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/bigdecimal'
linking shared-object nkf.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/nkf'
linking shared-object date_core.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/date'
linking shared-object ripper.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/ripper'
linking shared-object dl/callback.so
make[2]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545/ext/dl/callback'
make[1]: Leaving directory `/var/vcap/data/compile/ruby_aws_cpi/ruby-1.9.3-p545'
, Truncated stderr: example-deconstructor.c: In function 'main':
example-deconstructor.c:1037:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
                         parser.problem_value, parser.problem_offset);
                         ^
example-deconstructor.c:1041:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_offset);
                         ^
example-deconstructor.c:1051:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor.c:1051:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor.c:1051:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-deconstructor.c:1051:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-deconstructor.c:1056:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor.c:1056:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor.c:1066:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor.c:1066:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor.c:1066:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-deconstructor.c:1066:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-deconstructor.c:1071:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor.c:1071:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
run-emitter.c: In function 'print_output':
run-emitter.c:208:5: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 2 has type 'size_t' [-Wformat=]
     printf(""#### (length: %!d(MISSING))\n"", total_size);
     ^
run-emitter.c:209:5: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 3 has type 'size_t' [-Wformat=]
     printf(""OUTPUT:\n%!s(MISSING)#### (length: %!d(MISSING))\n"", buffer, size);
     ^
run-emitter.c: In function 'main':
run-emitter.c:322:9: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 2 has type 'size_t' [-Wformat=]
         printf(""PASSED (length: %!d(MISSING))\n"", written);
         ^
run-dumper.c: In function 'print_output':
run-dumper.c:182:5: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 2 has type 'size_t' [-Wformat=]
     printf(""#### (length: %!d(MISSING))\n"", total_size);
     ^
run-dumper.c:183:5: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 3 has type 'size_t' [-Wformat=]
     printf(""OUTPUT:\n%!s(MISSING)#### (length: %!d(MISSING))\n"", buffer, size);
     ^
run-dumper.c: In function 'main':
run-dumper.c:306:9: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 2 has type 'size_t' [-Wformat=]
         printf(""PASSED (length: %!d(MISSING))\n"", written);
         ^
example-deconstructor-alt.c: In function 'main':
example-deconstructor-alt.c:707:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
                         parser.problem_value, parser.problem_offset);
                         ^
example-deconstructor-alt.c:711:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_offset);
                         ^
example-deconstructor-alt.c:721:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor-alt.c:721:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:721:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:721:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:726:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor-alt.c:726:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:736:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor-alt.c:736:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:736:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:736:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-deconstructor-alt.c:741:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-deconstructor-alt.c:741:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-reformatter.c: In function 'main':
example-reformatter.c:124:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
                         parser.problem_value, parser.problem_offset);
                         ^
example-reformatter.c:128:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_offset);
                         ^
example-reformatter.c:138:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-reformatter.c:138:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-reformatter.c:138:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-reformatter.c:138:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-reformatter.c:143:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-reformatter.c:143:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-reformatter.c:153:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-reformatter.c:153:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
example-reformatter.c:153:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 7 has type 'size_t' [-Wformat=]
example-reformatter.c:153:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 8 has type 'size_t' [-Wformat=]
example-reformatter.c:158:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 4 has type 'size_t' [-Wformat=]
                         parser.problem_mark.column+1);
                         ^
example-reformatter.c:158:25: warning: format '%!d(MISSING)' expects argument of type 'int', but argument 5 has type 'size_t' [-Wformat=]
+ make -j install
+ [[ 0 != 0 ]]
+ tar xzf ruby_aws_cpi/ruby-1.9.3-p545.tar.gz
+ set -e
+ cd ruby-1.9.3-p545
+ LDFLAGS='-Wl,-rpath -Wl,/var/vcap/packages/ruby_aws_cpi'
+ CFLAGS=-fPIC
+ ./configure --prefix=/var/vcap/packages/ruby_aws_cpi --disable-install-doc --with-opt-dir=/var/vcap/packages/ruby_aws_cpi
++ IFS=:
++ tr '\012' ' '
++ for dir in '$withval'
++ sed 's/^x *//;s:%!\(MISSING)$-s:/var/vcap/packages/ruby_aws_cpi/lib:g;s:%!s(MISSING):/var/vcap/packages/ruby_aws_cpi/lib:g'
++ echo x ' -L%!$(MISSING)-s' ' -Wl,-R%!$(MISSING)-s'
+ val='-L/var/vcap/packages/ruby_aws_cpi/lib  -Wl,-R/var/vcap/packages/ruby_aws_cpi/lib '
+ set +x
+ make -j
make[2]: *** No rule to make target `../../.ext/common/digest', needed by `install-rb-default'.  Stop.
make[2]: *** Waiting for unfinished jobs....
/bin/mkdir: cannot create directory '../../../.ext/common/digest': File exists
make[2]: *** [.RUBYLIBDIR.time] Error 1
make[2]: *** Waiting for unfinished jobs....
/bin/mkdir: cannot create directory '../../.ext/common/digest': File exists
make[2]: *** [.RUBYLIBDIR.-.digest.time] Error 1
make[2]: *** No rule to make target `../../.ext/common/bigdecimal', needed by `install-rb-default'.  Stop.
make[2]: *** Waiting for unfinished jobs....
/bin/mkdir: cannot create directory '../../.ext/common/bigdecimal': File exists
make[2]: *** [.RUBYLIBDIR.-.bigdecimal.time] Error 1
old_thread_select.c: In function 'old_thread_select':
old_thread_select.c:57:5: warning: 'rb_thread_select' is deprecated (declared at ../../.././include/ruby/intern.h:390) [-Wdeprecated-declarations]
     rc = rb_thread_select(max, rp, wp, ep, tvp);
     ^
make[1]: *** [ext/digest/sha2/all] Error 2
make[1]: *** Waiting for unfinished jobs....
make[1]: *** [ext/digest/all] Error 2
make[1]: *** [ext/bigdecimal/all] Error 2
make: *** [build-ext] Error 2
```",,91046884,story,"[{'name': 'deprecate', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-29T21:46:23Z', 'id': 11546802, 'updated_at': '2015-04-29T21:46:23Z'}]",Failed to compile ruby on agent; maybe a bug introduced with make -j?,,[],956238,1348450,bug,2015-04-29T21:46:49Z,https://www.pivotaltracker.com/story/show/91046884
,2015-03-19T23:36:49Z,unscheduled,,"```
Turns out this causes more problems. The bosh director thought it had a compiled package, but it wasn't in the blob store. This caused creating the api_z1 to fail, and it couldn't recover as it then wanted to drain the job, which wasn't possible without the ruby package. We replaced the drain script with 'echo 0' and it got past the deletion. We then tried deleting the release, and got this:

Are you sure? (type 'yes' to continue): yes

Director task 27
  Started deleting packages
  Started deleting packages > acceptance-tests/0ee247095c4118d276ce981b2d239f7e39ea43c3. Done (00:00:00)
  Started deleting packages > buildpack_cache/4ced0bc62f12dcaa79121718ca3525253ede33b5. Done (00:00:00)
  Started deleting packages > buildpack_go/cb35d33ef9d379246bcdcccdad7858f8f37ab8cb. Done (00:00:00)
  Started deleting packages > buildpack_java/27d3334505188b901ec447e4e2b846a2ae34edb7. Done (00:00:00)
  Started deleting packages > buildpack_java_offline/ec0579afca7d3d05a9bd112e23ed9ffa4668fb25. Failed: PG::Error: ERROR:  update or delete on table ""packages"" violates foreign key constraint ""compiled_packages_package_id_fkey"" on table ""compiled_packages""
DETAIL:  Key (id)=(5) is still referenced from table ""compiled_packages"".
 (00:00:00)

Error 100: PG::Error: ERROR:  update or delete on table ""packages"" violates foreign key constraint ""compiled_packages_package_id_fkey"" on table ""compiled_packages""
DETAIL:  Key (id)=(5) is still referenced from table ""compiled_packages"".


Task 27 error

For a more detailed error report, run: bosh task 27 --debug
```",,90745272,story,[],deletion of a release results in a failure of deletion of compiled packages,,[],956238,81882,bug,2015-05-04T18:29:19Z,https://www.pivotaltracker.com/story/show/90745272
,2015-03-18T21:38:29Z,unscheduled,,"We had an errand VM still running from an earlier `run errand --keep-alive` command.

We updated our manifest to remove the network the errand job used and replace it with another. The new network had a different name and a different AWS subnet, but both were dynamic.

Then we deployed and ran the errand again, still with `--keep-alive`.

BOSH did not recreate our VM, so it was in the wrong subnet. Additionally, the update failed due to a nil pointer error. We'd expect BOSH to recreate our VM since its network configuration changed.",,90649556,story,[],Failed to recreate VM on dynamic network change,,[],956238,503831,bug,2015-03-18T21:38:44Z,https://www.pivotaltracker.com/story/show/90649556
,2015-03-18T21:23:34Z,unscheduled,,"Using bosh cli version 2865:

Missing spaces in ""Deleting old stemcells""

```
○ → bosh cleanup

Cleanup command will attempt to delete old unused
release versions and stemcells from your currently
targeted director at vpc-bosh-gocd.

Only 2 latest versions of each release
and 2 latest versions of each stemcell will be kept.

Releases and stemcells that are in use will not be affected.

Are you sure? (type 'yes' to continue): yes

Deleting old stemcells
  bosh-aws-xen-ubuntu-trusty-go_agent/2751DELETED
  bosh-aws-xen-ubuntu-trusty-go_agent/2778DELETED
  bosh-aws-xen-ubuntu-trusty-go_agent/2789DELETED
```",,90648438,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",MIssing space in bosh cleanup output.,,[],956238,756869,feature,2015-06-24T22:38:44Z,https://www.pivotaltracker.com/story/show/90648438
,2015-03-13T21:14:13Z,unscheduled,,"Currently, when the resurrector is enabled (via manifest) but paused (via `bosh vm resurrection off`) it is only paused on the director, not the resurrector. So the resurrector still tells the director to scan and fix the problem, which locks the deployment while running. The scan proceeds, but the fix is skipped (without telling the user via debug or event log).

The optimal solution would be to have the resurrector figure out if resurrection is paused via it's normal polling of the director for vms. Then the resurrector wouldn't need to tell the director to do anything and the director wouldn't lock the deployment.

The easy suboptimal fix would be to just add an event log message in the director saying that resurrection was skipped.",,90317828,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]","When resurrection is paused, the health monitor should not request that a missing/unresponsive vm be resurrected",,[],956238,1266616,feature,2015-03-13T21:14:41Z,https://www.pivotaltracker.com/story/show/90317828
,2015-03-13T01:06:17Z,unscheduled,,"During the deploy v203 on staging, one of the api_worker_z2/0 took over two hours to perform its update before we logged in and identified that the bosh-blobstore process that was hung. Unfortunately, the lsof output was not captured. It appears the kernel murdered the process or it failed in some way as shown below in the dmesg output. When we attempted to strace the process, strace itself hung, requiring a kill -9 to finish. We tried to save the strace process by killing bosh-blobstore process, which did not respond to a kill -15, then we kill -9'd it. 

After this was done, the deploy continued on. 


```
± |master ✗| → bosh task 1910

Director task 1910
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:01)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:01)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started preparing configuration > Binding configuration. Done (00:00:12)

  Started updating job nats_z2 > nats_z2/0 (canary). Done (00:00:31)
  Started updating job etcd_z1
  Started updating job etcd_z1 > etcd_z1/0 (canary). Done (00:02:43)
  Started updating job etcd_z1 > etcd_z1/1. Done (00:01:32)
     Done updating job etcd_z1 (00:04:15)
  Started updating job etcd_z2 > etcd_z2/0 (canary). Done (00:01:27)
  Started updating job stats_z1 > stats_z1/0 (canary). Done (00:00:21)
  Started updating job uaa_z1 > uaa_z1/0 (canary). Done (00:00:21)
  Started updating job uaa_z2 > uaa_z2/0 (canary). Done (00:00:31)
  Started updating job login_z1 > login_z1/0 (canary). Done (00:00:31)
  Started updating job login_z2 > login_z2/0 (canary). Done (00:00:32)
  Started updating job api_z1
  Started updating job api_z1 > api_z1/0 (canary). Done (00:02:23)
  Started updating job api_z1 > api_z1/1. Done (00:00:52)
     Done updating job api_z1 (00:03:15)
  Started updating job api_z2
  Started updating job api_z2 > api_z2/0 (canary). Done (00:00:52)
  Started updating job api_z2 > api_z2/1. Done (00:00:50)
     Done updating job api_z2 (00:01:42)
  Started updating job clock_global > clock_global/0 (canary). Done (00:01:13)
  Started updating job api_worker_z1 > api_worker_z1/0 (canary). Done (00:00:43)
  Started updating job api_worker_z2 > api_worker_z2/0 (canary). Done (02:36:27)
  Started updating job hm9000_z1 > hm9000_z1/0 (canary). Done (00:01:33)




[774960.108076] INFO: task bosh-blobstore-:14456 blocked for more than 120 seconds.
[774960.108155]       Not tainted 3.16.0-31-generic #41~14.04.1-Ubuntu
[774960.108214] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[774960.108289] bosh-blobstore- D 0000000000000000     0 14456    775 0x00000000
[774960.108293]  ffff880005553b50 0000000000000282 ffff8800e656bd20 ffff880005553fd8
[774960.108295]  0000000000013440 0000000000013440 ffff8800e6621e90 ffff880005553d58
[774960.108296]  ffff880005553d60 7fffffffffffffff ffff8800e656bd20 ffff880005553d10
[774960.108298] Call Trace:
[774960.108305]  [<ffffffff817675c9>] schedule+0x29/0x70
[774960.108307]  [<ffffffff817669a9>] schedule_timeout+0x229/0x2a0
[774960.108309]  [<ffffffff8176715f>] ? __schedule+0x35f/0x7a0
[774960.108311]  [<ffffffff817680c6>] wait_for_completion+0xa6/0x160
[774960.108315]  [<ffffffff810a1840>] ? wake_up_state+0x20/0x20
[774960.108318]  [<ffffffff8122d655>] do_coredump+0x335/0xe60
[774960.108322]  [<ffffffff8168ef8d>] ? netlink_broadcast+0x1d/0x20
[774960.108325]  [<ffffffff814b2718>] ? cn_netlink_send_mult+0x1a8/0x200
[774960.108330]  [<ffffffff810804d7>] get_signal_to_deliver+0x1c7/0x6f0
[774960.108334]  [<ffffffff81012548>] do_signal+0x48/0xad0
[774960.108337]  [<ffffffff81070000>] ? do_exit+0x20/0xa60
[774960.108339]  [<ffffffff8107d7eb>] ? recalc_sigpending+0x1b/0x50
[774960.108341]  [<ffffffff811d5a80>] ? __fput+0x190/0x220
[774960.108344]  [<ffffffff8100a140>] ? xen_clocksource_get_cycles+0x20/0x30
[774960.108346]  [<ffffffff81013039>] do_notify_resume+0x69/0xb0
[774960.108349]  [<ffffffff8176c462>] retint_signal+0x48/0x86

[775080.108076] INFO: task bosh-blobstore-:14456 blocked for more than 120 seconds.
[775080.108155]       Not tainted 3.16.0-31-generic #41~14.04.1-Ubuntu
[775080.108214] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[775080.108289] bosh-blobstore- D 0000000000000000     0 14456    775 0x00000000
[775080.108292]  ffff880005553b50 0000000000000282 ffff8800e656bd20 ffff880005553fd8
[775080.108294]  0000000000013440 0000000000013440 ffff8800e6621e90 ffff880005553d58
[775080.108296]  ffff880005553d60 7fffffffffffffff ffff8800e656bd20 ffff880005553d10
[775080.108298] Call Trace:
[775080.108305]  [<ffffffff817675c9>] schedule+0x29/0x70
[775080.108307]  [<ffffffff817669a9>] schedule_timeout+0x229/0x2a0
[775080.108308]  [<ffffffff8176715f>] ? __schedule+0x35f/0x7a0
[775080.108310]  [<ffffffff817680c6>] wait_for_completion+0xa6/0x160
[775080.108314]  [<ffffffff810a1840>] ? wake_up_state+0x20/0x20
[775080.108317]  [<ffffffff8122d655>] do_coredump+0x335/0xe60
[775080.108322]  [<ffffffff8168ef8d>] ? netlink_broadcast+0x1d/0x20
[775080.108324]  [<ffffffff814b2718>] ? cn_netlink_send_mult+0x1a8/0x200
[775080.108328]  [<ffffffff810804d7>] get_signal_to_deliver+0x1c7/0x6f0
[775080.108332]  [<ffffffff81012548>] do_signal+0x48/0xad0
[775080.108335]  [<ffffffff81070000>] ? do_exit+0x20/0xa60
[775080.108337]  [<ffffffff8107d7eb>] ? recalc_sigpending+0x1b/0x50
[775080.108339]  [<ffffffff811d5a80>] ? __fput+0x190/0x220
[775080.108342]  [<ffffffff8100a140>] ? xen_clocksource_get_cycles+0x20/0x30
[775080.108344]  [<ffffffff81013039>] do_notify_resume+0x69/0xb0
[775080.108347]  [<ffffffff8176c462>] retint_signal+0x48/0x86
```",,90247022,story,[],vm hang during deploy,,[],956238,1591058,bug,2015-03-13T16:58:05Z,https://www.pivotaltracker.com/story/show/90247022
,2015-03-11T18:16:56Z,unscheduled,,"`bosh init release asdf --git` should create `.gitignore`, but it doesn't appear to work:
```
ruby 2.1.4p265 tara in ~/workspace
○ → bosh init release asdf --git
Release directory initialized

ruby 2.1.4p265 tara in ~/workspace
○ → ls asdf/.gitignore
ls: asdf/.gitignore: No such file or directory

ruby 2.1.4p265 tara in ~/workspace
○ → bosh version
BOSH 1.2865.0
```
Or possibly the [documentation](http://bosh.io/docs/create-release.html#prep) is wrong: ""*If you used the --git option with bosh init release (as recommended), the correct .gitignore file has been automatically created for you.*""",,90127268,story,[],`bosh init release asdf --git` should create .gitignore,,[],956238,353433,bug,2015-12-27T21:56:02Z,https://www.pivotaltracker.com/story/show/90127268
,2015-03-13T18:50:01Z,unscheduled,,"WIP; Maybe something like this:

PUT /self-update?id=ae9f6a79facd862
-> responds with 200 or ??? when install.sh finishes

GET /self-update-status?id=ae9f6a79facd862
-> responds with 200 and ""status=ok"" or ""error"" or ""still running""

GET /self-update-log?id=ae9f6a79facd862
-> responds with streaming stderr log",2.0,90307350,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Caller of PUT /self-update can see installation progress (streamed logs?) and install script output somehow.,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307350
,2015-03-13T22:28:05Z,unscheduled,,active mode.,1.0,90322302,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",boostrapper in active mode verifies download server's cert against local ca,,[],956238,553935,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90322302
,2015-03-13T18:50:01Z,unscheduled,,,2.0,90307362,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper can be replaced with agent in bootstrapped tarball.,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307362
,2015-03-13T22:33:27Z,unscheduled,,,1.0,90322480,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",boostrapper configures ntp,,[],956238,553935,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90322480
,2015-03-13T18:50:02Z,unscheduled,,,2.0,90307364,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bootstrapper configures network interfaces according to metadata.,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307364
,2015-03-13T18:50:02Z,unscheduled,,split to separate cpi stories,4.0,90307366,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]","When bosh detects a bootstrappable stemcell, it tells the stemcell (via CPI metadata) to bootstrap instead of starting agent [feature flagged].",,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307366
,2015-03-13T18:50:02Z,unscheduled,,"either director endpoint or blobstore 
",2.0,90307368,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}, {'name': 'perf-issues', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-16T18:30:45Z', 'id': 11131764, 'updated_at': '2015-03-16T18:30:45Z'}]",Bosh provides a downloadable agent tarball for active bootstrappers.,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307368
,2015-03-13T18:50:02Z,unscheduled,,,,90307372,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]","Investigate if we can impose TLS security stuff described above on agent https, since it has existing clients?",,[],956238,1348450,chore,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307372
,2015-03-13T18:50:01Z,unscheduled,,,,90307358,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Agent from tarball comes up and does things!,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307358
,2015-03-13T22:21:24Z,unscheduled,,,,90321998,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",boostrapper is included in the stemcell,,[],956238,553935,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90321998
,2015-03-13T18:52:08Z,unscheduled,,,,90307536,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Changes to agent don't kick off full stemcell build pipeline,,[],956238,1348450,chore,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307536
,2015-03-13T18:50:01Z,unscheduled,,,,90307360,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]",Bosh connects to bootstrapper to upload agent tarball when needed (in passive bootstrapper case).,,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307360
,2015-03-13T18:50:01Z,unscheduled,,,,90307356,story,"[{'name': 'bootstrap', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-13T18:50:01Z', 'id': 11114046, 'updated_at': '2015-03-13T18:50:01Z'}]","Bosh generates a tarball appropriate for the os+arch being bootstrapped, which includes {agent sources + buildpack | agent executables} — how do we compile go code? how do we distribute code and buildpack?",,[],956238,1348450,feature,2015-03-31T21:27:33Z,https://www.pivotaltracker.com/story/show/90307356
,2015-03-06T00:26:53Z,unscheduled,,,,89762040,story,[],Bosh should provide an option to clean up compiled packages,,[],956238,553935,feature,2015-03-06T00:26:53Z,https://www.pivotaltracker.com/story/show/89762040
,2015-03-05T22:02:52Z,unscheduled,,"Right now we only update the gocd repository image. The images on docker hub become out of date (e.g., one was missing git user configuration).

Concourse pipelines should publish to docker hub so that:
A) The other Concourse pipelines will use up-to-date images (Concourse/Garden cannot access the private registry).
B) External developers can run our tests in a docker container the same way we do.
",,89750758,story,"[{'name': 'build-pair', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-05-21T23:13:09Z', 'id': 11746952, 'updated_at': '2015-05-21T23:13:09Z'}, {'name': 'ci', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034568, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'concourse', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-04-28T21:22:36Z', 'id': 11534756, 'updated_at': '2015-04-28T21:22:36Z'}]",Publish our docker images to docker hub when building a new image on Concourse,,[],956238,687691,feature,2015-10-08T17:40:47Z,https://www.pivotaltracker.com/story/show/89750758
,2015-03-05T19:28:15Z,unscheduled,,"To reproduce:
1. Log in as default admin
2. Create a new user
3. Try to use bosh command that requires login
4. See error

```
$ bosh login
Your username: admin
Enter password: *****
Logged in as `admin'

$ bosh create user test
Enter new password: ****
Verify new password: ****
User `test' has been 

$ bosh upload release  https://bosh.io/d/github.com/cloudfoundry/bosh?v=149

Using remote release `https://bosh.io/d/github.com/cloudfoundry/bosh?v=149'

Task exited with status failed
```

** Expected: ** `Please log in first`
** Actual: ** `Task exited with status failed`",,89731856,story,[],CLI gives unhelpful error (*Task exited with status failed*) when logged in with invalid user and running commands,,[],956238,553935,bug,2015-03-05T19:31:31Z,https://www.pivotaltracker.com/story/show/89731856
,2015-02-20T17:41:17Z,unscheduled,,"we should see only './P/F' (hopefully only the former)

If there is some log output that is useful for debugging we should write that to log files not to stdout.",,88801800,story,[],Clean up unit test output,,[],956238,119,chore,2015-02-20T17:41:17Z,https://www.pivotaltracker.com/story/show/88801800
,2015-02-13T00:28:53Z,unscheduled,,"Per [docs](http://docs.cloudfoundry.org/bosh/networks.html), gateway *should* be optional:
```
gateway [String, optional]: Subnet gateway IP
```
**but**, if the gateway is left out on Ubuntu stemcell, deployment will produce with a VM with invalid networking configuration (`/etc/network/interfaces`) (blank gateway field):
```
....
gateway
```",,88292094,story,[],Ubuntu stemcell networking is broken if no gateway specified in manifest,,[],956238,756869,feature,2015-02-13T00:28:54Z,https://www.pivotaltracker.com/story/show/88292094
,2015-02-12T23:57:43Z,unscheduled,,"Currently just says it did not finish successfully.  Note `bosh ssh` works just fine.

```
 > bosh scp --download cell_z1 0 /var/vcap/sys/log/executor/executor.stdout.log.1 /tmp/cell_z1-0/executor/executor.stdout.log.1.manual

Processing deployment manifest
------------------------------
Executing file operations on job cell_z1

Processing deployment manifest
------------------------------
Target deployment is `cf-ketchup-diego'

Setting up ssh artifacts

Director task 20027

Task 20027 done

Cleaning up ssh artifacts

Director task 20028

Task 20028 done
/Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-scp-1.1.2/lib/net/scp.rb:359:in `block (3 levels) in start_command': SCP did not finish successfully (1) (Net::SCP::Error)
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/channel.rb:591:in `call'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/channel.rb:591:in `do_close'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:587:in `channel_close'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:466:in `dispatch_incoming_packets'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:222:in `preprocess'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:206:in `process'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:170:in `block in loop'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:170:in `loop'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:170:in `loop'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh/connection/session.rb:119:in `close'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh.rb:217:in `ensure in start'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/net-ssh-2.9.2/lib/net/ssh.rb:217:in `start'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:263:in `with_ssh'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:209:in `block (2 levels) in perform_operation'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:204:in `each'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:204:in `block in perform_operation'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:153:in `setup_ssh'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:203:in `perform_operation'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/commands/ssh.rb:64:in `scp'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.rubies/ruby-1.9.3-p547/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2840.0/bin/bosh:7:in `<top (required)>'
	from /usr/bin/bosh:23:in `load'
	from /usr/bin/bosh:23:in `<main>'
```",,88290280,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",bosh scp failure message should be more informative,,[],956238,668075,feature,2015-06-24T22:38:38Z,https://www.pivotaltracker.com/story/show/88290280
,2015-02-24T17:22:40Z,unscheduled,,seen on cw openstack environment.,,89019098,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should not fail to bootstrap due to connection reuse in golang,,[],956238,81882,feature,2015-02-24T23:03:56Z,https://www.pivotaltracker.com/story/show/89019098
,2015-02-09T23:20:41Z,unscheduled,,"in [Bosh::Director::CompiledPackage](https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/compiled_package/compiled_package_inserter.rb) when the [stemcell model returns nil](https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/compiled_package/compiled_package_inserter.rb#L13), the compiled package upload task will fail with:

```
Error 100: PG::Error: ERROR:  column ""stemcell"" does not exist
LINE 1: ...HERE ((""compiled_packages"".""package_id"" = 1) AND (""stemcell""...
                                                             ^
```

Because the [compiled package query](https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/compiled_package/compiled_package_inserter.rb#L18) interprets `stemcell: nil` as a query on a column called `stemcell`, not a query on a NULL association of stemcell (`stemcell_id = NULL`)

We encountered this when uploading compiled packages for which there was no stemcell present on the director",,87998118,story,[],Compiled package upload fails if stemcell is not found,,[],956238,1054467,bug,2015-02-09T23:23:23Z,https://www.pivotaltracker.com/story/show/87998118
,2015-02-10T19:06:17Z,unscheduled,,"- BOSH deployment manifest has ""somenetwork"" as the network name

- vSphere datacenter has a DVS name ""somenetwork""

- ""somenetwork"" DVS has a portgroup also named ""somenetwork""

BOSH deploys without fail but the VM created has a disconnected nick. The dropdown in Edit Settings shows the network name with no parentheses and is in a broken state.

The vSphere CPI uses find_by_inventory_path to find the network mob:

https://github.com/cloudfoundry/bosh/blob/2840387d687a44569edd9cdc2e49ecba129634b8/bosh_vsphere_cpi/lib/cloud/vsphere/vm_creator.rb#L59

This method finds the DVS instead of the portgroup because it has the same name.

Note that after the deploy has run ""successfully"" a pseudo network with that name will exist on the vSphere and any calls to find_by_inventory_path after that will return this broken network.",,88072960,story,[],vSphere network assignment fails if a distributed virtual switch has the same name as a distributed virtual portgroup.,,[],956238,756869,bug,2015-02-10T19:39:09Z,https://www.pivotaltracker.com/story/show/88072960
,2015-02-09T23:20:45Z,unscheduled,,,,87998132,story,[],Release .blobs should be cached outside the release source tree,,[],956238,553935,feature,2015-02-09T23:23:27Z,https://www.pivotaltracker.com/story/show/87998132
,2015-02-09T20:44:40Z,unscheduled,,"If you don't put a gateway for *both* interfaces on a multi-homed host, *neither* interface will come up (your networking config will be broken):
* affects ubuntu (we didn't try CentOS)

(Note: unless you're a BOSH-deploying a networking router (which you're not) you only want one default route; you don't want two)

Our manifest's network section (note: *ten_gig* has no *gateway*):
```
networks:
- name: default
  subnets:
  - cloud_properties:
      name: admin
    range: 172.16.74.0/23
    dns:
      - 10.80.130.1
    gateway: 172.16.74.1
    static:
    - 172.16.74.61 - 172.16.74.62
    reserved:
    - 172.16.74.2  - 172.16.74.59
    - 172.16.74.63 - 172.16.75.254
- name: ten_gig
  subnets:
  - cloud_properties:
      name: nested-vmotion
    range: 192.168.198.0/23
    static:
    - 192.168.198.15 - 192.168.198.30
    reserved:
    - 192.168.198.2  - 192.168.198.14
    - 192.168.198.31 - 192.168.199.254
```

Produces `/etc/network/interfaces` similar to the following:
```
# Generated by bosh-agent
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
    address 172.16.74.62
    network 172.16.74.0
    netmask 255.255.254.0
    broadcast 172.16.75.255
    gateway 172.16.74.1
auto eth1
iface eth1 inet static
    address 192.168.198.16
    network 192.168.198.0
    netmask 255.255.254.0
    broadcast 192.168.199.255
    gateway
```
Typing `ifup eth0 eth1` gives the following error:
```
/etc/network/interfaces:18: option with empty value
```",,87982930,story,[],Multi-homed hosts don't handle single gateway properly,,[],956238,353433,bug,2015-02-13T01:03:01Z,https://www.pivotaltracker.com/story/show/87982930
,2015-02-04T19:46:44Z,unscheduled,,Currently there is a race condition when stopping workers/director(check others?) - https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/worker_ctl.erb#L69 if some other process starts at that PID it just loops forever spiking the CPU.,,87653946,story,[],Control scripts in bosh release should only stop current process,,[],956238,553935,feature,2015-02-04T19:46:44Z,https://www.pivotaltracker.com/story/show/87653946
,2015-02-04T18:57:10Z,unscheduled,,"Currently logs don't include timestamps for some reason, which makes it hard to debug things.",,87649460,story,[],Worker logs should include timestamps,,[],956238,553935,feature,2015-02-04T18:57:10Z,https://www.pivotaltracker.com/story/show/87649460
,2015-01-20T21:37:05Z,unscheduled,,"[Bosh::Director::DownloadHelper](https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/download_helper.rb#L18) disables SSL cert verification.

Using OpenSSL::SSL::VERIFY_NONE is a [Very Bad Idea](http://www.rubyinside.com/how-to-cure-nethttps-risky-default-https-behavior-4010.html) because security.

(There are several other places where we do this, including the agent client, the blobstore client, the cli, and the vsphere cpi. They probably all deserve to at least be audited.)",,86564282,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Director should verify SSL certificates when downloading stemcells/releases,,[],956238,1348450,feature,2015-06-24T22:38:24Z,https://www.pivotaltracker.com/story/show/86564282
,2015-01-29T19:35:44Z,unscheduled,,"saw this log on staging while restar bosh monitor

`bosh-monitor-1.2778.0`

```
I, [2015-01-29T19:19:59.055761 #23421]  INFO : HealthMonitor exiting!
I, [2015-01-29T19:20:40.105234 #31386]  INFO : HealthMonitor starting...
I, [2015-01-29T19:20:40.107680 #31386]  INFO : Logging delivery agent is running...
I, [2015-01-29T19:20:40.108088 #31386]  INFO : Resurrector is running...
I, [2015-01-29T19:20:40.108233 #31386]  INFO : DataDog plugin is running...
I, [2015-01-29T19:20:40.109565 #31386]  INFO : HTTP server is starting on port 25923...
I, [2015-01-29T19:20:40.164778 #31386]  INFO : BOSH HealthMonitor 1.2778.0 is running...
I, [2015-01-29T19:20:40.167106 #31386]  INFO : Connected to NATS at `nats://10.10.0.7:4222'
I, [2015-01-29T19:20:40.665075 #31386]  INFO : Found deployment `cf-staging'
W, [2015-01-29T19:20:40.974739 #31386]  WARN : Received heartbeat from unmanaged agent: 8699a59f-6832-408f-9e5e-e25f6bc4c453
F, [2015-01-29T19:20:41.047330 #31386] FATAL : expected string value for key value of member 4 of key dimensions of member 1 of option metric_data
F, [2015-01-29T19:20:41.047613 #31386] FATAL : /var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:93:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:346:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:342:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:342:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:245:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:243:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:243:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:346:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:342:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:342:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:245:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:243:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:243:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:590:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:586:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:586:in `validate'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/option_grammar.rb:601:in `request_params'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/query_request_builder.rb:37:in `populate_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:740:in `block (2 levels) in define_client_method'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:560:in `build_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:491:in `block (3 levels) in client_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/response.rb:175:in `call'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/response.rb:175:in `build_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/response.rb:114:in `initialize'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:203:in `new'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:203:in `new_response'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:490:in `block (2 levels) in client_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:391:in `log_client_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:477:in `block in client_request'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:373:in `return_or_raise'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `put_metric_data'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/cloud_watch.rb:98:in `put_metric_data'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/plugins/cloud_watch.rb:19:in `process'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/event_processor.rb:104:in `plugin_process'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/event_processor.rb:50:in `block in process'
/var/vcap/packages/ruby/lib/ruby/2.1.0/set.rb:263:in `each_key'
/var/vcap/packages/ruby/lib/ruby/2.1.0/set.rb:263:in `each'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/event_processor.rb:49:in `process'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/agent_manager.rb:289:in `on_heartbeat'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/agent_manager.rb:257:in `process_event'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/agent_manager.rb:53:in `block (2 levels) in setup_events'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/nats-0.5.0.beta.12/lib/nats/client.rb:503:in `call'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/nats-0.5.0.beta.12/lib/nats/client.rb:503:in `on_msg'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/nats-0.5.0.beta.12/lib/nats/client.rb:563:in `receive_data'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/eventmachine-1.0.3/lib/eventmachine.rb:187:in `run_machine'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/eventmachine-1.0.3/lib/eventmachine.rb:187:in `run'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/lib/bosh/monitor/runner.rb:26:in `run'
/var/vcap/packages/health_monitor/gem_home/ruby/2.1.0/gems/bosh-monitor-1.2778.0/bin/bosh-monitor:31:in `<top (required)>'
/var/vcap/packages/health_monitor/bin/bosh-monitor:16:in `load'
/var/vcap/packages/health_monitor/bin/bosh-monitor:16:in `<main>'
```
",,87234700,story,"[{'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",cloudwatch plugin of bosh monitor is broken,,[],956238,676405,bug,2015-06-24T22:38:32Z,https://www.pivotaltracker.com/story/show/87234700
,2015-01-15T23:29:10Z,unscheduled,,"We are working on the bosh director backup by using the ""bosh backup"" command.
This command creates temporary files in the default TMPDIR that the director_ctl job sets to:
/var/vcap/data/tmp/director
The final backup.tgz file is then stored in #{Config.base_dir} that by default is:
/var/vcap/store/director
Our concern in using these two dirs is that very large deployments can fill up the filesystem.
We would have a way to configure the director to use a specific directory for backup actions, for example with a new option in the director.yml.",,86261522,story,[],Bosh backup filling up disk,,[],956238,1210852,feature,2015-05-29T20:44:56Z,https://www.pivotaltracker.com/story/show/86261522
,2015-01-15T22:15:44Z,unscheduled,,"Bill Keenan and I are talking about a problem that he hit at a customer site. The problem comes up frequently. The example that he hit was with the following error, but this is one example of many, so let's not focus on fixing this specific problem per se, but spend time thinking about the idea of general improvements to error messages.

Error 100: No available resources has been reported in conjunction with ""bound VMs"" - another snippet of English you can grep for if you look at the task logs. It is still a mystery what resources we are talking about here - disk? network IPs? 

There are a number of examples where the initial error you get leads you to look at the task log for the error, which is not always possible, especially for people in the field who are trying to do an installation at a customer using a remote terminal session. Can we work on making some of these errors more instructive - instead of using vague terms like ""resources?""

Goal is rapid mitigation of an issue. Is it Disk, Syntax, CPU, Memory, etc...

The way I would approach this is to Google Group and search for the word ""error"" - see which ones come back and talk about whether that error gives the user a bread crumb to begin fact finding.",,86256386,story,[],** Spike ** on how to better reveal errors in BOSH logs,,[],956238,548715,feature,2015-01-15T22:16:28Z,https://www.pivotaltracker.com/story/show/86256386
,2015-01-15T22:10:38Z,unscheduled,,"From exploration in #85663616:

Here are the logs for a fresh riak tile installation on a new Ops Manager containing only the runtime tile. Looks like it's still choosing to ""skip"" uploading certain packages, then compiling them at deploy (ruby, golang, common, broker-registrar). Let us know if you need any additional information about this behavior.

Here's a gist of the install log:
https://gist.github.com/shalako/4639ec96327046e04799

Env may be recreated at any time, but here's the current direct link:
https://pcf.bear.wild.cf-app.com/installation_logs/3",,86255972,story,[],bosh skips precompiled packages,,[],956238,58676,bug,2015-01-15T22:51:10Z,https://www.pivotaltracker.com/story/show/86255972
,2015-01-15T22:06:24Z,unscheduled,,"to reproduce:

1. get unresponsive agent vm
2. recreate vm using last knowm spec
3. see the error:


```
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: (0.001178s) SELECT * FROM `vms` WHERE `id` = 38
I, [2015-01-15 21:59:24 #9922] [task:170]  INFO -- DirectorJobRunner: missing_vm 38: Recreate VM using last known apply spec
E, [2015-01-15 21:59:24 #9922] [task:170] ERROR -- DirectorJobRunner: Error resolving problem `2': undefined method `fetch' for ""package_compiler"":String
E, [2015-01-15 21:59:24 #9922] [task:170] ERROR -- DirectorJobRunner: /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/cloudcheck_helper.rb:95:in `recreate_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_handlers/missing_vm.rb:21:in `block (2 levels) in <class:MissingVM>'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:59:in `block in apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:21:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:58:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:42:in `block in apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `block in each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block (2 levels) in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:89:in `_execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `block in execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:801:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:171:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:140:in `fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/problem_resolver.rb:33:in `apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:38:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:37:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2778.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2778.0/bin/bosh-director-worker:83:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:gocd-oss
D, [2015-01-15 21:59:24 #9922] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:gocd-oss
I, [2015-01-15 21:59:24 #9922] [task:170]  INFO -- DirectorJobRunner: Done
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: (0.000706s) BEGIN
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: (0.001227s) UPDATE `tasks` SET `state` = 'done', `timestamp` = '2015-01-15 21:59:24', `description` = 'apply resolutions', `result` = '0 resolved', `output` = '/var/vcap/store/director/tasks/170', `user_id` = 1, `checkpoint_time` = '2015-01-15 21:59:24', `type` = 'cck_apply' WHERE (`id` = 170) LIMIT 1
D, [2015-01-15 21:59:24 #9922] [task:170] DEBUG -- DirectorJobRunner: (0.036758s) COMMIT
I, [2015-01-15 21:59:24 #9922] []  INFO -- DirectorJobRunner: Task took 0.098941456 seconds to process.
 INFO  DirectorTask : Director Version : 1.2778.0
 INFO  DirectorTask : Enqueuing task: 171
 INFO  DirectorTask : Director Version : 1.2778.0
 INFO  DirectorTask : Enqueuing task: 172
```",,86255686,story,[],bosh cck should not fail with unknown fetch for package compiler error,,[],956238,353433,bug,2015-01-15T22:16:11Z,https://www.pivotaltracker.com/story/show/86255686
,2015-01-14T19:04:30Z,unscheduled,,"we run into a corrupted compile package issue, https://www.pivotaltracker.com/story/show/86076508

to fix it, we need manually remove the bad ones in global cache  bucket and local bosh db and local blobstore bucket.

Hope bosh can make this process less painful",,86139922,story,[],bosh should be able to delete corrupted compile package in local blob store and global cache,,[],956238,676405,feature,2015-01-27T20:16:27Z,https://www.pivotaltracker.com/story/show/86139922
,2015-01-14T00:26:04Z,unscheduled,,"Trying to upload blobs from the bosh-vcloud-cpi-release failed with a Broken Pipe error. We also saw a similar 'connection reset by peer' error from the same command.

We thought it might be an old openssl being used, so we brew installed a new one, but got the same error.

On mac os x, ruby 2.1.2:

```
~/workspace/bosh-vcloud-cpi-release
→ bosh upload blobs
Total: 0, 0B

You have some blobs that need to be uploaded:
new	ruby_vcloud_cpi/ruby-1.9.3-p545.tar.gz	12.0M
new	ruby_vcloud_cpi/rubygems-2.1.11.tgz	358.6K
new	ruby_vcloud_cpi/yaml-0.1.5.tar.gz	493.1K

When ready please run `bosh upload blobs'

Upload blob ruby_vcloud_cpi/ruby-1.9.3-p545.tar.gz? (type 'yes' to continue): yes
ruby_vcloud_cpi/ruby-1.9.3-p545.tar.gz uploading...
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/base.rb:39:in `rescue in create': Failed to create object, underlying error: #<Errno::EPIPE: Broken pipe> /Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/openssl/buffering.rb:326:in `syswrite' (Bosh::Blobstore::BlobstoreError)
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/openssl/buffering.rb:326:in `do_write'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/openssl/buffering.rb:344:in `write'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http/generic_request.rb:202:in `copy_stream'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http/generic_request.rb:202:in `send_request_with_body_stream'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http/generic_request.rb:132:in `exec'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http.rb:1406:in `block in transport_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http.rb:1405:in `catch'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http.rb:1405:in `transport_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/net/http.rb:1378:in `request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/http/connection_pool.rb:350:in `request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/http/net_http_handler.rb:63:in `block in handle'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/http/connection_pool.rb:129:in `session_for'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/http/net_http_handler.rb:55:in `handle'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:253:in `block in make_sync_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:289:in `retry_server_errors'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:249:in `make_sync_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:511:in `block (2 levels) in client_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:391:in `log_client_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:477:in `block in client_request'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:373:in `return_or_raise'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `put_object'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:1752:in `write_with_put_object'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:607:in `write'
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/s3_blobstore_client.rb:147:in `block in store_in_s3'
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/s3_blobstore_client.rb:146:in `open'
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/s3_blobstore_client.rb:146:in `store_in_s3'
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/s3_blobstore_client.rb:74:in `create_file'
/Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/base.rb:27:in `create'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/forwardable.rb:183:in `create'
/Users/pivotal/.rubies/ruby-2.1.2/lib/ruby/2.1.0/forwardable.rb:183:in `create'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/blob_manager.rb:269:in `upload_blob'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:41:in `block in upload'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:38:in `each'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:38:in `upload'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/command_handler.rb:57:in `run'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/runner.rb:56:in `run'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/runner.rb:16:in `run'
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/bin/bosh:7:in `<top (required)>'
/Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
/Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/blobstore_client-1.2811.0/lib/blobstore_client/base.rb:26:in `create'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/blob_manager.rb:269:in `upload_blob'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:41:in `block in upload'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:38:in `each'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/commands/blob_management.rb:38:in `upload'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2811.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'
```",,86071882,story,[],upload blobs fails with Broken pipe,,[],956238,1266616,bug,2015-01-14T01:26:19Z,https://www.pivotaltracker.com/story/show/86071882
,2015-01-13T21:49:25Z,unscheduled,,"This page references having a bin/run script in the release, but has no examples or links to documentation that tells how that is/can be done.

http://docs.cloudfoundry.org/bosh/jobs.html

",,86059206,story,[],Errand documentation should have or link to instructions for creating a job errand run script,,[],956238,344,chore,2015-01-13T21:49:25Z,https://www.pivotaltracker.com/story/show/86059206
,2015-01-13T01:37:09Z,unscheduled,,"Reported by runtime, their openstack was updated (on live VM) and persistent disk got  mounted as read only:

```
cat /proc/mounts
rootfs / rootfs rw 0 0
none /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
none /proc proc rw,nosuid,nodev,noexec,relatime 0 0
none /dev devtmpfs rw,relatime,size=1021944k,nr_inodes=255486,mode=755 0 0
none /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/disk/by-uuid/6d4b9136-ca02-4f8c-8e49-3178a8311735 / ext4 rw,relatime,user_xattr,barrier=1,data=ordered 0 0
none /sys/fs/fuse/connections fusectl rw,relatime 0 0
none /sys/kernel/debug debugfs rw,relatime 0 0
none /sys/kernel/security securityfs rw,relatime 0 0
none /dev/shm tmpfs rw,nosuid,nodev,relatime 0 0
none /var/run tmpfs rw,nosuid,relatime,mode=755 0 0
none /var/lock tmpfs rw,nosuid,nodev,noexec,relatime 0 0
none /lib/init/rw tmpfs rw,nosuid,relatime,mode=755 0 0
/dev/vdb2 /var/vcap/data ext4 rw,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
/dev/loop0 /tmp ext4 rw,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
/dev/vdc1 /var/vcap/store ext4 ro,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
```

So everything was failing to use persistent disk with error (e.g. postgres):

```
chown: changing ownership of `/var/vcap/store/postgres/base/16384/16702': Read-only file system
```

It seems that sometimes fs can get into read-only state (http://askubuntu.com/a/197468)

Agent logs:

```
2015-01-13_01:36:24.38694 [File System] 2015/01/13 01:36:24 DEBUG - Reading file /proc/mounts
2015-01-13_01:36:24.38694 [File System] 2015/01/13 01:36:24 DEBUG - Read content
2015-01-13_01:36:24.38695 ********************
2015-01-13_01:36:24.38695 rootfs / rootfs rw 0 0
2015-01-13_01:36:24.38695 none /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
2015-01-13_01:36:24.38695 none /proc proc rw,nosuid,nodev,noexec,relatime 0 0
2015-01-13_01:36:24.38696 none /dev devtmpfs rw,relatime,size=1021944k,nr_inodes=255486,mode=755 0 0
2015-01-13_01:36:24.38697 none /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
2015-01-13_01:36:24.38697 /dev/disk/by-uuid/6d4b9136-ca02-4f8c-8e49-3178a8311735 / ext4 rw,relatime,user_xattr,barrier=1,data=ordered 0 0
2015-01-13_01:36:24.38697 none /sys/fs/fuse/connections fusectl rw,relatime 0 0
2015-01-13_01:36:24.38697 none /sys/kernel/debug debugfs rw,relatime 0 0
2015-01-13_01:36:24.38698 none /sys/kernel/security securityfs rw,relatime 0 0
2015-01-13_01:36:24.38698 none /dev/shm tmpfs rw,nosuid,nodev,relatime 0 0
2015-01-13_01:36:24.38698 none /var/run tmpfs rw,nosuid,relatime,mode=755 0 0
2015-01-13_01:36:24.38699 none /var/lock tmpfs rw,nosuid,nodev,noexec,relatime 0 0
2015-01-13_01:36:24.38705 none /lib/init/rw tmpfs rw,nosuid,relatime,mode=755 0 0
2015-01-13_01:36:24.38706 /dev/vdb2 /var/vcap/data ext4 rw,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
2015-01-13_01:36:24.38706 /dev/loop0 /tmp ext4 rw,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
2015-01-13_01:36:24.38706 /dev/vdc1 /var/vcap/store ext4 ro,relatime,user_xattr,acl,barrier=1,data=ordered 0 0
2015-01-13_01:36:24.38707
2015-01-13_01:36:24.38707 ********************
2015-01-13_01:36:24.38707 [platform] 2015/01/13 01:36:24 DEBUG - Mounting persistent disk /dev/sdc at /var/vcap/store
2015-01-13_01:36:24.38707 [File System] 2015/01/13 01:36:24 DEBUG - Making dir /var/vcap/store with perm 448
2015-01-13_01:36:24.38708 [File System] 2015/01/13 01:36:24 DEBUG - Checking if file exists /dev/xvdc
2015-01-13_01:36:24.38708 [File System] 2015/01/13 01:36:24 DEBUG - Checking if file exists /dev/vdc
2015-01-13_01:36:24.38708 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Running command: sfdisk -d /dev/vdc
2015-01-13_01:36:24.39302 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Stdout:
2015-01-13_01:36:24.39303 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Stderr: read: Input/output error
2015-01-13_01:36:24.39303
2015-01-13_01:36:24.39303 sfdisk: read error on /dev/vdc - cannot read sector 0
2015-01-13_01:36:24.39303  /dev/vdc: unrecognized partition table type
2015-01-13_01:36:24.39304 No partitions found
2015-01-13_01:36:24.39304 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Successful: true (0)
2015-01-13_01:36:24.39304 [SfdiskPartitioner] 2015/01/13 01:36:24 INFO - Partitioning /dev/vdc with ,,L
2015-01-13_01:36:24.39305 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Running command: sfdisk -uM /dev/vdc
2015-01-13_01:36:24.39567 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Stdout:
2015-01-13_01:36:24.39568 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Stderr: Checking that no-one is using this disk right now ...
2015-01-13_01:36:24.39568 BLKRRPART: Device or resource busy
2015-01-13_01:36:24.39569
2015-01-13_01:36:24.39569 This disk is currently in use - repartitioning is probably a bad idea.
2015-01-13_01:36:24.39569 Umount all file systems, and swapoff all swap partitions on this disk.
2015-01-13_01:36:24.39569 Use the --no-reread flag to suppress this check.
2015-01-13_01:36:24.39570 Use the --force flag to overrule all checks.
2015-01-13_01:36:24.39570 [Cmd Runner] 2015/01/13 01:36:24 DEBUG - Successful: false (1)
2015-01-13_01:36:24.40435 [main] 2015/01/13 01:36:24 ERROR - App setup Running bootstrap: Mounting persistent disk: Partitioning disk: Shelling out to sfdisk: Running command: 'sfdisk -uM /dev/vdc', stdout: '', stderr: 'Checking that no-one is using this disk right now ...
2015-01-13_01:36:24.40435 BLKRRPART: Device or resource busy
2015-01-13_01:36:24.40436
2015-01-13_01:36:24.40436 This disk is currently in use - repartitioning is probably a bad idea.
2015-01-13_01:36:24.40436 Umount all file systems, and swapoff all swap partitions on this disk.
2015-01-13_01:36:24.40437 Use the --no-reread flag to suppress this check.
2015-01-13_01:36:24.40437 Use the --force flag to overrule all checks.
2015-01-13_01:36:24.40437 ': exit status 1
```",,85976540,story,[],Agent should keep running if persistent disk is mounted as read only,,[],956238,1379888,feature,2015-01-13T01:43:41Z,https://www.pivotaltracker.com/story/show/85976540
,2015-01-08T23:36:43Z,unscheduled,,"It does not look like we verify tarballs before uploading them to s3: https://github.com/cloudfoundry/bosh-agent/blob/master/platform/commands/tarball_compressor.go#L20

http://stackoverflow.com/a/2001749

See also -W function in tar http://linux.die.net/man/1/tar",,85746604,story,[],Verify tarball is valid after compressing compiled package before uploading to blobstore,,[],956238,553935,feature,2015-01-08T23:36:43Z,https://www.pivotaltracker.com/story/show/85746604
,2015-01-07T19:46:49Z,unscheduled,,"On create_vm call we wait for instance to be in running state. We noticed in CI once that it went from running to pending and failed to associate elastic IP:

```
I, [2015-01-07 18:02:17 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: i-e8f6f416 is now running, took 31.030608243s
I, [2015-01-07 18:02:17 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: Creating new instance 'i-e8f6f416'
I, [2015-01-07 18:02:17 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: Associating instance `i-e8f6f416' with elastic IP `54.209.143.31'
I, [2015-01-07 18:02:18 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: [AWS EC2 200 0.48656 0 retries] describe_addresses(:public_ips=>[""54.209.143.31""])  

I, [2015-01-07 18:02:18 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: [AWS EC2 400 0.133962 0 retries] associate_address(:allocation_id=>""eipalloc-d9a0b1bb"",:instance_id=>""i-e8f6f416"") AWS::EC2::Errors::IncorrectInstanceState The pending-instance-running instance to which 'eni-f011efdd' is attached is not in a valid state for this operation

I, [2015-01-07 18:02:19 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)]  INFO -- DirectorJobRunner: [AWS EC2 400 0.096295 0 retries] associate_address(:allocation_id=>""eipalloc-d9a0b1bb"",:instance_id=>""i-e8f6f416"") AWS::EC2::Errors::InvalidInstanceID The pending instance 'i-e8f6f416' is not in a valid state for this operation.

E, [2015-01-07 18:02:19 #3243] [create_vm(28e8c3e9-fe5b-4b64-9157-ed2b9b7bc53d, ...)] ERROR -- DirectorJobRunner: Failed to create instance: The pending instance 'i-e8f6f416' is not in a valid state for this operation.
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:375:in `return_or_raise'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `associate_address'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/ec2/instance.rb:614:in `associate_elastic_ip'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/instance.rb:19:in `associate_elastic_ip'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/vip_network.rb:37:in `block in configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/retryable.rb:28:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/retryable.rb:28:in `block in retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/retryable.rb:26:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/retryable.rb:26:in `retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/common.rb:119:in `retryable'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/vip_network.rb:36:in `configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/network_configurator.rb:75:in `configure'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/cloud.rb:103:in `block in create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2807.0/lib/cloud/aws/cloud.rb:86:in `create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/vm_creator.rb:41:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/vm_creator.rb:9:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/vm_updater.rb:93:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/vm_updater.rb:27:in `block in update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/vm_updater.rb:22:in `times'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/vm_updater.rb:22:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/network_updater.rb:42:in `rescue in update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater/network_updater.rb:13:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater.rb:267:in `update_networks'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater.rb:70:in `block in update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater.rb:37:in `step'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/instance_updater.rb:70:in `update'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/job_updater.rb:74:in `block (2 levels) in update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/job_updater.rb:72:in `block in update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/job_updater.rb:71:in `update_canary_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2807.0/lib/bosh/director/job_updater.rb:65:in `block (2 levels) in update_canaries'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2807.0/lib/common/thread_pool.rb:63:in `block in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'
```",,85644958,story,[],[Tracking] failed AWS deploy when instance goes from running state to pending,,[],956238,553935,chore,2015-01-07T19:46:49Z,https://www.pivotaltracker.com/story/show/85644958
,2015-01-05T22:02:06Z,unscheduled,,"```
E, [2015-01-05 21:03:25 #12037] [task:50315] ERROR -- DirectorJobRunner: Error resolving problem `5758': Timed out waiting for i-abd0d34a to be terminated, took 1606.546409371s
E, [2015-01-05 21:03:25 #12037] [task:50315] ERROR -- DirectorJobRunner: /var/vcap/data/packages/director/26ccce83dc8980002916238201af3f00c013d377.1-a5e09fcf3838dc23f794753e1b0196cddf3e5304/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/helpers.rb:14:in `cloud_error'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/resource_wait.rb:169:in `block in for_resource'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2786.0/lib/common/retryable.rb:43:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2786.0/lib/common/retryable.rb:43:in `ensure in retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2786.0/lib/common/retryable.rb:43:in `retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/resource_wait.rb:173:in `for_resource'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/resource_wait.rb:28:in `for_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/instance.rb:73:in `terminate'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/cloud.rb:137:in `block in delete_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2786.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2786.0/lib/cloud/aws/cloud.rb:135:in `delete_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/cloudcheck_helper.rb:109:in `recreate_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_handlers/unresponsive_agent.rb:39:in `block (2 levels) in <class:UnresponsiveAgent>'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:59:in `block in apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/event_log.rb:97:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/event_log.rb:50:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:21:in `track_and_log'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:58:in `apply_resolution'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:42:in `block in apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `block in each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block (2 levels) in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:154:in `block in fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:89:in `_execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `block in execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/shared/mysql_prepared_statements.rb:34:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:801:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:171:in `execute'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/mysql2.rb:140:in `fetch_rows'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:152:in `each'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/problem_resolver.rb:33:in `apply_resolutions'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:37:in `block in perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:30:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/job_runner.rb:108:in `perform_job'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/job_runner.rb:31:in `block in run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2786.0/lib/common/thread_formatter.rb:49:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/job_runner.rb:31:in `run'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in `perform'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:in `block in work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in `work'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2786.0/bin/bosh-director-worker:83:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:16:in `<main>'
```

```
Director task 50315
  Started scanning 1 vms
  Started scanning 1 vms > Checking VM states. Done (00:00:10)
  Started scanning 1 vms > 0 OK, 1 unresponsive, 0 missing, 0 unbound, 0 out of sync. Done (00:00:00)
     Done scanning 1 vms (00:00:10)

  Started applying problem resolutions > unresponsive_agent 2357: Recreate VM using last known apply spec. Failed: Timed out waiting for i-abd0d34a to be terminated, took 1606.546409371s (00:26:59)

Task 50315 done

Started		2015-01-05 20:36:16 UTC
Finished	2015-01-05 21:03:25 UTC
Duration	00:27:09
```",,85473654,story,[],Failed resurrection should show up as error not done,,[],956238,1139722,bug,2015-01-05T22:02:06Z,https://www.pivotaltracker.com/story/show/85473654
,2015-01-05T19:35:26Z,unscheduled,,"Stack trace during the bosh deploy:
```
/Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/ssl_config.rb:194:in `add_file': system lib (OpenSSL::X509::StoreError)
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/ssl_config.rb:194:in `add_trust_ca_to_store'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/ssl_config.rb:405:in `load_cacerts'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/ssl_config.rb:201:in `load_trust_ca'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/ssl_config.rb:278:in `set_context'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:397:in `create_openssl_socket'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:295:in `initialize'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:826:in `new'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:826:in `create_ssl_socket'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:757:in `block in connect'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/2.1.0/timeout.rb:91:in `block in timeout'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/2.1.0/timeout.rb:101:in `call'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/2.1.0/timeout.rb:101:in `timeout'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/2.1.0/timeout.rb:127:in `timeout'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:751:in `connect'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:609:in `query'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient/session.rb:164:in `query'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient.rb:1087:in `do_get_block'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient.rb:891:in `block in do_request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient.rb:985:in `protect_keep_alive_disconnected'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient.rb:890:in `do_request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/httpclient-2.4.0/lib/httpclient.rb:778:in `request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:696:in `perform_http_request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:669:in `block in try_to_perform_http_request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:667:in `downto'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:667:in `try_to_perform_http_request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:613:in `request'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:549:in `block (2 levels) in <class:Director>'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:475:in `get_task'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:491:in `get_task_state'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/director_task.rb:14:in `state'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/task_tracking/task_tracker.rb:57:in `poll'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/task_tracking/task_tracker.rb:45:in `track'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:575:in `request_and_track'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/client/director.rb:242:in `deploy'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/commands/deployment.rb:104:in `perform'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bosh_cli-1.2739.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.rbenv/versions/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.rbenv/versions/2.1.2/bin/bosh:23:in `<main>'
```",,85458530,story,[],cli should retry connecting to the director for task tracking on OpenSSL::X509::StoreError error,,[],956238,721203,bug,2015-01-06T19:30:11Z,https://www.pivotaltracker.com/story/show/85458530
,2014-12-30T22:23:51Z,unscheduled,,"@calebamiles raised and issue is that running bosh delete --force exits with non-0 status, even though it says it will ignore all the errors. 

IMO, the command should exit with non-zero status if it fails so may be we should change the message so it is not misleading",,85291742,story,[],Discuss with @dk if commands with force option should always succeed,,[],956238,553935,feature,2015-01-07T20:06:19Z,https://www.pivotaltracker.com/story/show/85291742
,2014-12-30T03:55:32Z,unscheduled,,"reported be @tlabeeuw our s3 uploads are not multipart: https://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb#L143

Might be interesting:
http://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html
https://github.com/dallasmarlow/s3_multi_upload
http://baldowl.github.io/2011/02/18/multipart-uploads-with-fog.html",,85254570,story,[],Uploading blobs to s3 should be multipart/parallel,,[],956238,553935,feature,2014-12-30T17:04:08Z,https://www.pivotaltracker.com/story/show/85254570
,2014-12-29T20:00:01Z,unscheduled,,@dk This is to discuss. @aram mentioned that ops manager currently manually deletes folders in vsphere. This is helpful also for their CI.,,85233160,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",Delete deployment should delete folders for vms and templates,,[],956238,553935,feature,2014-12-29T20:45:23Z,https://www.pivotaltracker.com/story/show/85233160
,2014-11-25T00:38:16Z,unscheduled,,,,83361392,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",user should be able to see which tasks took locks,,[],956238,81882,feature,2015-06-24T22:46:07Z,https://www.pivotaltracker.com/story/show/83361392
,2014-11-25T00:37:11Z,unscheduled,,"- start time
- end time
- deployment",,83361334,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",user should be able to see more info about tasks,,[],956238,81882,feature,2015-06-24T22:46:01Z,https://www.pivotaltracker.com/story/show/83361334
,2014-11-24T19:21:25Z,unscheduled,,"Goal: Customers would like to see exactly what went into a stemcell. Right now, it's the laborious process of downloading the stemcell and getting the packages file.

Acceptance - there's a public way to see the full manifest of what's in the OS.",,83337650,story,[],BOSH should copy out the stemcell packages list and add it to the repo,,[],956238,637633,feature,2014-11-24T19:23:34Z,https://www.pivotaltracker.com/story/show/83337650
,2014-10-27T22:57:20Z,unscheduled,,"Use case: When the director is starting, it logs errors about services it can't talk to (likely because they are not started). But it doesn't log when it connects to services. 

This causes misunderstandings with customers because all they read is the error:
```
, [2014-10-27T22:12:42.642469 #21612]  INFO -- : Failed to start worker : #<Redis::CannotConnectError: Error connecting to Redis on 127.0.0.1:25255 (ECONNREFUSED)>
I, [2014-10-27T22:14:26.959800 #22043]  INFO -- : Starting worker 95fe0e12-a200-4be0-9dbb-e5b095e9900c:22043:normal
```

For errors - It'd be clearer to log something like:
```
INFO -- : Failed to start worker : #<Redis::CannotConnectError: Error connecting to Redis on 127.0.0.1:25255 (ECONNREFUSED)> System coming up?
```

And for start it'd be nice to see:
```
I, [2014-10-27T22:14:26.959800 #22043]  INFO -- : Starting worker 95fe0e12-a200-4be0-9dbb-e5b095e9900c:22043:normal (connected to DB, Redis, NATs)
```
",,81544908,story,[],"BOSH logging: as cloud operator, I should be able to see when the director and it's works connect to a BOSH component",,[],956238,637633,feature,2014-11-24T22:14:10Z,https://www.pivotaltracker.com/story/show/81544908
,2014-11-21T18:00:52Z,unscheduled,,Reported by diego team. They were seeing a bunch of ec2 timeout errors when running errands and doing deploys. At the same time they were seeing that deployments/errands were failing to acquire deployment lock.,,83223918,story,[],Investigate if failed errand does not release the lock ,,[],956238,668075,feature,2014-11-21T18:24:39Z,https://www.pivotaltracker.com/story/show/83223918
,2014-11-21T17:49:45Z,unscheduled,,"In case of ec2 failure, user see unpretty error message 

```
E, [2014-11-21T15:02:25.654916 #27848] [create_missing_vm(small_errand, 0/1)] ERROR -- : error creating vm: The service is unavailable. Please try again shortly.
I, [2014-11-21T15:02:25.654964 #27848] [create_missing_vm(small_errand, 0/1)]  INFO -- : Cleaning up the created VM due to an error: The service is unavailable. Please try again shortly.
D, [2014-11-21T15:02:25.655153 #27848] [0x3fc3db5a4978] DEBUG -- : Worker thread raised exception: The service is unavailable. Please try again shortly. - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:375:in `return_or_raise'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `run_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/aws-sdk-1.44.0/lib/aws/ec2/instance_collection.rb:303:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2749.0/lib/cloud/aws/instance_manager.rb:49:in `block in create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/retryable.rb:28:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/retryable.rb:28:in `block in retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/retryable.rb:26:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/retryable.rb:26:in `retryer'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/common.rb:119:in `retryable'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2749.0/lib/cloud/aws/instance_manager.rb:46:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2749.0/lib/cloud/aws/cloud.rb:91:in `block in create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_aws_cpi-1.2749.0/lib/cloud/aws/cloud.rb:86:in `create_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/vm_creator.rb:41:in `create'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/resource_pool_updater.rb:51:in `create_missing_vm'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/resource_pool_updater.rb:34:in `block (4 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/resource_pool_updater.rb:32:in `block (3 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/event_log.rb:36:in `track'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/resource_pool_updater.rb:31:in `block (2 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `block in create_thread'
D, [2014-11-21T15:02:25.655271 #27848] [0x3fc3db5a4978] DEBUG -- : Thread is no longer needed, cleaning up
D, [2014-11-21T15:02:25.655492 #27848] [task:21164] DEBUG -- : Shutting down pool
I, [2014-11-21T15:02:25.655564 #27848] [task:21164]  INFO -- : Starting to delete job instances
I, [2014-11-21T15:02:25.655648 #27848] [task:21164]  INFO -- : Deleting errand instances
D, [2014-11-21T15:02:25.655815 #27848] [task:21164] DEBUG -- : Creating new thread
D, [2014-11-21T15:02:25.656792 #27848] [task:21164] DEBUG -- : Waiting for tasks to complete
D, [2014-11-21T15:02:25.657093 #27848] [0x3fc3db31bbd4] DEBUG -- : Worker thread raised exception: undefined method `cid' for nil:NilClass - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/instance_deleter.rb:31:in `delete_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/instance_deleter.rb:21:in `block (3 levels) in delete_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `block in create_thread'
D, [2014-11-21T15:02:25.657153 #27848] [0x3fc3db31bbd4] DEBUG -- : Thread is no longer needed, cleaning up
D, [2014-11-21T15:02:25.657267 #27848] [task:21164] DEBUG -- : Shutting down pool
D, [2014-11-21T15:02:25.657336 #27848] [task:21164] DEBUG -- : Deleting lock: lock:deployment:cf-acceptance-tests
D, [2014-11-21T15:02:25.657470 #27848] [0x3fc3d95ecba8] DEBUG -- : Lock renewal thread exiting
D, [2014-11-21T15:02:25.658333 #27848] [task:21164] DEBUG -- : Deleted lock: lock:deployment:cf-acceptance-tests
E, [2014-11-21T15:02:25.658479 #27848] [task:21164] ERROR -- : undefined method `cid' for nil:NilClass
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/instance_deleter.rb:31:in `delete_instance'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2749.0/lib/bosh/director/instance_deleter.rb:21:in `block (3 levels) in delete_instances'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `call'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2749.0/lib/common/thread_pool.rb:63:in `block in create_thread'
D, [2014-11-21T15:02:25.658910 #27848] [task:21164] DEBUG -- : (0.000132s) BEGIN
D, [2014-11-21T15:02:25.659936 #27848] [task:21164] DEBUG -- : (0.000449s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2014-11-21 15:02:25.658592+0000', ""description"" = 'run errand cf_acceptance_tests from deployment cf-acceptance-tests', ""result"" = 'undefined method `cid'' for nil:NilClass', ""output"" = '/var/vcap/store/director/tasks/21164', ""user_id"" = 1, ""checkpoint_time"" = '2014-11-21 15:02:15.364552+0000', ""type"" = 'run_errand' WHERE (""id"" = 21164)
D, [2014-11-21T15:02:25.681978 #27848] [task:21164] DEBUG -- : (0.021923s) COMMIT
I, [2014-11-21T15:02:25.682106 #27848] [0x3fc3d83f5328]  INFO -- : Task took 10.31945761 seconds to process.
```",,83222968,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",deploy should not try to delete vm that was never created,,[],956238,553935,feature,2015-06-24T22:45:48Z,https://www.pivotaltracker.com/story/show/83222968
,2014-11-13T00:04:58Z,unscheduled,,"talk to @dk ""bosh task prod reports success even though failed""",1.0,82614344,story,"[{'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",investigate why bosh considered failing task as done instead of error,,[],956238,81882,feature,2014-11-13T00:04:58Z,https://www.pivotaltracker.com/story/show/82614344
,2014-11-12T22:50:56Z,unscheduled,,"- originally errand vm stuck around (agent was responsive)

- next errand ran and failed

```
+ bosh run errand push-broker

Director task 187
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

Error 100: undefined method `cid' for nil:NilClass

Task 187 error
```

- failure from the task: https://gist.github.com/cppforlife/d060831f4aac0205cded

- next time running bosh run errand made vm unresponsive",,82609412,story,"[{'name': 'improved errands', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-08-11T21:00:44Z', 'id': 9165874, 'updated_at': '2014-08-11T21:00:44Z'}]",bosh run errand fails to delete vms that was left around,,[],956238,163328,bug,2014-11-12T22:50:56Z,https://www.pivotaltracker.com/story/show/82609412
,2014-11-11T23:33:17Z,unscheduled,,"OpenStack lifecycle can leave lingering volumes, potentially exhausting quota and causing every subsequent Jenkins run to fail.",,82524942,story,[],OpenStack lifecycle_spec not cleaning up environment when encountering exception during attach volume ,,[],956238,1426194,bug,2014-11-11T23:33:17Z,https://www.pivotaltracker.com/story/show/82524942
,2014-10-31T15:12:33Z,unscheduled,,"We found that this line: https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/blob_manager.rb#L315-Lundefined

will throw an exception when the -q option is used:
https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/runner.rb#L125-Lundefined",,81848052,story,[],bosh cli may error on -q,,[],956238,1218388,bug,2014-10-31T15:12:34Z,https://www.pivotaltracker.com/story/show/81848052
,2014-10-28T22:12:00Z,unscheduled,,"The tests don't clean up volumes. There are 500+ volumes, snapshots, and AMIs behind.
Find a way to clean them up.",,81632246,story,[],CI/dev accounts leave un-used EBS volumes and AMIs,,[],956238,514635,chore,2014-10-30T00:13:58Z,https://www.pivotaltracker.com/story/show/81632246
,2014-10-21T00:17:43Z,unscheduled,,"This may have contributed to the un-stoppable instance we had recently (`Failed: `uaa_z2/0' is still running despite the stop command`), though I haven't been able to reproduce that much.

This behavior doesn't seem to show up in bosh-lite.",,81068496,story,[],"in aws, if i bosh stop a job and then terminate the instance, the resurrector brings it back started",,[],956238,381857,bug,2014-10-21T00:17:43Z,https://www.pivotaltracker.com/story/show/81068496
,2014-10-20T22:58:28Z,unscheduled,,"Today the BOSH CLI sends everything up all over again, even if the compiled packages already exist. It would save the user a lot of time if we uploaded only what's missing.",,81064446,story,[],Should be able to upload only the compiled packages I need,,[],956238,548715,feature,2014-11-06T18:57:38Z,https://www.pivotaltracker.com/story/show/81064446
,2014-10-20T18:07:54Z,unscheduled,,"We uploaded a stemcell but our RDS instance wasn't responding. Amazon restored it automatically for us but our director never recovered. Canceling the task had no effect.

```
$ bosh -n -t bosh.lakitu.cf-app.com upload stemcell https://s3.amazonaws.com/bosh-jenkins-artifacts/bosh-stemcell/aws/bosh-stemcell-2749-aws-xen-ubuntu-trusty-go_agent.tgz --skip-if-exists

Using remote stemcell `https://s3.amazonaws.com/bosh-jenkins-artifacts/bosh-stemcell/aws/bosh-stemcell-2749-aws-xen-ubuntu-trusty-go_agent.tgz'

Director task 129
```

```
$ bosh task 129 --debug

Director task 129
# Logfile created on 2014-10-20 17:15:39 +0000 by logger.rb/44203
I, [2014-10-20T17:15:39.527666 #1680]  INFO -- : Director Version : 1.2749.0
I, [2014-10-20T17:15:39.527714 #1680]  INFO -- : Enqueuing task: 129
I, [2014-10-20T17:15:39.700776 #2297] [0x3fbe3549f330]  INFO -- : Looking for task with task id 129
E, [2014-10-20T17:15:39.701943 #2297] [0x3fbe3549f330] ERROR -- : Mysql2::Error: MySQL server has gone away: SELECT * FROM `tasks` WHERE `id` = 129
```",,81037402,story,[],director and workers should automatically reconnect to the database if db connection is severed,,[],956238,1123776,feature,2014-10-21T19:22:46Z,https://www.pivotaltracker.com/story/show/81037402
,2014-10-17T01:21:12Z,unscheduled,,"There are many http clients. As far as we know, none of them need to talk to old servers (except maybe external blobstores). Disable SSLv3 on as many of them as possible.

Also consider limiting the set of ciphers allowed to the ""modern"" set recommended by Mozilla: https://wiki.mozilla.org/Security/Server_Side_TLS",,80879374,story,[],Disable SSLv3 in bosh http clients,,[],956238,1266616,feature,2014-10-17T01:21:12Z,https://www.pivotaltracker.com/story/show/80879374
,2014-10-17T00:46:23Z,unscheduled,,"The micro server int he agent has been configured to disallow SSLv3 already, but the http/tls clients need to also disallow it and preferably also use the same set of ciphers as the server.",,80878244,story,[],Disable SSLv3 in bosh-agent http clients,,[],956238,1266616,feature,2014-10-17T00:46:48Z,https://www.pivotaltracker.com/story/show/80878244
,2014-10-15T20:55:19Z,unscheduled,,"SHA-1 is only required for compatibility with ancient browsers, like IE6. SHA-256 is significantly more secure, especially since no browsers are used to talk to the director.

SHA-1 is not quite 'insecure' but not recommended.",,80778012,story,[],Upgrade Director SSL certificate to use SHA-256,,[],956238,1429768,feature,2014-10-15T20:55:20Z,https://www.pivotaltracker.com/story/show/80778012
,2014-10-15T20:57:40Z,unscheduled,,"The director uses a self-signed ssl certificate, which is not validated by the cli. Since the user can specify a custom SSL certificate for the director, we need to also be able to supply that certificate to the cli and use it for authentication when specified by the user.",,80778202,story,[],Add support for SSL certificate validation on the BOSH CLI,,[],956238,1429768,feature,2014-10-15T20:57:43Z,https://www.pivotaltracker.com/story/show/80778202
,2014-10-14T18:53:00Z,unscheduled,,"bosh vms sometimes fails with too many connections:

```
Error 100: Mysql2::Error: Too many connections

Task 7189 error
Failed to fetch VMs information from director
```

Our t1.micro RDS database has max_connections set to 34. Our bosh director has max_connections set to 32, but somehow the connections get up to 34:

```
mysql> show status where Variable_name = 'Threads_connected';
+-------------------+-------+
| Variable_name     | Value |
+-------------------+-------+
| Threads_connected | 34    |
+-------------------+-------+
```

This may be happening because there are connections from the director and the workers.

Is there something smart bosh could do here like or should we just up the number of connections on the server?",,80675796,story,[],bosh vms fails with too many connections to mysql,,[],956238,756869,feature,2014-10-15T04:14:04Z,https://www.pivotaltracker.com/story/show/80675796
,2014-10-09T22:46:06Z,unscheduled,,"When trying to upload to the Ireland region

```
Upload blob gemfire/Pivotal_GemFire_800_b48398.zip? (type 'yes' to continue): yes
gemfire/Pivotal_GemFire_800_b48398.zip uploading.../Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/base.rb:39:in `rescue in create': Failed to create object, underlying error: #<Errno::EPIPE: Broken pipe> /Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/openssl/buffering.rb:318:in `syswrite' (Bosh::Blobstore::BlobstoreError)
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/openssl/buffering.rb:318:in `do_write'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/openssl/buffering.rb:336:in `write'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/protocol.rb:199:in `write0'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/protocol.rb:173:in `block in write'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/protocol.rb:190:in `writing'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/protocol.rb:172:in `write'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1956:in `send_request_with_body_stream'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1922:in `exec'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1318:in `block in transport_request'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1317:in `catch'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1317:in `transport_request'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/net/http.rb:1294:in `request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/http/connection_pool.rb:350:in `request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/http/net_http_handler.rb:63:in `block in handle'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/http/connection_pool.rb:129:in `session_for'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/http/net_http_handler.rb:55:in `handle'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:253:in `block in make_sync_request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:289:in `retry_server_errors'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:249:in `make_sync_request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:511:in `block (2 levels) in client_request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:391:in `log_client_request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:477:in `block in client_request'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:373:in `return_or_raise'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/core/client.rb:476:in `client_request'
(eval):3:in `upload_part'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/s3/multipart_upload.rb:206:in `add_part'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:1723:in `block in write_with_multipart'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:724:in `multipart_upload'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:1722:in `write_with_multipart'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/aws-sdk-1.44.0/lib/aws/s3/s3_object.rb:605:in `write'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/s3_blobstore_client.rb:147:in `block in store_in_s3'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/s3_blobstore_client.rb:146:in `open'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/s3_blobstore_client.rb:146:in `store_in_s3'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/s3_blobstore_client.rb:74:in `create_file'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/base.rb:27:in `create'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/forwardable.rb:201:in `create'
/Users/pivotal/.rvm/rubies/ruby-1.9.3-p545/lib/ruby/1.9.1/forwardable.rb:201:in `create'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/blob_manager.rb:253:in `upload_blob'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:41:in `block in upload'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:38:in `each'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:38:in `upload'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/command_handler.rb:57:in `run'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/runner.rb:56:in `run'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/runner.rb:16:in `run'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/bin/bosh:7:in `<top (required)>'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/bosh:23:in `load'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/bosh:23:in `<main>'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/ruby_executable_hooks:15:in `eval'
/Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/ruby_executable_hooks:15:in `<main>'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/blobstore_client-1.2732.0/lib/blobstore_client/base.rb:26:in `create'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/blob_manager.rb:253:in `upload_blob'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:41:in `block in upload'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:38:in `each'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/commands/blob_management.rb:38:in `upload'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/command_handler.rb:57:in `run'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/runner.rb:56:in `run'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/lib/cli/runner.rb:16:in `run'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/gems/bosh_cli-1.2732.0/bin/bosh:7:in `<top (required)>'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/bosh:23:in `load'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/bosh:23:in `<main>'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/ruby_executable_hooks:15:in `eval'
        from /Users/pivotal/.rvm/gems/ruby-1.9.3-p545/bin/ruby_executable_hooks:15:in `<main>'
```

using a bucket in the `us-standard` region produces no such error.



",,80420836,story,[],bosh should support uploading blobs to non-us s3 regions,,[],956238,1406536,bug,2014-10-09T22:46:07Z,https://www.pivotaltracker.com/story/show/80420836
,2014-10-07T22:00:25Z,unscheduled,,"Trying to deploy but ran into an issue with relative location of release file. The err message does not provide adequate info.

Steps taken:
1. current location/dir: ~/workspace/cf-release/releases
2. bosh upload release cf-188.yml
3. err msg: Cannot find file `../releases/cf-188.yml'
",,80250858,story,[],misleading error while trying to upload release yml file by specifying relative file path,,[],956238,1092360,bug,2014-10-08T22:27:20Z,https://www.pivotaltracker.com/story/show/80250858
,2014-10-03T16:34:54Z,unscheduled,,"ruby 2.1.2 + cli 1.2719.0
Director task succeeded, but cli reported an error. Getting the task afterwards works fine.

```
± ms+ki |master ✗| → bosh deploy

Processing deployment manifest
------------------------------
Getting deployment properties from director...
Unable to get properties list from director, trying without it...
Compiling deployment manifest...
Cannot get current deployment information from director, possibly a new deployment
Please review all changes carefully

Deploying
---------
Deployment name: `dummy.yml'
Director name: `micro-piston'
Are you sure you want to deploy? (type 'yes' to continue): yes

Director task 5
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:01)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:01)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started creating bound missing vms > default/0ruby(40312,0x7fff79ccc180) malloc: *** error for object 0x7ffd74d1eef0: pointer being freed was not allocated
*** set a breakpoint in malloc_error_break to debug
Abort trap: 6
```

```
→ bosh task 5

Director task 5
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:01)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:01)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started creating bound missing vms > default/0. Done (00:02:01)

  Started binding instance vms > dummy/0. Done (00:00:01)

  Started preparing configuration > Binding configuration. Done (00:00:00)

  Started updating job dummy > dummy/0 (canary). Done (00:00:20)

Task 5 done

Started		2014-10-03 16:26:59 UTC
Finished	2014-10-03 16:29:22 UTC
Duration	00:02:23
```",,80034088,story,[],bosh cli should not fail with malloc error,,[],956238,553935,bug,2014-10-03T16:34:54Z,https://www.pivotaltracker.com/story/show/80034088
,2014-10-01T23:27:30Z,unscheduled,,"ssh-add -L shows a list of keys I've added, including keys for github, etc. When I want to `bosh ssh` to an environment that requires keys, I add the key to my agent. However, `bosh ssh` fails because the key is not the first in the list. This renders `bosh ssh` useless for AWS environments, at least.",,79915822,story,[],bosh ssh only works if the necessary key is the first in the list ,,[],956238,58676,bug,2014-10-01T23:27:30Z,https://www.pivotaltracker.com/story/show/79915822
,2014-10-01T00:12:40Z,unscheduled,,"During a micro deploy, our auth token timed out and we could not proceed. The [OpenStack API docs](http://docs.openstack.org/api/quick-start/content/) tell you to get another token when this happens:

> If the 401 Unauthorized error occurs, request another token.

Re-authentication should only happen on the first 401 in a single CPI method.

This is blocking microbosh deploy to Mirantis Express.",,79837634,story,[],OpenStack CPI should request another auth token on 401 Unauthorized,,[],956238,1266616,feature,2014-10-01T00:14:51Z,https://www.pivotaltracker.com/story/show/79837634
,2014-09-30T18:16:36Z,unscheduled,,https://github.com/cloudfoundry/bosh-lite/pull/195/commits,,79807510,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}, {'name': 'pull-request', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-13T17:49:44Z', 'id': 7458340, 'updated_at': '2014-01-13T17:49:44Z'}]",account for iptables nat chains renaming in bosh-lite,,[],956238,81882,feature,2014-10-14T19:54:45Z,https://www.pivotaltracker.com/story/show/79807510
,2014-09-29T16:45:07Z,unscheduled,,"on ketchup, see @astegman for credentials

* reporting 68% usage (same as system), when there is no persistent disk",,79713842,story,[],bosh vms --vitals reporting wrong persistent disk usage,,[],956238,1266616,bug,2014-09-29T23:17:45Z,https://www.pivotaltracker.com/story/show/79713842
,2014-09-28T18:24:29Z,unscheduled,,"```
  Started preparing configuration > Binding configuration. Failed: undefined method `to_sym' for true:TrueClass (00:00:00)

Error 100: undefined method `to_sym' for true:TrueClass

Task 256 error
```

tracked it down to my deployment manifest having `on` as a key, which YAML helpfully turns into `true`.

full backtrace:

https://gist.github.com/vito/d6fdb012ea210d9e3bd3",,79660590,story,[],obscure deploy failure when a property has a boolean key,,[],956238,381857,bug,2014-09-28T18:25:47Z,https://www.pivotaltracker.com/story/show/79660590
,2014-09-24T00:20:49Z,unscheduled,,"We saw a bug when reordering the networks for a job in the deployment manifest, which is that the output is the following:

```
Jobs
dummy2
  ± networks:
```

It probably should show something regarding the reordering, or nothing at all.",,79410280,story,[],bosh deploy should show differences when deployment manifest networks are reordered,,[],956238,320647,bug,2014-09-24T00:20:49Z,https://www.pivotaltracker.com/story/show/79410280
,2014-09-24T00:15:38Z,unscheduled,,"When removing persistent_disk from our deployment manifest and redeploying, no lines in the output included the removals of the persistent disk.",,79409906,story,[],bosh cli should show removals from the deployment manifest,,[],956238,320647,bug,2014-09-24T00:15:38Z,https://www.pivotaltracker.com/story/show/79409906
,2014-09-23T00:44:05Z,unscheduled,,"if two vms come up on the network with the same ip, one of the vms might received messages addressed to another (diff agent id). agent should discard that message and log that as an error.",,79331162,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh agent should not reply to messages that are not addressed to it,,[],956238,81882,feature,2014-09-23T00:44:05Z,https://www.pivotaltracker.com/story/show/79331162
,2014-09-22T17:10:29Z,unscheduled,,"some passwords are not being redacted
ex: ""excon.request"" body.auth.passwordCredentials.password",,79295834,story,[],openstack cpi logs should redact all passwords,,[],956238,1266616,bug,2014-09-22T17:10:54Z,https://www.pivotaltracker.com/story/show/79295834
,2014-09-19T22:15:22Z,unscheduled,,"```
$ bosh scp riak-cs 0 --download
Please enter valid source and destination paths

$ bosh help scp
scp [--download] [--upload] [--public_key FILE] [--gateway_host HOST]
[--gateway_user USER] [--gateway_identity_file FILE]
    upload/download the source file to the given job. Note: for download /path/to/destination is a directory
    --download                                                Download file
    --upload                                                  Upload file
    --public_key FILE                                         Public key
    --gateway_host HOST                                       Gateway host
    --gateway_user USER                                       Gateway user
    --gateway_identity_file FILE                              Gateway identity file
```

- What is the syntax for providing the source and destination paths??
- The job and job index are also missing from the usage syntax


For example, the usage should say
```
scp JOB [JOB_INDEX] [--download] [--upload] [--public_key FILE] [--gateway_host HOST]
[--gateway_user USER] [--gateway_identity_file FILE] SOURCE DESTINATION
```
Tried this syntax btw; got ruby errors. https://gist.github.com/shalako/309f389f6f244417c68a",,79214942,story,[],bosh scp usage doesn't provide necessary syntax,,[],956238,58676,bug,2014-09-19T22:15:22Z,https://www.pivotaltracker.com/story/show/79214942
,2014-09-03T17:26:55Z,unscheduled,,"- fix yagnats to not infinitely try to reconnect
- it should retry and return an error after 5 times (waiting 20 secs in between) if it cannot send the msg after which agent should restart 
  - (new method PublishWithMaxRetries(..., 5) for yagnats?)",2.0,78133114,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should retry sending heart beat to HM several times before returning an error,,[],956238,81882,feature,2014-10-02T16:37:50Z,https://www.pivotaltracker.com/story/show/78133114
,2014-09-19T02:21:54Z,unscheduled,,"A helper that does the equivalent of:

https://github.com/concourse/concourse/blob/56361c6bdd7069314e3b26043dbbbce807e9ccad/jobs/atc/templates/atc_ctl.erb#L12-L32

would be nice.  Right now, people do gross things like in cf-release: `spec.networks.send(properties.networks.apps)`.

(Related issue in loggregator's syslog forwarder configuration: https://github.com/cloudfoundry/loggregator/issues/29)",,79158540,story,[],BOSH template helpers should make it easy to get a job's IP from its default network.,,[],956238,668075,feature,2014-09-19T02:21:54Z,https://www.pivotaltracker.com/story/show/79158540
,2014-09-17T19:26:38Z,unscheduled,,We have no acceptance test for the case of an attached ephemeral disk being partitioned and used at /var/vcap/data. We added tests for the *root* disk but we have run into problems on OpenStack with an attached ephemeral disk that these theoretical tests would probably have caught.,,79035188,story,[],Add acceptance test for ephemeral disk,,[],956238,1123776,feature,2014-09-17T19:26:38Z,https://www.pivotaltracker.com/story/show/79035188
,2014-09-17T00:04:54Z,unscheduled,,"https://github.com/cloudfoundry/bosh/blob/3e8bee41eb89fac812e74ae78cbb3e69f1202424/bosh_cli/lib/cli/client/director.rb#L645

We swallow the actual exception that occurred during the HTTP request and do not print a stack trace. This makes it significantly harder to debug. Would be nice if we could print the stacktrace out (or log it, or something).",,78972136,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",Log stacktrace from CLI on director request failure,,[],956238,1123776,feature,2015-06-24T22:39:08Z,https://www.pivotaltracker.com/story/show/78972136
,2014-09-16T19:07:14Z,unscheduled,,For some reason this happens from time to time on Piston Cloud. We saw monologger gem files were empty. And bundle file was reported to be empty. ,,78947016,story,[],Investigate why package files are empty,,[],956238,553935,chore,2014-09-16T19:07:14Z,https://www.pivotaltracker.com/story/show/78947016
,2014-09-16T04:01:49Z,unscheduled,,`env` on a resource pool configuration?,,78883928,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'field', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-16T04:02:06Z', 'id': 9451556, 'updated_at': '2014-09-16T04:02:06Z'}]",deployment operator should be able to configure vms to be in the specific timezone,,[],956238,81882,feature,2014-09-16T04:02:06Z,https://www.pivotaltracker.com/story/show/78883928
,2014-09-13T02:49:49Z,unscheduled,,"In the Warden CPI, there are no required `cloud_properties`, but the value must be present. An empty hash is valid.",,78761554,story,[],cloud_properties should be optional and defaulted to an empty hash,,[],956238,381857,feature,2014-09-30T18:22:44Z,https://www.pivotaltracker.com/story/show/78761554
,2014-09-13T00:09:12Z,unscheduled,,"I'd like to say:

```
bosh --deployment-name=concourse ssh atc 0
```

instead of

```
bosh download manifest concourse > /tmp/concourse.yml
bosh -d /tmp/concourse.yml ssh atc 0
```

This applies to ssh, cck, logs, etc.
",,78758592,story,[],authenticated CLI user should be able to run commands passing deployment name as flag.,,[],956238,668075,feature,2014-09-13T00:09:12Z,https://www.pivotaltracker.com/story/show/78758592
,2014-08-13T21:48:58Z,unscheduled,,"After bringing up a a 3-node cluster, we detached the disk on vcenter and ran `bosh recreate mysql 0`, which resulted in this error:

```
  Started updating job mysql > mysql/0 (canary). Failed: Action Failed get_task: Task 781b6ba8-ad6d-4836-5128-6ebd98596434 result: Stopping Monitored Services: Stopping service gra-log-purger-executable: Stopping Monit service gra-log-purger-executable: Request failed with 503 Service Unavailable: <html><head><title>503 Service Unavailable</title></head><body bgcolor=#FFFFFF><h2>Service Unavailable</h2>Other action already in progress -- please try again later<p><hr><a href='http://mmonit.com/monit/'><font size=-1>monit 5.2.4</font></a></body></html>
 (00:00:23)
```

This occurs when following the steps for recovering from a detached (but not deleted) disk at the following URL: https://github.com/pivotal-cf/docs-mysql/blob/feature/mysql-durable/persistent_disk.html.md

Based on monit logs, it doesn't seem like its stop script was executed for any job. We do not observe multiple processes of galera_healthcheck or log_purger as is observed in https://www.pivotaltracker.com/story/show/76087362 . PID files look accurate and are present.

Interestingly, mysqld is not running, but there is no record of monit having stopped it.

Based on the monit source, it looks like this error arises when monit is asked to stop a service while already performing some other action.",,76912442,story,[],[Inconsistent] gra_log_purge_executable gives HTML error after timing out when trying to recover from detached disk,,[],956238,1061299,bug,2014-09-13T00:08:21Z,https://www.pivotaltracker.com/story/show/76912442
,2014-09-10T19:17:38Z,unscheduled,,We shouldn't need to add the public vagrant key to our AWS boxes because we set a keypair for AWS to use.,1.0,78580108,story,"[{'name': 'bosh-lite', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-02T18:30:55Z', 'id': 8346944, 'updated_at': '2014-05-02T18:30:55Z'}]",Remove public vagrant key from AWS box,,[],956238,320647,feature,2014-09-10T19:17:44Z,https://www.pivotaltracker.com/story/show/78580108
,2014-09-10T15:30:44Z,unscheduled,,"We saw this in one of our Hetzner Ops Managers (contact us for details I don't want to post them publicly).

```
Started deleting errand instances broker-deregistrar > vm-c3c0d2d3-0d0b-4910-9c53-7c47d992590d. Failed: Unable to communicate with the remote host, since it is disconnected. (00:00:05)

Error 100: Unable to communicate with the remote host, since it is disconnected.

Task 34 error

Errand `broker-deregistrar' did not complete
```",,78568454,story,[],"error running errand (""Unable to communicate with the remote host"")",,[],956238,1218342,bug,2014-09-10T15:30:44Z,https://www.pivotaltracker.com/story/show/78568454
,2014-09-10T09:10:38Z,unscheduled,,"1. Deploy with a spot bid in a resource pool, instance type c3.2xlarge (seems to take long time to fulfill)
2. Eventually looks like BOSH cancels the request and makes a new one.  Does this a few times
3. Cancel task
4. Keeps acquiring lock on the deployment, and re-issuing new spot instance requests.
5. Had to manually stop VM via AWS web console.",,78543706,story,[],deployment gets stuck in infinite loop if spot request isn't fulfilled soon enough,,[],956238,668075,bug,2014-09-10T09:10:38Z,https://www.pivotaltracker.com/story/show/78543706
,2014-09-10T08:47:21Z,unscheduled,,"When I deploy to AWS with one of my resources pools (spot instance) being type `r3.2xlarge`, I get the following error:

```
Started refilling resource pools > default/0. Failed: #<AWS::EC2::Errors::InvalidParameterCombination: Virtualization type 'hvm' is required for instances of type 'r3.2xlarge'.> (00:00:01)
    
Error 100: #<AWS::EC2::Errors::InvalidParameterCombination: Virtualization type 'hvm' is required for instances of type 'r3.2xlarge'.>
```

However, according to [this](http://aws.amazon.com/amazon-linux-ami/instance-type-matrix/), the virtualization type *is* hvm.",,78542420,story,[],bosh deploy gives incorrect virtualization type error with r3.2xlarge on AWS,,[],956238,668075,bug,2014-09-10T08:48:15Z,https://www.pivotaltracker.com/story/show/78542420
,2014-09-10T06:41:21Z,unscheduled,,"Taken from https://www.gocd.cf-app.com/go/tab/build/detail/runtime-clean-latest-ova/88/deploy-runtime/1/deploy-runtime but relevant bosh logs attached.

Was deploying, and saw:

>  Started updating job ha_proxy-partition-default_az_guid > ha_proxy-partition-default_az_guid/0 (canary). Done (00:00:39)
>  Started updating job nats-partition-default_az_guid > nats-partition-default_az_guid/0 (canary). Done (00:00:39)
>  Started updating job etcd_server-partition-default_az_guid > etcd_server-partition-default_az_guid/0 (canary). Failed: Could not upload file: https://10.84.32.61/folder/vm-8f6688b7-6654-4438-a773-dae16e2c25cc/env.json?dcPath=private&dsName=private, status code: 500 (00:00:08)

> Error 100: Could not upload file: https://10.84.32.61/folder/vm-8f6688b7-6654-4438-a773-dae16e2c25cc/env.json?dcPath=private&dsName=private, status code: 500

> Task 6 error

Then, tasks 7 and its successive retries failed with 

>   Started updating job etcd_server-partition-default_az_guid > etcd_server-partition-default_az_guid/0 (canary). Failed: Unable to load env.json (00:00:01)

> Error 100: Unable to load env.json",,78537660,story,[],"On vsphere, 500 when uploading env.json results in undeployable state",,[],956238,465545,bug,2014-09-10T06:41:22Z,https://www.pivotaltracker.com/story/show/78537660
,2014-09-09T22:48:22Z,unscheduled,,http://docs.cloudfoundry.org/bosh/create-release.html describes the basic directory layout of a bosh release but does not describe which directories are intended for bosh's own use and which are to be input by the user.,,78508966,story,"[{'name': 'docs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-09-09T22:51:05Z', 'id': 9406128, 'updated_at': '2014-09-09T22:51:05Z'}]",Describe usage of directories in a bosh release,,[],956238,96590,feature,2015-02-01T11:11:36Z,https://www.pivotaltracker.com/story/show/78508966
,2014-09-09T18:42:07Z,unscheduled,,"Ephemeral & Persistant disk partitioner _currently_ uses sfdisk and rewrites the entire partition table when it doesn't match the desired configuration. 

To be more defensive and less destructive to existing partitions, the bosh-agent should instead error if the partitions are in an unexpected state (ex: a manually created 2nd partition).

The partitioner _should_ allow the entire disk to be partitioned, to handle the case where the infrastructure pre-partitions/formats.",,78488430,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",Ephemeral & Persistant partitioner should not delete unexpected partitions,,[],956238,1266616,feature,2014-09-09T18:42:19Z,https://www.pivotaltracker.com/story/show/78488430
,2014-09-08T19:27:06Z,unscheduled,,"It should be possible to partition the root disk CentOS without rebooting by using partx (unlike just using parted on ubuntu).
https://access.redhat.com/solutions/57542",,78408220,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",Enable root device ephemeral partitioning on CentOS,,[],956238,1266616,feature,2014-09-08T19:27:06Z,https://www.pivotaltracker.com/story/show/78408220
,2014-09-08T16:30:02Z,unscheduled,,"BOSH director version: 1.2652.0

```
Bosh::Director::Models::Task.filter(""state NOT IN ('processing', 'queued')"").filter(""output IS NULL"").all
=> [#<Bosh::Director::Models::Task @values={:id=>18676, :state=>""cancelling"", :timestamp=>2014-09-02 13:33:42 +0000, :description=>""scan and fix"", :result=>nil, :output=>nil, :user_id=>1, :checkpoint_time=>2014-09-02 13:33:42 +0000, :type=>""cck_scan_and_fix""}>, #<Bosh::Director::Models::Task @values={:id=>18682, :state=>""timeout"", :timestamp=>2014-09-02 13:36:43 +0000, :description=>""delete release: cassandra"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:36:43 +0000, :type=>""delete_release""}>, #<Bosh::Director::Models::Task @values={:id=>18681, :state=>""timeout"", :timestamp=>2014-09-02 13:36:00 +0000, :description=>""delete release: cassandra"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:36:00 +0000, :type=>""delete_release""}>, #<Bosh::Director::Models::Task @values={:id=>18675, :state=>""timeout"", :timestamp=>2014-09-02 13:25:54 +0000, :description=>""create deployment"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:25:54 +0000, :type=>""update_deployment""}>, #<Bosh::Director::Models::Task @values={:id=>18674, :state=>""timeout"", :timestamp=>2014-09-02 13:17:53 +0000, :description=>""delete deployment cf-cfintegration"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:17:53 +0000, :type=>""delete_deployment""}>, #<Bosh::Director::Models::Task @values={:id=>18673, :state=>""timeout"", :timestamp=>2014-09-02 13:12:47 +0000, :description=>""delete deployment cf-cfintegration"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:12:47 +0000, :type=>""delete_deployment""}>, #<Bosh::Director::Models::Task @values={:id=>18672, :state=>""timeout"", :timestamp=>2014-09-02 13:12:37 +0000, :description=>""delete deployment cf-cfintegration"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:12:37 +0000, :type=>""delete_deployment""}>, #<Bosh::Director::Models::Task @values={:id=>18671, :state=>""timeout"", :timestamp=>2014-09-02 13:12:18 +0000, :description=>""create deployment"", :result=>nil, :output=>nil, :user_id=>2, :checkpoint_time=>2014-09-02 13:12:18 +0000, :type=>""update_deployment""}>]
```

```
E, [2014-09-08T16:16:48.330849 #14391] [0x7ce084] ERROR -- : TypeError - can't convert nil into String:
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:1508:in `path'
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:1508:in `block in fu_list'
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:1508:in `map'
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:1508:in `fu_list'
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:619:in `rm_r'
/var/vcap/packages/ruby/lib/ruby/1.9.1/fileutils.rb:648:in `rm_rf'
/var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2652.0/lib/bosh/director/api/task_remover.rb:10:in `block in remove'
```",,78391490,story,[],Sometimes a task can have a NULL output in the database which causes them to fail when cleaning them up,,[],956238,1218342,bug,2014-09-08T16:30:02Z,https://www.pivotaltracker.com/story/show/78391490
,2014-09-05T19:32:22Z,unscheduled,,"Error at top of stack trace:

```
undefined method `cid' for nil:NilClass - /var/vcap/packages/director/gem_home/ruby/1.9.1/gems/bosh-director-1.2690.0/lib/bosh/director/instance_deleter.rb:31:in `delete_instance'
```

There was another execution expired stack trace that was from httpclient and openssl but starting in vsphere cpi cloud/vsphere/file_provider.rb.",,78295116,story,[],vSphere VM had an issue (execution expired) and now an errand always fails to execute,,[],956238,465545,bug,2014-09-05T19:32:22Z,https://www.pivotaltracker.com/story/show/78295116
,2014-09-04T22:42:51Z,unscheduled,,"Currently, if the agent crashes & thrashes then future get_state requests from the director timeout after 10 minutes. It would be better if there were some minimal server state that could respond to get_state with the last 'crashing' error message.",,78236318,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",bosh-agent should respond to get_state after failing to bootstrap,,[],956238,1266616,feature,2014-09-05T00:59:10Z,https://www.pivotaltracker.com/story/show/78236318
,2014-09-04T21:04:09Z,unscheduled,,"bosh vms currently shows only the percent of the persistent, ephemeral & root disks. 
Add used and total disk space:  ""xx% (xxx/xxx)"" 
Like memory, disk size should be displayed in bytes.",,78229682,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",add used & total disk size to `bosh vms`,,[],956238,1266616,feature,2014-09-04T21:04:09Z,https://www.pivotaltracker.com/story/show/78229682
,2014-08-28T22:49:56Z,unscheduled,,@killfill found this problem when killing off a vm and waiting for it to come back. vm was on a dynamic network. resurrector recreate_vm should probably be refactored to use instance updater flow.,,77843762,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",bosh director should update dns entries when resurrector brings back the vm,,[],956238,81882,feature,2014-08-28T22:51:50Z,https://www.pivotaltracker.com/story/show/77843762
,2014-08-28T22:42:27Z,unscheduled,,"Health monitor was unable to connect to datadog for an extended period of time. STDERR has

```
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
Timeout::Error
```

which is not particularly useful. The error logging should include where the error occurred for debugging.",,77843322,story,[],health_monitor should log error messaging with stacktrace,,[],956238,1406536,feature,2014-08-28T22:42:27Z,https://www.pivotaltracker.com/story/show/77843322
,2014-08-27T19:02:45Z,unscheduled,,"currently #update_persistent_disk method only deletes old_disk and forgets about deleting other inactive disks. this would cause deploy to eventually fail because vm now has multiple disks in its settings.json.

- should we improve attach_disk vsphere CPI method to roll back changes to env.json if env.iso writing out fails?",,77747704,story,[],bosh deploy should delete inactive disks when creating a new disk,,[],956238,81882,feature,2014-08-27T19:13:24Z,https://www.pivotaltracker.com/story/show/77747704
,2014-08-26T21:45:31Z,unscheduled,,"The task is still running but CLI prints this error: System call error while talking to director: Operation timed out

Noticed that on ruby 2.1.2
",,77665416,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]","bosh deploy should not fail with ""System call error while talking to director: Operation timed out""",,[],956238,553935,bug,2015-06-24T22:39:29Z,https://www.pivotaltracker.com/story/show/77665416
,2014-08-26T21:10:21Z,unscheduled,,,,77661302,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}]",bosh cck should be able to delete compilation vms when they stick around,,[],956238,81882,feature,2014-08-26T21:10:28Z,https://www.pivotaltracker.com/story/show/77661302
,2014-08-21T18:39:07Z,unscheduled,,"We saw this in prod yesterday.

If an unknown instance queries the registry it crashes with an unhandled exception here:

https://github.com/cloudfoundry/bosh/blob/master/bosh-registry/lib/bosh/registry/instance_manager.rb#L57

That error then appears on the bosh vm in /var/vcap/sys/log/registry/registry.stderr.log

You can hopefully verify by deleting a VM reference from the registry?
bosh VMs like to query the registry really often, which causes this crash to happen so fast it looks like a local failure. Dmitry was very helpful and sorted this out for us, but since it's pretty confusing to the cloud ops team, it would totally confuse an operator outside the building!",,77395646,story,[],Director registry task crashes when queried by/about an unknown instance,,[],956238,1092360,feature,2014-08-21T18:39:07Z,https://www.pivotaltracker.com/story/show/77395646
,2014-08-20T23:37:54Z,unscheduled,,"AFAIK the only way to cancel a bosh task is to send to attach to it, then SIGINT and say ""y"" when prompted.

On GoCD clicking the X will sends a SIGTERM to the process that GoCD started, unfortunately if that process was running a bosh deploy or a bosh errand, then these bosh tasks happily continue in the background. We'd like someway to start these task so they cancel cleanly in GoCD.",,77342216,story,[],Running bosh tasks should be cancelled on GoCD when their GoCD Job is cancelled (i.e when they receive a SIGTERM),,[],956238,5637,feature,2014-08-20T23:38:22Z,https://www.pivotaltracker.com/story/show/77342216
,2014-08-18T22:59:24Z,unscheduled,,"User assigns static ip addresses .1, .2, and .3 to a job, then deploys. Now job_0 has .1, job_1 has .2, and job_2 has .3

User then changes the assignment to .1, .3, and .4

If user generates the manifest with [.1, .3, .4], then the deploy will fail because .3 is swapped from job_2 to job_1, and swapping is not allowed.

bosh could be smart enough to notice that .1 and .3 are still in the manifest, and choose only to update job_1 to use static ip address .4",,77163812,story,[],bosh should manage static ip address assignment to avoid needless swaps,,[],956238,285989,feature,2014-08-18T22:59:25Z,https://www.pivotaltracker.com/story/show/77163812
,2014-08-18T19:26:43Z,unscheduled,,"At ATT we saw a error in a deploy where we timed out on a get_state to agent that looks like it working fine - but may have been doing a long untar that kept it busy longer than the timeout for the director.

There a UX issues in bosh agent making a bit hard to tell what's going on: an agent logs geting a 'get_task' with reply-to guid, but doesn't seem to log when it responds to the guid. If it did, we could narrow the problem more. ",,77145812,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'seen-on-prod', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-10-03T18:06:39Z', 'id': 9608880, 'updated_at': '2014-10-03T18:06:39Z'}]",Issue with BOSH director and  go agent ,,[],956238,637633,feature,2014-10-08T22:59:09Z,https://www.pivotaltracker.com/story/show/77145812
,2014-08-18T16:59:14Z,unscheduled,,in manifest using `latest` with stemcell version 0000 is failing to find stemcell. We build 0000 if CANDIDATE_BUILD_NUMBER is not specified.,,77131820,story,[],stemcell version 0000 fails to match latest definition,,[],956238,553935,bug,2014-08-25T21:40:10Z,https://www.pivotaltracker.com/story/show/77131820
,2014-08-15T05:57:22Z,unscheduled,,"on trusty golang stemcells 2686 and 2690, if I deploy with 127.0.0.1 as the first entry in my network's dns, /etc/resolv.conf gets generated weirdly.

the file is a symlink, pointing to a file with *only* the 127.0.0.1 entry present. if the file is removed, and resolv.conf is generated, it is generated correctly.

if more dns entries are added *before* 127.0.0.1, they are included before it in the resulting /etc/resolv.conf, but no entries *after* 127.0.0.1 are present (including the base ones, i.e. microbosh).",,77004134,story,[],"when 127.0.0.1 is listed as a dns server, the resulting /etc/resolv.conf is truncated past the line with 127.0.0.1",,[],956238,381857,bug,2014-08-15T05:57:22Z,https://www.pivotaltracker.com/story/show/77004134
,2014-08-12T18:18:36Z,unscheduled,,"- unattach disk from vm using vcenter
- bosh recreate to tell bosh there's a problem with the disk
- bosh cck to attach disk and reboot vm
- bosh start/restart to monit start all the things, except for one
```
Process 'mariadb_ctrl-executable'   running
Process 'galera-healthcheck-executable' running
Process 'gra-log-purger-executable' running
System 'system_944b2d7f-4d8a-450e-89f0-9b748de709bc' not monitored
```

I don't know what `system_*` does, but it seems to be it should be running.",,76813394,story,[],system_* job isn't monitored with bosh start/restart,,[],956238,58676,bug,2014-08-12T18:18:36Z,https://www.pivotaltracker.com/story/show/76813394
,2014-08-06T22:38:22Z,unscheduled,,"```
$ bosh releases

+----------+----------+-------------+
| Name     | Versions | Commit Hash |
+----------+----------+-------------+
| cf       | 176*     | 4c9c6d0a+   |
| cf-mysql | 8        | 199c2433+   |
|          | 8+dev.8  | 87b885a8    |
|          | 9        | bcd336df+   |
|          | 9+dev.1  | 8385fe2b    |
|          | 9+dev.2  | 97fbd5e2    |
|          | 9+dev.3  | 36393a26    |
|          | 9+dev.4  | 36393a26+   |
|          | 10+dev.2 | 83e9b2d5+   |
+----------+----------+-------------+
(*) Currently deployed
(+) Uncommitted changes

Releases total: 2

$ bosh help delete release
delete release <name> [<version>] [--force]
    Delete release (or a particular release version)
    --force         ignore errors during deletion

$ bosh delete release cf-mysql 10+dev.2
Deleting `cf-mysql/10+dev.2'
Are you sure? (type 'yes' to continue): yes

Director task 138
Error 30010: Release version invalid: 10 dev.2

Task 138 error

For a more detailed error report, run: bosh task 138 --debug
```

If this is not the correct syntax, there's no way of knowing. Update either: 
- the command to work as attempted
- the output of `bosh releases` to have a version format consistent with syntax that works (copy and paste should ""just work"")
- or the help file to better describe the syntax that works",,76487058,story,[],can't delete a release,,[],956238,58676,bug,2014-08-06T22:38:22Z,https://www.pivotaltracker.com/story/show/76487058
,2014-08-06T19:33:08Z,unscheduled,,"Deploy to AWS failed with error: Timed out waiting for volume vol-981108d4 to be detached to instance i-e9d70ec5 as device /dev/sdg to be detached, took 1609.395351906s""

```
E, [2014-08-06T00:19:12.272883 #6144] [task:21846] ERROR -- : Timed out waiting for volume vol-981108d4 to be detached to instance i-e9d70ec5 as device /dev/sdg to be detached, took 1609.395351906s
/var/vcap/data/packages/director/44.1-cfe285b5933ec7b0a10562b33f89c8d22fb6e3b3/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/helpers.rb:14:in `cloud_error'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/resource_wait.rb:165:in `block in for_resource'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/retryable.rb:35:in `call'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/retryable.rb:35:in `ensure in retryer'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/retryable.rb:35:in `retryer'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/resource_wait.rb:169:in `for_resource'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/resource_wait.rb:48:in `for_attachment'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/cloud.rb:594:in `detach_ebs_volume'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/cloud.rb:249:in `block in detach_disk'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.2200.0/lib/cloud/aws/cloud.rb:239:in `detach_disk'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:263:in `delete_disk'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:417:in `rescue in update_persistent_disk'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:413:in `update_persistent_disk'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:77:in `block in update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:40:in `step'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/instance_updater.rb:77:in `update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_updater.rb:86:in `block (2 levels) in update_canary_instance'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_updater.rb:84:in `block in update_canary_instance'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_updater.rb:81:in `update_canary_instance'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_updater.rb:75:in `block (2 levels) in update_canaries'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_pool.rb:79:in `call'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_pool.rb:79:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_pool.rb:63:in `loop'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_pool.rb:63:in `block in create_thread'
```

The subsequent deploy was running ""mount disk"" for 18 hours.

The agent was responding with task state as ""running"".
Canceling deployment did not have any effect. Director did not event send 'cancel_task' to the agent.
",,76473356,story,[],"Bosh should be able to cancel ""mount_disk"" agent task",,[],956238,1061299,bug,2014-08-06T20:00:26Z,https://www.pivotaltracker.com/story/show/76473356
,2014-08-04T17:52:49Z,unscheduled,,"ruby 2.1.2p95 athens in ~/workspace/p-mysql
± dr+rmd |develop-1.3 ✓| → bosh releases
/Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/commands/release.rb:585:in `block (2 levels) in build_releases_table': undefined method `join' for nil:NilClass (NoMethodError)
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/commands/release.rb:583:in `each'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/commands/release.rb:583:in `block in build_releases_table'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/terminal-table-1.4.5/lib/terminal-table/table.rb:210:in `yield_or_eval'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/terminal-table-1.4.5/lib/terminal-table/table.rb:17:in `initialize'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/terminal-table-1.4.5/lib/terminal-table/table_helper.rb:5:in `new'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/terminal-table-1.4.5/lib/terminal-table/table_helper.rb:5:in `table'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/commands/release.rb:580:in `build_releases_table'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/commands/release.rb:156:in `list'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.gem/ruby/2.1.2/gems/bosh_cli-1.2366.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/2.1.2/bin/bosh:23:in `<main>'",,76297486,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",bosh releases should be more resilient to returned output from the director,,[],956238,1017737,feature,2014-08-04T19:41:26Z,https://www.pivotaltracker.com/story/show/76297486
,2014-04-29T18:31:22Z,unscheduled,,,,70384252,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]",bosh run errand command should provide an option to not save the logs file and show output immediately on screen,,[],956238,81882,feature,2014-08-05T07:22:16Z,https://www.pivotaltracker.com/story/show/70384252
,2014-03-10T17:58:53Z,unscheduled,,currently user has to use `--no-filter` flag to show errand tasks via `bosh tasks`,1.0,67240490,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]",bosh tasks should include run_errand tasks by default,,[],956238,81882,feature,2014-08-05T07:22:16Z,https://www.pivotaltracker.com/story/show/67240490
,2014-03-18T23:48:36Z,unscheduled,,"When a deployed job has ""lifecycle: errand"" added to it, bosh does not delete the VM.",2.0,67804108,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]",Converting a deployed job to an errand job does not delete the instance.,,[],956238,756869,feature,2014-08-05T07:22:16Z,https://www.pivotaltracker.com/story/show/67804108
,2014-06-20T00:32:02Z,unscheduled,,"When I run cats I don't want to wait 20 minutes to know that something is happening.  Similarly if I was migrating a database I'd want to watch the output, not trust that my migration was happening.",,73594450,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]","When running an errand, output should be streamed to the stream as the errand runs",,[],956238,46031,feature,2014-08-05T07:22:16Z,https://www.pivotaltracker.com/story/show/73594450
,2014-05-21T17:10:50Z,unscheduled,,"it appears that if an error happens after running an errand (e.g. refilling) errand output is not shown (though is saved in result file). we should try to show it.

on embarcadero:
```
±  |develop ✗| → bosh run errand acceptance_tests

Director task 68
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:02)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:02)

  Started preparing package compilation > Finding packages to compile. Done (00:00:01)

  Started binding instance vms > acceptance_tests/0. Done (00:00:00)

  Started updating job acceptance_tests > acceptance_tests/0. Done (00:00:27)

  Started running errand > acceptance_tests/0. Done (00:01:16)

  Started fetching logs for acceptance_tests/0 > Finding and packing log files. Done (00:00:01)

  Started deleting instances acceptance_tests > i-ae308dfd. Done (00:01:05)

  Started refilling resource pools > small_errand/0. Failed: Your quota allows for 0 more running instance(s). You requested at least 1 (00:00:01)

Error 100: Your quota allows for 0 more running instance(s). You requested at least 1

Task 68 error

Errand `acceptance_tests' did not complete

For a more detailed error report, run: bosh task 68 --debug

 |1.9.3-p448| gorgas in ~/workspace/cf-release
```",,71775642,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}]",user should see stdout and stderr from the director even if running errand failed to refill resource pool,,[],956238,81882,feature,2014-08-05T07:22:16Z,https://www.pivotaltracker.com/story/show/71775642
,2014-01-23T22:29:54Z,unscheduled,,"Example output where bosh tried to update api_z1/0 even through we were trying to recreate runner_z1 0

We've been experiencing issues with our API instance so it's possible api_z1 0 was failing in the background and the deployment plan created to recreate a runner decided to rectify this?
 
```
bosh -n recreate runner_z1 0
You are about to recreate runner_z1/0

Performing `recreate runner_z1/0'...

Director task 16

Preparing deployment
  binding deployment (00:00:00)
  binding releases (00:00:00)
  binding existing deployment (00:00:00)
  binding resource pools (00:00:00)
  binding stemcells (00:00:00)
  binding templates (00:00:00)
  binding properties (00:00:00)
  binding unallocated VMs (00:00:00)
  binding instance networks (00:00:00)
Done                    9/9 00:00:00

Preparing package compilation

Preparing DNS
  binding DNS (00:00:00)
Done                    1/1 00:00:00

Preparing configuration
  binding configuration (00:00:01)
Done                    1/1 00:00:01

  Started updating job api_z1: api_z1/0
     Done updating job api_z1: api_z1/0

Error 400007: `api_z1/0' is not running after update

Task 16 error

For a more detailed error report, run: bosh task 16 --debug
```
",,64424664,story,[],bosh recreate foo_job 1 should not affect any other jobs/instances,,[],956238,5637,feature,2014-08-05T08:07:18Z,https://www.pivotaltracker.com/story/show/64424664
,2014-08-01T23:28:32Z,unscheduled,,"volumes cannot be deleted before all snapshots are delete.

current workarounds:
- delete snapshots manually from iaas
- disable enable_snapshots setting before allowing bosh to create volumes

http://docs.openstack.org/api/openstack-block-storage/2.0/content/Delete_Volume.html",,76164066,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",bosh should delete disk snapshots before deleting volumes before scaling down instances,,[],956238,81882,feature,2014-08-01T23:30:31Z,https://www.pivotaltracker.com/story/show/76164066
,2014-08-01T23:28:03Z,unscheduled,,,,76164056,story,"[{'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",bosh delete deployment should delete disk snapshots before deleting volume,,[],956238,81882,feature,2014-08-01T23:28:59Z,https://www.pivotaltracker.com/story/show/76164056
,2014-07-31T16:30:06Z,unscheduled,,"on bosh-lite, migration disk stays mounted if persistent disk migration fails.

When increasing the size of the persistent disk, if the migration fails for some reason, the new volume stays mounted.

Migration failed because no space left on device:

```
Error 450001: Action Failed get_task: Task 2e578ab3-bd1e-469f-6918-5f22542a7976 result: Migrating persistent disk: Copying files from old disk to new disk: Running command: 'sh -c (tar -C /var/vcap/store -cf - .) | (tar -C /var/vcap/store_migration_target -xpf -)', stdout: '', stderr: 'tar: ./riak/rel/lib/eleveldb/205523667749658222872393179600727299639115513856/MANIFEST-000002: Wrote only 1024 of 10240 bytes
tar: ./riak/rel/lib/eleveldb/205523667749658222872393179600727299639115513856/LOG.old: Cannot write: No space left on device
tar: ./riak/rel/lib/eleveldb/205523667749658222872393179600727299639115513856/sst_2: Cannot mkdir: No space left on device
```

Later deploys fails until /var/vcap/store_migration_target is manually unmounted:

```
  Started updating job riak-cs
  Started updating job riak-cs > riak-cs/0 (canary). Failed: Action Failed get_task: Task 617978d7-1b13-40f9-6268-9551671e3865 result: Mounting persistent disk: Mounting partition: Device /warden-cpi-dev/disk-717194a7-1463-4f03-a4d2-6687ad247814 is already mounted to /var/vcap/store_migration_target, can't mount /warden-cpi-dev/disk-1a8687fe-c1f0-4c0b-b5f3-afc8f9f06388 (00:00:04)

Error 450001: Action Failed get_task: Task 617978d7-1b13-40f9-6268-9551671e3865 result: Mounting persistent disk: Mounting partition: Device /warden-cpi-dev/disk-717194a7-1463-4f03-a4d2-6687ad247814 is already mounted to /var/vcap/store_migration_target, can't mount /warden-cpi-dev/disk-1a8687fe-c1f0-4c0b-b5f3-afc8f9f06388
```",,76067080,story,"[{'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",user should be able to increase persistent disk size and successfully redeploy after an unsuccessful deploy with a smaller disk,,[],956238,756869,bug,2014-08-01T00:49:18Z,https://www.pivotaltracker.com/story/show/76067080
,2013-11-20T20:09:51Z,unscheduled,,"Both these blobstore implementations do not support choosing the key for uploaded objects, a feature that we've needed on a few occasions.

The simple blobstore server has also been considered superseded by the DAV blobstore for some time now.",,61159896,story,"[{'name': 'blobstore', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-15T18:33:26Z', 'id': 8216944, 'updated_at': '2014-04-15T18:33:26Z'}]",Formally remove support for the simple and Atmos blobstores,,[],956238,5637,feature,2015-05-22T01:52:41Z,https://www.pivotaltracker.com/story/show/61159896
,2014-07-30T21:55:06Z,unscheduled,,"Looks like passing log filters to `bosh logs` via `--only` is broken; seems like a simple typo in the job:

```ruby
@logs_fetcher.fetch(instance, @log_type, @filter) # should be @filters
```",,76014944,story,[],bosh logs --only doesn't seem to work,,[],956238,1123776,bug,2014-07-30T21:55:30Z,https://www.pivotaltracker.com/story/show/76014944
,2014-05-29T04:16:25Z,unscheduled,,is it possible that multipart threshold properties is causing s3 upload issues while bosh upload blobs. increasing it to large number helped.,,72240378,story,[],investigate failing to upload ~170mb blob to s3 with 2579,,[],956238,81882,feature,2014-08-05T07:43:45Z,https://www.pivotaltracker.com/story/show/72240378
,2014-07-30T18:34:08Z,unscheduled,,"Currently  if targeting previously targeted director`bosh upload release` will show usage error, and if you re-login everything works.",,75997452,story,[],"When user updates the microbosh and needs to relogin he should see Unathorised error, not the Usage error",,[],956238,553935,bug,2014-07-30T18:34:08Z,https://www.pivotaltracker.com/story/show/75997452
,2014-07-29T09:16:15Z,unscheduled,,"Streaming the contents of a log file on disk would be really handy, and it looks like rsyslogd can do this easily using the `imfile` module. Unfortunately this module doesn't seem to be available on recent stemcells (checked: bosh-aws-xen-ubuntu-lucid-go_agent/2611& bosh-warden-boshlite-ubuntu-lucid-go_agent/60).",,75877134,story,[],rsyslod should have the `imfile` module available,,[],956238,1139720,feature,2014-07-29T09:16:16Z,https://www.pivotaltracker.com/story/show/75877134
,2013-11-26T00:49:00Z,unscheduled,,,1.0,61441144,story,[],Investigate implementing ability to change DNS server IP address ,,[],956238,1068489,feature,2014-08-05T07:43:21Z,https://www.pivotaltracker.com/story/show/61441144
,2013-12-08T07:16:32Z,unscheduled,,"as reported here: https://groups.google.com/a/cloudfoundry.org/d/msg/vcap-dev/bUcPnE5Axs4/dnb0eKgIcdsJ

latest openstack kvm ubuntu stemcells like 1436 and 1484 use the path:
/usr/local/sbin/rsyslogd

cf-release v147 syslog_aggregator_ctl script attempts to execute rsyslogd from /usr/sbin/rsyslogd

somewhere along the line this changed because openstack-kvm-ubuntu stemcell version 1256 uses /usr/sbin/rsyslogd

was that intentional? does the runtime team need to update their job ctl scripts as a result?",,62108468,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",stemcells should support rsyslog at /usr/sbin/rsyslogd,,[],956238,494053,bug,2014-09-03T17:13:26Z,https://www.pivotaltracker.com/story/show/62108468
,2014-03-06T07:27:02Z,unscheduled,,"To reproduce:
1. Deploy something to vSphere using BOSH
1. Note that for each deployed VM, there's a folder in the datastore with the same name as the VM containing that VM's ephemeral disk, env.json, env.iso, and probably some other stuff
1. Delete the deployment
1. Note that those folders are still there in the datastore (although now they only contain env.json and env.iso)",,67006974,story,[],"When deleting a deployment (on vSphere), BOSH doesn't clean up folders in the datastore.",,[],956238,668075,bug,2014-08-05T07:43:21Z,https://www.pivotaltracker.com/story/show/67006974
,2014-03-12T18:06:05Z,unscheduled,,"https_handler.go#Start
https_dispatcher.go#Start",,67415208,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Propagate the error when agent fails to start http server,,[],956238,553935,chore,2014-08-05T07:43:21Z,https://www.pivotaltracker.com/story/show/67415208
,2014-05-17T16:35:10Z,unscheduled,,"CLI version: BOSH 1.2559.0
Director Stemcell version: bosh-aws-xen-centos/2200

To reproduce with cf-release:

1. upload cf-release/releases/cf-161.yml
2. remove the libyaml package from cf-release (it is no longer used by the ruby package)
3. create a dev release
4. upload the dev release
5. deploy the dev release

The deploy fails trying to compile the libyaml package even though it has been deleted:

```
Getting deployment properties from director...
Compiling deployment manifest...

Director task 13555

Preparing deployment

                                                                                                                        
Binding deployment                         |                            | 0/9
                                                                                                                        
  binding deployment (00:00:00)

                                                                                                                        
                                           |ooo                         | 1/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding releases                           |ooo                         | 1/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding releases (00:00:00)

                                                                                                                        
                                           |oooooo                      | 2/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding existing deployment                |oooooo                      | 2/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding existing deployment (00:00:00)

                                                                                                                        
                                           |ooooooooo                   | 3/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding resource pools                     |ooooooooo                   | 3/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding resource pools (00:00:00)

                                                                                                                        
                                           |oooooooooooo                | 4/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding stemcells                          |oooooooooooo                | 4/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding stemcells (00:00:00)

                                                                                                                        
                                           |ooooooooooooooo             | 5/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding templates                          |ooooooooooooooo             | 5/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding templates (00:00:00)

                                                                                                                        
                                           |oooooooooooooooooo          | 6/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding properties                         |oooooooooooooooooo          | 6/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding properties (00:00:00)

                                                                                                                        
                                           |ooooooooooooooooooooo       | 7/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding unallocated VMs                    |ooooooooooooooooooooo       | 7/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding unallocated VMs (00:00:00)

                                                                                                                        
                                           |oooooooooooooooooooooooo    | 8/9 00:00:00  ETA: --:--:--
                                                                                                                        
Binding instance networks                  |oooooooooooooooooooooooo    | 8/9 00:00:00  ETA: --:--:--
                                                                                                                        
  binding instance networks (00:00:00)

                                                                                                                        
                                           |oooooooooooooooooooooooooooo| 9/9 00:00:00  ETA: --:--:--
                                                                                                                        
Done                                        9/9 00:00:00

Preparing package compilation

                                                                                                                        
Finding packages to compile                |                            | 0/1
                                                                                                                        
  finding packages to compile: key not found: ""libyaml"" (00:00:00)

                                                                                                                        
                                           |oooooooooooooooooooooooooooo| 1/1 00:00:00  ETA: --:--:--
                                                                                                                        
Error                                       1/1 00:00:00

Error 100: key not found: ""libyaml""

Task 13555 error

For a more detailed error report, run: bosh task 13555 --debug
```

The stacktrace from `bosh task 13555 --debug` was:

```
E, [2014-05-17T08:26:18.885989 #8379] [task:13555] ERROR -- : key not found: ""libyaml""
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:21:in `fetch'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:21:in `package_by_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:16:in `block in dependencies'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `block in each'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each_key'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:15:in `map'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:15:in `dependencies'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/models/release_version.rb:25:in `package_dependency_key'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/compile_task_generator.rb:30:in `generate!'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:202:in `block (4 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:201:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:201:in `block (3 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:200:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:200:in `block (2 levels) in prepare_tasks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:187:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:187:in `block in prepare_tasks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/event_log.rb:83:in `call'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/event_log.rb:83:in `advance_and_track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/event_log.rb:36:in `track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:186:in `prepare_tasks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/package_compiler.rb:52:in `compile'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/jobs/update_deployment.rb:45:in `prepare'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/jobs/update_deployment.rb:73:in `block in perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/jobs/update_deployment.rb:71:in `perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_runner.rb:98:in `perform_job'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.2200.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2200.0/bin/bosh-director-worker:76:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `<main>'
```

NB: I was able to this reproduce on bosh-lite yesterday, but I am no longer able to reproduce this with the newer ""186"" bosh-lite box. I wonder if switching to fingerprints from package versions somehow fixed/masked this?",,71524826,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",Deleting an unused package from a release prevents the release from being deployed,,[],956238,5637,bug,2014-08-22T13:16:14Z,https://www.pivotaltracker.com/story/show/71524826
,2014-07-01T20:33:22Z,unscheduled,,,,74257126,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",Do not create /var/vcap/store directory if there is no persistent disk mounted,,[],956238,1123776,feature,2014-08-01T00:49:18Z,https://www.pivotaltracker.com/story/show/74257126
,2014-06-20T00:03:54Z,unscheduled,,"as a deployment operator, I should be able to have a persistent volume remount when a VM reboots so that I don't have to manually take it from attached to mount state.

Current state:  VM reboots & persistent volume does not com back.  Disk is attached but it is not mounted",,73593666,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",Persistent Volume is Mounted After VM Reboot,,[],956238,1210852,feature,2014-08-30T14:04:08Z,https://www.pivotaltracker.com/story/show/73593666
,2014-01-24T17:38:09Z,unscheduled,,"On an existing deployment, reduce the number of instances in a job and redeploy. Bosh will delete the now unused instances. At the end of the deployment when it refills resources pools, it will try to re-use IPs that are still held by the deleted instances because they have not finished deleting yet. AWS will return an error.

Jobs were not using static IPs.

Try reducing the number of instances to 0 if you can't reproduce.",,64477156,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",On AWS reducing the number of instances of a job causes IP conflicts.,,[],956238,756869,bug,2014-07-31T05:03:44Z,https://www.pivotaltracker.com/story/show/64477156
,2014-07-07T21:35:27Z,unscheduled,,"As a deployment operator on vSphere, I should be able to specify multiple resource pools per cluster in my BOSH manifest so that 

It would be great to be able to specify multiple resource pools per cluster.

Currently, you use a snippet ala:
```
  vcenter:
    address: 1.2.3.4
    user: ****
    password: *******
    datacenters:
      - name: TEST_DATACENTER
        vm_folder:       SYSTEM_MICRO_VSPHERE_VMs
        template_folder: SYSTEM_MICRO_VSPHERE_Templates
        disk_path:       SYSTEM_MICRO_VSPHERE_Disks
        datastore_pattern:            datastore1
        persistent_datastore_pattern: datastore1
        allow_mixed_datastores: true
        clusters:
          - TEST_CLUSTER:
              resource_pool: TEST_RP
```

However, this ties you to only one resource pool for the `TEST_CLUSTER` cluster.  It would be wonderful to be able to have something more like:

```
  vcenter:
    address: 1.2.3.4
    user: ****
    password: *******
    datacenters:
      - name: TEST_DATACENTER
        vm_folder:       SYSTEM_MICRO_VSPHERE_VMs
        template_folder: SYSTEM_MICRO_VSPHERE_Templates
        disk_path:       SYSTEM_MICRO_VSPHERE_Disks
        datastore_pattern:            datastore1
        persistent_datastore_pattern: datastore1
        allow_mixed_datastores: true
        clusters:
          - cluster1:
              cluster_name: TEST_CLUSTER
              resource_pool: TEST_RP
          - cluster2:
              cluster_name: TEST_CLUSTER
              resource_pool: ANOTHER_TEST_RP
```",2.0,74560930,story,"[{'name': 'storage and affinity', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-13T22:18:17Z', 'id': 7254216, 'updated_at': '2013-12-13T22:18:17Z'}]",specify multiple resource pools per cluster,,[],956238,1068489,feature,2014-07-28T18:08:26Z,https://www.pivotaltracker.com/story/show/74560930
,2014-07-09T21:40:34Z,unscheduled,,"Runtime has seen frequent failed initial deploys of cf-release to bosh-lite VMs where the small errand resource pool fails to fill after the service jobs are provisioned. This has occurred on both vmware_fusion and AWS bosh-lite boxes, at least.

vmware_fusion failure snippet:
```
  Started refilling resource pools
  Started refilling resource pools > small_errand/0
  Started refilling resource pools > small_errand/1
   Failed refilling resource pools > small_errand/0: command exited with failure (00:00:02)
     Done refilling resource pools > small_errand/1 (00:00:03)
   Failed refilling resource pools (00:00:03)

Error 100: command exited with failure
```

AWS failure example: see http://server.gocd.cf-app.com/go/tab/build/detail/cf-test/109/deploy/1/deploy

Bosh debug task logs attached in the activity below.

Steps to reproduce:
- `vagrant up` a new bosh-lite
- upload a stemcell and upload and deploy a recent cf-release
- sometimes the errand resource pools fail to fill on the initial deploy

This seems to happen more frequently on AWS bosh-lites. We've been using m3.2xlarge instances for those VMs.",,74735440,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",Filling errand resource pool unreliable on bosh-lite,,[],956238,1158132,bug,2014-07-28T17:41:13Z,https://www.pivotaltracker.com/story/show/74735440
,2014-07-24T17:28:14Z,unscheduled,,"After #75622904 is fixed, the user will be able to deploy --recreate in order to propagate blobstore credentials, but if the deployer doesn't also manage the bosh instance they may not know that the blobstore credentials have changed. For better UX, deploy should handle this transparently.

One potential solution would be to send the blobstore credentials in the apply-spec.",,75623174,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",User should be able to propagate blobstore credentials on deploy without recreate,,[],956238,1266616,feature,2014-07-31T04:37:02Z,https://www.pivotaltracker.com/story/show/75623174
,2014-04-05T00:18:27Z,unscheduled,,"While trying to deploy microbosh to OpenStack, we found it could hang. Dmitry came over and found out this was because of an ordering issue where there was a check for an SSH connection; there was a select() which would hang forever.

Reversing the order so the wait happens first solved the problem for us; this issue is to get that work included in microbosh.",,68922318,story,[],Ordering bug when microbosh checks for presence of SSL connection,,[],956238,1092360,bug,2014-07-28T17:12:49Z,https://www.pivotaltracker.com/story/show/68922318
,2014-07-28T11:13:07Z,unscheduled,,"These look like warning but everything seems to still work.

```
   28 ?        S      0:02 runsvdir -P /etc/service log: already exist.?rsyslogd: error: option -c is no longer supported - ignored?rsyslogd: invalid or yet-unknown config file command 'KLogPath' - have you forgotten to load a module? [try http://www.rsyslog.com/e/3003 ]?rsyslogd: action '*' treated as ':omusrmsg:*' - please change syntax, '*' will not be supported in the future [try http://www.rsyslog.com/e/2184 ]?Pidfile (and pid) already exist.?
```

This is with stemcell version *bosh-warden-boshlite-ubuntu-lucid-go_agent/60*.",,75796214,story,[],rsyslog startup errors in warden stemcell,,[],956238,1218342,bug,2014-07-28T11:13:07Z,https://www.pivotaltracker.com/story/show/75796214
,2013-09-18T17:02:13Z,unscheduled,,"For instance during failover, or when modifying the RDS database (e.g. we resized dijon's db recently and the director lost its connection to RDS)",2.0,57242346,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",The bosh director & its workers should automatically reconnect to the database if this connection is lost for any reason.,,[],956238,5637,feature,2014-08-01T23:16:20Z,https://www.pivotaltracker.com/story/show/57242346
,2014-01-20T18:08:29Z,unscheduled,,See https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/TG7Jlkfov5s/1p3Qs79k2NwJ,,64169780,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]","If the resurrector is not enabled, warn user when he uses the 'bosh vm resurrection' command.",,[],956238,1015645,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/64169780
,2013-12-11T00:45:40Z,unscheduled,,"@mattr, Matt Parker asked why we need to download manifest and then run bosh deployment before bosh ssh. 

Currently the reason why we need to download manifest is to be able to set deployment name and validate that job exists in the manifest.

1. Set deployment name via bosh deployment without needing a manifest so that bosh ssh knows which deployment
1. Do not check that the jobs exists in the deploy manifest.  `bosh ssh` should never require a deploy manifest at all.

Acceptance:
```
$ bosh target UNKOWN_DIRECTOR
$ bosh vms
$ bosh ssh VM
```",,62280574,story,"[{'name': 'ssh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T15:14:05Z', 'id': 8094772, 'updated_at': '2014-04-01T15:14:05Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh ssh should not require deployment manifest and then doing bosh deployment,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/62280574
,2014-01-16T19:48:46Z,unscheduled,,,,63997218,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh target should print out bosh status automatically,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/63997218
,2013-10-23T23:41:50Z,unscheduled,,"password was fixed in #55322154

here is most of the vcenter conf that goes into micro bosh:

    agent:
      ntp: [""us.pool.ntp.org""]

    vcenters:
    - host: 172.16.68.3
      user: root
      password: vmware

      datacenters:
      - name: BOSH_DC
        vm_folder: ACCEPTANCE_BOSH_VMs
        template_folder: ACCEPTANCE_BOSH_Templates
        disk_path: ACCEPTANCE_BOSH_Disks
        datastore_pattern: 'jalapeno'
        persistent_datastore_pattern: 'jalapeno'
        allow_mixed_datastores: true
        clusters:
        - BOSH_CL

All of the hash values above may start with a exclamation mark especially when users generate those values automatically (one of the customers had them generated automatically).",4.0,59455700,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",valid configurations in director job templates that start with an exclamation mark should not cause problems when interpolated on director,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/59455700
,2014-06-06T19:11:05Z,unscheduled,,"evaluate templates using manifest and spec files.  output the result.  if you hit a real director, the output is real/sensical (e.g. - a real IP is given back)...true to the environment that it runs in.  

But when not hitting real director, the runtime specific info is skipped but other info is still valuable. 

skips compilation & gives feedback to user so use can see what inputs are given back to job?
jobs and errands

",,72801676,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh deploy  --dry-run  gives valid feedback,,[],956238,707557,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72801676
,2014-05-30T21:13:52Z,unscheduled,,"We've been developing this release from two different machines, and admittedly it's about time we set up a shared blobstore, but when we forgot to copy/generate our private.yml, the error messages were unclear.",,72371118,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","When our release has no config/private.yml, 'bosh blobs' just echoed 'usage: 'blobs'",,[],956238,801085,bug,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72371118
,2013-12-03T22:09:04Z,unscheduled,,"If a dev release is uploaded without a `--rebase`, bosh can end up with orphaned jobs/packages.  `bosh cleanup` should clean these out.",2.0,61854464,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh cleanup` should delete orphaned jobs/packages,,[],956238,1068489,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/61854464
,2014-06-10T16:52:05Z,unscheduled,,"Example bellow shows that the affected jobs are shown both under ""Jobs"" with ""new version"" and under ""Jobs affected by changes in this release"". This info is duplicate and unnecessary.

```
bosh create release --force
Syncing blobs...
Please enter development release name: |dummy| dummy2

Building DEV release
---------------------------------

Building packages
-----------------
Building bad_package...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version e05b55beba82daac75cf5a941429374a7b60c815

Building dummy_package...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version 089602e69964da63bc5f797caa85d8d5e955973d


Resolving dependencies
----------------------
Dependencies resolved, correct build order is:
- bad_package
- dummy_package


Building jobs
-------------
Building dummy...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version 289ecbf3fa7359e84d84e5d7c5edd22689ad81d4

Building dummy_with_bad_package...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version de7713ed0ead9954a444c65348ed27890728cfd3

Building dummy_with_package...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version b819fee41c8e2c461e26f338bc0a510841d65c48

Building dummy_with_properties...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
  Generated version a933a391fdf14df1a68aa3d51c5e8089dfb89962


Building release
----------------

Generating manifest...
----------------------
Writing manifest...

Release summary
---------------
Packages
+---------------+------------------------------------------+-------------+
| Name          | Version                                  | Notes       |
+---------------+------------------------------------------+-------------+
| bad_package   | e05b55beba82daac75cf5a941429374a7b60c815 | new version |
| dummy_package | 089602e69964da63bc5f797caa85d8d5e955973d | new version |
+---------------+------------------------------------------+-------------+

Jobs
+------------------------+------------------------------------------+-------------+
| Name                   | Version                                  | Notes       |
+------------------------+------------------------------------------+-------------+
| dummy                  | 289ecbf3fa7359e84d84e5d7c5edd22689ad81d4 | new version |
| dummy_with_bad_package | de7713ed0ead9954a444c65348ed27890728cfd3 | new version |
| dummy_with_package     | b819fee41c8e2c461e26f338bc0a510841d65c48 | new version |
| dummy_with_properties  | a933a391fdf14df1a68aa3d51c5e8089dfb89962 | new version |
+------------------------+------------------------------------------+-------------+

Jobs affected by changes in this release
+------------------------+------------------------------------------+
| Name                   | Version                                  |
+------------------------+------------------------------------------+
| dummy                  | 289ecbf3fa7359e84d84e5d7c5edd22689ad81d4 |
| dummy_with_bad_package | de7713ed0ead9954a444c65348ed27890728cfd3 |
| dummy_with_package     | b819fee41c8e2c461e26f338bc0a510841d65c48 |
| dummy_with_properties  | a933a391fdf14df1a68aa3d51c5e8089dfb89962 |
+------------------------+------------------------------------------+
```",,72983450,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",create release should not print duplicate job info,,[],956238,1266616,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72983450
,2014-04-24T17:21:29Z,unscheduled,,"ERROR: bosh -n -c /tmp/d20140424-28693-3fcp99/bosh_config.yml vms --details failed with output:
       Deployment `simple'
       
       Director task 15
       Error 100: undefined method `each_value' for nil:NilClass

This was part of integration tests so we don't have debug logs, but we can trace where we call each_value when doing bosh vms
",,70105122,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",Bosh vms should not fail with `undefined method `each_value' for nil:NilClass`,,[],956238,1123776,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/70105122
,2014-02-20T06:18:20Z,unscheduled,,"If a operator configures a Bosh with a maximum of threads, director should honor this configured value when using ThreadPool. An example is the job updater process: it will use the max_in_flight value regardless of what max_threads is.",,66121256,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Bosh should honor max_threads,,[],956238,1015645,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/66121256
,2014-03-11T23:01:10Z,unscheduled,,"it's not clear how useful today's `bosh validate jobs` functionality is. 

Investigate what `bosh validate jobs` does and suggest more thorough set of validations for release+dep-manifest combination",,67355502,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'validate jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-16T18:15:59Z', 'id': 8226276, 'updated_at': '2014-04-16T18:15:59Z'}]",investigate bosh validate jobs functionality,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/67355502
,2014-02-16T01:43:37Z,unscheduled,,"https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/command_handler.rb#L59

The CLI rescues ArgumentError, a super generic Ruby exception, and assumes the user gave the wrong number of arguments. This masks bugs in the CLI and makes it look like the user messed up. I saw this previously when running 'bosh aws create' which is kind of scary.

Gist showing a particular manifestation of the issue:

https://gist.github.com/vito/c0ad1dca068ac1f02a5f",2.0,65855040,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","in CLI, ArgumentError should not be treated as a user error",,[],956238,381857,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/65855040
,2014-06-26T18:50:07Z,unscheduled,,,,73989466,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","""bosh create release"" should warn that your working dir is dirty and recommend using ""--force"" BEFORE doing time consuming things like pulling down blobs and building packages.",,[],956238,668075,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/73989466
,2014-05-07T22:54:00Z,unscheduled,,"Tried to set the micro deployment manifest without realizing that the bosh-deployments.yml contained a reference for an instance that did not exists with the following result. 

```
/Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2524.0/lib/bosh/deployer/instance_manager/openstack.rb:114:in `discover_client_services_ip': undefined method `floating_ip_address' for nil:NilClass (NoMethodError)
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2524.0/lib/bosh/deployer/instance_manager/openstack.rb:63:in `client_services_ip'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2524.0/lib/bosh/cli/commands/micro.rb:55:in `set_current'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2524.0/lib/bosh/cli/commands/micro.rb:32:in `micro_deployment'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2524.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2524.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2524.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2524.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/bin/bosh:23:in `load'
	from /Users/pivotal/.rbenv/versions/1.9.3-p484/bin/bosh:23:in `<main>'
```

It would be great if an error message was returned instead.",,70904152,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",BOSH CLI should gracefully error when setting a the micro deployment manifest with a non-existent instance,,[],956238,1406536,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/70904152
,2014-04-05T01:00:33Z,unscheduled,,,,68923208,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-17T21:58:30Z', 'id': 8237838, 'updated_at': '2014-04-17T21:58:30Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh disks command - list disks and associations,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68923208
,2014-04-09T21:23:51Z,unscheduled,,"`bosh validate jobs` currently only works when a job is defined using 'template'.  If you use the new 'templates' format of defining a job, you get an error something like 

```
Job `nats_z1' doesn't have a template
```",,69220986,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'validate jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-16T18:15:59Z', 'id': 8226276, 'updated_at': '2014-04-16T18:15:59Z'}]",`bosh validate jobs` should properly looks at 'templates' hash along with 'template',,[],956238,1068489,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/69220986
,2014-04-02T19:13:01Z,unscheduled,,"as a bosh operator, i should be able to reset a bosh target and have it make no change to the deployment manifest in use.

Currently, if you did this:
1. `bosh deployment /path/to/manifest1.yml`
2.  `bosh target bosh.bosh-cf-app.com`
3.  `bosh deployment /path/to/manifest2.yml`
4.  `bosh target bosh-vcenter.orsomething.com`
5.  `bosh target bosh.bosh-cf-app.com`
--->  `bosh deployment` will show /path/to/manifest1.yml though you did not explicitly reset to that manifest.  

Desired state after this story is that changing a target does not change the deployment manifest.",,68747578,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Disconnect resetting `bosh target` from resetting last manifest file behind the scenes,,[],956238,1210852,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68747578
,2014-04-01T16:58:47Z,unscheduled,,"Since you can run `bosh target`, and get the list of deployments you should be able to use an existing deployment name when running commands like `bosh ssh` this prevents people from using the following workflow

1. `bosh target <some-target>`
2. `bosh deployments`
3. `bosh download manifest <some-deployment`
4. `bosh deployment <some-deployment-manifest>`
5. `bosh ssh <some-job-in-the-deployment>`

Especially in the case where the deployment name is known in advance this would be a huge time saver.",,68649420,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",the -d flag should work with a deployment name as well as a file,,[],956238,1406536,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68649420
,2014-03-28T21:34:28Z,unscheduled,,"It should have some basic contents (similar to the `monit` and `spec` files), for example:

```
case $1 in

  start)
    ;;
  stop)
    ;;
  *)
    echo ""Usage: ctl {start|stop}""
    ;;

esac
```",,68475966,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh generate job <jobname>` should generate a starter `ctl.erb` file.,,[],956238,1194390,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68475966
,2014-03-27T01:42:14Z,unscheduled,,"Looks like error was not associated properly with job failure.

```
{""time"":1395878724,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1395878725,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":12}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":25}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":37}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":50}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":62}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":75}
{""time"":1395878727,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":87}
{""time"":1395878758,""stage"":""Updating job"",""tags"":[""clock_global""],""total"":1,""task"":""clock_global/0 (canary)"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1395878758,""error"":{""code"":450001,""message"":""#<Bosh::Blobstore::BlobstoreError: sha1 mismatch expected=43da1a6240499cd2e6cbb425998c6bf891aa4b95 actual=a3e5c198843ab40f97a70edea6926c6c294a93fb>: [\""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/util.rb:31:in `rescue in unpack_blob'\"", \""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/util.rb:27:in `unpack_blob'\"", \""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/helpers.rb:26:in `fetch_bits'\"", \""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/helpers.rb:31:in `fetch_bits_and_symlink'\""
```

Rendered output becomes the following:

```
Done                                        1/1 00:00:38

  Started updating job clock_global
  Started updating job clock_global: clock_global/0 (canary)
     Done updating job clock_global: clock_global/0 (canary) (00:01:19)
     Done updating job clock_global (00:01:19)


Error 450001: #<Bosh::Blobstore::BlobstoreError: sha1 mismatch expected=43da1a6240499cd2e6cbb425998c6bf891aa4b95 actual=8aa2fb0678ca8dc5932cd992d54039c05abce385>: [""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/util.rb:31:in `rescue in unpack_blob'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/util.rb:27:in `unpack_blob'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/helpers.rb:26:in `fetch_bits'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/helpers.rb:31:in `fetch_bits_and_symlink'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/package.rb:33:in `install_for_job'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/plan.rb:77:in `block (2 levels) in install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/plan.rb:76:in `each'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/plan.rb:76:in `block in install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/plan.rb:75:in `each'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/apply_plan/plan.rb:75:in `install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.local/lib/bosh_agent/message/apply.rb:120:i
```",,68332682,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",director should properly associate error to a task that failed in the event log,,[],956238,81882,bug,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68332682
,2014-07-03T17:03:48Z,unscheduled,,"Currently it shows ' Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent > client/ccea3aab18e497796b2f213c58ac4bb68353a156. Done (00:00:00)'

but should show ' Started deleting compiled packages > client/ccea3aab18e497796b2f213c58ac4bb68353a156. Done (00:00:00)' (no need to repeat stemcell name/ver)

Full current output:
```
○ → bosh delete stemcell bosh-vsphere-esxi-ubuntu-trusty-go_agent 2624
Checking if stemcell exists...
You are going to delete stemcell `bosh-vsphere-esxi-ubuntu-trusty-go_agent/2624'
Are you sure? (type 'yes' to continue): yes

Director task 40
  Started deleting stemcell from cloud > Delete stemcell. Done (00:00:04)

  Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent
  Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent > client/ccea3aab18e497796b2f213c58ac4bb68353a156. Done (00:00:00)
  Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent > golang/aa5f90f06ada376085414bfc0c56c8cd67abba9c. Done (00:00:00)
  Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent > common/e7481927a203a7ec6c424cdba0b2248c958433c9. Done (00:00:00)
  Started deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent > server/83f8e6322d06d9c01493ede0faf211e92a532311. Done (00:00:00)
     Done deleting compiled packages 2624, bosh-vsphere-esxi-ubuntu-trusty-go_agent (00:00:00)

  Started deleting stemcell metadata > Deleting stemcell metadata. Done (00:00:00)

Task 40 done

Started		2014-06-24 15:03:44 UTC
Finished	2014-06-24 15:03:48 UTC
Duration	00:00:04

Deleted stemcell `bosh-vsphere-esxi-ubuntu-trusty-go_agent/2624'
```",1.0,74400396,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh delete stemcell X should properly format deleting compiled packages line,,[],956238,1123776,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/74400396
,2014-06-20T00:17:39Z,unscheduled,,"Currently, drain script output is being saved in memory and passed around. This can crash go-agent if the output is sufficiently large...

Drain script should use the same solution as the Packaging script in #72740894",1.0,73594030,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}, {'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Large drain STDOUT/STDERR should not crash go-agent,,[],956238,1266616,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/73594030
,2014-05-23T20:29:45Z,unscheduled,,,,71968430,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",director should clean up temporary uploaded asset if task fails to complete (or kick off),,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/71968430
,2014-03-18T23:47:40Z,unscheduled,,"Persistent disks on errand jobs are not currently supported. At deploy time, fail the deploy with an error message that says ""Errand jobs with persistent disks are not supported.""",,67804072,story,"[{'name': 'errand2', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-16T21:13:48Z', 'id': 8466458, 'updated_at': '2014-07-31T05:46:59Z'}, {'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Deploying errand job with persistent disks gives helpful error.,,[],956238,756869,feature,2014-07-31T05:46:59Z,https://www.pivotaltracker.com/story/show/67804072
,2014-03-13T07:17:58Z,unscheduled,,More info at http://docs.cloudfoundry.org/bosh/create-manifest.html#jobs,,67454566,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Provide a better error message if user doesn't  specify correctly the 'templates' Hash at the jobs section.,,[],956238,1015645,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/67454566
,2014-07-03T22:26:32Z,unscheduled,,"This happened when we had targeted but not logged into the director on ketchup
```
ruby 1.9.3p547 clement in ~/workspace/cf-release
± ah+sc |develop ✓| → bosh -n upload release
Usage: upload release [<release_file>] [--rebase] [--skip-if-exists]
```

we were running 1.2624.0 version",,74421630,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh upload release` should not fail with a misleading error when not logged in,,[],956238,5637,bug,2014-10-22T19:10:48Z,https://www.pivotaltracker.com/story/show/74421630
,2014-06-17T19:01:16Z,unscheduled,,"Instance is being shut down in AWS console and BOSH does not allow to delete the reference to it. This blocks any other BOSH commands. So you have to delete VM in director database manually to resolve this issue. Users should be able to delete reference to VM in cli.

```
Problem 1 of 1: gocd_server/0 (i-cbcbe59b) is not responding.
  1. Ignore problem
  2. Reboot VM
  3. Recreate VM using last known apply spec
  4. Delete VM reference (DANGEROUS!)
Please choose a resolution [1 - 4]: 4

Below is the list of resolutions you've provided
Please make sure everything is fine and confirm your changes

  1. gocd_server/0 (i-cbcbe59b) is not responding
     Delete VM reference (DANGEROUS!)

Apply resolutions? (type 'yes' to continue): yes
Applying resolutions...

Director task 619
  Started applying problem resolutions > unresponsive_agent 139: Delete VM reference (DANGEROUS!). Failed: VM `139' has a cloud id, please use a different resolution. (00:00:10)

```",,73414624,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Bosh cck should allow to delete VM reference,,[],956238,553935,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/73414624
,2014-06-05T16:45:18Z,unscheduled,,"bosh help is more helpful than bosh --help.
--help should auto-generate the usage pattern

This has especially confused users, because it doesn't show non-flag parameters specified by command method arguments.

```
$ bosh version
BOSH 1.2579.0
$ bosh help create release
create release [<manifest_file>] [--force] [--final] [--with-tarball]
[--dry-run]
    Create release (assumes current directory to be a release repository)
    --force                       bypass git dirty state check
    --final                       create final release
    --with-tarball                create release tarball
    --dry-run                     stop before writing release manifest
$ bosh create release --help
Usage: bosh [options]
        --force                      bypass git dirty state check
        --final                      create final release
        --with-tarball               create release tarball
        --dry-run                    stop before writing release manifest
```",,72714220,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh cmd --help` should show usage pattern the way `bosh help cmd` does,,[],956238,1266616,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72714220
,2014-03-31T21:14:10Z,unscheduled,,"The output very kindly prints a list of all buckets:
```
○ → time bosh aws destroyyesNo EC2 instances found
No EBS volumes found
THIS IS A VERY DESTRUCTIVE OPERATION AND IT CANNOT BE UNDONE!

Database Instances:
	
Are you sure you want to delete all databases? (type 'yes' to continue): THIS IS A VERY DESTRUCTIVE OPERATION AND IT CANNOT BE UNDONE!

Buckets:
	riak-cs-release-blobstore
	toland-cf-app-com-bosh-artifacts
	toolbox-cf-app-com-bosh-artifacts
	townsend-cf-app-com-bosh-artifacts
	townsend-cf-app-com-bosh-blobstoreAre you sure you want to empty and delete all buckets? (type 'yes' to continue): yes
```

But not all `Elastic IPs` or `SSH Keypairs`, so I feel like I am flying blind, for these decisions.",,68588408,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",I would like to see 'bosh aws destroy' list all resources before it deletes them.,,[],956238,951283,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/68588408
,2014-06-10T17:03:38Z,unscheduled,,"It's possible to request vms while a vm is being deployed such that the vm does not have an agent_id assigned to it. When this happens the `bosh vms` call explodes with the following error. This happens occasionally on travis. 

To repro locally you would have to disconnect from the deploy task and call vms while it's still running. However, it's unlikely that the case would be triggered at the exact moment unless you were spamming vms calls.

```
E, [2014-06-10T02:46:18.306716 #3759] [task:8] ERROR -- : undefined method `credentials' for nil:NilClass
/home/travis/build/cloudfoundry/bosh/bosh-director/lib/bosh/director/agent_client.rb:24:in `with_defaults'
/home/travis/build/cloudfoundry/bosh/bosh-director/lib/bosh/director/jobs/vm_state.rb:47:in `process_vm'
/home/travis/build/cloudfoundry/bosh/bosh-director/lib/bosh/director/jobs/vm_state.rb:27:in `block (3 levels) in perform'
/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:79:in `call'
/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:79:in `block (2 levels) in create_thread'
/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:in `loop'
/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:in `block in create_thread'
D, [2014-06-10T02:46:18.307050 #3759] [task:8] DEBUG -- : Acquired connection: 12249960
D, [2014-06-10T02:46:18.307232 #3759] [task:8] DEBUG -- : (0.000092s) BEGIN
D, [2014-06-10T02:46:18.308714 #3759] [task:8] DEBUG -- : (0.000892s) UPDATE ""tasks"" SET ""state"" = 'error', ""timestamp"" = '2014-06-10 02:46:18.306887+0000', ""description"" = 'retrieve vm-stats', ""result"" = 'undefined method `credentials'' for nil:NilClass', ""output"" = '/tmp/d20140610-8858-xizts7/boshdir/tasks/8', ""user_id"" = NULL, ""checkpoint_time"" = '2014-06-10 02:46:18.207347+0000', ""type"" = 'vms' WHERE (""id"" = 8)
D, [2014-06-10T02:46:18.308997 #3759] [task:8] DEBUG -- : (0.000121s) COMMIT
D, [2014-06-10T02:46:18.309065 #3759] [task:8] DEBUG -- : Released connection: 12249960
I, [2014-06-10T02:46:18.309152 #3759] [0x5b87f4]  INFO -- : Task took 0.102921423 seconds to process.
Task 8 error
I, [2014-06-10T02:46:21.895039 #8858]  INFO -- : ******************** end Director task 8 ********************
I, [2014-06-10T02:46:21.995575 #8858]  INFO -- : Terminating bosh-monitor (e42e7f3a) with PID=31715
F
Failures:
  1) health_monitor: 2 resurrects stateful nodes if fix_stateful_nodes director option is set
     Failure/Error: resurrected_vm = director.wait_for_vm('foobar/0', 150)
     RuntimeError:
       ERROR: bosh -n -c /tmp/d20140610-8858-18arzco/bosh_config.yml vms --details failed with output:
       Deployment `simple'
       
       Director task 8
       Error 100: undefined method `credentials' for nil:NilClass
       
       Task 8 error
       Failed to fetch VMs information from director
     # ./spec/support/bosh_runner.rb:36:in `run_in_current_dir'
     # ./spec/support/bosh_runner.rb:11:in `block in run'
     # ./spec/support/bosh_runner.rb:11:in `chdir'
     # ./spec/support/bosh_runner.rb:11:in `run'
     # ./spec/support/director.rb:51:in `fetch_bosh_vms_vitals'
     # ./spec/support/director.rb:14:in `vms'
     # ./spec/support/director.rb:38:in `block in wait_for_vm'
     # ./spec/support/director.rb:37:in `loop'
     # ./spec/support/director.rb:37:in `wait_for_vm'
     # ./spec/integration/hm_2_spec.rb:33:in `block (2 levels) in <top (required)>'
Finished in 21 minutes 59 seconds
12 examples, 1 failure
```",,72984276,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh vms should error in a more descriptive way when the agent_id is not set for a vm,,[],956238,1266616,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72984276
,2014-06-10T22:05:12Z,unscheduled,,"As a deployment operator I would like to cancel a deployment immediately when job update fails.

A PHD developer requested the feature because they had an issue while deploying which would result in a 20 minute timeout occurring. They tried to cancel it but it was waiting for the checkpoint and was not canceling for 20 minutes.

Ideas:
* Provide an option to release current lock
* Cancel forcefully which will interrupt the deployment task.
* Improve the user experience by adding more checkpoints for canceling during job update.",,73010540,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Bosh should allow to cancel deployment immediately,,[],956238,553935,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/73010540
,2014-06-11T19:30:05Z,unscheduled,,both for local and remote stemcells. This would necessitate some metadata,,73081570,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh upload stemcell` should check before uploading to see if the stemcell has already been uploaded,,[],956238,1123776,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/73081570
,2014-05-06T15:24:32Z,unscheduled,,"from Github:  https://github.com/cloudfoundry/bosh/issues/315#issuecomment-42243741

",,70769956,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","investigate Merge ""Creating bound missing VMs"" and ""Binding instance VMs"" stages during deployment",,[],956238,1210852,feature,2014-07-31T04:37:02Z,https://www.pivotaltracker.com/story/show/70769956
,2013-10-23T18:23:31Z,unscheduled,,"The CF deploy on tabasco was left in an undeployable state from an automated deploy prior to:
* https://deploy-jenkins.cf-app.com/view/Runtime/job/tabasco-runtime-cf-deploy/1323/

The result was that when the deploy above (1323) ran, the dea drain script failed because /var/vcap/packages/ruby/bin/ruby was no longer in the system.

The error from bosh was unhelpful in that it was complaining about the drain script itself not being present:
""Errno::ENOENT: No such file or directory - /var/vcap/jobs/dea_next/bin/drain""
",,59431560,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",cf-release deploy fails with unhelpful error,,[],956238,1054467,bug,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/59431560
,2014-03-14T22:38:18Z,unscheduled,,"This tends to take a long time on AWS, and on a real deployment when investigating an issue you'll probably be SSHing to machines left and right. My muscle memory is now Ctrl+D + hold Ctrl+C until I'm freed back to my prompt. Pretty annoying.

Related: when bosh SSHing and something fails (i.e. I don't have the key on my keychain), the feedback for the user is '<10 seconds to make user><10 seconds to clean up user><error message>'. If this was moved out that experience would be much less painful.

The director probably shouldn't be relying on the commandline for garbage collecting these users. There already exists a 'bosh cleanup ssh' command; if the user never had to see this and instead the director periodically did that automatically, this pain would go away. (Though I'm not sure if it has a distinction between active and inactive (garbage) users.)",1.0,67590322,story,"[{'name': 'ssh', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T15:14:05Z', 'id': 8094772, 'updated_at': '2014-04-01T15:14:05Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",'bosh ssh' should not force an operator to wait for the artifacts to be cleaned up,,[],956238,381857,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/67590322
,2013-12-03T01:26:04Z,unscheduled,,"When user adds new file to blobs and try to upload blobs, without the necessary S3 credentials, one should get meaningful error message like ""needs credentials to upload blobs""

Current behavior prints out a stack trace and returns
lib/blobstore_client/simple_blobstore_client.rb:43:in `get_file': Could not fetch object, 403/ (Bosh::Blobstore::BlobstoreError)",,61782502,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",User should see error message when 'bosh upload blobs' fails,,[],956238,1062777,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/61782502
,2014-05-14T00:26:59Z,unscheduled,,"During the CF v171 deploy, we observed the following warnings:

```
Detecting changes in deployment...

Releases
cf
  changed version:
    - 170
    + 171
Release version has changed: 170 -> 171
Are you sure you want to deploy this version? (type 'yes' to continue): yes

Compilation
No changes

Update
No changes

Resource pools
small_errand
  cloud_properties
    added availability_zone: us-east-1d
    added instance_type: m1.small
  added name: small_errand
  added network: cf1
  added size: 2
  stemcell
    added name: bosh-aws-xen-ubuntu
    added version: 2366
Stemcell update has been detected. Are you sure you want to update stemcells? (type 'yes' to continue): yes
Stemcell update seems to be inconsistent with current deployment. Please carefully review changes above.
Are you sure this configuration is correct? (type 'yes' to continue): yes
```

We were only adding a new resource pool, but got a couple of pretty strong warnings (the second one was even in red) about ""updating"" a stemcell, which of course we didn't want to do accidentally. 

Consulting with the bosh team we determined that it was okay to proceed, as the warnings were only referring to adding a stemcell along with the new resource pool.

Bosh's warnings around updating stemcells shouldn't trigger when adding a resource pool.",,71284058,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","bosh warns about ""updating stemcell"" when adding new resource pool",,[],956238,1196628,bug,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/71284058
,2014-01-10T19:56:35Z,unscheduled,,and what package they're compiling.,,63638622,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",`bosh vms` should show compile vms as compile vms ,,[],956238,46031,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/63638622
,2013-06-28T02:30:32Z,unscheduled,,,1.0,52514619,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","improve the health  monitor logs, it should say something about whether it sent data to datadog",,[],956238,5637,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/52514619
,2014-04-11T15:54:41Z,unscheduled,,"Specified a security group name in my deployment manifest that I hadn't created yet but the error returned was

Error 100: When specifying a security group you must specify a group id for each item

Which isn't particularly helpful.",,69351546,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Specifying a nonexistant security group in a deployment manifest should return a better error,,[],956238,1406536,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/69351546
,2014-05-29T17:07:21Z,unscheduled,,"Currently the message seen by the user is ""release is invalid, please fix and upload again"" which isn't necessarily true.",,72276592,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Better error message from cli when tarball extraction fails client side when uploading release,,[],956238,1017737,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/72276592
,2014-04-10T17:52:10Z,unscheduled,,"During apply, we rescue a system call failed error and print its message, but not its backtrace. This makes it hard to discover *what* system call failed.",,69285360,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Return better error when ephemeral disk fills up during deploy,,[],956238,1123776,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/69285360
,2014-01-18T01:57:45Z,unscheduled,,"During a bosh lite deploy, the agent exploded on a checksum mismatch.  Doing The Same Thing And Expecting Different Results, I hit deploy again and it worked. Bosh should have retried itself.

rumor has it the s3 blobstore client code already does this.

https://gist.github.com/mkocher/afb4ee5df049c9f7018e",,64091126,story,"[{'name': 'automate', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T18:44:00Z', 'id': 8097282, 'updated_at': '2014-04-01T18:44:00Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh agent should retry downloads in case of checksum mismatch for all blobstores,,[],956238,46031,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/64091126
,2013-10-30T21:16:30Z,unscheduled,,"vms were deleted manually from aws console. then i tried to recreate them:

```
±  |develop ✓| → bo cck
WARNING: loading local plugin: lib/bosh/cli/commands/aws.rb
Performing cloud check...

Director task 195

Scanning 19 VMs
  checking VM states (00:00:22)
  1 OK, 0 unresponsive, 18 missing, 0 unbound, 0 out of sync (00:00:00)
Done                    2/2 00:00:22

Scanning 1 persistent disks
  looking for inactive disks (00:00:00)
  1 OK, 0 inactive, 0 mount-info mismatch (00:00:00)
Done                    2/2 00:00:00

Task 195 done
Started		2013-10-30 21:13:41 UTC
Finished	2013-10-30 21:14:03 UTC
Duration	00:00:22

Scan is complete, checking if any problems found...

Found 18 problems

Problem 1 of 18: VM with cloud ID `i-5f0f3627' missing.
  1. Ignore problem
  2. Recreate VM using last known apply spec
  3. Delete VM reference (DANGEROUS!)
Please choose a resolution [1 - 3]: 2
...

Problem 18 of 18: VM with cloud ID `i-1d427879' missing.
  1. Ignore problem
  2. Recreate VM using last known apply spec
  3. Delete VM reference (DANGEROUS!)
Please choose a resolution [1 - 3]:
Please enter a number between 1 and 3
Please choose a resolution [1 - 3]: 2

Below is the list of resolutions you've provided
Please make sure everything is fine and confirm your changes

  1. VM with cloud ID `i-5f0f3627' missing.
     Recreate VM using last known apply spec
...
  18. VM with cloud ID `i-1d427879' missing.
      Recreate VM using last known apply spec

Apply resolutions? (type 'yes' to continue): yes
Applying resolutions...

Director task 196

Applying problem resolutions
  missing_vm 18: Recreate VM using last known apply spec: The instance ID 'i-5f0f3627' does not exist (00:00:11)
...
  missing_vm 27: Recreate VM using last known apply spec: The instance ID 'i-1d427879' does not exist (00:00:00)
Done                    18/18 00:00:16

Task 196 done
Started		2013-10-30 21:14:33 UTC
Finished	2013-10-30 21:14:49 UTC
Duration	00:00:16
Cloudcheck is finished
```",,59869122,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",bosh cck should allow to recreate vm even if vm is missing,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/59869122
,2014-01-10T23:54:10Z,unscheduled,,"currently bosh_cli does not show started/finished times when task errored out. it is very useful to see that information regardless of success or failure. (e.g. did we fail to do something when aws was having problems)

see task_tracker.rb",,63653228,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should be able to see how long it took for a task to error out,,[],956238,81882,feature,2014-07-31T03:16:47Z,https://www.pivotaltracker.com/story/show/63653228
,2014-02-21T22:29:39Z,unscheduled,,"currently the only way to see it is to use `bosh tasks recent --no-filter` e.g.

```
±  |master ✗| → bosh tasks recent --no-filter

+----+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
| #  | State | Timestamp               | User | Description       | Result                                                      |
+----+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
| 10 | done  | 2014-02-21 22:25:47 UTC | hm   | scan and fix      | scan and fix complete                                       |
| 9  | done  | 2014-02-21 22:19:33 UTC | hm   | scan and fix      | scan and fix complete                                       |
| 8  | done  | 2014-02-21 22:09:48 UTC | hm2  | retrieve vm-stats |                                                             |
| 7  | done  | 2014-02-21 20:05:14 UTC | hm2  | retrieve vm-stats |                                                             |
| 6  | done  | 2014-02-21 19:29:23 UTC | hm   | scan and fix      | scan and fix complete                                       |
| 5  | done  | 2014-02-21 19:14:23 UTC | hm   | create deployment | /deployments/dummy                                          |
| 4  | error | 2014-02-21 19:05:43 UTC | hm   | create deployment | Cannot update settings for 'i-67b3b947', got HTTP 500       |
| 3  | error | 2014-02-21 19:01:23 UTC | hm   | create deployment | Could not reserve network for package compilation: capacity |
| 2  | done  | 2014-02-21 18:56:15 UTC | hm   | create stemcell   | /stemcells/bosh-aws-xen-centos/0000                         |
| 1  | done  | 2014-02-21 18:55:33 UTC | hm   | create release    | Created release `dummy/0.3-dev'                             |
+----+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
```

vs

```
±  |master ✗| → bosh tasks recent

+---+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
| # | State | Timestamp               | User | Description       | Result                                                      |
+---+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
| 5 | done  | 2014-02-21 19:14:23 UTC | hm   | create deployment | /deployments/dummy                                          |
| 4 | error | 2014-02-21 19:05:43 UTC | hm   | create deployment | Cannot update settings for 'i-67b3b947', got HTTP 500       |
| 3 | error | 2014-02-21 19:01:23 UTC | hm   | create deployment | Could not reserve network for package compilation: capacity |
| 2 | done  | 2014-02-21 18:56:15 UTC | hm   | create stemcell   | /stemcells/bosh-aws-xen-centos/0000                         |
| 1 | done  | 2014-02-21 18:55:33 UTC | hm   | create release    | Created release `dummy/0.3-dev'                             |
+---+-------+-------------------------+------+-------------------+-------------------------------------------------------------+
```",1.0,66260378,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh should show 'scan and fix' tasks by default in bosh tasks recent,,[],956238,81882,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/66260378
,2014-02-14T18:57:08Z,unscheduled,,related to #62600628. should we delete a pid file from the cron task to signify job failure? ,,65818946,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should be alerted if route53_backup fails,,[],956238,81882,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/65818946
,2014-03-25T17:09:41Z,unscheduled,,"```
  Started creating new jobs > director/16. Done (00:00:00)
  Started creating new jobs > redis/3. Done (00:00:00)
  Started creating new jobs > registry/2. Done (00:00:00)
  Started creating new jobs > postgres/6. Done (00:00:01)
  Started creating new jobs > health_monitor/6. Done (00:00:00)
     Done creating new jobs (00:00:01)

  Started release has been created > bosh/47. Done (00:00:00)

Task 6 done

Started		2014-03-25 17:07:32 UTC
Finished	2014-03-25 17:07:49 UTC
Duration	00:00:17
```",1.0,68207758,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh cli should say 'Started creating release' instead of 'Started release has been created',,[],956238,81882,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/68207758
,2013-11-01T21:57:12Z,unscheduled,,"Given a release
And a bosh deployment manifest
When I run 'bosh deployment' with the manifest
And I run 'bosh validate jobs'
Then I should have files on my local disk that contain the job templates with their values filed in from the manifest


Currently, the only way to validate the result of the ERB in a job template is to do a deployment, bosh ssh into a VM that has the job template, and then read the file on that disk. It would be much faster for us to have a file we could inspect on our local disk without having to do an actual deploy. 

This would also allow us to write tests against the bosh release that do not require us to do a full deploy.",,60013218,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'validate jobs', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-16T18:15:59Z', 'id': 8226276, 'updated_at': '2014-04-16T18:15:59Z'}]",release developer should be able to inspect compiled job templates after running 'bosh validate jobs',,[],956238,285989,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/60013218
,2013-12-08T00:02:20Z,unscheduled,,"a problem encountered on the mailing lists that took a lot of troubleshooting:
https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-users/5LMUDN5C3jw/jVBuKYn_UxMJ",,62105170,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","BOSH should warn users in logs when NATS password contains an ""@"" or other unparsable yml symbol",,[],956238,494053,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/62105170
,2013-10-31T23:30:44Z,unscheduled,,"for:
- any error that is not handled
- argument errors that are caught right now by command_handler

there is not good way to debug what's going on without a stacktrace. there is no downside of showing the stacktrace to the user since cli is running on their machine and will not reveal any sensitive details. 

we can pretty format backtrace.

at the least exception class and message should be shown.",,59952026,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh cli should show stacktrace to the user when any error occurs,,[],956238,756869,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/59952026
,2013-12-17T01:33:35Z,unscheduled,,"Currently, our DEA drain script has 3 code paths:

1. Evacuation; prints -N until all instances gone and then shut down DEA and Warden
2. Snapshots; prints -N until all staging tasks gone and then shut down DEA and Warden

We determine whether to evacuate based on job_shutdown being passed. However, when the agent invokes the script again, it invokes it with job_check_status, so our script cannot determine whether to continue down path 1 or 2.",,62604304,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","when a drain script is invoked with job_shutdown and tells the agent to try again in N seconds, it should indicate that the job is still shutting down when retrying",,[],956238,381857,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/62604304
,2013-12-06T01:11:08Z,unscheduled,,currently instance_updater does not do anything with apply state result. it will fail later after trying to wait for agent to start...,,62025430,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",director should fail immediately if apply message resulted in a failed result,,[],956238,81882,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/62025430
,2014-03-31T23:43:01Z,unscheduled,,"When updating 2 jobs in parallel, one of them generated an error, and the other update succeeded.

The cli displayed the error, but did not indicate which update failed.",,68597484,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'et', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-18T17:12:53Z', 'id': 7982484, 'updated_at': '2014-03-18T17:12:53Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh_cli does not indicate which instance update caused an error,,[],956238,1165386,bug,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/68597484
,2014-02-28T18:50:47Z,unscheduled,,"Saw this in a CI run (http://bosh-jenkins.cf-app.com/:8080/job/bat_micro_vsphere_ubuntu/867/console) - create stemcell task failed with 

```
E, [2014-02-28T18:01:40.513593 #2483] [0x82887c] ERROR -- : Redis::TimeoutError - Connection timed out:
/var/vcap/packages/director/gem_home/gems/redis-3.0.3/lib/redis/client.rb:208:in `rescue in io'
...
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2093.0/lib/bosh/director/job_queue.rb:11:in `enqueue'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2093.0/lib/bosh/director/api/stemcell_manager.rb:40:in `create_stemcell'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.2093.0/lib/bosh/director/api/controllers/stemcells_controller.rb:7:in `block in <class:StemcellsController>'
```

but `bosh tasks` returns

```
± th+yt |develop ✓| → bosh tasks

+---+--------+-------------------------+-------+-----------------+--------+
| # | State  | Timestamp               | User  | Description     | Result |
+---+--------+-------------------------+-------+-----------------+--------+
| 1 | queued | 2014-02-28 18:00:20 UTC | admin | create stemcell |        |
+---+--------+-------------------------+-------+-----------------+--------+

Total tasks running now: 1
```

These 'queued' tasks also will continue to be considered to be 'running', so they will skew the 'Total tasks running now' numbers",,66686584,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","Tasks that error on enqueueing should display status of 'error', rather than 'queued'",,[],956238,1068489,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/66686584
,2014-03-11T23:50:47Z,unscheduled,,"There is a common issue with the agent task failing and if VM is down, HM recreates it and we loosing all agent logs. To better troubleshoot the problem we should save all agent logs for some period.",,67357864,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Aggregate all agent logs ,,[],956238,553935,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/67357864
,2014-05-21T16:41:27Z,unscheduled,,"Some jobs don't need a monit file (e.g. testing packaging, errands), so we shouldn't require one. Current workaround is an empty one.",0.0,71773540,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Don't require a monit file,,[],956238,1123776,feature,2014-08-05T06:43:56Z,https://www.pivotaltracker.com/story/show/71773540
,2014-05-16T17:45:38Z,unscheduled,,If the credentials for the blobstore specified in final.yml are not found an unhelpful 'bosh usage' error message is returned,,71492882,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh create release` should have a helpful error message when blobstore credentials are missing,,[],956238,514635,feature,2014-07-31T04:37:02Z,https://www.pivotaltracker.com/story/show/71492882
,2014-03-19T19:17:58Z,unscheduled,,"bosh ssh currently validates that a job is defined in the manifest before performing the ssh. This story is about breaking that dependency on the manifest and allowing a user to bosh ssh onto a vm regardless of whether that vm is defined in the manifest or not so that users can do things like get onto 'rogue' vms and examine them.

Background:
This was a new bosh jobs. The deployment failed.  We were trying to get into the box to see what failed.

±  |master ✓| → bosh vms
Deployment `cf-warden'

Director task 139

Task 139 done

+------------------------------------+---------+---------------+--------------+
| Job/index                          | State   | Resource Pool | IPs          |
+------------------------------------+---------+---------------+--------------+
| api_z1/0                           | running | large_z1      | 10.244.0.138 |
| clock_global/0                     | failing | medium_z1     | 10.244.0.146 |
| etcd_leader_z1/0                   | running | medium_z1     | 10.244.0.38  |
| ha_proxy_z1/0                      | running | router_z1     | 10.244.0.34  |
| hm9000_z1/0                        | running | medium_z1     | 10.244.0.142 |
| loggregator_trafficcontroller_z1/0 | running | small_z1      | 10.244.0.10  |
| loggregator_z1/0                   | running | medium_z1     | 10.244.0.14  |
| login_z1/0                         | running | medium_z1     | 10.244.0.134 |
| nats_z1/0                          | running | medium_z1     | 10.244.0.6   |
| postgres_z1/0                      | running | medium_z1     | 10.244.0.30  |
| router_z1/0                        | running | router_z1     | 10.244.0.22  |
| runner_z1/0                        | running | runner_z1     | 10.244.0.26  |
| uaa_z1/0                           | running | medium_z1     | 10.244.0.130 |
+------------------------------------+---------+---------------+--------------+

VMs total: 13

ruby 1.9.3p484 brookdale in ~/workspace/bosh-lite
±  |master ✓| → bosh ssh clock_global 0
Job `clock_global' doesn't exist

We fixed the issue by re-downloading the manifest and pointing our deployment at it.  ",,67868618,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",SSH into BOSH VM even if job is not defined in manifest,,[],956238,1097582,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/67868618
,2014-02-12T22:59:58Z,unscheduled,,"After compiling list of in-scope commands in #66362140, this story will have a task for each command that needs work.",2.0,65681144,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",VM naming / addressing should be consistent in commands and error messages,,[],956238,1054467,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/65681144
,2014-01-29T19:15:38Z,unscheduled,,"When trying to deploy with a manifest whose director UUID doesn't match what we are targeting, we just get `Bosh::Cli::AuthError`. It would have helped to have some idea of what to look for, like UUID, `bosh login`, etc.",,64766026,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Better error message for mismatched director UUID,1210852.0,[1210852],956238,1123776,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/64766026
,2014-03-11T18:11:35Z,unscheduled,,"If accessing an S3 blobstore fails for some reason, it would be useful to have the User and Account reported.   These could be stored in prod/config/private.yml along with the other S3 blobstore key info.",,67329696,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",BOSH error message should tell you the user & account in question,,[],956238,637633,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/67329696
,2014-03-19T18:28:15Z,unscheduled,,"today bosh deploy only shows diff (below) when it is running in interactive mode:

```
$ bosh deploy
Getting deployment properties from director...
Compiling deployment manifest...
Detecting changes in deployment...

Release
No changes

Releases
No changes

Compilation
No changes

Update
No changes

Resource pools
No changes

Networks
default
  subnets
    172.16.79.0/24
      changed static:
        - 172.16.79.11
        + 172.16.79.12

Jobs
dummy
  changed networks:
    - {""name""=>""default"", ""static_ips""=>[""172.16.79.11""]}
    + {""name""=>""default"", ""static_ips""=>[""172.16.79.12""]}

Properties
No changes

Please review all changes carefully
Deploying `manifest.yml' to `config-net-bosh' (type 'yes' to continue): yes
```",,67863666,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should see diff of manifest changes even if bosh deploy is run in non-interactive mode,,[],956238,81882,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/67863666
,2014-03-05T18:45:28Z,unscheduled,,"Caleb has context on the issue.

The director debug log says this:

 D, [2014-03-04T17:08:25.956289 #9846] [task:12035] DEBUG -- : (0.000705s) UPDATE `tasks` SET `stat     e` = 'error', `timestamp` = '2014-03-04 17:08:25', `description` = 'create deployment', `result` =      'Failed to fetch object, underlying error: #<Timeout::Error: execution expired>...', `output` = '     /var/vcap/store/director/tasks/12035', `user_id` = 1, `checkpoint_time` = '2014-03-04 17:08:17', `     type` = 'update_deployment' WHERE (`id` = 12035) LIMIT 1

but doesn't tell us the object or bucket name, neither does the stack trace. Full debug log attached in a gdoc.

",,66966186,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",BOSH error messages from Blobstore client should tell the Op the bucket / object id it errored on,,[],956238,637633,feature,2014-07-31T03:21:40Z,https://www.pivotaltracker.com/story/show/66966186
,2014-02-06T18:46:16Z,unscheduled,,"We have a hunch that this is due to a compile VM not being provisioned in time.  Attempting to deploy again. 

```
Compiling packages
  gorouter/22 (00:01:21)
  hm9000/11 (00:01:24)
  uaa/44 (00:02:03)
  collector/21 (00:00:48)
  dea_next/39 (00:02:46)
  cloud_controller_ng/37 (00:05:15)
  pivotal_login/6: Timed out pinging to 3403511b-1502-4260-9512-ffd181bd9288 after 600 seconds (00:13:01)
Error                          7/7 00:13:01

Error 450002: Timed out pinging to 3403511b-1502-4260-9512-ffd181bd9288 after 600 seconds

Task 37384 error

For a more detailed error report, run: bosh task 37384 --debug
error: Command failed: ""set -o pipefail &&  bosh -t https://bosh.run.pivotal.io:25555 deploy"" (options: {})    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/command_runner/spawner.rb:21:in `wait'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/command_runner.rb:16:in `run!'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/bosh.rb:143:in `run_bosh'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/bosh.rb:57:in `deploy'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/final_deployment_strategy.rb:24:in `do_deploy'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/deployment_strategy.rb:18:in `deploy!'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/lib/cf_deployer/cf_deploy.rb:95:in `deploy'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/.gem/ruby/1.9.3/bundler/gems/cf-deployer-7ebef094afec/bin/cf_deploy:14:in `<top (required)>'    # 2014-02-06 18:42:02 +0000
  `- /home/mariash/workspace/ci/bin/cf_deploy:16:in `load'                                   # 2014-02-06 18:42:02 +0000
  `- /home/mariash/workspace/ci/bin/cf_deploy:16:in `<main>'                                 # 2014-02-06 18:42:02 +0000
```",,65305076,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",Timeout compiling package - compilation vm issue,,[],956238,14062,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/65305076
,2014-02-10T20:32:45Z,unscheduled,,,,65506060,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh init release should include .gitkeep files in directories that it creates if --git is specified,,[],956238,81882,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/65506060
,2014-02-11T01:17:54Z,unscheduled,,"...so that user is not waiting for compilation vms to spin up/down every time they bosh deploy. 

this would be especially useful while iterating on release making.


Deployments to Tabasco failed a couple of times with similar errors:

```
cloud_controller_ng/41.7-dev,...           |                            | 0/2 00:13:11  ETA: --:--:--
                                                                                                                        
  cloud_controller_ng/41.7-dev: Timed out pinging to 3a97fb16-f9f1-4877-9251-8ed0bdad5021 after 600 seconds (00:13:14)

                                                                                                                        
dea_next/42.3-dev                          |oooooooooooooo              | 1/2 00:13:14  ETA: --:--:--
                                                                                                                        
  dea_next/42.3-dev: Timed out pinging to 96eed456-d840-43a5-a4d0-78822c8a362d after 600 seconds (00:13:14)

                                                                                                                        
                                           |oooooooooooooooooooooooooooo| 2/2 00:13:14  ETA: --:--:--
                                                                                                                        
Error                                       2/2 00:13:14

Error 450002: Timed out pinging to 3a97fb16-f9f1-4877-9251-8ed0bdad5021 after 600 seconds

Task 11240 error

For a more detailed error report, run: bosh task 11240 --debug
```",,65525694,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",user should be able to specify that compilation vms should stick around,,[],956238,81882,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/65525694
,2014-03-17T23:46:02Z,unscheduled,,"as a bosh operator, i should be able to suspend health monitoring/resurrection for a deployment so that I can do things like perform maintenance on a VM or group of VMs and not have an attempted resurrection during that maintenance.

1 pair - 1 day to spike on LOE for this request.",,67709956,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",investigate suspension of HM across a deployment,,[],956238,1210852,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/67709956
,2014-02-13T18:30:40Z,unscheduled,,"let's be proactive at showing more usage details

Currently:
±  |master ✗| → bosh delete release
Usage: delete release <name> [<version>] [--force]

Proposed:
±  |master ✗| → bosh delete release
delete release <name> [<version>] [--force]
    Delete release (or a particular release version)
    --force         ignore errors during deletion
",1.0,65745846,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should see full help message for the command when they incorrectly used a command,,[],956238,81882,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/65745846
,2014-03-05T22:47:01Z,unscheduled,,"These commands are all about the *public* blobstore used when creating releases, as opposed to ""The Blobstore.""

Could we improve the help entries to make that more clear?

add blob <local_path> [<blob_dir>]
    Add a local file as BOSH blob
blobs
    Print current blobs status
    Downloads a stemcell from the public blobstore
sync blobs
    Sync blob with the blobstore
upload blobs
    Upload new and updated blobs to the blobstore",,66986902,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Improve help entries for public blobstore interface,,[],956238,1202076,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/66986902
,2014-03-06T19:10:56Z,unscheduled,,"We've created an IAM user with ""power user"" permissions:

```
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""NotAction"": ""iam:*"",
      ""Resource"": ""*""
    }
  ]
}
```

Our private.yml looks like this:

```
blobstore:
  s3:
    bucket_name: a-bucket-that-really-exists
    access_key_id: REDACTED
    secret_access_key: REDACTED
```

Then during the create release phase in cf_deployer (running `bosh -t  -u  -p  -n create release --final`) we receive this error:

> Blobstore error: Failed to create object, S3 response error: AWS::Errors::Base

This error does not provide any information to assist with troubleshooting.",,67051620,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Error message during blobstore upload is not helpful or actionable,,[],956238,465545,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/67051620
,2013-12-10T18:20:18Z,unscheduled,,"Currently the release creation `--with-tarball` option will do nothing if a file exists at `releases/bosh-<version>.tgz`, regardless of what that file looks like.  This logic should pay attention to the release manifest instead.",,62252966,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Creating final release tarball should not skip tarball creation if release version not in manifest,,[],956238,1068489,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/62252966
,2014-01-14T22:03:36Z,unscheduled,,"```
 |1.9.3-p484| mason in ~
○ → bosh download manifest /tmp/a1.yml # Oops, forgot the deployment name
```

...returns...

```
HTTP 404: <!DOCTYPE html>
<html>
<head>
  <style type=""text/css"">
  body { text-align:center;font-family:helvetica,arial;font-size:22px;
    color:#888;margin:20px}
  #c {margin:0 auto;width:500px;text-align:left}
  </style>
</head>
<body>
  <h2>Sinatra doesn&rsquo;t know this ditty.</h2>
  <img src='https://54.236.221.107/__sinatra__/404.png'>
  <div id=""c"">
    Try this:
    <pre># in controller.rb
class Bosh::Director::Api::Controller
  get '/deployments/tmp/a1.yml' do
    ""Hello World""
  end
end
</pre>
  </div>
</body>
</html>
```

```
 |1.9.3-p484| mason in ~
○ → bosh download manifest cf-a1 /tmp/a1.yml # Got it right this time!
Deployment manifest saved to `/tmp/a1.yml'

 |1.9.3-p484| mason in ~
○ → bosh -v
BOSH 1.1722.0
```",,63844840,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh download manifest` should have a more intuitive error when an invalid set of arguments is passed to it,,[],956238,5637,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/63844840
,2014-03-06T02:35:21Z,unscheduled,,"The following is necessary in order to backup various runtime databases for the elastic runtime:
```
→ bosh stop cloud_controller 
You are about to stop cloud_controller/0
Detecting changes in deployment...

Releases
No changes

Compilation
No changes

Update
No changes

Resource pools
No changes

Networks
No changes

Jobs
No changes

Properties
No changes

Cannot perform job management when other deployment changes are present. Please use `--force' to override.
```
Note that ""No changes"" is displayed under all categories, but we still get a message informing us that we must use`--force`. There is a one-line change in the deployment manifest, but this does not appear anywhere in this output, which is very confusing.

When there are changes, the user should always be told where/what they are.",,66998132,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","`bosh` commands that detect changes in deployment should indicate what has changed, even when changes are not captured under the existing categories",,[],956238,1196628,feature,2014-07-31T03:22:32Z,https://www.pivotaltracker.com/story/show/66998132
,2014-05-29T21:39:32Z,unscheduled,,"Currently the order of arguments is not specified in the help:

```
scp [--download] [--upload] [--public_key FILE] [--gateway_host HOST]
[--gateway_user USER] [--gateway_identity_file FILE]
    upload/download the source file to the given job. Note: for download /path/to/destination is a directory
    --download                                                Download file
    --upload                                                  Upload file
    --public_key FILE                                         Public key
    --gateway_host HOST                                       Gateway host
    --gateway_user USER                                       Gateway user
    --gateway_identity_file FILE                              Gateway identity file
```

Would be helpful to see where should be the source and destination.",,72303396,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Improve CLI help for `bosh scp`,,[],956238,553935,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/72303396
,2014-02-24T20:56:45Z,unscheduled,,"Running `BOSH recreate` without any argument should error out but does not.  Instead it attempts to recreate vms. 

The syntax should require a defined VM or set of Vms and should error our if the correct info is not supplied",,66368276,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Running `BOSH recreate` without any argument should error out,,[],956238,1210852,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66368276
,2014-02-24T20:55:21Z,unscheduled,,"Running `BOSH ssh` without any argument should error out but does not.  Instead it attempts to use a defined gateway host if one exists. 

The syntax should require a defined set of ssh credentials and should error our if one is not supplied. ",,66368168,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Running `BOSH ssh` without any argument should error out,,[],956238,1210852,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66368168
,2014-02-20T21:55:46Z,unscheduled,,"Having the blobs/ directory exposed misleads the user into thinking that they can e.g. copy files into it (just like src/) during development when updating a blob. What you're supposed to run is ""bosh add blob"" which does all the internal plumbing. If the user copies over the file in blobs/ it'll just be reverted during ""bosh create release"" (checksum mismatch).",,66187982,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","exposing the ""blobs"" directory is confusing",,[],956238,381857,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66187982
,2014-02-11T21:01:06Z,unscheduled,,"If I'm the bootstrap admin user, and create another user, it succeeds but then locks me out with no indication.

$ bosh target xxx
Target already set to `xxx'

$ bosh create user
Enter new username: ci
Enter new password: *
Verify new password: *
User `ci' has been created

$ bosh vms
Bosh::Cli::AuthError

$ bosh login
Your username: admin
Enter password: *****
Cannot log in as `admin', please try again
Enter password:
Exiting...

$ bosh login
Your username: ci
Enter password: *
Logged in as `ci'

$ bosh vms
Deployment `xxx'

...",,65589752,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","As a bosh admin, when creating a user I should be told to log in as them",,[],956238,381857,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/65589752
,2014-02-10T22:11:00Z,unscheduled,,"we have `bosh init release`, `bosh generate job ...`, and `bosh generate package ...`. Would be nice if all of them were generate.",,65514768,story,"[{'name': 'bosh syntax inconsistency', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-10T23:04:43Z', 'id': 7686248, 'updated_at': '2014-02-10T23:04:43Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",make generate bosh cli tasks consistent in terms of naming,,[],956238,81882,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/65514768
,2014-01-29T20:50:54Z,unscheduled,,See https://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-users/DVmXkDILqGQ,,64774796,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",BOSH should error gracefully if user specifies internal dns and hardcoded dns IPs for a network,1210852.0,[1210852],956238,14062,feature,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/64774796
,2014-02-26T21:07:03Z,unscheduled,,"We deployed a dummy release with the network section of manifest YAML file incorrectly indented and BOSH attempted to deploy the release and timed out.  The created VMs did not have have the correct networking setup.  We believe that this is due to not being able to find the network instead of failing early with incorrect network configuration.

This is the incorrect YAML:

```
 21 networks:
 22 - name: default
 23   subnets:
 24   - range: 172.16.79.0/24
 25       reserved:
 26       - 172.16.79.1 - 172.16.79.1
 27       - 172.16.79.3 - 172.16.79.200
 28       gateway: 172.16.79.2
 29     dns:
 30     - 10.80.0.44
 31     cloud_properties:
 32       name: VM Network
```",,66543458,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Trying to deploy a dummy release with incorrect network information results in deployment timeout vs. saying that network configuration was erroneous,,[],956238,756869,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66543458
,2014-02-26T21:11:31Z,unscheduled,,"On vsphere if your deployment manifest has a nil env: bosh the agent will get a traceback here and die before setting up networking:

https://github.com/cloudfoundry/bosh/blob/master/bosh_agent/lib/bosh_agent/platform/linux/password.rb#L9

Example resource pool section from manifest:

```
resource_pools:
- name: default
  network: default
  size: 1
  stemcell:
    name: bosh-vsphere-esxi-centos
    version: latest
  cloud_properties:
    ram: 1024
    disk: 512
    cpu: 1
  env:
    bosh:
```",,66543766,story,"[{'name': 'bosh manifest', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T21:41:22Z', 'id': 8099338, 'updated_at': '2014-04-01T21:41:22Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","in vsphere, nil bosh in env results in agent failure",,[],956238,756869,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66543766
,2014-02-21T19:53:15Z,unscheduled,,"Currently it looks like it always set uncommitted_changes: true because it generates *.yml files in .final_builds and releases and these files should not be part of .gitignore.

e.g. creating bat release #2

```
± th+yt |develop ✓| → bosh create release --final
WARNING: Could not load IOV methods. Check your GSSAPI C library for an update
WARNING: Could not load AEAD methods. Check your GSSAPI C library for an update
WARNING: Nokogiri was built against LibXML version 2.8.0, but has dynamically loaded 2.7.8
Syncing blobs...
Are you sure you want to generate final version? yes

Building FINAL release
-----------------------------------

Building packages
-----------------
Building batarang...
  Final version:   FOUND LOCAL
Uploading final version 1...
This package has already been uploaded

Building batlight...
  Final version:   FOUND LOCAL
Uploading final version 1...
This package has already been uploaded


Resolving dependencies
----------------------
Dependencies resolved, correct build order is:
- batarang
- batlight


Building jobs
-------------
Building batarang...
  Final version:   NOT FOUND
  Dev version:     FOUND LOCAL
Uploading final version 1.1-dev...
Uploaded, blobstore id b86504a2-ae38-4371-8993-8957bfd0c76e

Building batlight...
  Final version:   FOUND LOCAL
Uploading final version 1...
This package has already been uploaded


Building release
----------------

Generating manifest...
----------------------
Writing manifest...

Release summary
---------------
Packages
+----------+---------+-------+
| Name     | Version | Notes |
+----------+---------+-------+
| batarang | 1       |       |
| batlight | 1       |       |
+----------+---------+-------+

Jobs
+----------+---------+-------------+
| Name     | Version | Notes       |
+----------+---------+-------------+
| batarang | 2       | new version |
| batlight | 1       |             |
+----------+---------+-------------+

Jobs affected by changes in this release
+----------+---------+
| Name     | Version |
+----------+---------+
| batarang | 2       |
+----------+---------+

Release version: 2
Release manifest: /Users/pivotal/workspace/bosh/bat/spec/system/assets/bat-release/releases/bat-2.yml

 |1.9.3-p448| dolores in ~/workspace/bosh/bat/spec/system/assets/bat-release
± th+yt |develop ✗| → cat /Users/pivotal/workspace/bosh/bat/spec/system/assets/bat-release/releases/bat-2.yml
---
packages:
- name: batarang
  version: 1
  sha1: !binary |-
    M2MxZWZlODBhZWM2OTg0OGViNjQzNTA2NjhhNzVhNTFjYWFiMTc1NA==
  fingerprint: !binary |-
    MGVhZDVlMDA3OGQ2OGE1YTBiZDcyOTM1NmE1NDA1MGFjMDAzMzUyZg==
  dependencies: []
- name: batlight
  version: 1
  sha1: !binary |-
    YjdlZjE2NTYwMjUxOTM0MDA1NDIzYmRhZTQ5NjE4NWM3NmZhNDFhYw==
  fingerprint: !binary |-
    NGZmNWM1Y2IwZDFmNmQxMTlhYmNmYWFkZTg3M2Q2MzA3ZGNlMDM2NA==
  dependencies: []
jobs:
- name: batarang
  version: 2
  fingerprint: !binary |-
    ZjM3YjNlMzIzMjlkMGYwNmFkYWQ5YTA2ZDBmNjZmNWQ0MWYyN2RjNA==
  sha1: !binary |-
    ZDU3MjQ5N2VjNWVmYmIwNTg1ZjNhZmI1NzAxOTYxMWFlNTk5M2U4Yw==
- name: batlight
  version: 1
  fingerprint: !binary |-
    MmUyNjQzYjg3NzJkMDkxYmJjOTIyMDlmOWRhMmY1M2I2MDI2NDNhOQ==
  sha1: !binary |-
    NzFkZjMzZGMxZGVjNjU5ZTI0MzIzMzc2NzcwNGRmNmQwYjE4Y2JkOQ==
commit_hash: 367a07d8
uncommitted_changes: true
name: bat
version: 2

 |1.9.3-p448| dolores in ~/workspace/bosh/bat/spec/system/assets/bat-release
± th+yt |develop ✗| → git st
# On branch develop
# Your branch is ahead of 'origin/develop' by 3 commits.
#   (use ""git push"" to publish your local commits)
#
# Changes not staged for commit:
#   (use ""git add <file>..."" to update what will be committed)
#   (use ""git checkout -- <file>..."" to discard changes in working directory)
#
#	modified:   .final_builds/jobs/batarang/index.yml
#	modified:   releases/index.yml
#
# Untracked files:
#   (use ""git add <file>..."" to include in what will be committed)
#
#	releases/bat-2.yml
no changes added to commit (use ""git add"" and/or ""git commit -a"")

 |1.9.3-p448| dolores in ~/workspace/bosh/bat/spec/system/assets/bat-release
± th+yt |develop ✗| →
```",,66250838,story,"[{'name': 'bosh release', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T21:41:09Z', 'id': 8099334, 'updated_at': '2014-04-01T21:41:09Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh create release --final should not always generate a release with uncommitted_changes: true,,[],956238,81882,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/66250838
,2014-01-03T18:48:50Z,unscheduled,,"to reproduce try deploying vm with 1mb ram which seems to be not allowed on vcenter 5.1

printing out @logger.info(""methods - #{result.not_attempted} - #{result.recommendations}"") gave the following.

D, [2014-01-03T18:41:04.955129 #13799] [0x21fd1bc] DEBUG -- : Redis >> 0.19ms
D, [2014-01-03T18:41:04.955177 #13799] [0x21fd1bc] DEBUG -- : Redis >> GET lock:deployment:p-redis-b0c242add7d13677f422
D, [2014-01-03T18:41:04.955328 #13799] [0x21fd1bc] DEBUG -- : Redis >> 0.11ms
D, [2014-01-03T18:41:04.955408 #13799] [0x21fd1bc] DEBUG -- : Redis >> MULTI 
D, [2014-01-03T18:41:04.955444 #13799] [0x21fd1bc] DEBUG -- : Redis >> SET lock:deployment:p-redis-b0c242add7d13677f422 1388774475.9553685:d3ad8435-1a29-4649-a6be-2da127028070
D, [2014-01-03T18:41:04.955470 #13799] [0x21fd1bc] DEBUG -- : Redis >> EXEC 
D, [2014-01-03T18:41:04.955681 #13799] [0x21fd1bc] DEBUG -- : Redis >> 0.16ms


I, [2014-01-03T18:41:05.843952 #13799] [create_vm(82c71e5b-cfe6-4224-bca9-713e394e9248, ...)]  INFO -- : methods - [#<VimSdk::Vim::Cluster::NotAttemptedVmInfo:0x00000003f94ba8 @dynamic_type=nil, @dynamic_property=[], @vm=<[Vim.VirtualMachine] vm-642808>, @fault=#<VimSdk::Vim::Fault::NoCompatibleHost:0x00000003f92880 @dynamic_type=nil, @dynamic_property=[], @msg=""No host is compatible with the virtual machine."", @fault_cause=nil, @fault_message=[], @host=[<[Vim.HostSystem] host-11>], @error=[#<VimSdk::Vim::Fault::MemorySizeNotSupported:0x00000003f978f8 @dynamic_type=nil, @dynamic_property=[], @msg=""Virtual machine has 1 megabytes of memory, which is outside the range of 4 to 261,120 megabytes supported on the host. This may be a general limitation of the host software, or specific to the guest OS selected for the virtual machine."", @fault_cause=nil, @fault_message=[], @memory_size_mb=1, @min_memory_size_mb=4, @max_memory_size_mb=261120>]>>] - []



I, [2014-01-03T18:41:05.844113 #13799] [create_vm(82c71e5b-cfe6-4224-bca9-713e394e9248, ...)]  INFO -- : undefined method `task' for nil:NilClass - /var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.5.0.pre.1266/lib/cloud/vsphere/client.rb:162:in `power_on_vm'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.5.0.pre.1266/lib/cloud/vsphere/cloud.rb:257:in `block in create_vm'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh_vsphere_cpi-1.5.0.pre.1266/lib/cloud/vsphere/cloud.rb:163:in `create_vm'
/var/vcap/packages/ruby/lib/ruby/1.9.1/forwardable.rb:201:in `create_vm'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/vm_creator.rb:44:in `create'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/resource_pool_updater.rb:54:in `create_missing_vm'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/resource_pool_updater.rb:37:in `block (4 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/resource_pool_updater.rb:35:in `block (3 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/event_log.rb:58:in `track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.1266/lib/bosh/director/resource_pool_updater.rb:34:in `block (2 levels) in create_missing_vms'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_pool.rb:83:in `call'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_pool.rb:83:in `block (2 levels) in create_thread'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_pool.rb:67:in `loop'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.1266/lib/common/thread_pool.rb:67:in `block in create_thread'
I, [2014-01-03T18:41:05.844181 #13799] [delete_vm(vm-8c4e4a4d-bb75-4a63-b757-bff579dd30b8)]  INFO -- : Deleting vm: vm-8c4e4a4d-bb75-4a63-b757-bff579dd30b8
I, [2014-01-03T18:41:06.891964 #13799] [delete_vm(vm-8c4e4a4d-bb75-4a63-b757-bff579dd30b8)]  INFO -- : Deleted vm: vm-8c4e4a4d-bb75-4a63-b757-bff579dd30b8
E, [2014-01-03T18:41:07.912886 #13799] [create_missing_vm(redis, 0/1)] ERROR -- : error creating vm: undefined method `task' for nil:NilClass


code used at the time of error:

def power_on_vm(datacenter, vm)
      task = datacenter.power_on_vm([vm], nil)
      result = wait_for_task(task)
      if result.attempted.empty?
        raise ""Could not power on VM: #{result.not_attempted.msg}""
      else
        @logger.info(""methods - #{result.not_attempted} - #{result.recommendations}"") # <---- added
        task = result.attempted.first.task
        wait_for_task(task)
      end
    end",,63218560,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",user should see full description of the error message coming from vcenter when failed to create vm,,[],956238,81882,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/63218560
,2013-12-30T22:18:53Z,unscheduled,,"After applying resolution we are seeing this error:

unresponsive_agent 61: Delete VM reference (DANGEROUS!): VM `61' has a cloud id, please use a different resolution.",,63087750,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Bosh cloud check should not offer to delete reference if VM has cloud id,,[],956238,553935,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/63087750
,2013-12-17T00:29:31Z,unscheduled,,"# Attempts to allocate a dynamic IP address for all idle VMs
    # (unless they already have one). This allows us to fail earlier
    # in case any of resource pools is not big enough to accommodate
    # those VMs.
    def reserve_networks
      network = @resource_pool.network

      each_idle_vm do |idle_vm|
        unless idle_vm.network_reservation
          reservation = NetworkReservation.new(
              :type => NetworkReservation::DYNAMIC)
          network.reserve(reservation)

          unless reservation.reserved?
            case reservation.error
              when NetworkReservation::CAPACITY
                raise NetworkReservationNotEnoughCapacity,
                      ""`#{name}/#{index}' asked for a dynamic IP "" +
                      ""but there were no more available""
              else
                raise NetworkReservationError,
                      ""`#{name}/#{index}' failed to reserve "" +
                      ""dynamic IP: #{reservation.error}""
            end
          end

          idle_vm.network_reservation = reservation
        end
      end
    end






E, [2013-12-13T22:42:01.737910 #23771] [task:48] ERROR -- : undefined local variable or method `name' for #<Bosh::Director::ResourcePoolUpdater:0x00000002953308>
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/resource_pool_updater.rb:151:in `block in reserve_networks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/resource_pool_updater.rb:170:in `block in each_idle_vm'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/resource_pool_updater.rb:170:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/resource_pool_updater.rb:170:in `each_idle_vm'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/resource_pool_updater.rb:141:in `reserve_networks'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/deployment_plan/resource_pools.rb:43:in `block in refill'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/deployment_plan/resource_pools.rb:42:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/deployment_plan/resource_pools.rb:42:in `refill'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/deployment_plan/updater.rb:44:in `update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/jobs/update_deployment.rb:55:in `update'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/jobs/update_deployment.rb:81:in `block in perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/jobs/update_deployment.rb:74:in `perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/job_runner.rb:98:in `perform_job'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.5.0.pre.local/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.5.0.pre.local/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
...",,62602120,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh should not blow up with undefined var exception during a deploy when there is not enough dynamic ips,,[],956238,81882,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/62602120
,2013-10-17T19:17:21Z,unscheduled,,"Follow-on from #58474852

we can repro this issue in AWS on a fresh bosh micro deploy 100% of the time. this happens if you cancel in the window after the 'compile' phase starts, but before the vm is spun up.
",,59075156,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",director should always clean up compile vms when a deploy is cancelled,,[],956238,637633,bug,2014-07-31T03:23:07Z,https://www.pivotaltracker.com/story/show/59075156
,2013-12-14T02:03:50Z,unscheduled,,"if it runs out of resources it should say so. we saw some weird behaviour in InstanceUpdater when it tries to delete vm after it created it.

https://docs.google.com/a/pivotallabs.com/document/d/15IO3BdHVD1Sydpij4cx4JU4-oGybkM_agfM1OMnOcZI/edit",,62500104,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh deploy should not fail with vm not found when deploying from scratch,,[],956238,81882,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/62500104
,2013-12-13T18:35:49Z,unscheduled,,we had microbosh with 4gb of eph/pers disks. we uploaded bosh release and then tried to upload a stemcell. stacktrace we got back seems to include html.,,62478290,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh upload stemcell should result in a more user friendly error/stacktrace when MB runs out of space,,[],956238,81882,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/62478290
,2013-12-04T22:30:07Z,unscheduled,,"After deploying a new stemcell to a1, we ran a bosh cleanup. We noticed that rather than only keeping the latest two stemcells, it just deleted the two oldest.

",,61940228,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",bosh cleanup doesn't completely clean up vms,,[],956238,756869,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/61940228
,2013-11-19T01:52:37Z,unscheduled,,"We had a deploy on bosh-lite that failed while updating the api_z1 job. We ran `bosh recreate api_z1/0` and after it recreated the CC, it continued where the deploy left off, creating the HM, DEA, etc. jobs.",,61009266,story,"[{'name': 'automate', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T18:44:00Z', 'id': 8097282, 'updated_at': '2014-04-01T18:44:00Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh recreate $job seems to restart a botched deploy,,[],956238,465545,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/61009266
,2013-11-15T19:17:34Z,unscheduled,,"When a deployment fails, the deployment manifest is never written to the database. If you then ""bosh download manifest"" it successfully returns an empty string.

It should either error in this case or we might want to keep old deployment manifests around and add a flag indicating whether they are successful or not.
",,60868016,story,"[{'name': 'bosh manifest', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T21:41:22Z', 'id': 8099338, 'updated_at': '2014-04-01T21:41:22Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Deployment manifest on director can be empty string,,[],956238,756869,bug,2014-12-17T04:16:20Z,https://www.pivotaltracker.com/story/show/60868016
,2014-02-19T23:11:15Z,unscheduled,,"in select_active_host_mobs(), the code checks against the string value 'true' instead of the boolean value. This looks to be the cause of the errors. 

(Note: this code only executes if the user is deploy micro-bosh without a resource pools)",,66105696,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",hosts in maintenance mode still seen as active by micro deploy,,[],956238,756869,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/66105696
,2014-01-27T22:23:56Z,unscheduled,,"The OpenStack CPI never throws a Bosh::Clouds::VMNotFound error when #delete_cm from the OpenStack CPI fails in the case the IaaS has no record of the vm.

see lifecycle_spec in vsphere or aws. it uses:

```
extend Bosh::Cpi::CompatibilityHelpers
it_can_delete_non_existent_vm
```",,64617340,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",Resurrector fails to recreate vm on OpenStack if vm was deleted ,,[],956238,1406536,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/64617340
,2014-01-23T08:37:55Z,unscheduled,,"[JZ/JF] `httpclient` has a fix in their source control but haven't released the gem yet. We are waiting for upstream to release the gem containing the fix.

https://github.com/cloudfoundry/bosh/issues/504

As discussed on https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-dev/ygtyp9-CHEU/VfNik6H4MjwJ due to bug in http_client https://github.com/nahi/httpclient/pull/168 bosh_cli can't talk to the bosh director agent for deploying a micro bosh

Workaround is to manually unset https_proxy in initial bosh micro deploy steps.

Waiting for a new official version of http_client to upgrade gem dependencies

```
$ bosh micro deploy /home/gberche/bosh-stemcell-1782-vsphere-esxi-ubuntu.tgz
      unpacking stemcell (00:00:05)
      uploading stemcell (00:03:47)
      creating VM from urn:vcloud:catalogitem:1b847122-017a-4149-8b89-2a3232b8decf (00:03:36)
    Waiting for the agent               |oooooo                  | 3/11 00:12:48  ETA: 00:14:37Unable to connect to Bosh agent. Check logs for more details.



$ bosh micro agent ping 
    /opt/rh/ruby193/root/usr/local/share/gems/gems/agent_client-1.1761.0/lib/agent_client/http_client.rb:64:in `post_json': Agent HTTP 404 (Bosh::Agent::Error)
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/agent_client-1.1761.0/lib/agent_client/http_client.rb:23:in `handle_method'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/agent_client-1.1761.0/lib/agent_client/base.rb:19:in `method_missing'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/bosh_cli_plugin_micro-1.1761.0/lib/bosh/cli/commands/micro.rb:297:in `agent'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/bosh_cli-1.1761.0/lib/cli/command_handler.rb:57:in `run'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/bosh_cli-1.1761.0/lib/cli/runner.rb:56:in `run'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/bosh_cli-1.1761.0/lib/cli/runner.rb:16:in `run'
            from /opt/rh/ruby193/root/usr/local/share/gems/gems/bosh_cli-1.1761.0/bin/bosh:7:in `<top (required)>'
            from /opt/rh/ruby193/root/usr/local/bin/bosh:23:in `load'
            from /opt/rh/ruby193/root/usr/local/bin/bosh:23:in `<main>'
```

Filed by gberche-orange",,64371858,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",cloudfoundry/bosh #504: bosh_cli fails to contact bosh agent when https_proxy is set,786819.0,"[786819, 1338772]",956238,1134058,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/64371858
,2014-03-13T22:58:37Z,unscheduled,,"Compiling deployment manifest...
AWS spot instance support is an experimental feature.
Please log an issue at https://github.com/cloudfoundry/bosh/issues if you run into any issues related to spot instances.

Cannot get current deployment information from director, possibly a new deployment
Please review all changes carefully
Deploying `dummy.yml' to `vpc-bosh-bosh-pm' (type 'yes' to continue): yes

Director task 75
",,67518712,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh spot instance warning should be in director not CLI,,[],956238,1210852,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/67518712
,2014-03-12T19:04:37Z,unscheduled,,bosh cli shows a 'Done updating job' message on jobs,,67421198,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",UI strangeness when a stemcell update fails mid-deploy,,[],956238,637633,bug,2014-07-31T03:23:42Z,https://www.pivotaltracker.com/story/show/67421198
,2014-03-12T18:32:40Z,unscheduled,,We have to wait for a stemcell to be unpacked on the director side and possibly fail due to missing properties in the stemcell.MF. We should fail sooner.,,67417952,story,"[{'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",`bosh upload stemcell` should fail fast when uploading a stemcell to a director on a incompatible IaaS,,[],956238,1406536,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/67417952
,2014-02-05T22:13:03Z,unscheduled,,"○ → bosh vms --vitals
Deployment `cf-warden'

Director task 84

Task 84 done
/Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:87:in `block (2 levels) in show_deployment': undefined method `[]' for nil:NilClass (NoMethodError)
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:58:in `each'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:58:in `block in show_deployment'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/terminal-table-1.4.5/lib/terminal-table/table.rb:210:in `yield_or_eval'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/terminal-table-1.4.5/lib/terminal-table/table.rb:17:in `initialize'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/terminal-table-1.4.5/lib/terminal-table/table_helper.rb:5:in `new'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/terminal-table-1.4.5/lib/terminal-table/table_helper.rb:5:in `table'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:42:in `show_deployment'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:17:in `block in list'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:17:in `each'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/commands/vms.rb:17:in `list'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/command_handler.rb:57:in `run'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/runner.rb:56:in `run'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/lib/cli/runner.rb:16:in `run'
	from /Users/pivotal/.gem/ruby/1.9.3/gems/bosh_cli-1.1798.0/bin/bosh:7:in `<top (required)>'
	from /Users/pivotal/.gem/ruby/1.9.3/bin/bosh:23:in `load'
	from /Users/pivotal/.gem/ruby/1.9.3/bin/bosh:23:in `<main>'",,65238420,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",running bosh vms --vitals should not results in an exception,,[],956238,81882,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/65238420
,2014-02-19T21:56:18Z,unscheduled,,"○ → bosh ssh hm9000_z2
You should specify the job index. There is more than one instance of this job type.

 |1.9.3-p448| cf-doubletrouble-mbp in ~/workspace
○ → bosh vms
Deployment `cf-warden'

Director task 50

Task 50 done

+------------------------------------+----------+---------------+--------------+
| Job/index                          | State    | Resource Pool | IPs          |
+------------------------------------+----------+---------------+--------------+
| api_worker_z1/0                    | running  | small_z1      | 10.244.0.146 |
| api_z1/0                           | running  | large_z1      | 10.244.0.138 |
| clock_global/0                     | running  | medium_z1     | 10.244.0.142 |
| etcd_leader_z1/0                   | running  | medium_z1     | 10.244.0.38  |
| ha_proxy_z1/0                      | running  | router_z1     | 10.244.0.34  |
| hm9000_z1/0                        | running  | medium_z1     | 10.244.0.150 |
| loggregator_trafficcontroller_z1/0 | starting | small_z1      | 10.244.0.10  |
| loggregator_z1/0                   | running  | medium_z1     | 10.244.0.14  |
| login_z1/0                         | running  | medium_z1     | 10.244.0.134 |
| nats_z1/0                          | running  | medium_z1     | 10.244.0.6   |
| postgres_z1/0                      | running  | medium_z1     | 10.244.0.30  |
| router_z1/0                        | running  | router_z1     | 10.244.0.22  |
| runner_z1/0                        | running  | runner_z1     | 10.244.0.26  |
| uaa_z1/0                           | running  | medium_z1     | 10.244.0.130 |
+------------------------------------+----------+---------------+--------------+

VMs total: 14",,66099006,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","`bosh ssh job_with_zero_instances` should error out saying there are zero instances, not ""more than one instance of this job type.""",,[],956238,46031,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/66099006
,2013-12-02T09:40:46Z,unscheduled,,"https://github.com/cloudfoundry/bosh/issues/463

Amazon recently moved one of our S3 buckets from *US Standard* to *Oregon* without notifying us and just left a lot of 301 redirects in place of the files that were once there. This makes the blob syncing part of creating a release fail now with this error:

```
$ bosh create release --force
/var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/simple_blobstore_client.rb:43:in `get_file': Could not fetch object, 301/ (Bosh::Blobstore::BlobstoreError)
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb:88:in `get_file'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/base.rb:50:in `get'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/sha1_verifiable_blobstore_client.rb:19:in `get'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:19:in `block in get'
	from /var/vcap/store/gems/2.0.0p353/deploy/gems/bosh_common-1.5.0.pre.1244/lib/common/retryable.rb:21:in `block in retryer'
	from /var/vcap/store/gems/2.0.0p353/deploy/gems/bosh_common-1.5.0.pre.1244/lib/common/retryable.rb:19:in `loop'
	from /var/vcap/store/gems/2.0.0p353/deploy/gems/bosh_common-1.5.0.pre.1244/lib/common/retryable.rb:19:in `retryer'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/blobstore_client/lib/blobstore_client/retryable_blobstore_client.rb:18:in `get'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/blob_manager.rb:295:in `download_blob'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/blob_manager.rb:230:in `block in process_index'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/blob_manager.rb:211:in `each_pair'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/blob_manager.rb:211:in `process_index'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/blob_manager.rb:155:in `sync'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/commands/release.rb:350:in `dirty_blob_check'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/commands/release.rb:300:in `create_from_spec'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/commands/release.rb:48:in `create'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/command_handler.rb:57:in `run'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/runner.rb:59:in `run'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/lib/cli/runner.rb:18:in `run'
	from /var/vcap/store/gems/2.0.0p353/deploy/bundler/gems/bosh-1e760331719a/bosh_cli/bin/bosh:7:in `<top (required)>'
	from /var/vcap/store/gems/2.0.0p353/deploy/bin/bosh:23:in `load'
	from /var/vcap/store/gems/2.0.0p353/deploy/bin/bosh:23:in `<main>'
```

We're going to be moving the bucket back to the original region but it would be nice if the BOSH blobstore client understood redirect status codes and handled them with the correct semantics.

Filed by xoebus",,61711884,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",cloudfoundry/bosh #463: S3 Blobstore Client does not follow 301 MOVED PERMANENTLY responses,,[],956238,1134058,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/61711884
,2014-02-11T06:44:55Z,unscheduled,,"exception while doing bosh cck

I, [2014-02-11T06:24:13.808269 #6015] [task:17]  INFO -- : missing_vm 8: Recreate VM using last known apply spec
E, [2014-02-11T06:24:13.808754 #6015] [task:17] ERROR -- : Error resolving problem `1': undefined method `fetch' for ""package_compiler"":String
E, [2014-02-11T06:24:13.808825 #6015] [task:17] ERROR -- : /var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/cloudcheck_helper.rb:95:in `recreate_vm'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_handlers/missing_vm.rb:21:in `block (2 levels) in <class:MissingVM>'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_handlers/base.rb:83:in `instance_eval'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_handlers/base.rb:83:in `apply_resolution'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:82:in `block in apply_resolution'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:23:in `block in track_and_log'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/event_log.rb:75:in `call'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/event_log.rb:75:in `advance_and_track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/event_log.rb:36:in `track'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:21:in `track_and_log'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:81:in `apply_resolution'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:47:in `block in apply_resolutions'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:46:in `each'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/problem_resolver.rb:46:in `apply_resolutions'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:38:in `block in perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/lock_helper.rb:21:in `block in with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/lock.rb:58:in `lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/lock_helper.rb:21:in `with_deployment_lock'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/jobs/cloud_check/apply_resolutions.rb:37:in `perform'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/job_runner.rb:98:in `perform_job'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/job_runner.rb:29:in `block in run'
/var/vcap/packages/director/gem_home/gems/bosh_common-1.1868.0/lib/common/thread_formatter.rb:46:in `with_thread_name'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/job_runner.rb:29:in `run'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/lib/bosh/director/jobs/base_job.rb:10:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/job.rb:125:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:186:in `perform'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:149:in `block in work'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `loop'
/var/vcap/packages/director/gem_home/gems/resque-1.23.1/lib/resque/worker.rb:128:in `work'
/var/vcap/packages/director/gem_home/gems/bosh-director-1.1868.0/bin/bosh-director-worker:76:in `<top (required)>'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `load'
/var/vcap/packages/director/bin/bosh-director-worker:23:in `<main>'
",,65534654,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}, {'name': 'vms', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:44:51Z', 'id': 8096512, 'updated_at': '2014-04-01T17:44:51Z'}]",bosh cck should not raise an error when trying to recreate vms,,[],956238,381857,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/65534654
,2014-02-06T18:43:00Z,unscheduled,,"yes, this is a bit funny, but BOSH should probably handle this better.",,65304810,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'fail fast', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T14:57:21Z', 'id': 8094600, 'updated_at': '2014-04-01T14:57:21Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",setting max_in_flight to 0 with a job with 0 instances causes deploy to fail,,[],956238,786819,bug,2014-07-31T03:23:43Z,https://www.pivotaltracker.com/story/show/65304810
,2014-01-31T19:07:26Z,unscheduled,,"From Bill Keenan

Setting aside the non-reproducable failed install, which did/does not clean up the disk on delete, here is the disk state after deleting successful installs.

Successfully installed:
VMware vSphere
Elastic runtime
MySQL

Successfully deleted:
MySQL
Elastic runtime
VMware vSphere

Using vCenter Web Client, the image shows the directories left on the datastore. All of the vm-* directories contain an env.json and env.iso file. The two linked clone directories (sc-*), contain a large .vmdk (see the 2nd attached image).

BillK",,64925716,story,"[{'name': 'vsphere', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-21T01:09:33Z', 'id': 7518042, 'updated_at': '2014-01-21T01:09:33Z'}]",Deleting an install leaves orphan files in the datastore,,[],956238,548715,bug,2015-06-24T22:50:46Z,https://www.pivotaltracker.com/story/show/64925716
,2014-04-21T16:04:44Z,unscheduled,,"when using a dynamic network with the aws cpi plugin, the subnet is ignored.  The subnet is only respected if you set it with a manual network.

As a release developer, I should respect the user-specified subnet on a dynamic network because not everyone wants to use a manual network.  

New users just trying out something simple with bosh, configuring a manual network will feel very burdensome to many new users. if you specify the dynamic network now, then you are assumed to be using the ""default"" vpc, which you may not want to do.

manual network initialization here setting subnet info:
https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/manual_network.rb#L16

dynamic network here ignoring subnet info:
https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/dynamic_network.rb#L13-L14

then when the instance is created, you can see the vpc params only apply for the manual network and nothing happens in this code for dynamic:
https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/instance_manager.rb#L202-L208",,69850654,story,"[{'name': 'networking', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-21T16:04:53Z', 'id': 8250390, 'updated_at': '2014-04-21T16:04:53Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Subnet is ignored when using a dynamic network with the AWS CPI plugin,,[],956238,1210852,feature,2014-07-31T04:37:02Z,https://www.pivotaltracker.com/story/show/69850654
,2014-02-08T23:37:33Z,unscheduled,,"Often while doing a prod deploy we find DEAs have a process stuck in Disk Wait state, and have to do a `bosh recreate ...` of the VM in order to kill the process.

",,65419016,story,"[{'name': 'automate', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T18:44:00Z', 'id': 8097282, 'updated_at': '2014-04-01T18:44:00Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","as a bosh user, I should be able to tell bosh to recreate a VM if it is unable to stop a job during a deploy",,[],956238,46031,feature,2014-07-31T03:24:09Z,https://www.pivotaltracker.com/story/show/65419016
,2014-07-26T00:53:40Z,unscheduled,,"```
bo upload stemcell tmp/bosh-stemcell-0000-aws-xen-ubuntu-trusty-go_agent.tgz

Verifying stemcell...
File exists and readable                                     OK
Verifying tarball...
Read tarball                                                 OK
Manifest exists                                              OK
Stemcell image file                                          OK
Stemcell properties                                          OK

Stemcell info
-------------
Name:    bosh-aws-xen-ubuntu-trusty-go_agent
Version: 0000

Checking if stemcell already exists...
No

Uploading stemcell...

bosh-stemcell: 100% |oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo| 393.6MB   1.2MB/s Time: 00:05:22

Director task 1
System call error while talking to director: Operation timed out
```

But task 1 completed successfully. See attached debug log",,75753214,story,[],"Stemcell upload timed out, but actually completed successfully",,[],956238,1123776,feature,2014-07-26T00:53:40Z,https://www.pivotaltracker.com/story/show/75753214
,2014-07-25T21:54:22Z,unscheduled,,should there be a -v flag?,,75746976,story,[],user should be able to see more detailed information about steps taken by bosh deploy,,[],956238,81882,feature,2014-07-25T21:54:39Z,https://www.pivotaltracker.com/story/show/75746976
,2014-07-25T17:40:14Z,unscheduled,,"as a deployment operator deploying PHD, I should be able to deploy multiple data nodes across separate physical hosts so that the aggregate load for N job instances of data node is appropriately distributed

For example, if a vSphere cluster is made up of 4 physical machines and I am deploying an 8 node Hadoop cluster, I want 2 nodes on each host rather than 8 nodes on 1 host and 0 nodes on the other 3.

background found here:        https://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf

and here:        http://www.vmware.com/files/pdf/vmware-virtualizing-apache-hadoop.pdf",,75729022,story,"[{'name': 'phd', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-25T22:59:47Z', 'id': 8042040, 'updated_at': '2014-03-25T22:59:47Z'}]",Deploy multiple hadoop data nodes across separate physical hosts,,[],956238,1210852,feature,2014-07-25T18:18:45Z,https://www.pivotaltracker.com/story/show/75729022
,2014-07-25T17:39:07Z,unscheduled,,"as a deployment operator deploying PHD, I should be able to separate two master nodes (typically a primary active master and a backup non-active master) onto separate physical hosts so that my master pair is in an HA configuration from a physical hardware POV

background found here:        https://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf

and here:        http://www.vmware.com/files/pdf/vmware-virtualizing-apache-hadoop.pdf",,75728938,story,"[{'name': 'phd', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-25T22:59:47Z', 'id': 8042040, 'updated_at': '2014-03-25T22:59:47Z'}]",Deploy two hadoop master nodes on separate physical hosts,,[],956238,1210852,feature,2014-07-25T18:17:55Z,https://www.pivotaltracker.com/story/show/75728938
,2014-07-25T16:56:11Z,unscheduled,,"When rendering templates via the command-line gem, missing / mis-typed hash keys can silently return nil.  It would greatly aid debugging if these missing attributes were at least logged.

Possible implementation: http://stackoverflow.com/questions/16905191/raise-exception-when-accessing-attributes-that-doesnt-exist-in-openstruct",,75725562,story,"[{'name': 'juju', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-18T16:28:00Z', 'id': 8747846, 'updated_at': '2014-06-18T16:28:00Z'}]",Make Bosh.Template log warning when missing attributes are accessed in templates,,[],956238,5637,feature,2014-07-25T16:59:51Z,https://www.pivotaltracker.com/story/show/75725562
,2014-07-24T22:09:58Z,unscheduled,,,,75657602,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","bosh cli should retry tailing output when it receives ""System call error while talking to director: Operation timed out""  error",,[],956238,81882,feature,2014-07-24T22:21:44Z,https://www.pivotaltracker.com/story/show/75657602
,2014-07-23T19:18:02Z,unscheduled,,In an upgrade from PCF 1.2.0.0 to 1.2.2.0 customer got a 500 error in uploading release-repac.,,75541300,story,[],500 error occurred in PCF when uploading release-repac,,[],956238,548715,bug,2014-07-24T19:48:32Z,https://www.pivotaltracker.com/story/show/75541300
,2014-07-23T16:48:42Z,unscheduled,,"```
ruby 2.1.2p95 gorgas in ~/workspace/bosh/release/src/bosh/bosh-director
± al+as |develop ✓| → bosh task 13

Director task 13
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started compiling packages
  Started compiling packages > mysql/e5309aed88f5cc662bc77988a31874461f7c4fb8
  Started compiling packages > postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce
  Started compiling packages > genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201
  Started compiling packages > libpq/6aa19afb153dc276924693dc724760664ce61593
  Started compiling packages > powerdns/e41baf8e236b5fed52ba3c33cf646e4b2e0d5a4e
  Started compiling packages > nginx/fd1339e8f0ffe194e1c63b8651d7192a4468dbc5
  Started compiling packages > redis/ec27a0b7849863bc160ac54ce667ecacd07fc4cb
  Started compiling packages > ruby/4c712daa5e2268adeb50cfccb043a105a210a25e
     Done compiling packages > powerdns/e41baf8e236b5fed52ba3c33cf646e4b2e0d5a4e (00:01:34)
     Done compiling packages > mysql/e5309aed88f5cc662bc77988a31874461f7c4fb8 (00:01:57)
     Done compiling packages > genisoimage/008d332ba1471bccf9d9aeb64c258fdd4bf76201 (00:02:09)
     Done compiling packages > libpq/6aa19afb153dc276924693dc724760664ce61593 (00:02:11)
     Done compiling packages > redis/ec27a0b7849863bc160ac54ce667ecacd07fc4cb (00:02:27)
     Done compiling packages > postgres/aa7f5b110e8b368eeb8f5dd032e1cab66d8614ce (00:02:40)
     Done compiling packages > nginx/fd1339e8f0ffe194e1c63b8651d7192a4468dbc5 (00:02:55)
     Done compiling packages > ruby/4c712daa5e2268adeb50cfccb043a105a210a25e (00:12:15)
  Started compiling packages > registry/4035f8389f39a403b270044f55b781d578685d0b
  Started compiling packages > health_monitor/d6d9037d04ea93bf80e9b08d3e970773db04b3a4
  Started compiling packages > director/1677ac092b69e5ab845cba3b96678a1d1245a0b3
  Started compiling packages > nats/6a31c7bb0d5ffa2a9f43c7fd7193193438e20e92. Done (00:00:29)
   Failed compiling packages > health_monitor/d6d9037d04ea93bf80e9b08d3e970773db04b3a4: Action Failed get_task: Task 03af941d-b688-416c-743b-18b907f79879 result: Compiling package health_monitor: Running packaging script: Running command: 'bash -x packaging', stdout: 'Don't run Bundler as root. Bundler can ask for sudo if it is needed, and
installing your bundle as root will break this application for all non-root
users on this machine.
Resolving dependencies...
Installing addressable 2.3.4
Using json 1.8.1
Installing nokogiri 1.5.11
Installing aws-sdk 1.44.0
Installing dogapi 1.6.0
Installing escape_utils 1.0.1
Installing eventmachine 1.0.3
Installing em-http-request 0.3.0
Installing little-plugger 1.1.3
Installing logging 1.5.2
Installing daemons 1.1.9
Installing json_pure 1.8.1
Installing rack 1.5.2
Installing thin 1.5.1
Installing nats 0.5.0.beta.12
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.3
Installing yajl-ruby 1.1.0
Installing bosh-monitor 1.0000.0
Using bundler 1.6.3
Updating files in vendor/cache
Unfortunately, a fatal error has occurred. Please see the Bundler
troubleshooting documentation at http://bit.ly/bundler-issues. Thanks!
', stderr: '+ set -e
+ cd bosh/bosh-monitor
+ mkdir -p /var/vcap/packages/health_monitor/bin /var/vcap/packages/health_monitor/gem_home
+ cat
+ /var/vcap/packages/ruby/bin/bundle install --local --no-prune --binstubs /var/vcap/packages/health_monitor/bin --path /var/vcap/packages/health_monitor/gem_home
/var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:305:in `fetch_gem': undefined method `source_uri' for nil:NilClass (NoMethodError)
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:143:in `cached_built_in_gem'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:129:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:114:in `block in cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:112:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli/install.rb:79:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:146:in `install'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/command.rb:27:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/invocation.rb:121:in `invoke_command'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor.rb:363:in `dispatch'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/base.rb:440:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:9:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `block in <top (required)>'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/friendly_errors.rb:5:in `with_friendly_errors'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `<top (required)>'
	from /var/vcap/packages/ruby/bin/bundle:23:in `load'
	from /var/vcap/packages/ruby/bin/bundle:23:in `<main>'
': exit status 1 (00:00:44)
   Failed compiling packages > director/1677ac092b69e5ab845cba3b96678a1d1245a0b3: Action Failed get_task: Task 9178028b-8e03-4444-6438-212b4f2f83ac result: Compiling package director: Running packaging script: Running command: 'bash -x packaging', stdout: 'total 12K
drwxr-xr-x 3 root root 4.0K Jul 22 23:33 .
drwxr-xr-x 3 root root 4.0K Jul 22 23:33 ..
drwxr-xr-x 3 root root 4.0K Jul 22 23:00 vendor
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and
installing your bundle as root will break this application for all non-root
users on this machine.
Resolving dependencies...
Installing rake 10.3.2
Using json 1.8.1
Installing nokogiri 1.5.11
Installing aws-sdk 1.44.0
Installing bcrypt-ruby 3.0.1
Installing beefcake 0.3.7
Installing semi_semantic 1.1.0
Installing bosh_common 1.0000.0
Installing builder 3.1.4
Installing excon 0.25.3
Installing formatador 0.2.5
Installing mime-types 2.3
Installing multi_json 1.10.1
Installing net-ssh 2.9.1
Installing net-scp 1.1.2
Installing ruby-hmac 0.4.0
Installing fog 1.14.0
Installing httpclient 2.2.4
Installing log4r 1.1.10
Installing ruby-atmos-pure 1.0.5
Installing blobstore_client 1.0000.0
Installing gibberish 1.2.2
Installing yajl-ruby 1.1.0
Installing bosh-core 1.0000.0
Installing bosh-director-core 1.0000.0
Installing sequel 3.43.0
Installing rack 1.5.2
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.3
Installing daemons 1.1.9
Installing eventmachine 1.0.3
Installing thin 1.5.1
Installing bosh-registry 1.0000.0
Installing bosh_cpi 1.0000.0
Installing bosh_aws_cpi 1.0000.0
Installing bosh_openstack_cpi 1.0000.0
Installing rest-client 1.6.7
Installing bosh_vcloud_cpi 0.5.4
Installing membrane 0.0.2
Installing mysql2 0.3.11
Installing pg 0.15.1
Installing bosh_vsphere_cpi 1.0000.0
Installing warden-protocol 0.1.3
Installing warden-client 0.1.0
Installing bosh_warden_cpi 1.0000.0
Installing mono_logger 1.1.0
Installing json_pure 1.8.1
Installing nats 0.5.0.beta.12
Installing netaddr 1.5.0
Installing rack-test 0.6.2
Installing redis 3.0.3
Installing redis-namespace 1.3.1
Installing vegas 0.1.11
Installing resque 1.25.2
Installing resque-backtrace 0.0.1
Installing thread_safe 0.3.4
Installing tzinfo 1.2.1
Installing rufus-scheduler 2.0.24
Installing ffi 1.9.3
Installing sys-filesystem 1.1.2
Installing bosh-director 1.0000.0
Using bundler 1.6.3
Updating files in vendor/cache
Unfortunately, a fatal error has occurred. Please see the Bundler
troubleshooting documentation at http://bit.ly/bundler-issues. Thanks!
', stderr: '+ set -e
+ mkdir -p /var/vcap/packages/director/bin /var/vcap/packages/director/gem_home
+ cp bosh/REVISION /var/vcap/packages/director/gem_home
+ cd bosh/bosh-director
+ ls -lha
+ libpq_dir=/var/vcap/packages/libpq
+ mysqlclient_dir=/var/vcap/packages/mysql
+ cat
+ bundle_cmd=/var/vcap/packages/ruby/bin/bundle
+ /var/vcap/packages/ruby/bin/bundle config build.mysql2 --with-mysql-dir=/var/vcap/packages/mysql --with-mysql-include=/var/vcap/packages/mysql/include/mysql
+ /var/vcap/packages/ruby/bin/bundle config build.pg --with-pg-lib=/var/vcap/packages/libpq/lib --with-pg-include=/var/vcap/packages/libpq/include
+ /var/vcap/packages/ruby/bin/bundle install --local --no-prune --binstubs /var/vcap/packages/director/bin --path /var/vcap/packages/director/gem_home
/var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:305:in `fetch_gem': undefined method `source_uri' for nil:NilClass (NoMethodError)
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:143:in `cached_built_in_gem'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:129:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:114:in `block in cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:112:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli/install.rb:79:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:146:in `install'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/command.rb:27:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/invocation.rb:121:in `invoke_command'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor.rb:363:in `dispatch'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/base.rb:440:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:9:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `block in <top (required)>'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/friendly_errors.rb:5:in `with_friendly_errors'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `<top (required)>'
	from /var/vcap/packages/ruby/bin/bundle:23:in `load'
	from /var/vcap/packages/ruby/bin/bundle:23:in `<main>'
': exit status 1 (00:01:42)
     Done compiling packages > registry/4035f8389f39a403b270044f55b781d578685d0b (00:21:15)
   Failed compiling packages (00:33:31)

Error 450001: Action Failed get_task: Task 03af941d-b688-416c-743b-18b907f79879 result: Compiling package health_monitor: Running packaging script: Running command: 'bash -x packaging', stdout: 'Don't run Bundler as root. Bundler can ask for sudo if it is needed, and
installing your bundle as root will break this application for all non-root
users on this machine.
Resolving dependencies...
Installing addressable 2.3.4
Using json 1.8.1
Installing nokogiri 1.5.11
Installing aws-sdk 1.44.0
Installing dogapi 1.6.0
Installing escape_utils 1.0.1
Installing eventmachine 1.0.3
Installing em-http-request 0.3.0
Installing little-plugger 1.1.3
Installing logging 1.5.2
Installing daemons 1.1.9
Installing json_pure 1.8.1
Installing rack 1.5.2
Installing thin 1.5.1
Installing nats 0.5.0.beta.12
Installing rack-protection 1.5.3
Installing tilt 1.4.1
Installing sinatra 1.4.3
Installing yajl-ruby 1.1.0
Installing bosh-monitor 1.0000.0
Using bundler 1.6.3
Updating files in vendor/cache
Unfortunately, a fatal error has occurred. Please see the Bundler
troubleshooting documentation at http://bit.ly/bundler-issues. Thanks!
', stderr: '+ set -e
+ cd bosh/bosh-monitor
+ mkdir -p /var/vcap/packages/health_monitor/bin /var/vcap/packages/health_monitor/gem_home
+ cat
+ /var/vcap/packages/ruby/bin/bundle install --local --no-prune --binstubs /var/vcap/packages/health_monitor/bin --path /var/vcap/packages/health_monitor/gem_home
/var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:305:in `fetch_gem': undefined method `source_uri' for nil:NilClass (NoMethodError)
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:143:in `cached_built_in_gem'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/source/rubygems.rb:129:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:114:in `block in cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/runtime.rb:112:in `cache'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli/install.rb:79:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:146:in `install'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/command.rb:27:in `run'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/invocation.rb:121:in `invoke_command'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor.rb:363:in `dispatch'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/vendor/thor/base.rb:440:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/cli.rb:9:in `start'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `block in <top (required)>'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/lib/bundler/friendly_errors.rb:5:in `with_friendly_errors'
	from /var/vcap/packages/ruby/lib/ruby/gems/2.1.0/gems/bundler-1.6.3/bin/bundle:20:in `<top (required)>'
	from /var/vcap/packages/ruby/bin/bundle:23:in `load'
	from /var/vcap/packages/ruby/bin/bundle:23:in `<main>'
': exit status 1

Task 13 error

ruby 2.1.2p95 gorgas in ~/workspace/bosh/release/src/bosh/bosh-director
```",,75529146,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh director should not repeat error that was already recorded in the event log,,[],956238,5637,feature,2014-07-23T18:51:01Z,https://www.pivotaltracker.com/story/show/75529146
,2014-06-20T00:03:16Z,unscheduled,,"as a deployment operator, i should be able to upload a set of assets and them have them available across micro deployments so that I don't have to reupload them when I target a different micro deployment
",,73593646,story,"[{'name': 'not-yet', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T21:52:11Z', 'id': 9059872, 'updated_at': '2014-07-28T21:52:11Z'}]",Reuse Images across micro deploys,,[],956238,1210852,feature,2014-07-28T21:52:17Z,https://www.pivotaltracker.com/story/show/73593646
,2014-07-22T23:16:38Z,unscheduled,,"3mins:

```
2014-07-21_23:25:06.70410 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Successful: true (0)
2014-07-21_23:25:06.70411 [File System] 2014/07/21 23:25:06 DEBUG - Making dir /etc with perm 511
2014-07-21_23:25:06.70411 [File System] 2014/07/21 23:25:06 DEBUG - Making dir /var/vcap/bosh/etc with perm 511
2014-07-21_23:25:06.70412 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Running command: ntpdate
2014-07-21_23:25:06.70523 [File System] 2014/07/21 23:25:06 DEBUG - Checking if file exists /sys/class/net/eth0
2014-07-21_23:25:06.70525 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Running command: arping -c 1 -U -I eth0 10.0.0.46
2014-07-21_23:25:06.91474 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Stdout: ARPING 10.0.0.46 from 10.0.0.46 eth0
2014-07-21_23:25:06.91479 Sent 1 probes (1 broadcast(s))
2014-07-21_23:25:06.91480 Received 0 response(s)
2014-07-21_23:25:06.91489 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Stderr:
2014-07-21_23:25:06.91506 [Cmd Runner] 2014/07/21 23:25:06 DEBUG - Successful: true (0)
2014-07-21_23:28:59.32539 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Stdout:
2014-07-21_23:28:59.32542 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Stderr:
2014-07-21_23:28:59.32546 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Successful: true (0)
2014-07-21_23:28:59.32553 [File System] 2014/07/21 23:28:59 DEBUG - Making dir /var/vcap/data with perm 488
2014-07-21_23:28:59.32577 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Running command: sfdisk -s /dev/sdb
2014-07-21_23:28:59.58865 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Stdout: 10485760
2014-07-21_23:28:59.58867 [Cmd Runner] 2014/07/21 23:28:59 DEBUG - Stderr:
```",,75481708,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",do not block on ntpdate for more than X amount of time,,[],956238,553935,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/75481708
,2014-07-22T21:39:56Z,unscheduled,,"as a release developer, i should be able to see the contents of tmpdir (prepackaging build dir) after a failed release creation so that I can debug my prepackaging script

",,75475988,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",tmpdir removed after failed release creation,,[],956238,1210852,feature,2015-06-24T22:44:04Z,https://www.pivotaltracker.com/story/show/75475988
,2014-07-22T21:25:20Z,unscheduled,,"We changed the blobstore credentials for A1's full BOSH and redeployed via the microBOSH, but updates to our existing CF deployment failed:

```
Director task 20884
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:01)
  Started preparing deployment > Binding existing deployment. Done (00:00:02)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:03)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:00)

  Started preparing configuration > Binding configuration. Done (00:00:06)

  Started updating job api_z1
  Started updating job api_z1 > api_z1/0 (canary). Done (00:00:24)

Error 450001: #<Bosh::Blobstore::BlobstoreError: Failed to find object '1ca3e76d-8a74-4dd9-8784-6482c8b5dd47', S3 response error: The AWS Access Key Id you provided does not exist in our records.>: [""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/util.rb:31:in `rescue in unpack_blob'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/util.rb:27:in `unpack_blob'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/helpers.rb:26:in `fetch_bits'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/helpers.rb:31:in `fetch_bits_and_symlink'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/package.rb:33:in `install_for_job'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/plan.rb:77:in `block (2 levels) in install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/plan.rb:76:in `each'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/plan.rb:76:in `block in install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/plan.rb:75:in `each'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/apply_plan/plan.rb:75:in `install_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:120:in `apply_packages'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:73:in `apply'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:8:in `process'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:265:in `process'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:250:in `process_long_running'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:177:in `block in process_in_thread'"", ""<internal:prelude>:10:in `synchronize'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:175:in `process_in_thread'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:155:in `block in handle_message'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `call'"", ""/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `block in spawn_threadpool'""]
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:78:in `rescue in apply'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:70:in `apply'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/message/apply.rb:8:in `process'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:265:in `process'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:250:in `process_long_running'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:177:in `block in process_in_thread'
<internal:prelude>:10:in `synchronize'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:175:in `process_in_thread'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:155:in `block in handle_message'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `call'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `block in spawn_threadpool'

Task 20884 error
```",,75475050,story,[],"When Bosh blobstore credentials are changed, Bosh agents should be updated with the new credentials",,[],956238,1158132,feature,2014-07-22T21:25:21Z,https://www.pivotaltracker.com/story/show/75475050
,2014-07-22T17:42:48Z,unscheduled,,"as a bosh operator, i should be able to run `bosh cck` and  be able to delete a vm that has a persistent disk with full knowledge that the vm will be recreated and persistent disk reattached so that I can proceed with confidence through the troubleshooting and resolution process seen below

```
Found 1 problem

Problem 1 of 1: VM `623fbf45-f93d-477b-8bd2-6063760bbb66' is out of sync: expected `cloudfoundry: database/0', got `: unknown job/'.
  1. Ignore problem
  2. Delete VM (unless it has persistent disk)
```

The path to resolution is::
- delete the vm that was out of sync
- do `bosh cck`
- see that cck noticed that the vm was removed and choose the recreate option
- see that `bosh cck` will recreate the the vm and attach the persistent disk again.

",,75455810,story,"[{'name': 'cck', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-22T17:42:47Z', 'id': 9002620, 'updated_at': '2014-07-22T17:42:47Z'}, {'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh cck - delete vm with persistent disk,,[],956238,1210852,feature,2014-07-23T23:38:10Z,https://www.pivotaltracker.com/story/show/75455810
,2014-07-21T18:17:23Z,unscheduled,,,,75379900,story,"[{'name': 'storage and affinity', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-12-13T22:18:17Z', 'id': 7254216, 'updated_at': '2013-12-13T22:18:17Z'}]",investigate whether bosh resource pools allow specifying placement for vsphere RP,,[],956238,1210852,feature,2014-07-21T18:25:54Z,https://www.pivotaltracker.com/story/show/75379900
,2014-07-21T16:50:11Z,unscheduled,,"For operators where internet connection speeds are low, it would be helpful to have an estimate as to how long a blob will take to download if it has been found remotely during `bosh upload release`. This should probably be completed after #75371834",,75371988,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","As a BOSH operator, I should have some time estimate for how long a blob will take to download if it has been found remotely during `bosh upload release`",,[],956238,1406536,feature,2014-07-24T15:17:59Z,https://www.pivotaltracker.com/story/show/75371988
,2014-07-21T16:48:20Z,unscheduled,,`bosh upload release` should display how many jobs and packages will need to be uploaded and how many have been completed.,,75371834,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",As a BOSH deployment operator I should know how many jobs and packages need to be proceeded when uploading a release,,[],956238,1406536,feature,2014-07-24T15:18:07Z,https://www.pivotaltracker.com/story/show/75371834
,2014-07-03T17:56:24Z,unscheduled,,"Details of everything we tried are in here: https://gist.github.com/hiremaga/0e3ee19b8af6ccbdbcf9

This includes the CLI and director versions.",,74403926,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]",Could not delete old releases from ketchup's micro bosh,,[],956238,5637,bug,2015-06-24T22:40:11Z,https://www.pivotaltracker.com/story/show/74403926
,2014-07-21T08:28:53Z,unscheduled,,,,75337916,story,"[{'name': 'persistent-disk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-24T00:54:10Z', 'id': 8784634, 'updated_at': '2014-08-01T00:49:18Z'}]",vsphere disk full should be able to increase disk size,,[],956238,1218342,bug,2014-08-01T00:49:18Z,https://www.pivotaltracker.com/story/show/75337916
,2014-07-19T06:31:57Z,unscheduled,,"Losing an AWS compilation VM during a deploy to tabasco caused subsequent deploys to fail on compilation.

Failed deploy where compilation VM lost:
```
Director task 18602
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:07)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:02)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:01)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:10)

  Started preparing package compilation > Finding packages to compile. Done (00:00:01)

  Started compiling packages > cloud_controller_ng/8cd7552f2a4fd4b6ae77fc5a3aafcce9bea7e780. Failed: The service is unavailable. Please try again shortly. (00:00:06)

Error 100: The service is unavailable. Please try again shortly.

Task 18602 error
```

Next deploy:

```
Director task 18604
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:01)
  Started preparing deployment > Binding existing deployment. Done (00:00:07)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:01)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:09)

  Started preparing package compilation > Finding packages to compile. Done (00:00:01)

  Started compiling packages
  Started compiling packages > warden/5e6d50d31a878ee58e3fbfe397edd50a3877587b
  Started compiling packages > cloud_controller_ng/8cd7552f2a4fd4b6ae77fc5a3aafcce9bea7e780
   Failed compiling packages > warden/5e6d50d31a878ee58e3fbfe397edd50a3877587b: Address 10.10.17.2 is in use. (00:04:42)
     Done compiling packages > cloud_controller_ng/8cd7552f2a4fd4b6ae77fc5a3aafcce9bea7e780 (00:05:41)
   Failed compiling packages (00:05:41)

Error 100: Address 10.10.17.2 is in use.

Task 18604 error
```

Running `bosh cck` against the `cf-tabasco` deployment reports no issues, and `bosh vms` reports no VM with this IP in any of the bosh deployments. Inspection in the AWS console reveals there is an unnamed c1.medium VM with IP address 10.10.17.2. Deleting the VM manually unblocks the deploy.

Would there have been any way to remediate this issue through BOSH CLI commands, without going down to the IaaS layer?",,75305420,story,[],Rogue compilation VM caused deploys to fail on tabasco,,[],956238,1158132,bug,2014-07-19T06:57:13Z,https://www.pivotaltracker.com/story/show/75305420
,2014-07-18T19:28:18Z,unscheduled,,"`config/final.yml`:
```
---
final_name: dummy
blobstore:
  provider: local
  options:
    blobstore_path: /tmp/
```

`config/private.yml`:
```
---
final_name: dummy
blobstore:
  provider: local
  options:
    blobstore_path: /tmp/
```

`bosh create release`:
`blobstore private provider does not match final provider`",,75288010,story,[],"Using local blobstore, existence of private.yml causes `bosh create release` to think the blobstore providers do not match",,[],956238,1123776,bug,2014-07-18T19:28:18Z,https://www.pivotaltracker.com/story/show/75288010
,2014-07-18T08:48:38Z,unscheduled,,"We bundle a JDK with one of our releases which contains many thousands of very small files. When the Agent untars this and shows the list of files in the archive it goes over a threshold (1MB)[1] and fails to deploy.

```
$ bosh -n deploy
Getting deployment properties from director...
Compiling deployment manifest...

Director task 35
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:00)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:01)

  Started preparing configuration > Binding configuration. Done (00:00:00)

  Started updating job jenkins > jenkins/0 (canary). Failed: Response exceeded maximum allowed length (00:01:25)

Error 450001: Response exceeded maximum allowed length

Task 35 error

For a more detailed error report, run: bosh task 35 --debug
```

[1]: https://github.com/cloudfoundry/bosh/blob/3192bb051a1329c99df69e4818312574c8402297/go_agent/src/bosh/mbus/nats_handler_test.go#L91-L131",,75252598,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Deploy should not fail because untarring a package generates >1MB of logs,,[],956238,1218342,bug,2015-06-24T22:43:34Z,https://www.pivotaltracker.com/story/show/75252598
,2014-07-17T21:19:00Z,unscheduled,,"```
Apply resolutions? (type 'yes' to continue): yes
Applying resolutions...

Director task 15
  Started applying problem resolutions > unresponsive_agent 15: Recreate VM using last known apply spec. Failed: Agent is responding now, skipping resolution (00:00:00)

Task 15 done
```",,75229374,story,"[{'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]","If a resolution to a problem has already been applied when running bosh cloud check, do not print a failure message ",,[],956238,1406536,feature,2014-07-24T15:18:21Z,https://www.pivotaltracker.com/story/show/75229374
,2014-07-17T20:54:19Z,unscheduled,,"Acceptance Criteria:

Updated tasks and description for #75227324",,75227522,story,"[{'name': 'error messaging', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-01-30T18:25:58Z', 'id': 7601074, 'updated_at': '2014-01-30T18:25:58Z'}, {'name': 'openstack', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T22:51:45Z', 'id': 8783972, 'updated_at': '2014-06-23T22:51:45Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",Investigate the difficulty in returning a warning message for OpenStack deploys when a flavor with no ephemeral storage is used.,,[],956238,1406536,feature,2014-07-22T01:09:44Z,https://www.pivotaltracker.com/story/show/75227522
,2014-07-17T19:23:57Z,unscheduled,,"drain action in agent will unmonitor the services, then a successive stop action will stop them. If the unmonitor is pending long enough, the stop will fail with 503 service unavailable.

Should we wait for unmonitor to finish?",,75221318,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",stop action will fail if unmonitor is still pending,,[],956238,1123776,feature,2015-06-24T22:42:58Z,https://www.pivotaltracker.com/story/show/75221318
,2014-07-17T19:19:39Z,unscheduled,,"```
Director task 326
  Started applying problem resolutions > missing_vm 225: Recreate VM using last known apply spec. Failed: The service is unavailable. Please try again shortly. (00:00:14)

Task 326 done

Started		2014-07-17 19:16:13 UTC
Finished	2014-07-17 19:16:27 UTC
Duration	00:00:14
Cloudcheck is finished

ruby 1.9.3p547 clement in ~
○ → bosh cck
Performing cloud check...

Director task 327
  Started scanning 13 vms
  Started scanning 13 vms > Checking VM states. Done (00:00:00)
  Started scanning 13 vms > 13 OK, 0 unresponsive, 0 missing, 0 unbound, 0 out of sync. Done (00:00:00)
     Done scanning 13 vms (00:00:00)

  Started scanning 0 persistent disks
  Started scanning 0 persistent disks > Looking for inactive disks. Done (00:00:00)
  Started scanning 0 persistent disks > 0 OK, 0 inactive, 0 mount-info mismatch. Done (00:00:00)
     Done scanning 0 persistent disks (00:00:00)

Task 327 done

Started		2014-07-17 19:16:53 UTC
Finished	2014-07-17 19:16:53 UTC
Duration	00:00:00

Scan is complete, checking if any problems found...
No problems found
```",,75221032,story,[],"if bosh cck fails to recreate a vm, problems should be found upon rerunning the check",,[],956238,1406536,bug,2014-07-17T19:19:39Z,https://www.pivotaltracker.com/story/show/75221032
,2014-07-16T01:39:39Z,unscheduled,,there is a questionable task_cancelled? behaviour in BaseJob. user sees that task they were running was cancelled but it actually timed out.,,75083970,story,[],user should see that task timed out instead of cancelled when director marks tasks as timed out,,[],956238,81882,feature,2014-07-16T01:39:40Z,https://www.pivotaltracker.com/story/show/75083970
,2014-07-15T22:25:06Z,unscheduled,,"For some reason, uncommitted_changes is being set as true the first time a new release is created.

```
git status (should be clean)
bosh create release --final
cat releases/dummy-1.yml (should have ""uncommitted_changes""=>false)
```",,75075256,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",uncommitted_changes flag in release manifest should be false if there are no local changes,,[],956238,1266616,bug,2015-06-24T22:43:11Z,https://www.pivotaltracker.com/story/show/75075256
,2014-07-15T21:34:46Z,unscheduled,,"Relevant code: https://github.com/cloudfoundry/bosh/blob/stable-2624/bosh-director/lib/bosh/director/instance_updater.rb#L60

Asset downloads occur at line #60.  Ephemeral disk updates occur at line #69.  An out-of-space error will always error out before attempting to increase the disk space.",,75071684,story,[],"As a bosh user who encountered an 'insufficient disk' error on a deploy, I would like to increase the ephemeral disk size and successfully redeploy.",,[],956238,1043545,feature,2014-07-15T21:34:46Z,https://www.pivotaltracker.com/story/show/75071684
,2014-07-15T20:16:40Z,unscheduled,,"As a BOSH user, I would like to see information from the 'fault' field in error cases against the OpenStack CPI

The OpenStack CPI does not capture as much information as it could when, for example, a new server VM comes up in an error state during creation. The fault field on the server object that Fog generates can have more useful information, however, that should be visible it at least the debug logs. For example:

```
fault={""message""=>""No valid host was found. Exceeded max scheduling attempts 3 for instance 40c5734b-00c7-4a65-a0fa-1b9523121b9c"", ""code""=>500, ""details""=>""  File \""/usr/local/lib/python2.7/dist-packages/nova/scheduler/manager.py\"", line 147, in run_instance\n    legacy_bdm_in_spec)\n  File \""/usr/local/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\"", line 85, in schedule_run_instance\n    filter_properties, instance_uuids)\n  File \""/usr/local/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\"", line 306, in _schedule\n    self._populate_retry(filter_properties, properties)\n  File \""/usr/local/lib/python2.7/dist-packages/nova/scheduler/filter_scheduler.py\"", line 275, in _populate_retry\n    raise exception.NoValidHost(reason=msg)\n"", ""created""=>""2014-07-15T19:59:06Z""},
```",,75065622,story,[],"As a BOSH user, I would like to see information from the 'fault' field in error cases against the OpenStack CPI",,[],956238,1158132,feature,2014-07-21T22:11:45Z,https://www.pivotaltracker.com/story/show/75065622
,2014-07-15T16:32:45Z,unscheduled,,"as a deployment operator, i should be able to compile pkgs for a release before deployment and separate from deployment so that deploys are faster.

Max from IBM has requested this feature to make their deploys much more performant. Canonical has also requested this. I'm sure PCF could also use this.",,75046966,story,[],Add command to compile packages for a release,,[],956238,1123776,feature,2014-07-22T23:47:05Z,https://www.pivotaltracker.com/story/show/75046966
,2014-07-12T01:02:57Z,unscheduled,,,,74894802,story,[],Datastore pattern can not contain special symbols like dash,,[],956238,553935,bug,2014-07-12T01:02:58Z,https://www.pivotaltracker.com/story/show/74894802
,2014-07-11T17:10:08Z,unscheduled,,"```
Uploading the whole release

Uploading release
dummy-0.15-de: 100% |oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo|   4.6KB 213.6KB/s Time: 00:00:00

Director task 23
  Started extracting release > Extracting release. Done (00:00:00)

  Started verifying manifest > Verifying manifest. Done (00:00:00)

  Started resolving package dependencies > Resolving package dependencies. Done (00:00:00)

  Started creating new packages
  Started creating new packages > bad_package/0.1-dev. Done (00:00:00)
  Started creating new packages > dummy_package/0.1-dev. Done (00:00:00)
     Done creating new packages (00:00:00)

  Started creating new jobs
  Started creating new jobs > dummy/0.7-dev. Done (00:00:00)
  Started creating new jobs > dummy_with_bad_package/0.1-dev. Done (00:00:00)
  Started creating new jobs > dummy_with_package/0.1-dev. Done (00:00:00)
  Started creating new jobs > dummy_with_properties/0.1-dev. Done (00:00:00)
     Done creating new jobs (00:00:00)

  Started release has been created > dummy/0+dev.15. Done (00:00:00)

Task 23 done

Started		2014-07-11 17:01:18 UTC
Finished	2014-07-11 17:01:18 UTC
Duration	00:00:00

Release uploaded
```



```
±  |develop ✗| → be bosh -n upload stemcell ~/Downloads/bosh-stemcell-64-warden-boshlite-ubuntu-lucid-go_agent.tgz

Verifying stemcell...
File exists and readable                                     OK
Verifying tarball...
Read tarball                                                 OK
Manifest exists                                              OK
Stemcell image file                                          OK
Stemcell properties                                          OK

Stemcell info
-------------
Name:    bosh-warden-boshlite-ubuntu-lucid-go_agent
Version: 64

Checking if stemcell already exists...
No

Uploading stemcell...

bosh-stemcell: 100% |oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo| 341.4MB  49.8MB/s Time: 00:00:06

Director task 53
  Started update stemcell
  Started update stemcell > Extracting stemcell archive. Done (00:00:02)
  Started update stemcell > Verifying stemcell manifest. Done (00:00:00)
  Started update stemcell > Checking if this stemcell already exists. Done (00:00:00)
  Started update stemcell > Uploading stemcell bosh-warden-boshlite-ubuntu-lucid-go_agent/64 to the cloud. Done (00:00:09)
  Started update stemcell > Save stemcell bosh-warden-boshlite-ubuntu-lucid-go_agent/64 (stemcell-ce1e9ef2-5a99-4539-8c7f-88643698fe96). Done (00:00:00)
     Done update stemcell (00:00:11)

Task 53 done

Started		2014-07-11 17:08:03 UTC
Finished	2014-07-11 17:08:14 UTC
Duration	00:00:11

Stemcell uploaded and created.
```",,74867306,story,[],inconsistent style of the even log between upload stemcell and upload release,,[],956238,81882,feature,2014-07-11T17:10:08Z,https://www.pivotaltracker.com/story/show/74867306
,2014-07-10T15:31:16Z,unscheduled,,"Currently bosh ssh will fail is there is no key in the ssh-agent and  no ~/.ssh/id_[rd]sa. In that case, bosh should generate the keypair. 

When users hit this error (and it's mostly new users, who we'd like to have a smooth 1st experience), it's surprising and not clear what they should do to work around it. 

It's also not consistent with what the rest of bosh does: we'll go to the trouble of making a disposable user for the vm, we should go the next step and make disposable ssh key.

",,74782662,story,[],"iff no ~/.ssh/id_[rd]sa, bosh should generate the key pair",,[],956238,637633,feature,2014-07-10T15:31:16Z,https://www.pivotaltracker.com/story/show/74782662
,2014-07-09T00:12:19Z,unscheduled,,"The services team has a release (cf-mysql-release) with an haproxy job, and we recently needed to add the ability to log to disk #73932590.

Haproxy is only able to log using syslog.  Thus, when trying to log locally to `var/vcaps/sys/log/...` the syslog user must be able to write to that directory. This is currently impossible since `var/vcaps/sys` and `.../sys/log` do not allow global read and execute.

We found two workarounds:
1) `chmod a+rx /var/vcap/sys && chmod a+rx /var/vcap/sys/log`
2) `usermod -a -G vcap syslog` (update: this does not actually work!)

We would like either of these (perhaps the second since it's less invasive) to be a default provided by bosh, so that other jobs that  want to use syslog to log to files can do so.

Other releases that use haproxy+syslog include 'cf-riak-cs-release' and 'cf-release' and 'cf-cassandra-release' (though these currently use syslog to log to network destinations, not to files).",,74657962,story,[],Add syslog user to vcap group,,[],956238,721203,feature,2014-07-09T20:45:50Z,https://www.pivotaltracker.com/story/show/74657962
,2014-06-13T21:39:35Z,unscheduled,,"BOSH provides a syslogd on the stemcell, but in order to configure your vms to forward logs to an aggregator, you must modify the syslog configuration in your release.  This means that each release author must include that functionality, and the user must configure this stuff for each tile in OpsManager.

This is a pain point for Runtime, and for each of our 7 (SEVEN!!?!) deployments in London, not to mention MySQL, Blobstore, etc.

Is there a way for BOSH to handle this for us?  We'd like to configure our deploy manifest such that all VMs in a deployment should forward syslog to a single place.  

Even better, can we configure the microbosh deploy manifest to forward syslog to a single place across all deployments OMG MIND BLOWN!?!?!?  Not sure if there's precedent for this kind of global configuration in BOSH.",,73236914,story,[],configure deploy manifest so all VMs in a deployment forward syslog to a single place,,[],956238,1210852,feature,2014-07-12T14:23:20Z,https://www.pivotaltracker.com/story/show/73236914
,2014-06-23T21:30:12Z,unscheduled,,"as a deployment operator, I should be able to configure microBOSH/BOSH to use my own postgres db so that I have an HA db solution


",,73753092,story,[],How to configure MicroBOSH & BOSH directors to use an External Postgres,,[],956238,1210852,feature,2014-07-08T22:12:04Z,https://www.pivotaltracker.com/story/show/73753092
,2014-06-30T22:32:41Z,unscheduled,,"as a deployment operator, I should be able to statically set the search suffixes so that I can use short hostnames to resolve a hostname

Oracle databases often use a SCAN server to discover dynamically the location of their database at connect time.  The SCAN server dishes out a hostname like ""dbrack1234"", and the client is supposed to connect.  Bad news, it's documented to only give out a short hostname, not a FQDN.  That short hostname has to resolve at the app-level, and the normal way this happens is DNS search suffixes handed out by DHCP.  Sadly, BOSH won't let us statically set the search suffixes 

The workaround was setting an environment variable in every app: LOCALDOMAIN=something.net",,74182696,story,"[{'name': 'networking', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-21T16:04:53Z', 'id': 8250390, 'updated_at': '2014-04-21T16:04:53Z'}]",BOSH should allow setting of DNS search suffixes,,[],956238,1210852,feature,2014-07-08T18:24:16Z,https://www.pivotaltracker.com/story/show/74182696
,2014-07-07T22:13:55Z,unscheduled,,For later review,,74564408,story,[],Persist task arguments along with job type,,[],956238,1123776,feature,2014-07-07T22:13:55Z,https://www.pivotaltracker.com/story/show/74564408
,2014-06-19T18:16:26Z,unscheduled,,"as a deployment operator, I should be able to deploy so that I can upgrade my application.

Deployment is failing with this error:
```
Building packages
-----------------
Building gorouter...
  Final version:   NOT FOUND
  Dev version:     NOT FOUND
  Generating...
`buildpack_cache' (4ced0bc62f12dcaa79121718ca3525253ede33b5) is corrupted in blobstore (id=0f819b7e-38b2-45cc-8e71-f733b4fd4a2a), please remove it manually and re-generate the final release  Generated version ca12146d28be6443ef9aa73d40ab63b13fcb3b6c

Building buildpack_cache...
  Final version:   Downloading `buildpack_cache (4ced0bc62f12dcaa79121718ca3525253ede33b5)'...

error: Command failed: ""set -o pipefail && cd ./repos/cf-release && bosh -t https://bosh.tabasco.cf-app.com:25555 -u ci -p xxx -n create release"" (options: {})    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/command_runner/spawner.rb:21:in `wait'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/command_runner.rb:16:in `run!'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/bosh.rb:141:in `run_bosh'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/bosh.rb:123:in `bosh_create_release'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/bosh.rb:88:in `create_release'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/bosh.rb:30:in `create_dev_release'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/dev_deployment_strategy.rb:7:in `create_release'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/cf_deploy.rb:22:in `create_release'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/bin/cf_deploy:6:in `block in <top (required)>'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/lib/cf_deployer/cli.rb:14:in `start'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/.gem/ruby/1.9.3/bundler/gems/cf-deployer-47fad4a4e0f6/bin/cf_deploy:5:in `<top (required)>'    # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/jobs/tabasco-runtime-cf-deploy/workspace/ci/bin/cf_deploy:16:in `load'     # 2014-06-19 17:48:49 +0000
  `- /mnt/jenkins/jobs/tabasco-runtime-cf-deploy/workspace/ci/bin/cf_deploy:16:in `<main>'    # 2014-06-19 17:48:49 +0000
```",,73565650,story,[],Blobstore download failing,,[],956238,1210852,bug,2014-07-11T17:00:14Z,https://www.pivotaltracker.com/story/show/73565650
,2014-07-01T22:45:08Z,unscheduled,,"Currently if you pass a command to `bosh ssh` it will disable the password for that user. We have sudoers set up without the ""no password"" option, which means you cannot sudo without providing a password. This means non-interactive commands passed to bosh ssh cannot use sudo.

We should either allow `--default_password` to specify a password like it does for interactive ssh, or allow password-less sudo for users in the admin group.",,74264638,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",Allow sudo when passing a command to bosh ssh,,[],956238,1123776,feature,2015-06-24T22:41:34Z,https://www.pivotaltracker.com/story/show/74264638
,2014-06-19T15:23:38Z,unscheduled,,"Currently a deploy will fail if you try to move a previously used static ip to another job. 

TODO: exact error message

Cause speculation:
This behavior seems to be caused by the order of operations while preparing and updating a deploy. It seems that new job instances are assigned their static IPs before all unneeded static IPs have been released. 

It should already be possible to reuse a static IP on a job of the same name, even if a previous deploy fails and left a vm without an assigned job instance. #68325788

Acceptance scenarios:
- rename a job that has a static ip
- move a static ip from one job to another using the same resource pool
- move a static ip from one job to another using a different resource pool",,73552140,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'ux', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-02-11T21:01:06Z', 'id': 7695254, 'updated_at': '2014-02-11T21:01:06Z'}]",bosh deploy should allow recycling of static IPs,,[],956238,1266616,feature,2015-06-24T22:41:50Z,https://www.pivotaltracker.com/story/show/73552140
,2014-06-25T05:07:19Z,unscheduled,,"proxy, ha proxy, https proxy
PR#525, 535, github issues 504, 462, 547 (#71252782 is dup of #462)
 ",,73859548,story,[],charter: investigate state of proxy,,[],956238,1210852,feature,2014-06-25T05:08:39Z,https://www.pivotaltracker.com/story/show/73859548
,2013-11-27T00:56:40Z,unscheduled,,,,61521208,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",director should be indifferent to blobstore deletion errors when rendered job templates are being cleaned,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/61521208
,2014-06-10T23:42:02Z,unscheduled,,We had a situation where this occurred. Since the compile cache is a 'nice-to-have' I'd like it not to fail the deploy. ,,73015494,story,[],A failure to write the 'global compile cache' should not be fatal to a deploy. ,,[],956238,637633,feature,2014-06-16T17:27:44Z,https://www.pivotaltracker.com/story/show/73015494
,2014-03-17T18:03:39Z,unscheduled,,"pre_packaging scripts in cf-release is one area which makes building releases non-deterministic. rubygems.org goes down -> cannot build. incorrect ruby configure on local machine -> cannot build. etc.

we need to look at each pre_packaging script and convert it to either happen at the packaging stage if possible or if it's related to pulling down dependencies do that via git (e.g bundle package in cloud controller instead of on the fly in cf-release)

BOSH release itself is pre_packaging free now =). See pre_packagingics anonymous for more help.",,67680322,story,[],modify/remove pre_packaging scripts to not depend on env of machines `bosh create release` is being run on,,[],956238,81882,feature,2014-06-02T23:59:02Z,https://www.pivotaltracker.com/story/show/67680322
,2014-03-26T19:15:41Z,unscheduled,,"As a release developer who has to manage many releases I would like to be able to share packages between releases so that I can reduce maintenance overhead

Upgrading the Ruby or Go runtime identically 5 times is no fun at all.",,68304620,story,[],Ability to share packages between releases,,[],956238,1218342,feature,2014-06-18T05:54:52Z,https://www.pivotaltracker.com/story/show/68304620
,2014-06-02T20:52:03Z,unscheduled,,"`bosh create release` has been failing more often during the Syncing blobs... phase.

The download percentage repeats for a number of lines, then bosh reports the download as complete but fails with a checksum mismatch.

seen on a1 and tabasco jenkins.

",,72477458,story,[],checksum mismatch - downloading blob for bosh create release,,[],956238,1165386,bug,2014-06-02T22:22:46Z,https://www.pivotaltracker.com/story/show/72477458
,2014-04-17T19:29:26Z,unscheduled,,"We observed this in ""health monitor only resurrects stateless nodes that are configured to be resurrected"" - one of the two instances was marked as do-not-resuscitate (`bosh vm resurrection foobar 0 off`) and then both agents were killed. It appears that when the director got the scan_and_fix commands for the VMs, it always checked the DNR instance first, which meant the other instance's scan_and_fix always ran into the deployment lock and gave up.",,69728124,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",scan and fix may get stuck on a node that never resurrects,,[],956238,1123776,bug,2015-06-24T22:42:34Z,https://www.pivotaltracker.com/story/show/69728124
,2014-04-08T18:51:30Z,unscheduled,,"using stemcell 2355 vsphere esxi ubuntu (ruby and go)

We created a manifest with a job that had 2 templates that failed. (see attached).

The first deployment (from scratch) failed as expected.

When we redeployed the same manifest, both the ruby and go agents encountered the 503 error when sending the stop command to monit.

The 3rd and subsequent deploy attempts did not encounter the 503. Only the ""first redeploy after the initial deploy"" triggered the 503.",,69120210,story,"[{'name': 'et', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-18T17:12:53Z', 'id': 7982484, 'updated_at': '2014-03-18T17:12:53Z'}]","503 service unavailable from monit, deploying with failing job templates the second time",,[],956238,1123776,bug,2014-06-02T22:22:46Z,https://www.pivotaltracker.com/story/show/69120210
,2014-04-06T22:12:16Z,unscheduled,,"I have a release that vendors in Docker's source. Their repo has symlinks that are used for tests, and they intentionally point to bogus locations (i.e. `/b`).

`bosh create release` blows up like so:

https://gist.githubusercontent.com/vito/76cb981556a569b8b1bc/raw/ee3aa4a4fbd5c2e5eb337a4baa158c40af58bd11/gistfile1.txt

Could be wrong, but I don't think this fingerprinting stage should bother following symlinks (perhaps just fingerprint the link itself?).

I suspect later stages like the `tar`ing up won't work either; I've seen that follow symlinks before.",,68951036,story,[],bosh create release should not follow symlinks during fingerprinting/packaging,,[],956238,381857,bug,2014-06-02T22:22:46Z,https://www.pivotaltracker.com/story/show/68951036
,2014-04-04T01:21:44Z,unscheduled,,"output snippet
```
     Done preparing deployment (00:00:00)

  Started preparing package compilation > Finding packages to compile > Downloading 'haproxy/2' from global cache. Done (00:00:00) > Downloading 'syslog_aggregator/9' from global cache. Done (00:00:00) > Downloading 'ruby/12' from global cache. Done (00:00:00) > Downloading 'common/6' from global cache. Done (00:00:00) > Downloading 'gnatsd/4' from global cache. Done (00:00:00) > Downloading 'golang/6' from global cache. Done (00:00:01) > Downloading 'nats/10' from global cache. Done (00:00:01) > Downloading 'collector/27' from global cache. Done (00:00:00) > Downloading 'debian_nfs_server/4' from global cache. Done (00:00:00) [TRIMMED TO MAKE TRACKER HAPPY]
     Done preparing package compilation > Finding packages to compile (00:00:32)

  Started preparing dns > Binding DNS. Done (00:00:00)
```",,68853802,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}]","Bosh ""Started preparing package compilation"" output shows up on a single line",,[],956238,1054467,bug,2015-06-24T22:42:15Z,https://www.pivotaltracker.com/story/show/68853802
,2014-04-01T00:45:39Z,unscheduled,,"If an instance/vm becomes unresponsive during an update, and the recreate attempt for that vm fails, then the instance will ""disappear"":

* the instance will not show up in the output of `bosh vms`
* the instance will not be discovered as missing when you run `bosh cck`

Found while changing network config on vsphere. Using bosh-stemcell-2311-vsphere-esxi-ubuntu ruby and go agent versions.

I had an existing deployment (dummy). 2 jobs, 1 instance each. 1 job on stemcell with go agent, 1 on stemcell with ruby agent.

I changed the job config from dynamic ip address to static ip address, and ran `bosh deploy`

The go agent vm successfully reconfigured itself with the new ip address

The ruby agent vm failed to reconfigure itself (existing bug). `bosh deploy` eventually timed out.

While `bosh deploy` was still running, health monitor noticed that the vm was unresponsive, and asked director to scan and fix.

Scan and fix kept retrying until it could get the deployment lock. When it did run, it failed with `error creating vm: No available resources`

`bosh vms` shows the running vm, but no entry for the job that failed to update

`bosh cck` reports no problems.

`bosh deploy` with the same manifest does fix the problem - the ruby agent instance is recreated.

",,68600234,story,"[{'name': 'et', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-18T17:12:53Z', 'id': 7982484, 'updated_at': '2014-03-18T17:12:53Z'}]","job instance disappears, problem during update and problem during recreate",,[],956238,1165386,bug,2014-06-02T22:22:46Z,https://www.pivotaltracker.com/story/show/68600234
,2014-06-05T00:37:05Z,unscheduled,,"Specifically, after deleting an old vm the new inf settings should not include the old vm's dynamic IP. This results in new VM potentially thinking that it's at a wrong ip.

Impl: see VmUpdater::VmDeleter#delete and network_spec on Instance. current_state is not being reset which contain old vm's net settings.",,72664926,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh director should always populate new vm inf settings with empty dynamic network settings,,[],956238,81882,bug,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/72664926
,2014-05-22T17:06:51Z,unscheduled,,"One option is to add a ""bug version"" field to the Director's package model to distinguish between old and new sha1's. New packages could then be marked as having a valid sha1, while old package records might have an invalid sha1. The director would need to add a new flag in its call to the agent, to enable verification of the package blob against the provided sha1. This method is reverse compatible, but adds an amount of technical debt that would eventually need to be removed.

Another option is just to wait a reasonably long period of time (6mo+) and then re-enable package blob verification. At that point, old blobs affected by director bug #59776002 may fail to deploy, with a ""sha1 mismatch. expected ... actual ..."" error.",,71862108,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'blocked', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-03-21T16:40:23Z', 'id': 8013374, 'updated_at': '2014-03-21T16:40:23Z'}]",New source packages that will be compiled by compile_package action should be verified when downloaded by sha1,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/71862108
,2013-11-12T23:20:00Z,unscheduled,,should go agent keep allowing multiple tasks to run on go-agent. ruby agent allows only one tasks at a time afaik. provide cancel_task?,1.0,60643088,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",go agent should reject long running tasks if one is already enqueued/running,,[],956238,551901,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/60643088
,2014-01-08T18:21:48Z,unscheduled,,"They should check their config files during validation, rather than crashing later

extracted from #62280594",1.0,63478014,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",Require external blobstores to respond to validate cli arg,,[],956238,1076818,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/63478014
,2014-03-17T21:49:52Z,unscheduled,,"ruby agent supports 5 blobstores.  go agent supports three (S3, simple, local) and is missing atmos/swift

This feature is specifically: Add swift support

Story #68026514 is for atmos support

",,67701786,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",swift blobstore support,,[],956238,1210852,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/67701786
,2014-03-21T07:00:00Z,unscheduled,,"The agent already reports an instance as being  ""unhealthy"" to DataDog"" if monit isn't able to start it. This story is for a situation where a release developer has added complex monitoring (e.g. a memory threshold to as a tactical workaround for a memory leak) that results in processes being restarted.
The Runtime team does this for CC for example and LAMB might do this for loggregator. It'd be good to have these restarts be reported to DataDog to make sure the underlying issue does not get masked.",,68019502,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]","As an Operator, I'd like to be notified by the bosh agent when a process is restarted by monit",,[],956238,5637,feature,2014-09-09T23:29:16Z,https://www.pivotaltracker.com/story/show/68019502
,2014-03-24T21:56:26Z,unscheduled,,"go-agent will restart itself after several minutes (or so it seems) after it notices that heartbeats are not being sent. however, heartbeats are on 1 min interval so agent will not restart sooner rather than later which could mean that director will waste an extra couple of minutes.

in addition what we saw is that go-agent does not notice connection being down on the first missed heartbeat. we tried: ifdown eth0 && tail -f /var/vcap/bosh/log/current

related to https://www.pivotaltracker.com/story/show/62430844

after looking at yagnats it seems that it will not return an error if publish fails but rather will keep retrying. we should have a connectionChecker object that uses nats for pinging (ping in yagnats times out and returns an error).",,68148814,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",agent should restart when it loses internet connection after it received prepare_configure_networks,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68148814
,2014-03-14T00:14:28Z,unscheduled,,"depends on #67521532

make sure that we cap total time of waiting for monit to not be pending because otherwise we will have run away task.

to accept:
- set canary MAX watch time to <30 secs
- deploy job that takes long time to start ~30secs
- (at the same time check that monit summary is reporting start pending action; gone in 30 secs since start of the deploy)
- immediately after deploy try to bosh stop job
- see that job is successfully stopped",2.0,67521846,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]","go agent should wait for monit to become available to enqueue an action (start, stop, etc) before starting to count failed attempts of submitting an action",,[],956238,553935,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/67521846
,2014-03-28T17:57:07Z,unscheduled,,,,68460634,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",drain should send notify shutdown message to health monitor regardless if agent has an apply spec,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68460634
,2014-03-29T01:05:31Z,unscheduled,,"nats-pub 'agent.53d9d248-523e-49e0-9705-c1a5af96868e' '{""method"":""ping"",""arguments"":[]}' -s nats://nats:nats@172.16.79.16:4222 will trigger agent restart on the *next* heartbeat",,68483336,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",investigate behavior of go agent (restarts agent) if it receives message without reply_to field,,[],956238,81882,feature,2014-10-08T23:00:35Z,https://www.pivotaltracker.com/story/show/68483336
,2013-11-19T08:00:00Z,unscheduled,,Removing hardcoded initial networking configuration from the stemcell will help to remove operating system specific code from supposedly os agnostic infrastructure stages e.g. the system_aws_network and system_openstack_network stages both need information about the operating system being used in order to initially configure DHCP prior to the agent reconfiguring the network.,,68649748,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}]",Do Initial networking configuration in the agent,,[],956238,1406536,feature,2014-07-31T05:55:59Z,https://www.pivotaltracker.com/story/show/68649748
,2014-04-02T23:46:49Z,unscheduled,,"to reproduce:
- deploy with 2 net on vsphere (because it needs to use static networking)
- redeploy with 1 net 
- see that centos still have eth1 and eth0 even though there should only be eth0

files as feature because ruby agent never did this either",,68769422,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",agent should remove unused network interfaces after reconfiguring network in manual setup,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68769422
,2014-04-03T00:01:32Z,unscheduled,,ruby agent always restarts networking unlike for ubuntu. also see https://www.pivotaltracker.com/story/show/68769422 for related functionality.,,68770136,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",agent should not restart networking every time when it starts on centos for manual networking,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68770136
,2014-04-04T21:50:10Z,unscheduled,,"When cleaning up jobs and packages, the agent should either keep all of the previous and current assets, or keep only the current.

Keeping both is possible for the package and job content. With symlinks, the current spec and previous spec might have different versions of the same package/job. The symlink can only point to one of them, and must be the one in the current spec.

Since in this case we can't keep both previous and current, we should keep only current consistently.

though we should still keep prev and current downloaded content.",,68916632,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'improvement', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623542, 'updated_at': '2014-06-06T18:53:38Z'}]",agent should only keep symlinks for jobs and packages for the current deployment,,[],956238,81882,feature,2014-07-31T05:54:59Z,https://www.pivotaltracker.com/story/show/68916632
,2014-04-05T01:00:11Z,unscheduled,,,,68923200,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'disks', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-17T21:58:30Z', 'id': 8237838, 'updated_at': '2014-04-17T21:58:30Z'}]","does go agent nats client try to reconnect if nats drops it as a slow client, or does the entire agent process abort (like ruby)?",,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68923200
,2014-04-24T02:50:00Z,unscheduled,,"to reproduce:

1. clean up /etc/resolve.conf
2. revert dhclient config (remove prepend)
3. restart dhclient
4. restart agent
5. director won't be able to talk to it",,70062230,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Agent should not rely on DNS,,[],956238,1123776,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/70062230
,2014-04-24T20:57:23Z,unscheduled,,currently we just mount on top of store migration dir if store dir is a mount point. this could lead in some cases to mount over existing store migration mount point.,,70123342,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",agent should raise an error during mount_disk command if bosh store and store migration dirs are already mounted,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/70123342
,2014-04-24T23:57:21Z,unscheduled,,https://lwn.net/Articles/281157/,,70133650,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}, {'name': 'warden', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-06T18:53:38Z', 'id': 8623544, 'updated_at': '2014-06-06T18:53:38Z'}]",agent should remount source persistent disk as read-only when using bind-mount linux mounter,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/70133650
,2014-05-14T19:27:16Z,unscheduled,,,,71349108,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",bosh-blobstore-s3 should allow to specify encryption key,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/71349108
,2014-05-15T00:43:30Z,unscheduled,,"currently bosh will NOT add vip network to the vm but will also consider bosh deploy successful. this is weird and potentially dangerous since if a user wants to point their elastic ips at a vm they have to know to recreate the vm. 

tried with go-agent. this most likely involves changing cpi.",,71369164,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",user should be able to add vip network to a running vm after creating it by modifying networks section of the job,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/71369164
,2014-05-19T19:32:44Z,unscheduled,,"go & ruby agents currently implicitly require the DNS to resolve the time servers when ntpdate is called, and will block until they resolve. If the first DNS in the list does not respond within a minute (default timeout of dns), the next in the list is tried.",,71610516,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",go agent should not depend on dns presence when running through bootstrap - in particular running ntpdate,,[],956238,81882,feature,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/71610516
,2014-03-25T22:27:52Z,unscheduled,,"Steps to reproduce:

1) Deploy VM with persistent disk
2) Change the size of the disk
3) Remove the persistent disk or change the size again

Deploy fails with error:

```
Error 450001: Action Failed list_disk: Getting real device path: Timed out getting real device path for /dev/sdf
```

We found out that agent still keeps data about first disk /dev/sdf in its settings.json. So the list_disk action fails to get the real path as it is not listed in /proc/mounts

```
""Persistent"":{""vol-2ebbc76c"":""/dev/sdf"",""vol-9b463ad9"":""/dev/sdg""}}
```

When the agent executes `unmount` it does not update the settings.json. That behavior is the same in ruby agent.",,68237300,story,"[{'name': 'agent', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-05-29T20:48:11Z', 'id': 8561342, 'updated_at': '2014-07-31T05:54:48Z'}]",Agent fails to update persistent disk on aws more than twice,,[],956238,553935,bug,2014-07-31T05:54:48Z,https://www.pivotaltracker.com/story/show/68237300
,2014-06-04T15:59:40Z,unscheduled,,"> There area few warnings and deprecations related to rsyslog, I'm wondering if there's a story in the backlog to address them. We are using the bosh-warden-boshlite-ubuntu-lucid-go_agent v58 stemcell.
> 
> As I was working on the Cassandra syslog forwarding, I came across the following:
> 
> **/etc/rsyslog.d/50-default.conf**
> 
> 1. \*.emerg :omusrmsg:\* (missing the :omusrmsg:)
rsyslog error: action '\*' treated as ':omusrmsg:*' - please change syntax, '\*' will not be supported in the future
> 2. all lines related to xconsole
rsyslog error: Could no open output pipe '/dev/xconsole': No such file or directory
> 
> **/etc/rsyslog.conf**
> 
> 1. $KLogPath /proc/kmsg
rsyslog error: invalid or yet-unknown config file command 'KLogPath' - have you forgotten to load a module?
> 
> Cheers, Gerhard.",,72622648,story,[],Update rsyslog configuration syntax,,[],956238,1123776,feature,2014-06-05T22:26:29Z,https://www.pivotaltracker.com/story/show/72622648
,2014-05-14T00:31:05Z,unscheduled,,"During CF v171 deploy, one DEA got into a bad state. We tried to re-create it with `bosh recreate runner_z2 10 --force`, but it didn't work:

```
bosh recreate runner_z2 10 --force
You are about to recreate runner_z2/10
Detecting changes in deployment...

Releases
cf
  changed version:
    - 170
    + 171
Release version has changed: 170 -> 171
Are you sure you want to deploy this version? (type 'yes' to continue): yes

Compilation
No changes

Update
No changes

Resource pools
small_errand
  cloud_properties
    added availability_zone: us-east-1d
    added instance_type: m1.small
  added name: small_errand
  added network: cf1
  added size: 2
  stemcell
    added name: bosh-aws-xen-ubuntu
    added version: 2366
Stemcell update has been detected. Are you sure you want to update stemcells? (type 'yes' to continue): yes
Stemcell update seems to be inconsistent with current deployment. Please carefully review changes above.
Are you sure this configuration is correct? (type 'yes' to continue): yes

Networks
No changes

Jobs
acceptance_tests
  added instances: 1
  added lifecycle: errand
  added name: acceptance_tests
  added networks: [{""name""=>""cf1""}]
  added release: cf
  added resource_pool: small_errand
  added template: acceptance-tests
smoke_tests
  added instances: 1
  added lifecycle: errand
  added name: smoke_tests
  added networks: [{""name""=>""cf1""}]
  added release: cf
  added resource_pool: small_errand
  added template: smoke-tests

Properties
dea_next
  added staging_memory_limit_mb: 1024
smoke_tests
  added api: api.run.pivotal.io
  added apps_domain: [""cfapps.io""]
  added org: runtime-smoke-tests
  added password: sadaflja234uoiuflk542@Wfkjslk
  added space: runtime-smoke-tests
  added use_existing_org: true
  added user: cfaccounts+runtime-smoke-tests@pivotallabs.com

Meta
No changes

Recreate runner_z2/10? (type 'yes' to continue): yes

Performing `recreate runner_z2/10'...

Director task 40032
  Started preparing deployment
  Started preparing deployment > Binding deployment. Done (00:00:00)
  Started preparing deployment > Binding releases. Done (00:00:00)
  Started preparing deployment > Binding existing deployment. Done (00:00:03)
  Started preparing deployment > Binding resource pools. Done (00:00:00)
  Started preparing deployment > Binding stemcells. Done (00:00:00)
  Started preparing deployment > Binding templates. Done (00:00:00)
  Started preparing deployment > Binding properties. Done (00:00:00)
  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)
  Started preparing deployment > Binding instance networks. Done (00:00:00)
     Done preparing deployment (00:00:03)

  Started preparing package compilation > Finding packages to compile. Done (00:00:00)

  Started preparing dns > Binding DNS. Done (00:00:01)

  Started preparing configuration > Binding configuration. Done (00:00:06)

  Started updating job runner_z2
  Started updating job runner_z2 > runner_z2/10 (canary). Done (00:00:22)

Error 450001: Cannot stop job: Service Unavailable
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:428:in `rescue in process'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:417:in `process'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:265:in `process'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:250:in `process_long_running'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:177:in `block in process_in_thread'
<internal:prelude>:10:in `synchronize'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:175:in `process_in_thread'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2366.0/lib/bosh_agent/handler.rb:155:in `block in handle_message'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `call'
/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `block in spawn_threadpool'

Task 40032 error
```

In order to resolve this, we had to terminate the vm from the AWS console and then `bosh recreate runner_z2 10 --force` again. We would have preferred an option to terminate the VM and start over from scratch from within bosh, which was unable to recreate if it couldn't communicate with the agent.",,71284244,story,[],bosh should have an option to recreate vms within bosh even if jobs can't be stopped,,[],956238,1196628,feature,2014-06-18T20:59:27Z,https://www.pivotaltracker.com/story/show/71284244
,2014-06-17T03:08:15Z,unscheduled,,"This has come up at post mortems before, bosh really shouldn't report instances as failing when they're going through a healthy deploy. This noise causes us to not know when something has actually gone wrong.
",,73360712,story,[],Bosh reports instances as failing during a healthy deploy triggering many alerts,,[],956238,1210852,feature,2014-06-18T13:58:55Z,https://www.pivotaltracker.com/story/show/73360712
,2014-01-09T21:40:10Z,unscheduled,,"Related to #63572270

We got severals alerts via DataDog from the BOSH agents on the runner VMs while they were being updated",,63572422,story,[],BOSH agent should not report being unhealthy when it is being updated,,[],956238,5637,feature,2014-06-18T06:27:12Z,https://www.pivotaltracker.com/story/show/63572422
,2014-04-09T22:34:34Z,unscheduled,,"currently ```bosh create release -final --with-tarball``` creates a tarball but creates it locally while the various files are uploaded.  

the files themselves are not trustworthy for RelEng.  Uploading the tarball at the same time, gives them an artifact that has a single checksum that can be verified and trusted",,69226258,story,[],"for final release, add functionality to upload the tarball of the release",,[],956238,1210852,feature,2014-04-14T17:45:39Z,https://www.pivotaltracker.com/story/show/69226258
,2013-08-29T19:03:56Z,unscheduled,,"currently a user builds all the manifests up front (before microbosh was deployed) hence we need to be able to know uuid beforehand. 
possible solutions:
- specify microbosh guid in the manifest when it's deployed
- provide cli command to change it
- update user's manifests after it is deloyed (hmmm)",,56079684,story,"[{'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",User should be able to set microbosh director uuid to support specifying uuid in the manifest,,[],956238,81882,feature,2014-07-31T04:37:02Z,https://www.pivotaltracker.com/story/show/56079684
,2013-06-28T02:37:12Z,unscheduled,,we should fix the alerting and then mute this alert until BE deploys their router fixes,,52514773,story,"[{'name': 'production', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034566, 'updated_at': '2013-11-14T20:34:18Z'}]",system disk is full alert is not alerting us...,,[],956238,5637,bug,2014-04-01T23:57:37Z,https://www.pivotaltracker.com/story/show/52514773
,2013-12-20T17:50:20Z,unscheduled,,"Currently when you run `bosh create release` for the first time, bosh asks you what you want to call the release.  In the Cloud Foundry case this should always be 'cf', but we have to document this and people have to follow the directions.

Ideally, cf-rlease would specify this name, and `bosh create release` would generate a release named cf.  `bosh create release --name mks-fork-of-cf` would create a release named mks-fork-of-cf.

The alternate name functionality might not be necessary as I don't know of anyone using the ability to call a release something custom.",,62851864,story,[],"As a release developer, I should specify the default name of the release",,[],956238,46031,feature,2014-04-01T19:07:34Z,https://www.pivotaltracker.com/story/show/62851864
,2014-03-04T21:35:42Z,unscheduled,,,,66897670,story,[],Bosh agent should be able to respond to  prepare message,,[],956238,81882,bug,2014-04-02T22:37:27Z,https://www.pivotaltracker.com/story/show/66897670
,2014-02-11T01:32:07Z,unscheduled,,"in this case deleting unneeded instances grouping is incorrect:

  Started deleting unneeded instances                                                      #<-------------- start group
  Started deleting unneeded instances: i-1ad44734
  Started deleting unneeded instances: i-f5d340db
     Done deleting unneeded instances: i-f5d340db (00:01:26)
     Done deleting unneeded instances                                                      #<--------------- end group
     Done deleting unneeded instances: i-1ad44734 (00:01:26)           #<-- should show up before 'end group'

...
{""time"":1392081724,""stage"":""Preparing package compilation"",""tags"":[],""total"":null,""task"":""Finding packages to compile"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081724,""stage"":""Preparing package compilation"",""tags"":[],""total"":null,""task"":""Finding packages to compile"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081724,""stage"":""Preparing DNS"",""tags"":[],""total"":1,""task"":""Binding DNS"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081729,""stage"":""Preparing DNS"",""tags"":[],""total"":1,""task"":""Binding DNS"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081729,""stage"":""Creating bound missing VMs"",""tags"":[],""total"":1,""task"":""default/0"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081882,""stage"":""Creating bound missing VMs"",""tags"":[],""total"":1,""task"":""default/0"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081882,""stage"":""Binding instance VMs"",""tags"":[],""total"":1,""task"":""our_service_that_backs_up/0"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081883,""stage"":""Binding instance VMs"",""tags"":[],""total"":1,""task"":""our_service_that_backs_up/0"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081883,""stage"":""Preparing configuration"",""tags"":[],""total"":1,""task"":""Binding configuration"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081884,""stage"":""Preparing configuration"",""tags"":[],""total"":1,""task"":""Binding configuration"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081884,""stage"":""Deleting unneeded instances"",""tags"":[],""total"":2,""task"":""i-1ad44734"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081884,""stage"":""Deleting unneeded instances"",""tags"":[],""total"":2,""task"":""i-f5d340db"",""index"":2,""state"":""started"",""progress"":0}
{""time"":1392081970,""stage"":""Deleting unneeded instances"",""tags"":[],""total"":2,""task"":""i-f5d340db"",""index"":2,""state"":""finished"",""progress"":100}
{""time"":1392081970,""stage"":""Deleting unneeded instances"",""tags"":[],""total"":2,""task"":""i-1ad44734"",""index"":1,""state"":""finished"",""progress"":100}
{""time"":1392081970,""stage"":""Updating job"",""tags"":[""our_service_that_backs_up""],""total"":1,""task"":""our_service_that_backs_up/0 (canary)"",""index"":1,""state"":""started"",""progress"":0}
{""time"":1392081970,""stage"":""Updating job"",""tags"":[""our_service_that_backs_up""],""total"":1,""task"":""our_service_that_backs_up/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":12}
{""time"":1392081972,""stage"":""Updating job"",""tags"":[""our_service_that_backs_up""],""total"":1,""task"":""our_service_that_backs_up/0 (canary)"",""index"":1,""state"":""in_progress"",""progress"":25}
...",,65526210,story,[],bosh deploy should properly group start and end events,,[],956238,81882,bug,2014-04-01T15:07:44Z,https://www.pivotaltracker.com/story/show/65526210
,2014-02-21T18:50:30Z,unscheduled,,"possible unexpected heartbeat is unrelated.. need to confirm.

```
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/runner.rb:142:in `fetch_deployments'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/runner.rb:102:in `block in poll_director'
W, [2014-02-21T13:45:59.267139 #10327]  WARN : Received heartbeat from unmanaged agent: 2136ba97-52df-4910-8e2e-7d8b189841bc
F, [2014-02-21T13:45:59.436275 #10327] FATAL : expected string value for key value of member 1 of key dimensions of member 1 of option metric_data
F, [2014-02-21T13:45:59.436453 #10327] FATAL : /var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:93:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:346:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:342:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:342:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:245:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:243:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:243:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:346:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:342:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:342:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:245:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:243:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:243:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:587:in `block in validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:583:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:583:in `validate'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/option_grammar.rb:598:in `request_params'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/query_request_builder.rb:37:in `populate_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:635:in `block (2 levels) in define_client_method'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:524:in `build_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:455:in `block (3 levels) in client_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/response.rb:168:in `call'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/response.rb:168:in `build_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/response.rb:108:in `initialize'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:184:in `new'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:184:in `new_response'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:454:in `block (2 levels) in client_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:355:in `log_client_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:441:in `block in client_request'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:337:in `return_or_raise'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:440:in `client_request'
(eval):3:in `put_metric_data'
/var/vcap/packages/health_monitor/gem_home/gems/aws-sdk-1.8.5/lib/aws/cloud_watch.rb:99:in `put_metric_data'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/plugins/cloud_watch.rb:19:in `process'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/event_processor.rb:104:in `plugin_process'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/event_processor.rb:50:in `block in process'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `block in each'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each_key'
/var/vcap/packages/ruby/lib/ruby/1.9.1/set.rb:222:in `each'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/event_processor.rb:49:in `process'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/agent_manager.rb:284:in `on_heartbeat'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/agent_manager.rb:252:in `process_event'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/agent_manager.rb:52:in `block in setup_events'
/var/vcap/packages/health_monitor/gem_home/gems/nats-0.4.28/lib/nats/client.rb:475:in `call'
/var/vcap/packages/health_monitor/gem_home/gems/nats-0.4.28/lib/nats/client.rb:475:in `on_msg'
/var/vcap/packages/health_monitor/gem_home/gems/nats-0.4.28/lib/nats/client.rb:529:in `receive_data'
/var/vcap/packages/health_monitor/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run_machine'
/var/vcap/packages/health_monitor/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/lib/bosh/monitor/runner.rb:26:in `run'
/var/vcap/packages/health_monitor/gem_home/gems/bosh-monitor-1.0000.0/bin/bosh-monitor:28:in `<top (required)>'
/var/vcap/packages/health_monitor/bin/bosh-monitor:23:in `load'
/var/vcap/packages/health_monitor/bin/bosh-monitor:23:in `<main>'
I, [2014-02-21T13:45:59.436584 #10327]  INFO : HealthMonitor shutting down...
I, [2014-02-21T13:46:05.429257 #10377]  INFO : HealthMonitor starting...
```",,66246414,story,"[{'name': 'hm', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:32Z', 'id': 12049666, 'updated_at': '2015-06-24T22:38:32Z'}]",health monitor should not exit the process when it receives heartbeat from an unexpected agent,,[],956238,81882,bug,2015-06-24T22:40:31Z,https://www.pivotaltracker.com/story/show/66246414
,2014-02-04T17:30:03Z,unscheduled,,"When deploying 155 to prod, we found a package under 147 that had an incorrect SHA1 in the director database.  We had to delete and re-upload that release in order to proceed with the deployment.

It would have been great if we had a tool that would download all of the blobs, re-calculate their checksums, and compare with what's in the DB.  We don't know if the prod DB is currently correct for the rest of the uploaded packages, etc.",,65123088,story,[],"As a deploy operator, I would like to check the consistency of the bosh director database",,[],956238,14062,feature,2014-04-01T15:23:03Z,https://www.pivotaltracker.com/story/show/65123088
,2013-12-09T14:55:22Z,unscheduled,,"Came across this on a deploy to Tabasco and A1.  HM9000 has ~8 different processes that are monitored by monit.  I removed one by deleting its entry from the monit script and removing it's associated _ctl file.  The deploy went successfully and BOSH succesfully updated the monit script (so monit was no longer monitoring said process), but left the process running (with nothing monitoring it!)",,62152150,story,[],"When I remove a process from a job's monit script, Bosh does not stop that process -- it simply leaves the process unmonitored.",,[],956238,121297,bug,2015-06-03T16:42:07Z,https://www.pivotaltracker.com/story/show/62152150
,2013-11-06T18:26:38Z,unscheduled,,"We have migrations for job names in Pivotal CF. When you migrate a job - and change its name, the underlying job according to BOSH VMs shows the old name-guid. This will be confusing if someone goes into the BOSH CLI and looks at what the installation consists of - comparing it to what Pivotal CF Status displays.

There are two parts of this story. The first is that Pivotal CF can instruct BOSH to change the job ID. The second is that a changed job ID will present itself back to Pivotal CF once the ID has been changed / cascaded through all of the BOSH pieces / parts (Director, Agents, etc.)",,60270100,story,"[{'name': 'director', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-06-24T22:38:23Z', 'id': 12049664, 'updated_at': '2015-06-24T22:38:23Z'}, {'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}, {'name': 'pcf', 'project_id': 956238, 'kind': 'label', 'created_at': '2013-11-14T20:34:18Z', 'id': 7034572, 'updated_at': '2013-11-19T04:57:55Z'}]",BOSH should rename jobs when product version migrates job names,,[],956238,548715,feature,2015-06-24T22:40:45Z,https://www.pivotaltracker.com/story/show/60270100
,2013-11-20T07:26:57Z,unscheduled,,"I realized that changing the IP address of the internal DNS server may cause an entire system down.
I found a workaround to solve this problem as well, so share it for somebody accidentally released the floating IP address for his/her PowerDNS server.
(The problem might be a bug introduced by the CloudStack CPI, however, I assume that the OpenStack CPI and probably the AWS CPI have the same problem)

If you update your full bosh deployment with a new IP address for PowerDNS, that makes all BOSH agents down. The `bosh vms` shows all VMs as 'unknown/unknown | unresponsive agent' and you can do nothing with existing deployment. The VMs is still keeping the old DNS server address and agents on them cannot reconnect to the NATS server due to a name lookup error.

In this case, `bosh cloudcheck` does not work to recover the error. Both rebooting VMs and recreating VMs are useless.

Rebooting VMs:
  * Agents cannot resolve the domain name of the registry server because 'userdata' given by the meta-data server has the old DNS address
  * Local cache at /var/vcap/bosh/settings.json has the old DNS address as well
  * Agents cannot start

Recreating VMs:
  * Director uses the latest succeeded properties (probably stored in the vms table) which have the old DNS address
  * Created VMs will have the old DNS address in userdata saved by the director
  * Same as the above

To save your agents, you need to manually update the dns server address in '/var/vcap/bosh/settings.json' at every VMs created by the director.

I think this is not a serious bug at this moment. However we should have some measures to safely change the DNS IP address in the feature.

Thanks,
Yudai",8.0,61101360,story,[],changing bosh dns ip should change the agent configurations,,[],956238,494053,feature,2014-01-08T21:55:07Z,https://www.pivotaltracker.com/story/show/61101360
,2013-11-23T00:22:42Z,unscheduled,,,,61327868,story,"[{'name': 'dk', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-07-28T17:41:13Z', 'id': 9057098, 'updated_at': '2014-07-28T17:41:13Z'}]",Add hvc console on all stemcells,637633.0,[637633],956238,637633,feature,2014-08-05T05:08:08Z,https://www.pivotaltracker.com/story/show/61327868
,2013-10-17T20:48:29Z,unscheduled,,"Is this the behavior we want?

bosh releases ; echo $? ; bosh stemcells  ; echo $? ; bosh deployments ; echo $?
No releases
1
No stemcells
1
No deployments
1",,59082126,story,"[{'name': 'cli', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-04-01T17:34:48Z', 'id': 8096418, 'updated_at': '2014-04-01T17:34:48Z'}]",bosh_cli returns 1 on 'bosh releases' when there is no release,,[],956238,637633,feature,2015-06-24T22:40:22Z,https://www.pivotaltracker.com/story/show/59082126
,2013-10-18T17:54:59Z,unscheduled,,"verification time = canary time and update time, etc.",,59146642,story,[],bosh should allow to deploy job and skip verification time to make feedback cycle quicker when release makers make new jobs,,[],956238,81882,feature,2014-04-01T16:30:21Z,https://www.pivotaltracker.com/story/show/59146642
,2015-04-14T17:38:19Z,unscheduled,,,,92455766,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",install sublime,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92455766
,2015-03-24T19:04:55Z,unscheduled,,until we resolve #90209450 lets just not sprout install the go projects and let people install where they think best,,91037340,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Stop incorrectly checking out go projects into ~/workspace without correct go directories,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/91037340
,2015-04-14T17:24:39Z,unscheduled,,,,92454336,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]","no such darcularge font profile in RubyMine and intelij, only darcula
",,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92454336
,2015-03-10T00:22:44Z,unscheduled,,"the repo recipe should be able to be done using the git-projects

There is no reason for the bosh team to maintain private chef recipes for this project",,89972062,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",remove cf-bosh custom recipes,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/89972062
,2015-04-14T17:53:25Z,unscheduled,,,,92457060,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",^^ Sprout - Easy ^^  ------------  vv Sprout - Hard vv,,[],956238,119,release,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92457060
,2015-04-14T17:48:20Z,unscheduled,,,,92456590,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",install rspec-awesome plugin for RubyMine.,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92456590
,2015-04-14T17:47:57Z,unscheduled,,i've never been able to figure out how to fix...,,92456560,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",option-/ doesn't do cyclic complete thingy,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92456560
,2015-04-14T17:47:20Z,unscheduled,,,,92456526,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",change Mac scroll bar visibility preference to always.,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92456526
,2015-04-14T17:42:37Z,unscheduled,,,,92456152,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",configure iterm scroll buffer to 11,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92456152
,2015-04-14T17:37:34Z,unscheduled,,see tasks,,92455698,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",pre-install some gems in ruby 2.1.5,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92455698
,2015-04-14T17:36:56Z,unscheduled,,"
install bosh from local checkout of the head of the develop branch in ~/workspace/bosh ",,92455648,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",install HEAD of bosh gem 2.1.5 ruby,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92455648
,2015-04-14T17:26:44Z,unscheduled,,should symlink to the working bosh-init in ~/go,,92454476,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Add bosh-init executable to the PATH ,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/92454476
,2015-04-14T17:24:02Z,unscheduled,,"open finder window go to: Preferences -> General -> 'New Finder windows show:' choose 'pivotal'

open finder window go to: Preferences -> Sidebar.  Uncheck 
- `All My FIles`
- `iCloud Drive`
- `Hard disks`
- `CD's, DVD's, and iPods`
- `Recent Tags`",,92454292,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]","finder: remove ‘all my files” from sidebar _and_ also remove as startpage
",,[],956238,119,chore,2015-07-31T23:24:13Z,https://www.pivotaltracker.com/story/show/92454292
,2015-03-12T22:33:18Z,unscheduled,,,,90239920,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Install bundler in 2.1.5,,[],956238,344,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/90239920
,2015-03-09T23:58:56Z,unscheduled,,,,89970764,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Get 'workspace' and users home folder into 'Favorites' section of Finder,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/89970764
,2015-03-09T23:51:00Z,unscheduled,,"see and send pull request towards issue https://github.com/pivotal/pivotal_ide_prefs/issues/14

we want to automate changing (Preferences -> Version Control -> Git -> SSH Executable) from 'Built-In' to 'Native'",,89970316,story,"[{'name': 'sprout', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-03-09T23:50:59Z', 'id': 11066638, 'updated_at': '2015-03-09T23:50:59Z'}]",Setup Idea and RubyMine to use ssh executable for git by default,,[],956238,119,chore,2015-04-14T18:19:19Z,https://www.pivotaltracker.com/story/show/89970316
,2015-07-13T19:45:23Z,unscheduled,,,,98978642,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",--- stemcell hardening stories below ---,,[],956238,81882,release,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978642
,2015-07-13T19:42:40Z,unscheduled,,"Legitimate device files should only exist in the /dev directory. NFS mounts should not present device files to users.
---
None
---
SV-50453r2_rule
---
F-43601r1_fix
---
Add the ""nodev"" option to the fourth column of ""/etc/fstab"" for the line which controls mounting of any NFS mounts.
---
C-46212r2_chk
---
To verify the ""nodev"" option is configured for all NFS mounts, run the following command: 

$ mount | grep ""nfs ""

All NFS mounts should show the ""nodev"" setting in parentheses, along with other mount options. 
If the setting does not show, this is a finding.",1.0,98978104,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38652] [medium] Remote file systems must be mounted with the nodev option.,,[],956238,81882,feature,2015-10-29T21:08:32Z,https://www.pivotaltracker.com/story/show/98978104
,2015-07-13T19:42:40Z,unscheduled,,"NFS mounts should not present suid binaries to users. Only vendor-supplied suid executables should be installed to their default location on the local filesystem.
---
None
---
SV-50455r2_rule
---
F-43603r1_fix
---
Add the ""nosuid"" option to the fourth column of ""/etc/fstab"" for the line which controls mounting of any NFS mounts.
---
C-46214r3_chk
---
To verify the ""nosuid"" option is configured for all NFS mounts, run the following command: 

$ mount | grep nfs

All NFS mounts should show the ""nosuid"" setting in parentheses, along with other mount options. 
If the setting does not show, this is a finding.",1.0,98978108,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38654] [medium] Remote file systems must be mounted with the nosuid option.,,[],956238,81882,feature,2015-10-29T21:08:30Z,https://www.pivotaltracker.com/story/show/98978108
,2015-07-13T19:42:40Z,unscheduled,,"USB storage devices such as thumb drives can be used to introduce unauthorized software and other vulnerabilities. Support for these devices should be disabled and the devices themselves should be tightly controlled.
---
None
---
SV-50291r3_rule
---
F-43437r3_fix
---
To prevent USB storage devices from being used, configure the kernel module loading system to prevent automatic loading of the USB storage driver. To configure the system to prevent the ""usb-storage"" kernel module from being loaded, add the following line to a file in the directory ""/etc/modprobe.d"": 

install usb-storage /bin/true

This will prevent the ""modprobe"" program from loading the ""usb-storage"" module, but will not prevent an administrator (or another program) from using the ""insmod"" program to load the module manually.
---
C-46047r3_chk
---
If the system is configured to prevent the loading of the ""usb-storage"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r usb-storage /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978110,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38490] [medium] The operating system must enforce requirements for the connection of mobile devices to operating systems.,,[],956238,81882,feature,2015-10-29T21:08:17Z,https://www.pivotaltracker.com/story/show/98978110
,2015-07-13T19:42:39Z,unscheduled,,"The ""ip6tables"" service provides the system's host-based firewalling capability for IPv6 and ICMPv6.
---
None
---
SV-50352r3_rule
---
F-43499r2_fix
---
The ""ip6tables"" service can be enabled with the following commands: 

# chkconfig ip6tables on
# service ip6tables start
---
C-46109r3_chk
---
If the system is a cross-domain system, this is not applicable.

If IPV6 is disabled, this is not applicable.

Run the following command to determine the current status of the ""ip6tables"" service: 

# service ip6tables status

If the service is not running, it should return the following: 

ip6tables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978086,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38551] [medium] The operating system must connect to external networks or information systems only through managed IPv6 interfaces consisting of boundary protection devices arranged in accordance with an organizational security architecture.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978086
,2015-07-13T19:42:39Z,unscheduled,,"Setting the SELinux state to enforcing ensures SELinux is able to confine potentially compromised processes to the security policy, which is designed to prevent them from causing damage to the system or further elevating their privileges. 
---
None
---
SV-65573r1_rule
---
F-56165r1_fix
---
The SELinux state should be set to ""enforcing"" at system boot time. In the file ""/etc/selinux/config"", add or correct the following line to configure the system to boot into enforcing mode:

SELINUX=enforcing
---
C-53703r1_chk
---
Check the file ""/etc/selinux/config"" and ensure the following line appears:

SELINUX=enforcing

If SELINUX is not set to enforcing, this is a finding. ",1.0,98978088,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-51363] [medium] The system must use a Linux Security Module configured to enforce limits on system services.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978088
,2015-07-13T19:42:39Z,unscheduled,,"The addition/removal of kernel modules can be used to alter the behavior of the kernel and potentially introduce malicious code into kernel space. It is important to have an audit trail of modules that have been introduced into the kernel.
---
None
---
SV-50381r2_rule
---
F-43528r2_fix
---
Add the following to ""/etc/audit/audit.rules"" in order to capture kernel module loading and unloading events, setting ARCH to either b32 or b64 as appropriate for your system: 

-w /sbin/insmod -p x -k modules
-w /sbin/rmmod -p x -k modules
-w /sbin/modprobe -p x -k modules
-a always,exit -F arch=[ARCH] -S init_module -S delete_module -k modules
---
C-46138r3_chk
---
To determine if the system is configured to audit execution of module management programs, run the following commands:

$ sudo egrep -e ""(-w |-F path=)/sbin/insmod"" /etc/audit/audit.rules
$ sudo egrep -e ""(-w |-F path=)/sbin/rmmod"" /etc/audit/audit.rules
$ sudo egrep -e ""(-w |-F path=)/sbin/modprobe"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line.

To determine if the system is configured to audit calls to the ""init_module"" system call, run the following command:

$ sudo grep -w ""init_module"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

To determine if the system is configured to audit calls to the ""delete_module"" system call, run the following command:

$ sudo grep -w ""delete_module"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

If no line is returned for any of these commands, this is a finding. ",1.0,98978080,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38580] [medium] The audit system must be configured to audit the loading and unloading of dynamic kernel modules.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978080
,2015-07-13T19:42:41Z,unscheduled,,"Taking appropriate action in case of disk errors will minimize the possibility of losing audit records.
---
None
---
SV-50264r1_rule
---
F-43410r1_fix
---
Edit the file ""/etc/audit/auditd.conf"". Modify the following line, substituting [ACTION] appropriately: 

disk_error_action = [ACTION]

Possible values for [ACTION] are described in the ""auditd.conf"" man page. These include: 

""ignore""
""syslog""
""exec""
""suspend""
""single""
""halt""


Set this to ""syslog"", ""exec"", ""single"", or ""halt"".
---
C-46020r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to take appropriate action when disk errors occur:

# grep disk_error_action /etc/audit/auditd.conf
disk_error_action = [ACTION]


If the system is configured to ""suspend"" when disk errors occur or ""ignore"" them, this is a finding.",1.0,98978148,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38464] [medium] The audit system must take appropriate action when there are disk errors on the audit storage volume.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978148
,2015-07-13T19:42:40Z,unscheduled,,"Permissions on audit binaries and configuration files that are too generous could allow an unauthorized user to gain privileges that they should not have. The permissions set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50464r1_rule
---
F-43612r1_fix
---
The RPM package management system can restore file access permissions of the audit package files and directories. The following command will update audit files with permissions different from what is expected by the RPM database: 

# rpm --setperms audit
---
C-46223r1_chk
---
The following command will list which audit files on the system have permissions different from what is expected by the RPM database: 

# rpm -V audit | grep '^.M'

If there is any output, for each file or directory found, compare the RPM-expected permissions with the permissions on the file or directory:

# rpm -q --queryformat ""[%{FILENAMES} %{FILEMODES:perms}\n]"" audit | grep  [filename]
# ls -lL [filename]

If the existing permissions are more permissive than those expected by RPM, this is a finding.",1.0,98978140,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38663] [medium] The system package management tool must verify permissions on all files and directories associated with the audit package.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978140
,2015-07-13T19:42:40Z,unscheduled,,"Ownership of audit binaries and configuration files that is incorrect could allow an unauthorized user to gain privileges that they should not have. The ownership set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50465r1_rule
---
F-43613r1_fix
---
The RPM package management system can restore file ownership of the audit package files and directories. The following command will update audit files with ownership different from what is expected by the RPM database: 

# rpm --setugids audit
---
C-46224r1_chk
---
The following command will list which audit files on the system have ownership different from what is expected by the RPM database: 

# rpm -V audit | grep '^.....U'


If there is output, this is a finding.",1.0,98978134,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38664] [medium] The system package management tool must verify ownership on all files and directories associated with the audit package.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978134
,2015-07-13T19:42:40Z,unscheduled,,"Group-ownership of audit binaries and configuration files that is incorrect could allow an unauthorized user to gain privileges that they should not have. The group-ownership set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50466r1_rule
---
F-43614r1_fix
---
The RPM package management system can restore file group-ownership of the audit package files and directories. The following command will update audit files with group-ownership different from what is expected by the RPM database: 

# rpm --setugids audit
---
C-46225r1_chk
---
The following command will list which audit files on the system have group-ownership different from what is expected by the RPM database: 

# rpm -V audit | grep '^......G'


If there is output, this is a finding.",1.0,98978132,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38665] [medium] The system package management tool must verify group-ownership on all files and directories associated with the audit package.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978132
,2015-07-13T19:42:41Z,unscheduled,,"Taking appropriate action in case of a filled audit storage volume will minimize the possibility of losing audit records.
---
None
---
SV-50268r1_rule
---
F-43413r1_fix
---
The ""auditd"" service can be configured to take an action when disk space starts to run low. Edit the file ""/etc/audit/auditd.conf"". Modify the following line, substituting [ACTION] appropriately: 

disk_full_action = [ACTION]

Possible values for [ACTION] are described in the ""auditd.conf"" man page. These include: 

""ignore""
""syslog""
""exec""
""suspend""
""single""
""halt""


Set this to ""syslog"", ""exec"", ""single"", or ""halt"".
---
C-46023r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to take appropriate action when the audit storage volume is full:

# grep disk_full_action /etc/audit/auditd.conf
disk_full_action = [ACTION]


If the system is configured to ""suspend"" when the volume is full or ""ignore"" that it is full, this is a finding.",1.0,98978156,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38468] [medium] The audit system must take appropriate action when the audit storage volume is full.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978156
,2015-07-13T19:42:41Z,unscheduled,,"If users can write to audit logs, audit trails can be modified or destroyed.
---
None
---
SV-50299r1_rule
---
F-43445r1_fix
---
Change the mode of the audit log files with the following command: 

# chmod 0640 [audit_file]
---
C-46055r1_chk
---
Run the following command to check the mode of the system audit logs: 

grep ""^log_file"" /etc/audit/auditd.conf|sed s/^[^\/]*//|xargs stat -c %a:%n

Audit logs must be mode 0640 or less permissive. 
If any are more permissive, this is a finding.",1.0,98978160,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38498] [medium] Audit log files must have mode 0640 or less permissive.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978160
,2015-07-13T19:42:41Z,unscheduled,,"If users can delete audit logs, audit trails can be modified or destroyed.
---
None
---
SV-50294r1_rule
---
F-43440r1_fix
---
Change the mode of the audit log directories with the following command: 

# chmod go-w [audit_directory]
---
C-46050r1_chk
---
Run the following command to check the mode of the system audit directories: 

grep ""^log_file"" /etc/audit/auditd.conf|sed 's/^[^/]*//; s/[^/]*$//'|xargs stat -c %a:%n

Audit directories must be mode 0755 or less permissive. 
If any are more permissive, this is a finding.",1.0,98978166,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38493] [medium] Audit log directories must have mode 0755 or less permissive.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978166
,2015-07-13T19:42:42Z,unscheduled,,"If non-privileged users can write to audit logs, audit trails can be modified or destroyed.
---
None
---
SV-50245r1_rule
---
F-43390r1_fix
---
Change the group owner of the audit log files with the following command: 

# chgrp root [audit_file]
---
C-46000r1_chk
---
Run the following command to check the group owner of the system audit logs: 

grep ""^log_file"" /etc/audit/auditd.conf|sed s/^[^\/]*//|xargs stat -c %G:%n

Audit logs must be group-owned by root. 
If they are not, this is a finding.",1.0,98978198,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38445] [medium] Audit log files must be group-owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978198
,2015-07-13T19:42:44Z,unscheduled,,"If non-privileged users can write to audit logs, audit trails can be modified or destroyed.
---
None
---
SV-50296r1_rule
---
F-43443r1_fix
---
Change the owner of the audit log files with the following command: 

# chown root [audit_file]
---
C-46053r1_chk
---
Run the following command to check the owner of the system audit logs: 

grep ""^log_file"" /etc/audit/auditd.conf|sed s/^[^\/]*//|xargs stat -c %U:%n

Audit logs must be owned by root. 
If they are not, this is a finding.",1.0,98978292,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38495] [medium] Audit log files must be owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978292
,2015-07-13T19:42:46Z,unscheduled,,"The total storage for audit log files must be large enough to retain log information over the period required. This is a function of the maximum log file size and the number of logs retained.
---
None
---
SV-50437r1_rule
---
F-43585r1_fix
---
Determine how many log files ""auditd"" should retain when it rotates logs. Edit the file ""/etc/audit/auditd.conf"". Add or modify the following line, substituting [NUMLOGS] with the correct value: 

num_logs = [NUMLOGS]

Set the value to 5 for general-purpose systems. Note that values less than 2 result in no log rotation.
---
C-46195r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine how many logs the system is configured to retain after rotation: ""# grep num_logs /etc/audit/auditd.conf"" 

num_logs = 5


If the overall system log file(s) retention hasn't been properly set up, this is a finding.",1.0,98978356,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38636] [medium] The system must retain enough rotated audit logs to cover the required log retention period.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978356
,2015-07-13T19:42:46Z,unscheduled,,"The hash on important files like audit system executables should match the information given by the RPM database. Audit executables  with erroneous hashes could be a sign of nefarious activity on the system.
---
None
---
SV-50438r2_rule
---
F-43586r1_fix
---
The RPM package management system can check the hashes of audit system package files. Run the following command to list which audit files on the system have hashes that differ from what is expected by the RPM database: 

# rpm -V audit | grep '^..5'

A ""c"" in the second column indicates that a file is a configuration file, which may appropriately be expected to change. If the file that has changed was not expected to then refresh from distribution media or online repositories. 

rpm -Uvh [affected_package]

OR 

yum reinstall [affected_package]
---
C-46196r3_chk
---
The following command will list which audit files on the system have file hashes different from what is expected by the RPM database. 

# rpm -V audit | awk '$1 ~ /..5/ && $2 != ""c""'


If there is output, this is a finding.",1.0,98978358,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38637] [medium] The system package management tool must verify contents of all files associated with the audit package.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978358
,2015-07-13T19:42:46Z,unscheduled,,"Automatically rotating logs (by setting this to ""rotate"") minimizes the chances of the system unexpectedly running out of disk space by being overwhelmed with log data. However, for systems that must never discard log data, or which use external processes to transfer it and reclaim space, ""keep_logs"" can be employed.
---
None
---
SV-50435r1_rule
---
F-43583r1_fix
---
The default action to take when the logs reach their maximum size is to rotate the log files, discarding the oldest one. To configure the action taken by ""auditd"", add or correct the line in ""/etc/audit/auditd.conf"": 

max_log_file_action = [ACTION]

Possible values for [ACTION] are described in the ""auditd.conf"" man page. These include: 

""ignore""
""syslog""
""suspend""
""rotate""
""keep_logs""


Set the ""[ACTION]"" to ""rotate"" to ensure log rotation occurs. This is the default. The setting is case-insensitive.
---
C-46193r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to rotate logs when they reach their maximum size: ""# grep max_log_file_action /etc/audit/auditd.conf"" 

max_log_file_action ""rotate""


If the system has not been properly set up to rotate audit logs, this is a finding.",1.0,98978360,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38634] [medium] The system must rotate audit log files that reach the maximum file size.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978360
,2015-07-13T19:42:46Z,unscheduled,,"The total storage for audit log files must be large enough to retain log information over the period required. This is a function of the maximum log file size and the number of logs retained.
---
None
---
SV-50434r1_rule
---
F-43582r1_fix
---
Determine the amount of audit data (in megabytes) which should be retained in each log file. Edit the file ""/etc/audit/auditd.conf"". Add or modify the following line, substituting the correct value for [STOREMB]: 

max_log_file = [STOREMB]

Set the value to ""6"" (MB) or higher for general-purpose systems. Larger values, of course, support retention of even more audit data.
---
C-46192r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine how much data the system will retain in each audit log file: ""# grep max_log_file /etc/audit/auditd.conf"" 

max_log_file = 6


If the system audit data threshold hasn't been properly set up, this is a finding.",1.0,98978364,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38633] [medium] The system must set a maximum audit log file size.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978364
,2015-07-13T19:42:42Z,unscheduled,,"Notifying administrators of an impending disk space problem may allow them to take corrective action prior to any disruption.
---
None
---
SV-50270r2_rule
---
F-43415r2_fix
---
The ""auditd"" service can be configured to take an action when disk space starts to run low. Edit the file ""/etc/audit/auditd.conf"". Modify the following line, substituting [ACTION] appropriately: 

space_left_action = [ACTION]

Possible values for [ACTION] are described in the ""auditd.conf"" man page. These include: 

""ignore""
""syslog""
""email""
""exec""
""suspend""
""single""
""halt""


Set this to ""email"" (instead of the default, which is ""suspend"") as it is more likely to get prompt attention.  The ""syslog"" option is acceptable, provided the local log management infrastructure notifies an appropriate administrator in a timely manner.

RHEL-06-000521 ensures that the email generated through the operation ""space_left_action"" will be sent to an administrator.
---
C-46025r3_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to email the administrator when disk space is starting to run low: 

# grep space_left_action /etc/audit/auditd.conf
space_left_action = email


If the system is not configured to send an email to the system administrator when disk space is starting to run low, this is a finding.  The ""syslog"" option is acceptable when it can be demonstrated that the local log management infrastructure notifies an appropriate administrator in a timely manner.",1.0,98978190,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38470] [medium] The audit system must alert designated staff members when the audit storage volume approaches capacity.,,[],956238,81882,feature,2015-07-14T00:49:33Z,https://www.pivotaltracker.com/story/show/98978190
,2015-07-13T19:42:42Z,unscheduled,,"Email sent to the root account is typically aliased to the administrators of the system, who can take appropriate action.
---
None
---
SV-50481r1_rule
---
F-43629r1_fix
---
The ""auditd"" service can be configured to send email to a designated account in certain situations. Add or correct the following line in ""/etc/audit/auditd.conf"" to ensure that administrators are notified via email for those situations: 

action_mail_acct = root
---
C-46241r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to send email to an account when it needs to notify an administrator: 

action_mail_acct = root


If auditd is not configured to send emails per identified actions, this is a finding.",1.0,98978222,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38680] [medium] The audit system must identify staff members to receive notifications of audit log storage volume capacity issues.,,[],956238,81882,feature,2015-07-14T00:50:14Z,https://www.pivotaltracker.com/story/show/98978222
,2015-07-13T19:42:35Z,unscheduled,,"Arbitrary changes to the system time can be used to obfuscate nefarious activities in log files, as well as to confuse network services that are highly dependent upon an accurate system time (such as sshd). All changes to the system time should be audited.
---
None
---
SV-50328r3_rule
---
F-43475r2_fix
---
On a 32-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b32 -S clock_settime -k audit_time_rules

On a 64-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b64 -S clock_settime -k audit_time_rules

The -k option allows for the specification of a key in string form that can be used for better reporting capability through ausearch and aureport. Multiple system calls can be defined on the same line to save space if desired, but is not required. See an example of multiple combined syscalls: 

-a always,exit -F arch=b64 -S adjtimex -S settimeofday -S clock_settime -k audit_time_rules
---
C-46085r2_chk
---
To determine if the system is configured to audit calls to the ""clock_settime"" system call, run the following command:

$ sudo grep -w ""clock_settime"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

If the system is not configured to audit time changes, this is a finding. ",1.0,98977914,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38527] [low] The audit system must be configured to audit all attempts to alter system time through clock_settime.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977914
,2015-07-13T19:42:35Z,unscheduled,,"Arbitrary changes to the system time can be used to obfuscate nefarious activities in log files, as well as to confuse network services that are highly dependent upon an accurate system time (such as sshd). All changes to the system time should be audited.
---
None
---
SV-50326r4_rule
---
F-43473r4_fix
---
On a 32-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b32 -S stime -k audit_time_rules

On a 64-bit system, the ""-S stime"" is not necessary. The -k option allows for the specification of a key in string form that can be used for better reporting capability through ausearch and aureport. Multiple system calls can be defined on the same line to save space if desired, but is not required. See an example of multiple combined syscalls: 

-a always,exit -F arch=b64 -S adjtimex -S settimeofday -S clock_settime -k audit_time_rules
---
C-46083r3_chk
---
If the system is 64-bit only, this is not applicable.

To determine if the system is configured to audit calls to the ""stime"" system call, run the following command:

$ sudo grep -w ""stime"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

If the system is not configured to audit time changes, this is a finding. ",1.0,98977916,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38525] [low] The audit system must be configured to audit all attempts to alter system time through stime.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977916
,2015-07-13T19:42:35Z,unscheduled,,"Arbitrary changes to the system time can be used to obfuscate nefarious activities in log files, as well as to confuse network services that are highly dependent upon an accurate system time (such as sshd). All changes to the system time should be audited.
---
None
---
SV-50323r3_rule
---
F-43470r2_fix
---
On a 32-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b32 -S settimeofday -k audit_time_rules

On a 64-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b64 -S settimeofday -k audit_time_rules

The -k option allows for the specification of a key in string form that can be used for better reporting capability through ausearch and aureport. Multiple system calls can be defined on the same line to save space if desired, but is not required. See an example of multiple combined syscalls: 

-a always,exit -F arch=b64 -S adjtimex -S settimeofday -S clock_settime -k audit_time_rules
---
C-46080r2_chk
---
To determine if the system is configured to audit calls to the ""settimeofday"" system call, run the following command:

$ sudo grep -w ""settimeofday"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

If the system is not configured to audit time changes, this is a finding. ",1.0,98977918,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38522] [low] The audit system must be configured to audit all attempts to alter system time through settimeofday.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977918
,2015-07-13T19:42:36Z,unscheduled,,"Placing ""/var/log/audit"" in its own partition enables better separation between audit files and other files, and helps ensure that auditing cannot be halted due to the partition running out of space.
---
None
---
SV-50267r1_rule
---
F-43412r1_fix
---
Audit logs are stored in the ""/var/log/audit"" directory. Ensure that it has its own partition or logical volume at installation time, or migrate it later using LVM. Make absolutely certain that it is large enough to store all audit logs that will be created by the auditing daemon.
---
C-46022r1_chk
---
Run the following command to determine if ""/var/log/audit"" is on its own partition or logical volume: 

$ mount | grep ""on /var/log/audit ""

If ""/var/log/audit"" has its own partition or volume group, a line will be returned. 
If no line is returned, this is a finding.",1.0,98977932,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38467] [low] The system must use a separate file system for the system audit data path.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977932
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50353r3_rule
---
F-43500r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fchown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fchown -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fchown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fchown -F auid=0 -k perm_mod
---
C-46110r2_chk
---
To determine if the system is configured to audit calls to the ""fchown"" system call, run the following command:

$ sudo grep -w ""fchown"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977940,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38552] [low] The audit system must be configured to audit all discretionary access control permission modifications using fchown.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977940
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50351r3_rule
---
F-43498r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fchmodat -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fchmodat -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fchmodat -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fchmodat -F auid=0 -k perm_mod
---
C-46108r2_chk
---
To determine if the system is configured to audit calls to the ""fchmodat"" system call, run the following command:

$ sudo grep -w ""fchmodat"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977942,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38550] [low] The audit system must be configured to audit all discretionary access control permission modifications using fchmodat.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977942
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50358r3_rule
---
F-43505r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fsetxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fsetxattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fsetxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fsetxattr -F auid=0 -k perm_mod
---
C-46115r2_chk
---
To determine if the system is configured to audit calls to the ""fsetxattr"" system call, run the following command:

$ sudo grep -w ""fsetxattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977944,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38557] [low] The audit system must be configured to audit all discretionary access control permission modifications using fsetxattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977944
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50357r3_rule
---
F-43504r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fremovexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fremovexattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fremovexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fremovexattr -F auid=0 -k perm_mod
---
C-46114r2_chk
---
To determine if the system is configured to audit calls to the ""fremovexattr"" system call, run the following command:

$ sudo grep -w ""fremovexattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977948,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38556] [low] The audit system must be configured to audit all discretionary access control permission modifications using fremovexattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977948
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50355r3_rule
---
F-43502r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fchownat -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fchownat -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fchownat -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fchownat -F auid=0 -k perm_mod
---
C-46112r2_chk
---
To determine if the system is configured to audit calls to the ""fchownat"" system call, run the following command:

$ sudo grep -w ""fchownat"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977950,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38554] [low] The audit system must be configured to audit all discretionary access control permission modifications using fchownat.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977950
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50360r3_rule
---
F-43507r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S lremovexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S lremovexattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S lremovexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S lremovexattr -F auid=0 -k perm_mod
---
C-46117r2_chk
---
To determine if the system is configured to audit calls to the ""lremovexattr"" system call, run the following command:

$ sudo grep -w ""lremovexattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977952,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38559] [low] The audit system must be configured to audit all discretionary access control permission modifications using lremovexattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977952
,2015-07-13T19:42:36Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50359r3_rule
---
F-43506r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S lchown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S lchown -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S lchown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S lchown -F auid=0 -k perm_mod
---
C-46116r2_chk
---
To determine if the system is configured to audit calls to the ""lchown"" system call, run the following command:

$ sudo grep -w ""lchown"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977954,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38558] [low] The audit system must be configured to audit all discretionary access control permission modifications using lchown.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977954
,2015-07-13T19:42:37Z,unscheduled,,"The auditd service does not include the ability to send audit records to a centralized server for management directly.  It does, however, include an audit event multiplexor plugin (audispd) to pass audit records to the local syslog server.
---
None
---
SV-50271r1_rule
---
F-43416r1_fix
---
Set the ""active"" line in ""/etc/audisp/plugins.d/syslog.conf"" to ""yes"".  Restart the auditd process.

# service auditd restart
---
C-46026r1_chk
---
Verify the audispd plugin is active:

# grep active /etc/audisp/plugins.d/syslog.conf

If the ""active"" setting is missing or set to ""no"", this is a finding.",1.0,98977966,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38471] [low] The system must forward audit records to the syslog service.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977966
,2015-07-13T19:42:37Z,unscheduled,,"In addition to auditing new user and group accounts, these watches will alert the system administrator(s) to any modifications. Any unexpected users, groups, or modifications should be investigated for legitimacy.
---
None
---
SV-50337r1_rule
---
F-43484r1_fix
---
Add the following to ""/etc/audit/audit.rules"", in order to capture events that modify account changes: 

# audit_account_changes
-w /etc/group -p wa -k audit_account_changes
-w /etc/passwd -p wa -k audit_account_changes
-w /etc/gshadow -p wa -k audit_account_changes
-w /etc/shadow -p wa -k audit_account_changes
-w /etc/security/opasswd -p wa -k audit_account_changes
---
C-46094r1_chk
---
To determine if the system is configured to audit account changes, run the following command: 

auditctl -l | egrep '(/etc/passwd|/etc/shadow|/etc/group|/etc/gshadow|/etc/security/opasswd)'

If the system is configured to watch for account changes, lines should be returned for each file specified (and with ""perm=wa"" for each). 
If the system is not configured to audit account changes, this is a finding.",1.0,98977970,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38536] [low] The operating system must automatically audit account disabling actions.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977970
,2015-07-13T19:42:37Z,unscheduled,,"The network environment should not be modified by anything other than administrator action. Any change to network parameters should be audited.
---
None
---
SV-50341r2_rule
---
F-43488r2_fix
---
Add the following to ""/etc/audit/audit.rules"", setting ARCH to either b32 or b64 as appropriate for your system: 

# audit_network_modifications
-a always,exit -F arch=ARCH -S sethostname -S setdomainname -k audit_network_modifications
-w /etc/issue -p wa -k audit_network_modifications
-w /etc/issue.net -p wa -k audit_network_modifications
-w /etc/hosts -p wa -k audit_network_modifications
-w /etc/sysconfig/network -p wa -k audit_network_modifications
---
C-46098r1_chk
---
To determine if the system is configured to audit changes to its network configuration, run the following command: 

auditctl -l | egrep '(sethostname|setdomainname|/etc/issue|/etc/issue.net|/etc/hosts|/etc/sysconfig/network)'

If the system is configured to watch for network configuration changes, a line should be returned for each file specified (and ""perm=wa"" should be indicated for each). 
If the system is not configured to audit changes of the network configuration, this is a finding.",1.0,98977974,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38540] [low] The audit system must be configured to audit modifications to the systems network configuration.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977974
,2015-07-13T19:42:37Z,unscheduled,,"The system's mandatory access policy (SELinux) should not be arbitrarily changed by anything other than administrator action. All changes to MAC policy should be audited.
---
None
---
SV-50342r1_rule
---
F-43489r1_fix
---
Add the following to ""/etc/audit/audit.rules"": 

-w /etc/selinux/ -p wa -k MAC-policy
---
C-46099r1_chk
---
To determine if the system is configured to audit changes to its SELinux configuration files, run the following command: 

# auditctl -l | grep ""dir=/etc/selinux""

If the system is configured to watch for changes to its SELinux configuration, a line should be returned (including ""perm=wa"" indicating permissions that are watched). 
If the system is not configured to audit attempts to change the MAC policy, this is a finding.",1.0,98977976,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38541] [low] The audit system must be configured to audit modifications to the systems Mandatory Access Control (MAC) configuration (SELinux).,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977976
,2015-07-13T19:42:37Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50344r3_rule
---
F-43491r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S chmod -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S chmod -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S chmod -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S chmod -F auid=0 -k perm_mod
---
C-46101r2_chk
---
To determine if the system is configured to audit calls to the ""chmod"" system call, run the following command:

$ sudo grep -w ""chmod"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If the system is not configured to audit permission changes, this is a finding. ",1.0,98977978,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38543] [low] The audit system must be configured to audit all discretionary access control permission modifications using chmod.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977978
,2015-07-13T19:42:37Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50348r3_rule
---
F-43495r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S fchmod -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S fchmod -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S fchmod -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S fchmod -F auid=0 -k perm_mod
---
C-46105r2_chk
---
To determine if the system is configured to audit calls to the ""fchmod"" system call, run the following command:

$ sudo grep -w ""fchmod"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98977980,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38547] [low] The audit system must be configured to audit all discretionary access control permission modifications using fchmod.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977980
,2015-07-13T19:42:37Z,unscheduled,,"Arbitrary changes to the system time can be used to obfuscate nefarious activities in log files, as well as to confuse network services that are highly dependent upon an accurate system time (such as sshd). All changes to the system time should be audited.
---
None
---
SV-50331r1_rule
---
F-43477r1_fix
---
Add the following to ""/etc/audit/audit.rules"": 

-w /etc/localtime -p wa -k audit_time_rules

The -k option allows for the specification of a key in string form that can be used for better reporting capability through ausearch and aureport and should always be used.
---
C-46087r1_chk
---
To determine if the system is configured to audit attempts to alter time via the /etc/localtime file, run the following command: 

# auditctl -l | grep ""watch=/etc/localtime""

If the system is configured to audit this activity, it will return a line. 
If the system is not configured to audit time changes, this is a finding.",1.0,98977996,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38530] [low] The audit system must be configured to audit all attempts to alter system time through /etc/localtime.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977996
,2015-07-13T19:42:37Z,unscheduled,,"The actions taken by system administrators should be audited to keep a record of what was executed on the system, as well as, for accountability purposes.
---
None
---
SV-50379r1_rule
---
F-43526r1_fix
---
At a minimum, the audit system should collect administrator actions for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-w /etc/sudoers -p wa -k actions
---
C-46136r1_chk
---
To verify that auditing is configured for system administrator actions, run the following command: 

# auditctl -l | grep ""watch=/etc/sudoers""


If there is no output, this is a finding.",1.0,98978000,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38578] [low] The audit system must be configured to audit changes to the /etc/sudoers file.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978000
,2015-07-13T19:42:37Z,unscheduled,,"Auditing file deletions will create an audit trail for files that are removed from the system. The audit trail could aid in system troubleshooting, as well as detecting malicious processes that attempt to delete log files to conceal their presence.
---
None
---
SV-50376r4_rule
---
F-43523r4_fix
---
At a minimum, the audit system should collect file deletion events for all users and root. Add the following (or equivalent) to ""/etc/audit/audit.rules"", setting ARCH to either b32 or b64 as appropriate for your system: 

-a always,exit -F arch=ARCH -S rmdir -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete
-a always,exit -F arch=ARCH -S rmdir -S unlink -S unlinkat -S rename -S renameat -F auid=0 -k delete


---
C-46133r3_chk
---
To determine if the system is configured to audit calls to the ""rmdir"" system call, run the following command:

$ sudo grep -w ""rmdir"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. To determine if the system is configured to audit calls to the ""unlink"" system call, run the following command:

$ sudo grep -w ""unlink"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. To determine if the system is configured to audit calls to the ""unlinkat"" system call, run the following command:

$ sudo grep -w ""unlinkat"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. To determine if the system is configured to audit calls to the ""rename"" system call, run the following command:

$ sudo grep -w ""rename"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. To determine if the system is configured to audit calls to the ""renameat"" system call, run the following command:

$ sudo grep -w ""renameat"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line.

If no line is returned, this is a finding. ",1.0,98978002,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38575] [low] The audit system must be configured to audit user deletions of files and programs.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978002
,2015-07-13T19:42:38Z,unscheduled,,"The unauthorized exportation of data to external media could result in an information leak where classified information, Privacy Act information, and intellectual property could be lost. An audit trail should be created each time a filesystem is mounted to help identify and guard against information loss.
---
None
---
SV-50369r3_rule
---
F-43516r2_fix
---
At a minimum, the audit system should collect media exportation events for all users and root. Add the following to ""/etc/audit/audit.rules"", setting ARCH to either b32 or b64 as appropriate for your system: 

-a always,exit -F arch=ARCH -S mount -F auid>=500 -F auid!=4294967295 -k export
-a always,exit -F arch=ARCH -S mount -F auid=0 -k export
---
C-46126r2_chk
---
To verify that auditing is configured for all media exportation events, run the following command: 

$ sudo grep -w ""mount"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98978026,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38568] [low] The audit system must be configured to audit successful file system mounts.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978026
,2015-07-13T19:42:38Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50362r3_rule
---
F-43509r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S lsetxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S lsetxattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S lsetxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S lsetxattr -F auid=0 -k perm_mod
---
C-46119r2_chk
---
To determine if the system is configured to audit calls to the ""lsetxattr"" system call, run the following command:

$ sudo grep -w ""lsetxattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98978030,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38561] [low] The audit system must be configured to audit all discretionary access control permission modifications using lsetxattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978030
,2015-07-13T19:42:38Z,unscheduled,,"Unsuccessful attempts to access files could be an indicator of malicious activity on a system. Auditing these events could serve as evidence of potential system compromise.
---
None
---
SV-50367r2_rule
---
F-43514r2_fix
---
At a minimum, the audit system should collect unauthorized file accesses for all users and root. Add the following to ""/etc/audit/audit.rules"", setting ARCH to either b32 or b64 as appropriate for your system: 

-a always,exit -F arch=ARCH -S creat -S open -S openat -S truncate \
-S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access
-a always,exit -F arch=ARCH -S creat -S open -S openat -S truncate \
-S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access
-a always,exit -F arch=ARCH -S creat -S open -S openat -S truncate \
-S ftruncate -F exit=-EACCES -F auid=0 -k access
-a always,exit -F arch=ARCH -S creat -S open -S openat -S truncate \
-S ftruncate -F exit=-EPERM -F auid=0 -k access
---
C-46124r1_chk
---
To verify that the audit system collects unauthorized file accesses, run the following commands: 

# grep EACCES /etc/audit/audit.rules



# grep EPERM /etc/audit/audit.rules


If either command lacks output, this is a finding.",1.0,98978032,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38566] [low] The audit system must be configured to audit failed attempts to access files and programs.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978032
,2015-07-13T19:42:38Z,unscheduled,,"Privileged programs are subject to escalation-of-privilege attacks, which attempt to subvert their normal role of providing some necessary but limited capability. As such, motivation exists to monitor these programs for unusual activity.
---
None
---
SV-50368r4_rule
---
F-43515r6_fix
---
At a minimum, the audit system should collect the execution of privileged commands for all users and root. To find the relevant setuid / setgid programs, run the following command for each local partition [PART]:

$ sudo find [PART] -xdev -type f -perm /6000 2>/dev/null

Then, for each setuid / setgid program on the system, add a line of the following form to ""/etc/audit/audit.rules"", where [SETUID_PROG_PATH] is the full path to each setuid / setgid program in the list:

-a always,exit -F path=[SETUID_PROG_PATH] -F perm=x -F auid>=500 -F auid!=4294967295 -k privileged
---
C-46125r7_chk
---
To verify that auditing of privileged command use is configured, run the following command once for each local partition [PART] to find relevant setuid / setgid programs:

$ sudo find [PART] -xdev -type f -perm /6000 2>/dev/null

Run the following command to verify entries in the audit rules for all programs found with the previous command:

$ sudo grep path /etc/audit/audit.rules

It should be the case that all relevant setuid / setgid programs have a line in the audit rules. If that is not the case, this is a finding. ",1.0,98978034,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38567] [low] The audit system must be configured to audit all use of setuid and setgid programs.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978034
,2015-07-13T19:42:38Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50366r3_rule
---
F-43513r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S setxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S setxattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S setxattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S setxattr -F auid=0 -k perm_mod
---
C-46123r2_chk
---
To determine if the system is configured to audit calls to the ""setxattr"" system call, run the following command:

$ sudo grep -w ""setxattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding. ",1.0,98978036,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38565] [low] The audit system must be configured to audit all discretionary access control permission modifications using setxattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978036
,2015-07-13T19:42:38Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50346r3_rule
---
F-43493r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S chown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S chown -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S chown -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S chown -F auid=0 -k perm_mod
---
C-46103r2_chk
---
To determine if the system is configured to audit calls to the ""chown"" system call, run the following command:

$ sudo grep -w ""chown"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines.

If no line is returned, this is a finding. ",1.0,98978048,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38545] [low] The audit system must be configured to audit all discretionary access control permission modifications using chown.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978048
,2015-07-13T19:42:38Z,unscheduled,,"In addition to auditing new user and group accounts, these watches will alert the system administrator(s) to any modifications. Any unexpected users, groups, or modifications should be investigated for legitimacy.
---
None
---
SV-50339r1_rule
---
F-43486r1_fix
---
Add the following to ""/etc/audit/audit.rules"", in order to capture events that modify account changes: 

# audit_account_changes
-w /etc/group -p wa -k audit_account_changes
-w /etc/passwd -p wa -k audit_account_changes
-w /etc/gshadow -p wa -k audit_account_changes
-w /etc/shadow -p wa -k audit_account_changes
-w /etc/security/opasswd -p wa -k audit_account_changes
---
C-46096r1_chk
---
To determine if the system is configured to audit account changes, run the following command: 

auditctl -l | egrep '(/etc/passwd|/etc/shadow|/etc/group|/etc/gshadow|/etc/security/opasswd)'

If the system is configured to watch for account changes, lines should be returned for each file specified (and with ""perm=wa"" for each). 
If the system is not configured to audit account changes, this is a finding.",1.0,98978050,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38538] [low] The operating system must automatically audit account termination.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978050
,2015-07-13T19:42:39Z,unscheduled,,"In addition to auditing new user and group accounts, these watches will alert the system administrator(s) to any modifications. Any unexpected users, groups, or modifications should be investigated for legitimacy.
---
None
---
SV-50332r1_rule
---
F-43480r1_fix
---
Add the following to ""/etc/audit/audit.rules"", in order to capture events that modify account changes: 

# audit_account_changes
-w /etc/group -p wa -k audit_account_changes
-w /etc/passwd -p wa -k audit_account_changes
-w /etc/gshadow -p wa -k audit_account_changes
-w /etc/shadow -p wa -k audit_account_changes
-w /etc/security/opasswd -p wa -k audit_account_changes
---
C-46090r1_chk
---
To determine if the system is configured to audit account changes, run the following command: 

auditctl -l | egrep '(/etc/passwd|/etc/shadow|/etc/group|/etc/gshadow|/etc/security/opasswd)'

If the system is configured to watch for account changes, lines should be returned for each file specified (and with ""perm=wa"" for each). 
If the system is not configured to audit account changes, this is a finding.",1.0,98978054,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38531] [low] The operating system must automatically audit account creation.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978054
,2015-07-13T19:42:39Z,unscheduled,,"In addition to auditing new user and group accounts, these watches will alert the system administrator(s) to any modifications. Any unexpected users, groups, or modifications should be investigated for legitimacy.
---
None
---
SV-50335r1_rule
---
F-43482r1_fix
---
Add the following to ""/etc/audit/audit.rules"", in order to capture events that modify account changes: 

# audit_account_changes
-w /etc/group -p wa -k audit_account_changes
-w /etc/passwd -p wa -k audit_account_changes
-w /etc/gshadow -p wa -k audit_account_changes
-w /etc/shadow -p wa -k audit_account_changes
-w /etc/security/opasswd -p wa -k audit_account_changes
---
C-46092r1_chk
---
To determine if the system is configured to audit account changes, run the following command: 

auditctl -l | egrep '(/etc/passwd|/etc/shadow|/etc/group|/etc/gshadow|/etc/security/opasswd)'

If the system is configured to watch for account changes, lines should be returned for each file specified (and with ""perm=wa"" for each). 
If the system is not configured to audit account changes, this is a finding.",1.0,98978060,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38534] [low] The operating system must automatically audit account modification.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978060
,2015-07-13T19:42:39Z,unscheduled,,"Each process on the system carries an ""auditable"" flag which indicates whether its activities can be audited. Although ""auditd"" takes care of enabling this for all processes which launch after it does, adding the kernel argument ensures it is set for every process during boot.
---
None
---
SV-50238r2_rule
---
F-43382r2_fix
---
To ensure all processes can be audited, even those which start prior to the audit daemon, add the argument ""audit=1"" to the kernel line in ""/etc/grub.conf"", in the manner below:

kernel /vmlinuz-version ro vga=ext root=/dev/VolGroup00/LogVol00 rhgb quiet audit=1

UEFI systems may prepend ""/boot"" to the ""/vmlinuz-version"" argument. 
---
C-45992r2_chk
---
Inspect the kernel boot arguments (which follow the word ""kernel"") in ""/etc/grub.conf"". If they include ""audit=1"", then auditing is enabled at boot time. 

If auditing is not enabled at boot time, this is a finding.",1.0,98978064,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38438] [low] Auditing must be enabled at boot by setting a kernel parameter.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978064
,2015-07-13T19:42:39Z,unscheduled,,"Arbitrary changes to the system time can be used to obfuscate nefarious activities in log files, as well as to confuse network services that are highly dependent upon an accurate system time (such as sshd). All changes to the system time should be audited.
---
None
---
SV-50436r3_rule
---
F-43584r2_fix
---
On a 32-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b32 -S adjtimex -k audit_time_rules

On a 64-bit system, add the following to ""/etc/audit/audit.rules"": 

# audit_time_rules
-a always,exit -F arch=b64 -S adjtimex -k audit_time_rules

The -k option allows for the specification of a key in string form that can be used for better reporting capability through ausearch and aureport. Multiple system calls can be defined on the same line to save space if desired, but is not required. See an example of multiple combined syscalls: 

-a always,exit -F arch=b64 -S adjtimex -S settimeofday -S clock_settime -k audit_time_rules
---
C-46194r2_chk
---
To determine if the system is configured to audit calls to the ""adjtimex"" system call, run the following command:

$ sudo grep -w ""adjtimex"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return a line. 

If the system is not configured to audit time changes, this is a finding.",1.0,98978070,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38635] [low] The audit system must be configured to audit all attempts to alter system time through adjtimex.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978070
,2015-07-13T19:42:39Z,unscheduled,,"The changing of file permissions could indicate that a user is attempting to gain access to information that would otherwise be disallowed. Auditing DAC modifications can facilitate the identification of patterns of abuse among both authorized and unauthorized users.
---
None
---
SV-50364r3_rule
---
F-43511r2_fix
---
At a minimum, the audit system should collect file permission changes for all users and root. Add the following to ""/etc/audit/audit.rules"": 

-a always,exit -F arch=b32 -S removexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b32 -S removexattr -F auid=0 -k perm_mod

If the system is 64-bit, then also add the following: 

-a always,exit -F arch=b64 -S removexattr -F auid>=500 -F auid!=4294967295 \
-k perm_mod
-a always,exit -F arch=b64 -S removexattr -F auid=0 -k perm_mod
---
C-46121r2_chk
---
To determine if the system is configured to audit calls to the ""removexattr"" system call, run the following command:

$ sudo grep -w ""removexattr"" /etc/audit/audit.rules

If the system is configured to audit this activity, it will return several lines. 

If no line is returned, this is a finding.",1.0,98978074,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38563] [low] The audit system must be configured to audit all discretionary access control permission modifications using removexattr.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978074
,2015-07-13T19:42:41Z,unscheduled,,"Notifying administrators of an impending disk space problem may allow them to take corrective action prior to any disruption.
---
None
---
SV-50479r2_rule
---
F-43627r2_fix
---
The ""auditd"" service can be configured to take an action when disk space starts to run low. Edit the file ""/etc/audit/auditd.conf"". Modify the following line, substituting [num_megabytes] appropriately: 

space_left = [num_megabytes]

The ""num_megabytes"" value should be set to a fraction of the total audit storage capacity available that will allow a system administrator to be notified with enough time to respond to the situation causing the capacity issues.  This value must also be documented locally.
---
C-46240r1_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine whether the system is configured to email the administrator when disk space is starting to run low: 

# grep space_left /etc/audit/auditd.conf 

space_left = [num_megabytes]


If the ""num_megabytes"" value does not correspond to a documented value for remaining audit partition capacity or if there is no locally documented value for remaining audit partition capacity, this is a finding.",1.0,98978182,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38678] [medium] The audit system must provide a warning when allocated audit record storage volume reaches a documented percentage of maximum audit record storage capacity.,,[],956238,81882,feature,2015-07-14T00:57:44Z,https://www.pivotaltracker.com/story/show/98978182
,2015-07-13T19:42:43Z,unscheduled,,"Ensuring the ""auditd"" service is active ensures audit records generated by the kernel can be written to disk, or that appropriate actions will be taken if other obstacles exist.
---
None
---
SV-50433r2_rule
---
F-43581r2_fix
---
The ""auditd"" service is an essential userspace component of the Linux Auditing System, as it is responsible for writing audit records to disk. The ""auditd"" service can be enabled with the following commands: 

# chkconfig auditd on
# service auditd start
---
C-46191r1_chk
---
Run the following command to determine the current status of the ""auditd"" service: 

# service auditd status

If the service is enabled, it should return the following: 

auditd is running...


If the service is not running, this is a finding.",1.0,98978244,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38632] [medium] The operating system must produce audit records containing sufficient information to establish what type of events occurred.,,[],956238,81882,feature,2015-07-14T00:57:27Z,https://www.pivotaltracker.com/story/show/98978244
,2015-07-13T21:59:11Z,unscheduled,,,,98991360,story,"[{'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",extra hardening of stemcell - audit,,[],956238,81882,release,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98991360
,2015-07-13T19:42:34Z,unscheduled,,"Virus scanning software can be used to detect if a system has been compromised by computer viruses, as well as to limit their spread to other systems.
---
None
---
SV-50467r2_rule
---
F-43615r2_fix
---
Install virus scanning software, which uses signatures to search for the presence of viruses on the filesystem. 

The McAfee VirusScan Enterprise for Linux virus scanning tool is provided for DoD systems. Ensure virus definition files are no older than 7 days, or their last release. 

Configure the virus scanning software to perform scans dynamically on all accessed files. If this is not possible, configure the system to scan all altered files on the system on a daily basis. If the system processes inbound SMTP mail, configure the virus scanner to scan all received mail. 
---
C-46226r2_chk
---
Inspect the system for a cron job or system service which executes a virus scanning tool regularly.
To verify the McAfee VSEL system service is operational, run the following command:

# /etc/init.d/nails status

To check on the age of uvscan virus definition files, run the following command:

# cd /opt/NAI/LinuxShield/engine/dat
# ls -la avvscan.dat avvnames.dat avvclean.dat

If virus scanning software does not run continuously, or at least daily, or has signatures that are out of date, this is a finding. ",1.0,98977840,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38666] [high] The system must use and update an approved virus scan program.,,[],956238,81882,feature,2015-07-14T01:13:11Z,https://www.pivotaltracker.com/story/show/98977840
,2015-07-13T19:42:40Z,unscheduled,,"A log server (loghost) receives syslog messages from one or more systems. This data can be used as an additional log source in the event a system is compromised and its local logs are suspect. Forwarding log messages to a remote loghost also provides system administrators with a centralized place to view the status of multiple hosts within the enterprise.
---
None
---
SV-50321r1_rule
---
F-43468r1_fix
---
To configure rsyslog to send logs to a remote log server, open ""/etc/rsyslog.conf"" and read and understand the last section of the file, which describes the multiple directives necessary to activate remote logging. Along with these other directives, the system can be configured to forward its logs to a particular log server by adding or correcting one of the following lines, substituting ""[loghost.example.com]"" appropriately. The choice of protocol depends on the environment of the system; although TCP and RELP provide more reliable message delivery, they may not be supported in all environments. 
To use UDP for log message delivery: 

*.* @[loghost.example.com]


To use TCP for log message delivery: 

*.* @@[loghost.example.com]


To use RELP for log message delivery: 

*.* :omrelp:[loghost.example.com]
---
C-46078r1_chk
---
To ensure logs are sent to a remote host, examine the file ""/etc/rsyslog.conf"". If using UDP, a line similar to the following should be present: 

*.* @[loghost.example.com]

If using TCP, a line similar to the following should be present: 

*.* @@[loghost.example.com]

If using RELP, a line similar to the following should be present: 

*.* :omrelp:[loghost.example.com]


If none of these are present, this is a finding.",1.0,98978120,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38520] [medium] The operating system must back up audit records on an organization defined frequency onto a different system or media than the system being audited.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978120
,2015-07-13T19:42:40Z,unscheduled,,"Operating system backup is a critical step in maintaining data assurance and availability. User-level information is data generated by information system and/or application users. Backups shall be consistent with organizational recovery time and recovery point objectives.
---
None
---
SV-50289r1_rule
---
F-43435r1_fix
---
Procedures to back up user data from the system must be established and executed. The Red Hat operating system provides utilities for automating such a process.  Commercial and open-source products are also available.

Implement a process whereby user data is backed up from the system in accordance with local policies.
---
C-46045r1_chk
---
Ask an administrator if a process exists to back up user data from the system. 

If such a process does not exist, this is a finding.",1.0,98978118,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38488] [medium] The operating system must conduct backups of user-level information contained in the operating system per organization defined frequency to conduct backups consistent with recovery time and recovery point objectives.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978118
,2015-07-13T19:42:40Z,unscheduled,,"A log server (loghost) receives syslog messages from one or more systems. This data can be used as an additional log source in the event a system is compromised and its local logs are suspect. Forwarding log messages to a remote loghost also provides system administrators with a centralized place to view the status of multiple hosts within the enterprise.
---
None
---
SV-50322r1_rule
---
F-43656r1_fix
---
To configure rsyslog to send logs to a remote log server, open ""/etc/rsyslog.conf"" and read and understand the last section of the file, which describes the multiple directives necessary to activate remote logging. Along with these other directives, the system can be configured to forward its logs to a particular log server by adding or correcting one of the following lines, substituting ""[loghost.example.com]"" appropriately. The choice of protocol depends on the environment of the system; although TCP and RELP provide more reliable message delivery, they may not be supported in all environments. 
To use UDP for log message delivery: 

*.* @[loghost.example.com]


To use TCP for log message delivery: 

*.* @@[loghost.example.com]


To use RELP for log message delivery: 

*.* :omrelp:[loghost.example.com]
---
C-46269r1_chk
---
To ensure logs are sent to a remote host, examine the file ""/etc/rsyslog.conf"". If using UDP, a line similar to the following should be present: 

*.* @[loghost.example.com]

If using TCP, a line similar to the following should be present: 

*.* @@[loghost.example.com]

If using RELP, a line similar to the following should be present: 

*.* :omrelp:[loghost.example.com]


If none of these are present, this is a finding.",1.0,98978122,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38521] [medium] The operating system must support the requirement to centrally manage the content of audit records generated by organization defined information system components.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978122
,2015-07-13T19:42:40Z,unscheduled,,"Operating system backup is a critical step in maintaining data assurance and availability. System-level information includes system-state information, operating system and application software, and licenses. Backups must be consistent with organizational recovery time and recovery point objectives.
---
None
---
SV-50287r1_rule
---
F-43434r1_fix
---
Procedures to back up OS data from the system must be established and executed. The Red Hat operating system provides utilities for automating such a process.  Commercial and open-source products are also available.

Implement a process whereby OS data is backed up from the system in accordance with local policies.
---
C-46044r1_chk
---
Ask an administrator if a process exists to back up OS data from the system, including configuration data. 

If such a process does not exist, this is a finding.",1.0,98978126,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38486] [medium] The operating system must conduct backups of system-level information contained in the information system per organization defined frequency to conduct backups that are consistent with recovery time and recovery point objectives.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978126
,2015-07-13T19:42:40Z,unscheduled,,"Adding host-based intrusion detection tools can provide the capability to automatically take actions in response to malicious behavior, which can provide additional agility in reacting to network threats. These tools also often include a reporting capability to provide network awareness of system, which may not otherwise exist in an organization's systems management regime.
---
None
---
SV-50468r2_rule
---
F-43616r2_fix
---
The base Red Hat platform already includes a sophisticated auditing system that can detect intruder activity, as well as SELinux, which provides host-based intrusion prevention capabilities by confining privileged programs and user sessions which may become compromised.

In DoD environments, supplemental intrusion detection tools, such as, the McAfee Host-based Security System, are available to integrate with existing infrastructure. When these supplemental tools interfere with the proper functioning of SELinux, SELinux takes precedence. 
---
C-46227r1_chk
---
Inspect the system to determine if intrusion detection software has been installed. Verify the intrusion detection software is active. 
If no host-based intrusion detection tools are installed, this is a finding.",1.0,98978136,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38667] [medium] The system must have a host-based intrusion detection tool installed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978136
,2015-07-13T19:42:41Z,unscheduled,,"The ""iptables"" service provides the system's host-based firewalling capability for IPv4 and ICMP.
---
None
---
SV-50356r2_rule
---
F-43503r2_fix
---
The ""iptables"" service can be enabled with the following commands: 

# chkconfig iptables on
# service iptables start
---
C-46113r2_chk
---
If the system is a cross-domain system, this is not applicable.

Run the following command to determine the current status of the ""iptables"" service: 

# service iptables status

If the service is not running, it should return the following: 

iptables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978162,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38555] [medium] The system must employ a local IPv4 firewall.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978162
,2015-07-13T19:42:42Z,unscheduled,,"The ""ip6tables"" service provides the system's host-based firewalling capability for IPv6 and ICMPv6.
---
None
---
SV-50350r3_rule
---
F-43497r3_fix
---
The ""ip6tables"" service can be enabled with the following commands: 

# chkconfig ip6tables on
# service ip6tables start
---
C-46107r3_chk
---
If the system is a cross-domain system, this is not applicable.

If IPv6 is disabled, this is not applicable.

Run the following command to determine the current status of the ""ip6tables"" service: 

# service ip6tables status

If the service is not running, it should return the following: 

ip6tables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978208,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38549] [medium] The system must employ a local IPv6 firewall.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978208
,2015-07-13T19:42:42Z,unscheduled,,"Setting the password maximum age ensures users are required to periodically change their passwords. This could possibly decrease the utility of a stolen password. Requiring shorter password lifetimes increases the risk of users writing down the password in a convenient location subject to physical compromise.
---
None
---
SV-50279r1_rule
---
F-43424r1_fix
---
To specify password maximum age for new accounts, edit the file ""/etc/login.defs"" and add or correct the following line, replacing [DAYS] appropriately: 

PASS_MAX_DAYS [DAYS]

The DoD requirement is 60.
---
C-46034r1_chk
---
To check the maximum password age, run the command: 

$ grep PASS_MAX_DAYS /etc/login.defs

The DoD requirement is 60. 
If it is not set to the required value, this is a finding.",1.0,98978194,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38479] [medium] User passwords must be changed at least every 60 days.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978194
,2015-07-13T19:42:43Z,unscheduled,,"The AIDE package must be installed if it is to be available for integrity checking.
---
None
---
SV-50290r1_rule
---
F-43436r1_fix
---
Install the AIDE package with the command: 

# yum install aide
---
C-46046r1_chk
---
If another file integrity tool is installed, this is not a finding.

Run the following command to determine if the ""aide"" package is installed: 

# rpm -q aide


If the package is not installed, this is a finding.",1.0,98978256,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38489] [medium] A file integrity tool must be installed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978256
,2015-07-13T19:42:43Z,unscheduled,,"For AIDE to be effective, an initial database of ""known-good"" information about files must be captured and it should be able to be verified against the installed files. 
---
None
---
SV-65601r1_rule
---
F-56189r1_fix
---
Run the following command to generate a new database:

# /usr/sbin/aide --init

By default, the database will be written to the file ""/var/lib/aide/aide.db.new.gz"". Storing the database, the configuration file ""/etc/aide.conf"", and the binary ""/usr/sbin/aide"" (or hashes of these files), in a secure location (such as on read-only media) provides additional assurance about their integrity. The newly-generated database can be installed as follows:

# cp /var/lib/aide/aide.db.new.gz /var/lib/aide/aide.db.gz

To initiate a manual check, run the following command:

# /usr/sbin/aide --check

If this check produces any unexpected output, investigate. 
---
C-53727r1_chk
---
To find the location of the AIDE database file, run the following command:

# grep DBDIR /etc/aide.conf

Using the defined values of the [DBDIR] and [database] variables, verify the existence of the AIDE database file:

# ls -l [DBDIR]/[database_file_name]

If there is no database file, this is a finding. ",1.0,98978242,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-51391] [medium] A file integrity baseline must be created.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978242
,2015-07-13T19:42:41Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50471r2_rule
---
F-43619r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46229r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output, this is a finding.",1.0,98978174,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38670] [medium] The operating system must detect unauthorized changes to software and information.,,[],956238,81882,feature,2015-07-14T00:48:45Z,https://www.pivotaltracker.com/story/show/98978174
,2015-07-13T19:42:41Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50474r2_rule
---
F-43621r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46232r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output, this is a finding.",1.0,98978172,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38673] [medium] The operating system must ensure unauthorized, security-relevant configuration changes detected are tracked.",,[],956238,81882,feature,2015-07-14T00:48:50Z,https://www.pivotaltracker.com/story/show/98978172
,2015-07-13T19:42:44Z,unscheduled,,"Disabling a major host protection feature, such as SELinux, at boot time prevents it from confining system services at boot time. Further, it increases the chances that it will remain off during system operation.
---
None
---
SV-65547r1_rule
---
F-56147r1_fix
---
SELinux can be disabled at boot time by an argument in ""/etc/grub.conf"". Remove any instances of ""selinux=0"" from the kernel arguments in that file to prevent SELinux from being disabled at boot. 
---
C-54007r1_chk
---
Inspect ""/etc/grub.conf"" for any instances of ""selinux=0"" in the kernel boot arguments. Presence of ""selinux=0"" indicates that SELinux is disabled at boot time. If SELinux is disabled at boot time, this is a finding.",1.0,98978298,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-51337] [medium] The system must use a Linux Security Module at boot time.,,[],956238,81882,feature,2015-07-14T00:52:03Z,https://www.pivotaltracker.com/story/show/98978298
,2015-07-13T19:42:35Z,unscheduled,,"If a device file carries the SELinux type ""unlabeled_t"", then SELinux cannot properly restrict access to the device file. 
---
None
---
SV-65589r1_rule
---
F-56179r1_fix
---
Device files, which are used for communication with important system resources, should be labeled with proper SELinux types. If any device files carry the SELinux type ""unlabeled_t"", investigate the cause and correct the file's context. 
---
C-53719r1_chk
---
To check for unlabeled device files, run the following command:

# ls -RZ /dev | grep unlabeled_t

It should produce no output in a well-configured system. 

If there is output, this is a finding. ",1.0,98977912,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-51379] [low] All device files must be monitored by the system Linux Security Module.,,[],956238,81882,feature,2015-07-14T00:54:38Z,https://www.pivotaltracker.com/story/show/98977912
,2015-07-13T19:42:45Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50496r2_rule
---
F-43644r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46257r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output or if aide is not run at least weekly, this is a finding.",1.0,98978340,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38695] [medium] A file integrity tool must be used at least weekly to check for unauthorized file changes, particularly the addition of unauthorized system libraries or binaries, or for unauthorized modification to authorized system libraries or binaries.",,[],956238,81882,feature,2015-07-14T00:53:08Z,https://www.pivotaltracker.com/story/show/98978340
,2015-07-13T19:42:45Z,unscheduled,,"Ensuring the ""auditd"" service is active ensures audit records generated by the kernel can be written to disk, or that appropriate actions will be taken if other obstacles exist.
---
None
---
SV-50429r2_rule
---
F-43576r2_fix
---
The ""auditd"" service is an essential userspace component of the Linux Auditing System, as it is responsible for writing audit records to disk. The ""auditd"" service can be enabled with the following commands: 

# chkconfig auditd on
# service auditd start
---
C-46186r1_chk
---
Run the following command to determine the current status of the ""auditd"" service: 

# service auditd status

If the service is enabled, it should return the following: 

auditd is running...


If the service is not running, this is a finding.",1.0,98978334,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38628] [medium] The operating system must produce audit records containing sufficient information to establish the identity of any user/subject associated with the event.,,[],956238,81882,feature,2015-07-14T00:53:27Z,https://www.pivotaltracker.com/story/show/98978334
,2015-07-13T19:42:45Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50501r2_rule
---
F-43649r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46262r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output, this is a finding.",1.0,98978338,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38700] [medium] The operating system must provide a near real-time alert when any of the organization defined list of compromise or potential compromise indicators occurs.,,[],956238,81882,feature,2015-07-14T00:53:27Z,https://www.pivotaltracker.com/story/show/98978338
,2015-07-13T19:42:43Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50499r2_rule
---
F-43647r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46261r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output, this is a finding.",1.0,98978260,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38698] [medium] The operating system must employ automated mechanisms to detect the presence of unauthorized software on organizational information systems and notify designated organizational officials in accordance with the organization defined frequency.,,[],956238,81882,feature,2015-07-14T00:57:04Z,https://www.pivotaltracker.com/story/show/98978260
,2015-07-13T19:42:36Z,unscheduled,,"The risk of a system's physical compromise, particularly mobile systems such as laptops, places its data at risk of compromise. Encrypting this data mitigates the risk of its loss if the system is lost.
---
None
---
SV-50462r1_rule
---
F-43610r1_fix
---
Red Hat Enterprise Linux 6 natively supports partition encryption through the Linux Unified Key Setup-on-disk-format (LUKS) technology. The easiest way to encrypt a partition is during installation time. 

For manual installations, select the ""Encrypt"" checkbox during partition creation to encrypt the partition. When this option is selected the system will prompt for a passphrase to use in decrypting the partition. The passphrase will subsequently need to be entered manually every time the system boots. 

For automated/unattended installations, it is possible to use Kickstart by adding the ""--encrypted"" and ""--passphrase="" options to the definition of each partition to be encrypted. For example, the following line would encrypt the root partition: 

part / --fstype=ext3 --size=100 --onpart=hda1 --encrypted --passphrase=[PASSPHRASE]

Any [PASSPHRASE] is stored in the Kickstart in plaintext, and the Kickstart must then be protected accordingly. Omitting the ""--passphrase="" option from the partition definition will cause the installer to pause and interactively ask for the passphrase during installation. 

Detailed information on encrypting partitions using LUKS can be found on the Red Had Documentation web site:
https://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/sect-Security_Guide-LUKS_Disk_Encryption.html
---
C-46221r1_chk
---
Determine if encryption must be used to protect data on the system. 
If encryption must be used and is not employed, this is a finding.",1.0,98977926,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38661] [low] The operating system must protect the confidentiality and integrity of data at rest.,,[],956238,81882,feature,2015-07-14T00:55:35Z,https://www.pivotaltracker.com/story/show/98977926
,2015-07-13T22:03:31Z,unscheduled,,,,98991646,story,"[{'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",extra hardening of stemcell - vague/bigger,,[],956238,81882,release,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98991646
,2015-07-13T19:42:40Z,unscheduled,,"Earlier versions of SNMP are considered insecure, as they potentially allow unauthorized access to detailed system management information.

---
None
---
SV-50461r1_rule
---
F-43604r1_fix
---
Edit ""/etc/snmp/snmpd.conf"", removing any references to ""v1"", ""v2c"", or ""com2sec"". Upon doing that, restart the SNMP service: 

# service snmpd restart
---
C-46215r1_chk
---
To ensure only SNMPv3 or newer is used, run the following command: 

# grep 'v1\|v2c\|com2sec' /etc/snmp/snmpd.conf | grep -v '^#'

There should be no output. 
If there is output, this is a finding.",1.0,98978138,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38660] [medium] The snmpd service must use only SNMP protocol version 3 or newer.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978138
,2015-07-13T19:42:40Z,unscheduled,,"A number of system services utilize email messages sent to the root user to notify system administrators of active or impending issues.  These messages must be forwarded to at least one monitored email address.
---
None
---
SV-50246r1_rule
---
F-43391r1_fix
---
Set up an alias for root that forwards to a monitored email address:

# echo ""root: <system.administrator>@mail.mil"" >> /etc/aliases
# newaliases
---
C-46001r1_chk
---
Find the list of alias maps used by the Postfix mail server:

# postconf alias_maps

Query the Postfix alias maps for an alias for ""root"":

# postmap -q root <alias_map>

If there are no aliases configured for root that forward to a monitored email address, this is a finding.",1.0,98978142,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38446] [medium] The mail system must forward all mail for root to one or more system administrators.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978142
,2015-07-13T19:42:41Z,unscheduled,,"Preventing direct root login to virtual console devices helps ensure accountability for actions taken on the system using the root account.
---
None
---
SV-50293r1_rule
---
F-43439r2_fix
---
To restrict root logins through the (deprecated) virtual console devices, ensure lines of this form do not appear in ""/etc/securetty"": 

vc/1
vc/2
vc/3
vc/4

Note:  Virtual console entries are not limited to those listed above.  Any lines starting with ""vc/"" followed by numerals should be removed.
---
C-46049r1_chk
---
To check for virtual console entries which permit root login, run the following command: 

# grep '^vc/[0-9]' /etc/securetty

If any output is returned, then root logins over virtual console devices is permitted. 
If root login over virtual console devices is permitted, this is a finding.",1.0,98978152,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38492] [medium] The system must prevent the root account from logging in from virtual consoles.,,[],956238,81882,feature,2015-07-14T00:48:32Z,https://www.pivotaltracker.com/story/show/98978152
,2015-07-13T19:42:41Z,unscheduled,,"Disabling authentication for default system accounts makes it more difficult for attackers to make use of them to compromise a system.
---
None
---
SV-50297r3_rule
---
F-43442r2_fix
---
Some accounts are not associated with a human user of the system, and exist to perform some administrative function. An attacker should not be able to log into these accounts. 

Disable logon access to these accounts with the command: 

# passwd -l [SYSACCT]
---
C-46052r2_chk
---
To obtain a listing of all users and the contents of their shadow password field, run the command: 

$ awk -F: '$1 !~ /^root$/ && $2 !~ /^[!*]/ {print $1 "":"" $2}' /etc/shadow

Identify the operating system accounts from this listing. These will primarily be the accounts with UID numbers less than 500, other than root. 

If any default operating system account (other than root) has a valid password hash, this is a finding.",1.0,98978168,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38496] [medium] Default operating system accounts, other than root, must be locked.",,[],956238,81882,feature,2015-07-14T00:48:35Z,https://www.pivotaltracker.com/story/show/98978168
,2015-07-13T19:42:41Z,unscheduled,,"The ""ip6tables"" service provides the system's host-based firewalling capability for IPv6 and ICMPv6.
---
None
---
SV-50354r3_rule
---
F-43501r2_fix
---
The ""ip6tables"" service can be enabled with the following commands: 

# chkconfig ip6tables on
# service ip6tables start
---
C-46111r3_chk
---
If the system is a cross-domain system, this is not applicable.

If IPv6 is disabled, this is not applicable.

Run the following command to determine the current status of the ""ip6tables"" service: 

# service ip6tables status

If the service is not running, it should return the following: 

ip6tables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978158,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38553] [medium] The operating system must prevent public IPv6 access into an organizations internal networks, except as appropriately mediated by managed interfaces employing boundary protection devices.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978158
,2015-07-13T19:42:41Z,unscheduled,,"Users need to be aware of activity that occurs regarding their account. Providing users with information regarding the number of unsuccessful attempts that were made to login to their account allows the user to determine if any unauthorized activity has occurred and gives them an opportunity to notify administrators.
---
None
---
SV-66089r1_rule
---
F-56701r1_fix
---
To configure the system to notify users of last logon/access using ""pam_lastlog"", add the following line immediately after ""session required pam_limits.so"":

session required pam_lastlog.so showfailed
---
C-54013r1_chk
---
To ensure that last logon/access notification is configured correctly, run the following command:

# grep pam_lastlog.so /etc/pam.d/system-auth

The output should show output ""showfailed"". If that is not the case, this is a finding. ",1.0,98978164,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-51875] [medium] The operating system, upon successful logon/access, must display to the user the number of unsuccessful logon/access attempts since the last successful logon/access.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978164
,2015-07-13T19:42:41Z,unscheduled,,"The sendmail software was not developed with security in mind and its design prevents it from being effectively contained by SELinux. Postfix should be used instead.
---
None
---
SV-50472r1_rule
---
F-43620r1_fix
---
Sendmail is not the default mail transfer agent and is not installed by default. The ""sendmail"" package can be removed with the following command: 

# yum erase sendmail
---
C-46231r1_chk
---
Run the following command to determine if the ""sendmail"" package is installed: 

# rpm -q sendmail


If the package is installed, this is a finding.",1.0,98978176,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38671] [medium] The sendmail package must be removed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978176
,2015-07-13T19:42:41Z,unscheduled,,"Enabling idle activation of the screen saver ensures the screensaver will be activated after the idle delay. Applications requiring continuous, real-time screen display (such as network management products) require the login session does not have administrator rights and the display station is located in a controlled-access area.
---
None
---
SV-50431r3_rule
---
F-43579r1_fix
---
Run the following command to activate the screensaver in the GNOME desktop after a period of inactivity: 

# gconftool-2 --direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type bool \
--set /apps/gnome-screensaver/idle_activation_enabled true
---
C-46189r3_chk
---
If the GConf2 package is not installed, this is not applicable.

To check the screensaver mandatory use status, run the following command: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gnome-screensaver/idle_activation_enabled

If properly configured, the output should be ""true"". 

If it is not, this is a finding.",1.0,98978180,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38630] [medium] The graphical desktop environment must automatically lock after 15 minutes of inactivity and the system must require user reauthentication to unlock the environment.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978180
,2015-07-13T19:42:41Z,unscheduled,,"Requiring a minimum password length makes password cracking attacks more difficult by ensuring a larger search space. However, any security benefit from an onerous requirement must be carefully weighed against usability problems, support costs, or counterproductive behavior that may result.

While it does not negate the password length requirement, it is preferable to migrate from a password-based authentication scheme to a stronger one based on PKI (public key infrastructure).
---
None
---
SV-50275r1_rule
---
F-43419r1_fix
---
To specify password length requirements for new accounts, edit the file ""/etc/login.defs"" and add or correct the following lines: 

PASS_MIN_LEN 14



The DoD requirement is ""14"". If a program consults ""/etc/login.defs"" and also another PAM module (such as ""pam_cracklib"") during a password change operation, then the most restrictive must be satisfied.
---
C-46029r1_chk
---
To check the minimum password length, run the command: 

$ grep PASS_MIN_LEN /etc/login.defs

The DoD requirement is ""14"". 
If it is not set to the required value, this is a finding.",1.0,98978186,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38475] [medium] The system must require passwords to contain a minimum of 14 characters.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978186
,2015-07-13T19:42:42Z,unscheduled,,"Setting the minimum password age protects against users cycling back to a favorite password after satisfying the password reuse requirement.
---
None
---
SV-50277r1_rule
---
F-43422r1_fix
---
To specify password minimum age for new accounts, edit the file ""/etc/login.defs"" and add or correct the following line, replacing [DAYS] appropriately: 

PASS_MIN_DAYS [DAYS]

A value of 1 day is considered sufficient for many environments. The DoD requirement is 1.
---
C-46032r1_chk
---
To check the minimum password age, run the command: 

$ grep PASS_MIN_DAYS /etc/login.defs

The DoD requirement is 1. 
If it is not set to the required value, this is a finding.",1.0,98978188,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38477] [medium] Users must not be able to change passwords more than once every 24 hours.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978188
,2015-07-13T19:42:42Z,unscheduled,,"DHCP relies on trusting the local network. If the local network is not trusted, then it should not be used. However, the automatic configuration provided by DHCP is commonly used and the alternative, manual configuration, presents an unacceptable burden in many circumstances.
---
None
---
SV-50480r2_rule
---
F-43628r2_fix
---
For each interface [IFACE] on the system (e.g. eth0), edit ""/etc/sysconfig/network-scripts/ifcfg-[IFACE]"" and make the following changes. 

Correct the BOOTPROTO line to read:

BOOTPROTO=none


Add or correct the following lines, substituting the appropriate values based on your site's addressing scheme:

NETMASK=[local LAN netmask]
IPADDR=[assigned IP address]
GATEWAY=[local LAN default gateway]
---
C-46242r2_chk
---
To verify that DHCP is not being used, examine the following file for each interface. 

# /etc/sysconfig/network-scripts/ifcfg-[IFACE]

If there is any network interface without a associated ""ifcfg"" file, this is a finding.

Look for the following:

BOOTPROTO=none

Also verify the following, substituting the appropriate values based on your site's addressing scheme:

NETMASK=[local LAN netmask]
IPADDR=[assigned IP address]
GATEWAY=[local LAN default gateway]


If it does not, this is a finding.",1.0,98978192,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38679] [medium] The DHCP client must be disabled if not needed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978192
,2015-07-13T19:42:42Z,unscheduled,,"Administrators should be made aware of an inability to record audit records. If a separate partition or logical volume of adequate size is used, running low on space for audit records should never occur. 
---
None
---
SV-68627r1_rule
---
F-59235r2_fix
---
The ""auditd"" service can be configured to take an action when disk space is running low but prior to running out of space completely. Edit the file ""/etc/audit/auditd.conf"". Add or modify the following line, substituting [ACTION] appropriately:

admin_space_left_action = [ACTION]

Set this value to ""single"" to cause the system to switch to single-user mode for corrective action. Acceptable values also include ""suspend"" and ""halt"". For certain systems, the need for availability outweighs the need to log all actions, and a different setting should be determined. Details regarding all possible values for [ACTION] are described in the ""auditd.conf"" man page. 
---
C-54997r2_chk
---
Inspect ""/etc/audit/auditd.conf"" and locate the following line to determine if the system is configured to either suspend, switch to single-user mode, or halt when disk space has run low:

admin_space_left_action single

If the system is not configured to switch to single-user mode for corrective action, this is a finding. ",1.0,98978196,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-54381] [medium] The audit system must switch the system to single-user mode when available audit storage volume becomes dangerously low.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978196
,2015-07-13T19:42:42Z,unscheduled,,"Enabling reverse path filtering drops packets with source addresses that should not have been able to be received on the interface they were received on. It should not be used on systems which are routers for complicated networks, but is helpful for end hosts and routers serving small networks.
---
None
---
SV-50343r2_rule
---
F-43490r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.rp_filter"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.rp_filter=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.rp_filter = 1
---
C-46100r2_chk
---
The status of the ""net.ipv4.conf.all.rp_filter"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.rp_filter

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.rp_filter /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978200,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38542] [medium] The system must use a reverse-path filter for IPv4 network traffic when possible on all interfaces.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978200
,2015-07-13T19:42:42Z,unscheduled,,"Enabling reverse path filtering drops packets with source addresses that should not have been able to be received on the interface they were received on. It should not be used on systems which are routers for complicated networks, but is helpful for end hosts and routers serving small networks.
---
None
---
SV-50345r2_rule
---
F-43492r1_fix
---
To set the runtime status of the ""net.ipv4.conf.default.rp_filter"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.default.rp_filter=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.default.rp_filter = 1
---
C-46102r2_chk
---
The status of the ""net.ipv4.conf.default.rp_filter"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.default.rp_filter

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.default.rp_filter /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978202,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38544] [medium] The system must use a reverse-path filter for IPv4 network traffic when possible by default.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978202
,2015-07-13T19:42:42Z,unscheduled,,"Any unnecessary network stacks - including IPv6 - should be disabled, to reduce the vulnerability to exploitation.
---
None
---
SV-50347r2_rule
---
F-43494r2_fix
---
To prevent the IPv6 kernel module (""ipv6"") from binding to the IPv6 networking stack, add the following line to ""/etc/modprobe.d/disabled.conf"" (or another file in ""/etc/modprobe.d""): 

options ipv6 disable=1

This permits the IPv6 module to be loaded (and thus satisfy other modules that depend on it), while disabling support for the IPv6 protocol.
---
C-46104r2_chk
---
If the system uses IPv6, this is not applicable.

If the system is configured to disable the ""ipv6"" kernel module, it will contain a line of the form: 

options ipv6 disable=1

Such lines may be inside any file in ""/etc/modprobe.d"" or the deprecated ""/etc/modprobe.conf"". This permits insertion of the IPv6 kernel module (which other parts of the system expect to be present), but otherwise keeps it inactive. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r ipv6 /etc/modprobe.conf /etc/modprobe.d


If the IPv6 kernel module is not disabled, this is a finding.",1.0,98978204,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38546] [medium] The IPv6 protocol handler must not be bound to the network stack unless needed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978204
,2015-07-13T19:42:42Z,unscheduled,,"An appropriate warning message reinforces policy awareness during the logon process and facilitates possible legal action against attackers.
---
None
---
SV-50490r3_rule
---
F-43638r2_fix
---
To set the text shown by the GNOME Display Manager in the login screen, run the following command: 

# gconftool-2
--direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type string \
--set /apps/gdm/simple-greeter/banner_message_text \
""[required text]""

""I've read & consent to terms in IS user agreem't.""

When entering a warning banner that spans several lines, remember to begin and end the string with """""". This command writes directly to the file ""/var/lib/gdm/.gconf/apps/gdm/simple-greeter/%gconf.xml"", and this file can later be edited directly if necessary.
---
C-46252r3_chk
---
If the GConf2 package is not installed, this is not applicable.

To ensure login warning banner text is properly set, run the following: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gdm/simple-greeter/banner_message_text

If properly configured, the proper banner text will appear within this schema. 

The required text is either: 

""I've read & consent to terms in IS user agreem't.""

If the required banner text is not appear in the schema, this is a finding.",1.0,98978214,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38689] [medium] The login banner must be displayed immediately prior to, or as part of, graphical desktop environment login prompts.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978214
,2015-07-13T19:42:42Z,unscheduled,,"An appropriate warning message reinforces policy awareness during the logon process and facilitates possible legal action against attackers.
---
None
---
SV-50489r3_rule
---
F-43637r2_fix
---
To enable displaying a login warning banner in the GNOME Display Manager's login screen, run the following command: 

# gconftool-2 --direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type bool \
--set /apps/gdm/simple-greeter/banner_message_enable true

To display a banner, this setting must be enabled and then banner text must also be set.
---
C-46250r3_chk
---
If the GConf2 package is not installed, this is not applicable.

To ensure a login warning banner is enabled, run the following: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gdm/simple-greeter/banner_message_enable

Search for the ""banner_message_enable"" schema. If properly configured, the ""default"" value should be ""true"". 
If it is not, this is a finding.",1.0,98978216,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38688] [medium] A login banner must be displayed immediately prior to, or as part of, graphical desktop environment login prompts.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978216
,2015-07-13T19:42:42Z,unscheduled,,"In ""iptables"" the default policy is applied only after all the applicable rules in the table are examined for a match. Setting the default policy to ""DROP"" implements proper design for a firewall, i.e., any packets which are not explicitly permitted should not be accepted.
---
None
---
SV-50487r1_rule
---
F-43635r1_fix
---
To set the default policy to DROP (instead of ACCEPT) for the built-in FORWARD chain which processes packets that will be forwarded from one interface to another, add or correct the following line in ""/etc/sysconfig/iptables"": 

:FORWARD DROP [0:0]
---
C-46248r1_chk
---
Run the following command to ensure the default ""FORWARD"" policy is ""DROP"": 

grep "":FORWARD"" /etc/sysconfig/iptables

The output must be the following: 

# grep "":FORWARD"" /etc/sysconfig/iptables
:FORWARD DROP [0:0]

If it is not, this is a finding.",1.0,98978218,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38686] [medium] The systems local firewall must implement a deny-all, allow-by-exception policy for forwarded packets.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978218
,2015-07-13T19:42:43Z,unscheduled,,"Disabling the ""tftp"" service ensures the system is not acting as a tftp server, which does not provide encryption or authentication.
---
None
---
SV-50410r2_rule
---
F-43557r4_fix
---
The ""tftp"" service should be disabled. The ""tftp"" service can be disabled with the following command: 

# chkconfig tftp off
---
C-46166r2_chk
---
To check that the ""tftp"" service is disabled in system boot configuration, run the following command:

# chkconfig ""tftp"" --list

Output should indicate the ""tftp"" service has either not been installed, or has been disabled, as shown in the example below:

# chkconfig ""tftp"" --list
tftp off
OR
error reading information on service tftp: No such file or directory


If the service is running, this is a finding.",1.0,98978240,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38609] [medium] The TFTP service must not be running.,,[],956238,81882,feature,2015-07-14T00:50:41Z,https://www.pivotaltracker.com/story/show/98978240
,2015-07-13T19:42:42Z,unscheduled,,"Removing the ""tftp-server"" package decreases the risk of the accidental (or intentional) activation of tftp services.
---
None
---
SV-50407r2_rule
---
F-43554r1_fix
---
The ""tftp-server"" package can be removed with the following command: 

# yum erase tftp-server
---
C-46164r1_chk
---
Run the following command to determine if the ""tftp-server"" package is installed: 

# rpm -q tftp-server


If the package is installed, this is a finding.",1.0,98978224,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38606] [medium] The tftp-server package must not be installed unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978224
,2015-07-13T19:42:42Z,unscheduled,,"Due to its usage for maintenance and security-supporting tasks, enabling the cron daemon is essential.
---
None
---
SV-50406r2_rule
---
F-43553r2_fix
---
The ""crond"" service is used to execute commands at preconfigured times. It is required by almost all systems to perform necessary maintenance tasks, such as notifying root of system activity. The ""crond"" service can be enabled with the following commands: 

# chkconfig crond on
# service crond start
---
C-46163r1_chk
---
Run the following command to determine the current status of the ""crond"" service: 

# service crond status

If the service is enabled, it should return the following: 

crond is running...


If the service is not running, this is a finding.",1.0,98978226,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38605] [medium] The cron service must be running.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978226
,2015-07-13T19:42:43Z,unscheduled,,"Disabling the ""ypbind"" service ensures the system is not acting as a client in a NIS or NIS+ domain.
---
None
---
SV-50405r2_rule
---
F-43552r2_fix
---
The ""ypbind"" service, which allows the system to act as a client in a NIS or NIS+ domain, should be disabled. The ""ypbind"" service can be disabled with the following commands: 

# chkconfig ypbind off
# service ypbind stop
---
C-46162r2_chk
---
To check that the ""ypbind"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""ypbind"" --list

Output should indicate the ""ypbind"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""ypbind"" --list
""ypbind"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""ypbind"" is disabled through current runtime configuration: 

# service ypbind status

If the service is disabled the command will return the following output: 

ypbind is stopped


If the service is running, this is a finding.",1.0,98978228,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38604] [medium] The ypbind service must not be running.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978228
,2015-07-13T19:42:43Z,unscheduled,,"Removing the ""ypserv"" package decreases the risk of the accidental (or intentional) activation of NIS or NIS+ services.
---
None
---
SV-50404r1_rule
---
F-43551r1_fix
---
The ""ypserv"" package can be uninstalled with the following command: 

# yum erase ypserv
---
C-46161r1_chk
---
Run the following command to determine if the ""ypserv"" package is installed: 

# rpm -q ypserv


If the package is installed, this is a finding.",1.0,98978230,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38603] [medium] The ypserv package must not be installed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978230
,2015-07-13T19:42:43Z,unscheduled,,"Using a stronger hashing algorithm makes password cracking attacks more difficult.
---
None
---
SV-50375r2_rule
---
F-43522r2_fix
---
In ""/etc/pam.d/system-auth"" and ""/etc/pam.d/system-auth-ac"", among potentially other files, the ""password"" section of the files control which PAM modules execute during a password change. Set the ""pam_unix.so"" module in the ""password"" section to include the argument ""sha512"", as shown below: 

password sufficient pam_unix.so sha512 [other arguments...]

This will help ensure when local users change their passwords, hashes for the new passwords will be generated using the SHA-512 algorithm. This is the default.

Note that any updates made to ""/etc/pam.d/system-auth"" will be overwritten by the ""authconfig"" program.  The ""authconfig"" program should not be used.
---
C-46132r3_chk
---
Inspect the ""password"" section of ""/etc/pam.d/system-auth"", ""/etc/pam.d/system-auth-ac"", and other files in ""/etc/pam.d"" and ensure that the ""pam_unix.so"" module includes the argument ""sha512"".

$ grep password /etc/pam.d/* | grep pam_unix.so | grep sha512

If it does not, this is a finding.",1.0,98978250,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38574] [medium] The system must use a FIPS 140-2 approved cryptographic hashing algorithm for generating account password hashes (system-auth).,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978250
,2015-07-13T19:42:43Z,unscheduled,,"Using a stronger hashing algorithm makes password cracking attacks more difficult.
---
None
---
SV-50378r1_rule
---
F-43525r1_fix
---
In ""/etc/libuser.conf"", add or correct the following line in its ""[defaults]"" section to ensure the system will use the SHA-512 algorithm for password hashing: 

crypt_style = sha512
---
C-46135r1_chk
---
Inspect ""/etc/libuser.conf"" and ensure the following line appears in the ""[default]"" section: 

crypt_style = sha512


If it does not, this is a finding.",1.0,98978252,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38577] [medium] The system must use a FIPS 140-2 approved cryptographic hashing algorithm for generating account password hashes (libuser.conf).,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978252
,2015-07-13T19:42:43Z,unscheduled,,"Using a stronger hashing algorithm makes password cracking attacks more difficult.
---
None
---
SV-50377r1_rule
---
F-43524r1_fix
---
In ""/etc/login.defs"", add or correct the following line to ensure the system will use SHA-512 as the hashing algorithm: 

ENCRYPT_METHOD SHA512
---
C-46134r1_chk
---
Inspect ""/etc/login.defs"" and ensure the following line appears: 

ENCRYPT_METHOD SHA512


If it does not, this is a finding.",1.0,98978254,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38576] [medium] The system must use a FIPS 140-2 approved cryptographic hashing algorithm for generating account password hashes (login.defs).,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978254
,2015-07-13T19:42:43Z,unscheduled,,"Locking out user accounts after a number of incorrect attempts prevents direct password guessing attacks.
---
None
---
SV-50374r3_rule
---
F-43521r5_fix
---
To configure the system to lock out accounts after a number of incorrect logon attempts using ""pam_faillock.so"": 

Add the following lines immediately below the ""pam_unix.so"" statement in the AUTH section of ""/etc/pam.d/system-auth"" and ""/etc/pam.d/password-auth"": 

auth [default=die] pam_faillock.so authfail deny=3 unlock_time=604800 fail_interval=900

auth required pam_faillock.so authsucc deny=3 unlock_time=604800 fail_interval=900

Note that any updates made to ""/etc/pam.d/system-auth"" and ""/etc/pam.d/password-auth"" may be overwritten by the ""authconfig"" program.  The ""authconfig"" program should not be used.
---
C-46131r3_chk
---
To ensure the failed password attempt policy is configured correctly, run the following command: 

# grep pam_faillock /etc/pam.d/system-auth
# grep pam_faillock /etc/pam.d/password-auth

The output should show ""deny=3"" for both files. 
If that is not the case, this is a finding.",1.0,98978258,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38573] [medium] The system must disable accounts after three consecutive unsuccessful logon attempts.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978258
,2015-07-13T19:42:43Z,unscheduled,,"The log files generated by rsyslog contain valuable information regarding system configuration, user authentication, and other such information. Log files should be protected from unauthorized access.
---
None
---
SV-50320r2_rule
---
F-43466r1_fix
---
The group-owner of all log files written by ""rsyslog"" should be root. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" and typically all appear in ""/var/log"". For each log file [LOGFILE] referenced in ""/etc/rsyslog.conf"", run the following command to inspect the file's group owner:

$ ls -l [LOGFILE]

If the owner is not ""root"", run the following command to correct this:

# chgrp root [LOGFILE]
---
C-46076r2_chk
---
The group-owner of all log files written by ""rsyslog"" should be root. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" and typically all appear in ""/var/log"". To see the group-owner of a given log file, run the following command:

$ ls -l [LOGFILE]

Some log files referenced in /etc/rsyslog.conf may be created by other programs and may require exclusion from consideration.

If the group-owner is not root, this is a finding.",1.0,98978262,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38519] [medium] All rsyslog-generated log files must be group-owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978262
,2015-07-13T19:42:43Z,unscheduled,,"The log files generated by rsyslog contain valuable information regarding system configuration, user authentication, and other such information. Log files should be protected from unauthorized access.
---
None
---
SV-50319r2_rule
---
F-43465r1_fix
---
The owner of all log files written by ""rsyslog"" should be root. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" typically all appear in ""/var/log"". For each log file [LOGFILE] referenced in ""/etc/rsyslog.conf"", run the following command to inspect the file's owner:

$ ls -l [LOGFILE]

If the owner is not ""root"", run the following command to correct this:

# chown root [LOGFILE]
---
C-46075r2_chk
---
The owner of all log files written by ""rsyslog"" should be root. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" and typically all appear in ""/var/log"". To see the owner of a given log file, run the following command:

$ ls -l [LOGFILE]

Some log files referenced in /etc/rsyslog.conf may be created by other programs and may require exclusion from consideration. 

If the owner is not root, this is a finding. ",1.0,98978264,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38518] [medium] All rsyslog-generated log files must be owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978264
,2015-07-13T19:42:44Z,unscheduled,,"Disabling TIPC protects the system against exploitation of any flaws in its implementation.
---
None
---
SV-50318r3_rule
---
F-43464r3_fix
---
The Transparent Inter-Process Communication (TIPC) protocol is designed to provide communications between nodes in a cluster. To configure the system to prevent the ""tipc"" kernel module from being loaded, add the following line to a file in the directory ""/etc/modprobe.d"": 

install tipc /bin/true
---
C-46074r3_chk
---
If the system is configured to prevent the loading of the ""tipc"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r tipc /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978266,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38517] [medium] The Transparent Inter-Process Communication (TIPC) protocol must be disabled unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978266
,2015-07-13T19:42:44Z,unscheduled,,"Disabling SCTP protects the system against exploitation of any flaws in its implementation.
---
None
---
SV-50316r3_rule
---
F-43462r3_fix
---
The Stream Control Transmission Protocol (SCTP) is a transport layer protocol, designed to support the idea of message-oriented communication, with several streams of messages within one connection. To configure the system to prevent the ""sctp"" kernel module from being loaded, add the following line to a file in the directory ""/etc/modprobe.d"": 

install sctp /bin/true
---
C-46072r3_chk
---
If the system is configured to prevent the loading of the ""sctp"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r sctp /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978268,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38515] [medium] The Stream Control Transmission Protocol (SCTP) must be disabled unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978268
,2015-07-13T19:42:44Z,unscheduled,,"Disabling DCCP protects the system against exploitation of any flaws in its implementation.
---
None
---
SV-50315r3_rule
---
F-43461r3_fix
---
The Datagram Congestion Control Protocol (DCCP) is a relatively new transport layer protocol, designed to support streaming media and telephony. To configure the system to prevent the ""dccp"" kernel module from being loaded, add the following line to a file in the directory ""/etc/modprobe.d"": 

install dccp /bin/true
---
C-46071r3_chk
---
If the system is configured to prevent the loading of the ""dccp"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r dccp /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978270,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38514] [medium] The Datagram Congestion Control Protocol (DCCP) must be disabled unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978270
,2015-07-13T19:42:44Z,unscheduled,,"IP forwarding permits the kernel to forward packets from one network interface to another. The ability to forward packets between two networks is only appropriate for systems acting as routers.

---
None

---
SV-50312r2_rule

---
F-43458r2_fix
---
To set the runtime status of the ""net.ipv4.ip_forward"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.ip_forward=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.ip_forward = 0
---
C-46068r3_chk
---
The status of the ""net.ipv4.ip_forward"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.ip_forward

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.ip_forward /etc/sysctl.conf

The ability to forward packets is only appropriate for routers. If the correct value is not returned, this is a finding. ",1.0,98978274,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38511] [medium] IP forwarding for IPv4 must not be enabled, unless the system is a router.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978274
,2015-07-13T19:42:44Z,unscheduled,,"ExecShield uses the segmentation feature on all x86 systems to prevent execution in memory higher than a certain address. It writes an address as a limit in the code segment descriptor, to control where code can be executed, on a per-process basis. When the kernel places a process's memory regions such as the stack and heap higher than this address, the hardware prevents execution in that address range.
---
None
---
SV-50398r2_rule
---
F-43545r1_fix
---
To set the runtime status of the ""kernel.exec-shield"" kernel parameter, run the following command: 

# sysctl -w kernel.exec-shield=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

kernel.exec-shield = 1
---
C-46155r3_chk
---
The status of the ""kernel.exec-shield"" kernel parameter can be queried by running the following command: 

$ sysctl kernel.exec-shield
$ grep kernel.exec-shield /etc/sysctl.conf

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"". 
If the correct value is not returned, this is a finding.",1.0,98978276,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38597] [medium] The system must limit the ability of processes to have simultaneous write and execute access to memory.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978276
,2015-07-13T19:42:44Z,unscheduled,,"Address space layout randomization (ASLR) makes it more difficult for an attacker to predict the location of attack code he or she has introduced into a process's address space during an attempt at exploitation. Additionally, ASLR also makes it more difficult for an attacker to know the location of existing code in order to repurpose it using return oriented programming (ROP) techniques.
---
None
---
SV-50397r2_rule
---
F-43543r1_fix
---
To set the runtime status of the ""kernel.randomize_va_space"" kernel parameter, run the following command: 

# sysctl -w kernel.randomize_va_space=2

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

kernel.randomize_va_space = 2
---
C-46153r2_chk
---
The status of the ""kernel.randomize_va_space"" kernel parameter can be queried by running the following commands: 

$ sysctl kernel.randomize_va_space
$ grep kernel.randomize_va_space /etc/sysctl.conf

The output of the command should indicate a value of at least ""1"" (preferably ""2""). If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"". 
If the correct value is not returned, this is a finding.",1.0,98978278,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38596] [medium] The system must implement virtual address space randomization.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978278
,2015-07-13T19:42:44Z,unscheduled,,"Smart card login provides two-factor authentication stronger than that provided by a username/password combination. Smart cards leverage a PKI (public key infrastructure) in order to provide and verify credentials.
---
None
---
SV-50396r2_rule
---
F-43544r2_fix
---
To enable smart card authentication, consult the documentation at:

https://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/enabling-smart-card-login.html

For guidance on enabling SSH to authenticate against a Common Access Card (CAC), consult documentation at:

https://access.redhat.com/solutions/82273
---
C-46154r1_chk
---
Interview the SA to determine if all accounts not exempted by policy are using CAC authentication. For DoD systems, the following systems and accounts are exempt from using smart card (CAC) authentication: 

SIPRNET systems
Standalone systems
Application accounts
Temporary employee accounts, such as students or interns, who cannot easily receive a CAC or PIV
Operational tactical locations that are not collocated with RAPIDS workstations to issue CAC or ALT
Test systems, such as those with an Interim Approval to Test (IATT) and use a separate VPN, firewall, or security measure preventing access to network and system components from outside the protection boundary documented in the IATT.



If non-exempt accounts are not using CAC authentication, this is a finding.",1.0,98978280,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38595] [medium] The system must be configured to require the use of a CAC, PIV compliant hardware token, or Alternate Logon Token (ALT) for authentication.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978280
,2015-07-13T19:42:44Z,unscheduled,,"An appropriate warning message reinforces policy awareness during the logon process and facilitates possible legal action against attackers.
---
None
---
SV-50394r1_rule
---
F-43540r1_fix
---
To configure the system login banner: 

Edit ""/etc/issue"". Replace the default text with a message compliant with the local site policy or a legal disclaimer. The required text is either: 

""I've read & consent to terms in IS user agreem't.""
---
C-46150r1_chk
---
To check if the system login banner is compliant, run the following command: 

$ cat /etc/issue


If it does not display the required banner, this is a finding.",1.0,98978282,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38593] [medium] The login banner must be displayed immediately prior to, or as part of, console login prompts.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978282
,2015-07-13T19:42:44Z,unscheduled,,"Locking out user accounts after a number of incorrect attempts prevents direct password guessing attacks. Ensuring that an administrator is involved in unlocking locked accounts draws appropriate attention to such situations.
---
None
---
SV-50393r3_rule
---
F-43541r4_fix
---
To configure the system to lock out accounts after a number of incorrect logon attempts and require an administrator to unlock the account using ""pam_faillock.so"": 

Add the following lines immediately below the ""pam_unix.so"" statement in the AUTH section of
""/etc/pam.d/system-auth"" and ""/etc/pam.d/password-auth"": 

auth [default=die] pam_faillock.so authfail deny=3 unlock_time=604800 fail_interval=900

auth required pam_faillock.so authsucc deny=3 unlock_time=604800  fail_interval=900

Note that any updates made to ""/etc/pam.d/system-auth"" and ""/etc/pam.d/password-auth"" may be overwritten by the ""authconfig"" program.  The ""authconfig"" program should not be used.
---
C-46151r4_chk
---
To ensure the failed password attempt policy is configured correctly, run the following command: 

# grep pam_faillock /etc/pam.d/system-auth
# grep pam_faillock /etc/pam.d/password-auth

The output should show ""unlock_time=<some-large-number>""; the largest acceptable value is 604800 seconds (one week). 
If that is not the case, this is a finding.",1.0,98978288,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38592] [medium] The system must require administrator action to unlock an account locked by excessive failed login attempts.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978288
,2015-07-13T19:42:44Z,unscheduled,,"If the ""/etc/passwd"" file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the list of accounts on the system and associated information, and protection of this file is critical for system security.
---
None
---
SV-50257r1_rule
---
F-43397r1_fix
---
To properly set the permissions of ""/etc/passwd"", run the command: 

# chmod 0644 /etc/passwd
---
C-46007r1_chk
---
To check the permissions of ""/etc/passwd"", run the command: 

$ ls -l /etc/passwd

If properly configured, the output should indicate the following permissions: ""-rw-r--r--"" 
If it does not, this is a finding.",1.0,98978290,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38457] [medium] The /etc/passwd file must have mode 0644 or less permissive.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978290
,2015-07-13T19:42:44Z,unscheduled,,"Unencrypted passwords for remote FTP servers may be stored in "".netrc"" files. DoD policy requires passwords be encrypted in storage and not used in access scripts.
---
None
---
SV-50420r2_rule
---
F-43569r2_fix
---
The "".netrc"" files contain logon information used to auto-logon into FTP servers and reside in the user's home directory. These files may contain unencrypted passwords to remote FTP servers making them susceptible to access by unauthorized users and should not be used. Any "".netrc"" files should be removed.
---
C-46179r3_chk
---
To check the system for the existence of any "".netrc"" files, run the following command: 

$ sudo find /root /home -xdev -name .netrc

If any .netrc files exist, this is a finding.",1.0,98978294,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38619] [medium] There must be no .netrc files on the system.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978294
,2015-07-13T19:42:44Z,unscheduled,,"This setting will cause the system greeting banner to be used for FTP connections as well.
---
None
---
SV-50400r2_rule
---
F-43564r3_fix
---
Edit the vsftpd configuration file, which resides at ""/etc/vsftpd/vsftpd.conf"" by default. Add or correct the following configuration options. 

banner_file=/etc/issue

Restart the vsftpd daemon.

# service vsftpd restart
---
C-46174r1_chk
---
To verify this configuration, run the following command: 

grep ""banner_file"" /etc/vsftpd/vsftpd.conf

The output should show the value of ""banner_file"" is set to ""/etc/issue"", an example of which is shown below. 

# grep ""banner_file"" /etc/vsftpd/vsftpd.conf
banner_file=/etc/issue


If it does not, this is a finding.",1.0,98978296,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38599] [medium] The FTPS/FTP service on the system must be configured with the login banner.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978296
,2015-07-13T19:42:44Z,unscheduled,,"Leaving the user list enabled is a security risk since it allows anyone with physical access to the system to quickly enumerate known user accounts without logging in.
---
None
---
SV-55880r2_rule
---
F-48722r2_fix
---
In the default graphical environment, users logging directly into the system are greeted with a login screen that displays all known users. This functionality should be disabled.

Run the following command to disable the user list:

$ sudo gconftool-2 --direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type bool --set /apps/gdm/simple-greeter/disable_user_list true
---
C-49197r4_chk
---
If the GConf2 package is not installed, this is not applicable.

To ensure the user list is disabled, run the following command:

$ gconftool-2 --direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--get /apps/gdm/simple-greeter/disable_user_list

The output should be ""true"". If it is not, this is a finding. ",1.0,98978302,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-43150] [medium] The login user list must be disabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978302
,2015-07-13T19:42:44Z,unscheduled,,"Allowing users to execute binaries from world-writable directories such as ""/tmp"" should never be necessary in normal operation and can expose the system to potential compromise.
---
None
---
SV-71919r1_rule
---
F-62639r1_fix
---
The ""noexec"" mount option can be used to prevent binaries from being executed out of ""/tmp"". Add the ""noexec"" option to the fourth column of ""/etc/fstab"" for the line which controls mounting of ""/tmp"".
---
C-58279r1_chk
---
To verify that binaries cannot be directly executed from the /tmp directory, run the following command:

$ grep '\s/tmp' /etc/fstab

The resulting output will show whether the /tmp partition has the ""noexec"" flag set. If the /tmp partition does not have the noexec flag set, this is a finding.",1.0,98978304,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-57569] [medium] The noexec option must be added to the /tmp partition.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978304
,2015-07-13T19:42:44Z,unscheduled,,"The ""iptables"" service provides the system's host-based firewalling capability for IPv4 and ICMP.
---
None
---
SV-50361r2_rule
---
F-43508r2_fix
---
The ""iptables"" service can be enabled with the following commands: 

# chkconfig iptables on
# service iptables start
---
C-46118r2_chk
---
If the system is a cross-domain system, this is not applicable.

Run the following command to determine the current status of the ""iptables"" service: 

# service iptables status

If the service is not running, it should return the following: 

iptables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978306,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38560] [medium] The operating system must connect to external networks or information systems only through managed IPv4 interfaces consisting of boundary protection devices arranged in accordance with an organizational security architecture.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978306
,2015-07-13T19:42:44Z,unscheduled,,"In ""ip6tables"" the default policy is applied only after all the applicable rules in the table are examined for a match. Setting the default policy to ""DROP"" implements proper design for a firewall, i.e., any packets which are not explicitly permitted should not be accepted.
---
None
---
SV-50244r2_rule
---
F-43389r3_fix
---
To set the default policy to DROP (instead of ACCEPT) for the built-in INPUT chain which processes incoming packets, add or correct the following line in ""/etc/sysconfig/ip6tables"": 

:INPUT DROP [0:0]

Restart the IPv6 firewall:

# service ip6tables restart
---
C-45999r2_chk
---
If IPv6 is disabled, this is not applicable.

Inspect the file ""/etc/sysconfig/ip6tables"" to determine the default policy for the INPUT chain. It should be set to DROP:

# grep "":INPUT"" /etc/sysconfig/ip6tables

If the default policy for the INPUT chain is not set to DROP, this is a finding. ",1.0,98978308,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38444] [medium] The systems local IPv6 firewall must implement a deny-all, allow-by-exception policy for inbound packets.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978308
,2015-07-13T19:42:45Z,unscheduled,,"An account has root authority if it has a UID of 0. Multiple accounts with a UID of 0 afford more opportunity for potential intruders to guess a password for a privileged account. Proper configuration of sudo is recommended to afford multiple system administrators access to root privileges in an accountable manner.
---
None
---
SV-50301r2_rule
---
F-43447r1_fix
---
If any account other than root has a UID of 0, this misconfiguration should be investigated and the accounts other than root should be removed or have their UID changed.
---
C-46057r2_chk
---
To list all password file entries for accounts with UID 0, run the following command: 

# awk -F: '($3 == 0) {print}' /etc/passwd

This should print only one line, for the user root. 
If any account other than root has a UID of 0, this is a finding.",1.0,98978314,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38500] [medium] The root account must be the only account having a UID of 0.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978314
,2015-07-13T19:42:45Z,unscheduled,,"Locking out user accounts after a number of incorrect attempts within a specific period of time prevents direct password guessing attacks.
---
None
---
SV-50302r3_rule
---
F-43448r4_fix
---
Utilizing ""pam_faillock.so"", the ""fail_interval"" directive configures the system to lock out accounts after a number of incorrect logon attempts.

Add the following ""fail_interval"" directives to ""pam_faillock.so"" immediately below the ""pam_unix.so"" statement in ""/etc/pam.d/system-auth"" and ""/etc/pam.d/password-auth"":

auth [default=die] pam_faillock.so authfail deny=3 unlock_time=604800 fail_interval=900

auth required pam_faillock.so authsucc deny=3 unlock_time=604800 fail_interval=900
---
C-46058r2_chk
---
To ensure the failed password attempt policy is configured correctly, run the following command:

$ grep pam_faillock /etc/pam.d/system-auth /etc/pam.d/password-auth

For each file, the output should show ""fail_interval=<interval-in-seconds>"" where ""interval-in-seconds"" is 900 (15 minutes) or greater. If the ""fail_interval"" parameter is not set, the default setting of 900 seconds is acceptable. If that is not the case, this is a finding. ",1.0,98978316,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38501] [medium] The system must disable accounts after excessive login failures within a 15-minute interval.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978316
,2015-07-13T19:42:45Z,unscheduled,,"The ""/etc/shadow"" file contains the list of local system accounts and stores password hashes. Protection of this file is critical for system security. Failure to give ownership of this file to root provides the designated owner with access to sensitive information which could weaken the system security posture.
---
None
---
SV-50305r1_rule
---
F-43451r1_fix
---
To properly set the permissions of ""/etc/shadow"", run the command: 

# chmod 0000 /etc/shadow
---
C-46061r2_chk
---
To check the permissions of ""/etc/shadow"", run the command: 

$ ls -l /etc/shadow

If properly configured, the output should indicate the following permissions: ""----------"" 
If it does not, this is a finding.",1.0,98978310,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38504] [medium] The /etc/shadow file must have mode 0000.,,[],956238,81882,feature,2015-07-14T00:52:36Z,https://www.pivotaltracker.com/story/show/98978310
,2015-07-13T19:42:45Z,unscheduled,,"The ""/etc/shadow"" file contains the list of local system accounts and stores password hashes. Protection of this file is critical for system security. Failure to give ownership of this file to root provides the designated owner with access to sensitive information which could weaken the system security posture.
---
None
---
SV-50303r1_rule
---
F-43449r1_fix
---
To properly set the owner of ""/etc/shadow"", run the command: 

# chown root /etc/shadow
---
C-46059r1_chk
---
To check the ownership of ""/etc/shadow"", run the command: 

$ ls -l /etc/shadow

If properly configured, the output should indicate the following owner: ""root"" 
If it does not, this is a finding.",1.0,98978318,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38502] [medium] The /etc/shadow file must be owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978318
,2015-07-13T19:42:45Z,unscheduled,,"The ""/etc/shadow"" file stores password hashes. Protection of this file is critical for system security.
---
None
---
SV-50304r1_rule
---
F-43450r1_fix
---
To properly set the group owner of ""/etc/shadow"", run the command: 

# chgrp root /etc/shadow
---
C-46060r1_chk
---
To check the group ownership of ""/etc/shadow"", run the command: 

$ ls -l /etc/shadow

If properly configured, the output should indicate the following group-owner. ""root"" 
If it does not, this is a finding.",1.0,98978320,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38503] [medium] The /etc/shadow file must be group-owned by root.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978320
,2015-07-13T19:42:45Z,unscheduled,,"Synchronizing with an NTP server makes it possible to collate system logs from multiple sources or correlate computer events with real time events. Using a trusted NTP server provided by your organization is recommended.
---
None
---
SV-50422r1_rule
---
F-43570r1_fix
---
To specify a remote NTP server for time synchronization, edit the file ""/etc/ntp.conf"". Add or correct the following lines, substituting the IP or hostname of a remote NTP server for ntpserver. 

server [ntpserver]

This instructs the NTP software to contact that remote server to obtain time data.
---
C-46180r1_chk
---
A remote NTP server should be configured for time synchronization. To verify one is configured, open the following file. 

/etc/ntp.conf

In the file, there should be a section similar to the following: 

# --- OUR TIMESERVERS -----
server [ntpserver]


If this is not the case, this is a finding.",1.0,98978322,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38621] [medium] The system clock must be synchronized to an authoritative time source.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978322
,2015-07-13T19:42:45Z,unscheduled,,"Enabling the ""ntpd"" service ensures that the ""ntpd"" service will be running and that the system will synchronize its time to any servers specified. This is important whether the system is configured to be a client (and synchronize only its own clock) or it is also acting as an NTP server to other systems. Synchronizing time is essential for authentication services such as Kerberos, but it is also important for maintaining accurate logs and auditing possible security breaches.
---
None
---
SV-50421r1_rule
---
F-43568r1_fix
---
The ""ntpd"" service can be enabled with the following command: 

# chkconfig ntpd on
# service ntpd start
---
C-46178r1_chk
---
Run the following command to determine the current status of the ""ntpd"" service: 

# service ntpd status

If the service is enabled, it should return the following: 

ntpd is running...


If the service is not running, this is a finding.",1.0,98978324,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38620] [medium] The system clock must be synchronized continuously, or at least daily.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978324
,2015-07-13T19:42:45Z,unscheduled,,"Log files can contain valuable information regarding system configuration. If the system log files are not protected, unauthorized users could change the logged data, eliminating their forensic value.
---
None
---
SV-50424r2_rule
---
F-43571r1_fix
---
The file permissions for all log files written by rsyslog should be set to 600, or more restrictive. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" and typically all appear in ""/var/log"". For each log file [LOGFILE] referenced in ""/etc/rsyslog.conf"", run the following command to inspect the file's permissions:

$ ls -l [LOGFILE]

If the permissions are not 600 or more restrictive, run the following command to correct this:

# chmod 0600 [LOGFILE]
---
C-46181r2_chk
---
The file permissions for all log files written by rsyslog should be set to 600, or more restrictive. These log files are determined by the second part of each Rule line in ""/etc/rsyslog.conf"" and typically all appear in ""/var/log"". For each log file [LOGFILE] referenced in ""/etc/rsyslog.conf"", run the following command to inspect the file's permissions: 

$ ls -l [LOGFILE]

The permissions should be 600, or more restrictive. Some log files referenced in /etc/rsyslog.conf may be created by other programs and may require exclusion from consideration.

If the permissions are not correct, this is a finding.",1.0,98978326,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38623] [medium] All rsyslog-generated log files must have mode 0600 or less permissive.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978326
,2015-07-13T19:42:45Z,unscheduled,,"The ssl directive specifies whether to use ssl or not. If not specified it will default to ""no"". It should be set to ""start_tls"" rather than doing LDAP over SSL.
---
None
---
SV-50426r1_rule
---
F-43574r1_fix
---
Configure LDAP to enforce TLS use. First, edit the file ""/etc/pam_ldap.conf"", and add or correct the following lines: 

ssl start_tls

Then review the LDAP server and ensure TLS has been configured.
---
C-46184r1_chk
---
If the system does not use LDAP for authentication or account information, this is not applicable.

To ensure LDAP is configured to use TLS for all transactions, run the following command: 

$ grep start_tls /etc/pam_ldap.conf


If no lines are returned, this is a finding.",1.0,98978328,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38625] [medium] If the system is using LDAP for authentication or account information, the system must use a TLS connection using FIPS 140-2 approved cryptographic algorithms.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978328
,2015-07-13T19:42:45Z,unscheduled,,"The tls_cacertdir or tls_cacertfile directives are required when tls_checkpeer is configured (which is the default for openldap versions 2.1 and up). These directives define the path to the trust certificates signed by the site CA.
---
None
---
SV-50427r1_rule
---
F-43575r1_fix
---
Ensure a copy of the site's CA certificate has been placed in the file ""/etc/pki/tls/CA/cacert.pem"". Configure LDAP to enforce TLS use and to trust certificates signed by the site's CA. First, edit the file ""/etc/pam_ldap.conf"", and add or correct either of the following lines: 

tls_cacertdir /etc/pki/tls/CA

or 

tls_cacertfile /etc/pki/tls/CA/cacert.pem

Then review the LDAP server and ensure TLS has been configured.
---
C-46185r1_chk
---
If the system does not use LDAP for authentication or account information, this is not applicable.

To ensure TLS is configured with trust certificates, run the following command: 

# grep cert /etc/pam_ldap.conf


If there is no output, or the lines are commented out, this is a finding.",1.0,98978330,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38626] [medium] The LDAP client must use a TLS connection using trust certificates signed by the site CA.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978330
,2015-07-13T19:42:45Z,unscheduled,,"Setting the idle delay controls when the screensaver will start, and can be combined with screen locking to prevent access from passersby.
---
None
---
SV-50430r3_rule
---
F-43578r1_fix
---
Run the following command to set the idle time-out value for inactivity in the GNOME desktop to 15 minutes: 

# gconftool-2 \
--direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type int \
--set /apps/gnome-screensaver/idle_delay 15
---
C-46188r3_chk
---
If the GConf2 package is not installed, this is not applicable.

To check the current idle time-out value, run the following command: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gnome-screensaver/idle_delay

If properly configured, the output should be ""15"". 

If it is not, this is a finding.",1.0,98978332,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38629] [medium] The graphical desktop environment must set the idle timeout to no more than 15 minutes.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978332
,2015-07-13T19:42:45Z,unscheduled,,"Ensuring the validity of packages' cryptographic signatures prior to installation ensures the provenance of the software and protects against malicious tampering.
---
None
---
SV-50283r1_rule
---
F-43429r1_fix
---
The ""gpgcheck"" option should be used to ensure checking of an RPM package's signature always occurs prior to its installation. To configure yum to check package signatures before installing them, ensure the following line appears in ""/etc/yum.conf"" in the ""[main]"" section: 

gpgcheck=1
---
C-46039r1_chk
---
To determine whether ""yum"" is configured to use ""gpgcheck"", inspect ""/etc/yum.conf"" and ensure the following appears in the ""[main]"" section: 

gpgcheck=1

A value of ""1"" indicates that ""gpgcheck"" is enabled. Absence of a ""gpgcheck"" line or a setting of ""0"" indicates that it is disabled. 
If GPG checking is not enabled, this is a finding.

If the ""yum"" system package management tool is not used to update the system, verify with the SA that installed packages are cryptographically signed.",1.0,98978342,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38483] [medium] The system package management tool must cryptographically verify the authenticity of system software packages during installation.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978342
,2015-07-13T19:42:46Z,unscheduled,,"A TCP SYN flood attack can cause a denial of service by filling a system's TCP connection table with connections in the SYN_RCVD state. Syncookies can be used to track a connection when a subsequent ACK is received, verifying the initiator is attempting a valid connection and is not a flood source. This feature is activated when a flood condition is detected, and enables the system to continue servicing valid connection requests.
---
None
---
SV-50340r2_rule
---
F-43487r1_fix
---
To set the runtime status of the ""net.ipv4.tcp_syncookies"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.tcp_syncookies=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.tcp_syncookies = 1
---
C-46097r2_chk
---
The status of the ""net.ipv4.tcp_syncookies"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.tcp_syncookies

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.tcp_syncookies /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978344,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38539] [medium] The system must be configured to use TCP syncookies when experiencing a TCP SYN flood.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978344
,2015-07-13T19:42:46Z,unscheduled,,"In ""iptables"" the default policy is applied only after all the applicable rules in the table are examined for a match. Setting the default policy to ""DROP"" implements proper design for a firewall, i.e., any packets which are not explicitly permitted should not be accepted.
---
None
---
SV-50314r1_rule
---
F-43460r1_fix
---
To set the default policy to DROP (instead of ACCEPT) for the built-in INPUT chain which processes incoming packets, add or correct the following line in ""/etc/sysconfig/iptables"": 

:INPUT DROP [0:0]
---
C-46070r1_chk
---
Inspect the file ""/etc/sysconfig/iptables"" to determine the default policy for the INPUT chain. It should be set to DROP. 

# grep "":INPUT"" /etc/sysconfig/iptables

If the default policy for the INPUT chain is not set to DROP, this is a finding.",1.0,98978346,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38513] [medium] The systems local IPv4 firewall must implement a deny-all, allow-by-exception policy for inbound packets.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978346
,2015-07-13T19:42:46Z,unscheduled,,"The ""iptables"" service provides the system's host-based firewalling capability for IPv4 and ICMP.
---
None
---
SV-50313r2_rule
---
F-43459r2_fix
---
The ""iptables"" service can be enabled with the following commands: 

# chkconfig iptables on
# service iptables start
---
C-46069r2_chk
---
If the system is a cross-domain system, this is not applicable.

Run the following command to determine the current status of the ""iptables"" service: 

# service iptables status

If the service is not running, it should return the following: 

iptables: Firewall is not running.


If the service is not running, this is a finding.",1.0,98978350,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38512] [medium] The operating system must prevent public IPv4 access into an organizations internal networks, except as appropriately mediated by managed interfaces employing boundary protection devices.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978350
,2015-07-13T19:42:46Z,unscheduled,,"A comprehensive account management process that includes automation helps to ensure the accounts designated as requiring attention are consistently and promptly addressed. Enterprise environments make user account management challenging and complex. A user management process requiring administrators to manually address account management functions adds risk of potential oversight.
---
None
---
SV-50239r1_rule
---
F-43384r1_fix
---
Implement an automated system for managing user accounts that minimizes the risk of errors, either intentional or deliberate.  If possible, this system should integrate with an existing enterprise user management system, such as, one based Active Directory or Kerberos.
---
C-45994r1_chk
---
Interview the SA to determine if there is an automated system for managing user accounts, preferably integrated with an existing enterprise user management system.

If there is not, this is a finding.",1.0,98978352,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38439] [medium] The system must provide automated support for account management functions.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978352
,2015-07-13T19:42:46Z,unscheduled,,"Enabling the activation of the screen lock after an idle period ensures password entry will be required in order to access the system, preventing access by passersby.
---
None
---
SV-50439r3_rule
---
F-43587r1_fix
---
Run the following command to activate locking of the screensaver in the GNOME desktop when it is activated: 

# gconftool-2 --direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type bool \
--set /apps/gnome-screensaver/lock_enabled true
---
C-46198r3_chk
---
If the GConf2 package is not installed, this is not applicable. 

To check the status of the idle screen lock activation, run the following command: 

$ gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gnome-screensaver/lock_enabled

If properly configured, the output should be ""true"". 
If it is not, this is a finding.",1.0,98978354,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38638] [medium] The graphical desktop environment must have automatic lock enabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978354
,2015-07-13T19:42:46Z,unscheduled,,"By default, AIDE does not install itself for periodic execution. Periodically running AIDE may reveal unexpected changes in installed files.
---
None
---
SV-50497r2_rule
---
F-43645r1_fix
---
AIDE should be executed on a periodic basis to check for changes. To implement a daily execution of AIDE at 4:05am using cron, add the following line to /etc/crontab: 

05 4 * * * root /usr/sbin/aide --check

AIDE can be executed periodically through other means; this is merely one example.
---
C-46258r2_chk
---
To determine that periodic AIDE execution has been scheduled, run the following command: 

# grep aide /etc/crontab /etc/cron.*/*

If there is no output, this is a finding.",1.0,98978362,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38696] [medium] The operating system must employ automated mechanisms, per organization defined frequency, to detect the addition of unauthorized components/devices into the operating system.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978362
,2015-07-13T19:42:46Z,unscheduled,,"Ensuring the ""auditd"" service is active ensures audit records generated by the kernel can be written to disk, or that appropriate actions will be taken if other obstacles exist.
---
None
---
SV-50432r2_rule
---
F-43580r2_fix
---
The ""auditd"" service is an essential userspace component of the Linux Auditing System, as it is responsible for writing audit records to disk. The ""auditd"" service can be enabled with the following commands: 

# chkconfig auditd on
# service auditd start
---
C-46190r1_chk
---
Run the following command to determine the current status of the ""auditd"" service: 

# service auditd status

If the service is enabled, it should return the following: 

auditd is running...


If the service is not running, this is a finding.",1.0,98978370,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38631] [medium] The operating system must employ automated mechanisms to facilitate the monitoring and control of remote access methods.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978370
,2015-07-13T19:42:46Z,unscheduled,,"This ensures ""postfix"" accepts mail messages (such as cron job reports) from the local system only, and not from the network, which protects it from network attack.
---
None
---
SV-50423r2_rule
---
F-43572r1_fix
---
Edit the file ""/etc/postfix/main.cf"" to ensure that only the following ""inet_interfaces"" line appears: 

inet_interfaces = localhost
---
C-46182r2_chk
---
If the system is an authorized mail relay host, this is not applicable. 

Run the following command to ensure postfix accepts mail messages from only the local system: 

$ grep inet_interfaces /etc/postfix/main.cf

If properly configured, the output should show only ""localhost"". 
If it does not, this is a finding.",1.0,98978374,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38622] [medium] Mail relaying must be restricted.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978374
,2015-07-13T19:42:35Z,unscheduled,,"The qpidd service is automatically installed when the ""base"" package selection is selected during installation. The qpidd service listens for network connections which increases the attack surface of the system. If the system is not intended to receive AMQP traffic then the ""qpidd"" service is not needed and should be disabled or removed.
---
None
---
SV-50449r2_rule
---
F-43597r2_fix
---
The ""qpidd"" service provides high speed, secure, guaranteed delivery services. It is an implementation of the Advanced Message Queuing Protocol. By default the qpidd service will bind to port 5672 and listen for connection attempts. The ""qpidd"" service can be disabled with the following commands: 

# chkconfig qpidd off
# service qpidd stop
---
C-46208r2_chk
---
To check that the ""qpidd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""qpidd"" --list

Output should indicate the ""qpidd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""qpidd"" --list
""qpidd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""qpidd"" is disabled through current runtime configuration: 

# service qpidd status

If the service is disabled the command will return the following output: 

qpidd is stopped


If the service is running, this is a finding.",1.0,98977874,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38648] [low] The qpidd service must not be running.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977874
,2015-07-13T19:42:34Z,unscheduled,,"The umask value influences the permissions assigned to files when they are created. A misconfigured umask value could result in files with excessive permissions that can be read and/or written to by unauthorized users.
---
None
---
SV-50450r1_rule
---
F-43598r1_fix
---
To ensure the default umask for users of the C shell is set properly, add or correct the ""umask"" setting in ""/etc/csh.cshrc"" to read as follows: 

umask 077
---
C-46209r1_chk
---
Verify the ""umask"" setting is configured correctly in the ""/etc/csh.cshrc"" file by running the following command: 

# grep ""umask"" /etc/csh.cshrc

All output must show the value of ""umask"" set to 077, as shown in the below: 

# grep ""umask"" /etc/csh.cshrc
umask 077


If the above command returns no output, or if the umask is configured incorrectly, this is a finding.",1.0,98977872,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38649] [low] The system default umask for the csh shell must be 077.,,[],956238,81882,feature,2015-07-14T00:53:49Z,https://www.pivotaltracker.com/story/show/98977872
,2015-07-13T19:42:35Z,unscheduled,,"The umask influences the permissions assigned to files created by a process at run time. An unnecessarily permissive umask could result in files being created with insecure permissions.
---
None
---
SV-50443r1_rule
---
F-43592r1_fix
---
The file ""/etc/init.d/functions"" includes initialization parameters for most or all daemons started at boot time. The default umask of 022 prevents creation of group- or world-writable files. To set the default umask for daemons, edit the following line, inserting 022 or 027 for [UMASK] appropriately: 

umask [UMASK]

Setting the umask to too restrictive a setting can cause serious errors at runtime. Many daemons on the system already individually restrict themselves to a umask of 077 in their own init scripts.
---
C-46203r1_chk
---
To check the value of the ""umask"", run the following command: 

$ grep umask /etc/init.d/functions

The output should show either ""022"" or ""027"". 
If it does not, this is a finding.",1.0,98977876,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38642] [low] The system default umask for daemons must be 027 or 022.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977876
,2015-07-13T19:42:35Z,unscheduled,,"The ""atd"" service could be used by an unsophisticated insider to carry out activities outside of a normal login session, which could complicate accountability. Furthermore, the need to schedule tasks with ""at"" or ""batch"" is not common.
---
None
---
SV-50442r2_rule
---
F-43590r2_fix
---
The ""at"" and ""batch"" commands can be used to schedule tasks that are meant to be executed only once. This allows delayed execution in a manner similar to cron, except that it is not recurring. The daemon ""atd"" keeps track of tasks scheduled via ""at"" and ""batch"", and executes them at the specified time. The ""atd"" service can be disabled with the following commands: 

# chkconfig atd off
# service atd stop
---
C-46201r2_chk
---
If the system uses the ""atd"" service, this is not applicable.

To check that the ""atd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""atd"" --list

Output should indicate the ""atd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""atd"" --list
""atd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""atd"" is disabled through current runtime configuration: 

# service atd status

If the service is disabled the command will return the following output: 

atd is stopped


If the service is running, this is a finding.",1.0,98977878,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38641] [low] The atd service must be disabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977878
,2015-07-13T19:42:35Z,unscheduled,,"General-purpose systems typically have their network and routing information configured statically by a system administrator. Workstations or some special-purpose systems often use DHCP (instead of IRDP) to retrieve dynamic network configuration information.
---
None
---
SV-50451r2_rule
---
F-43599r2_fix
---
The ""rdisc"" service implements the client side of the ICMP Internet Router Discovery Protocol (IRDP), which allows discovery of routers on the local subnet. If a router is discovered then the local routing table is updated with a corresponding default route. By default this daemon is disabled. The ""rdisc"" service can be disabled with the following commands: 

# chkconfig rdisc off
# service rdisc stop
---
C-46210r1_chk
---
To check that the ""rdisc"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""rdisc"" --list

Output should indicate the ""rdisc"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""rdisc"" --list
""rdisc"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""rdisc"" is disabled through current runtime configuration: 

# service rdisc status

If the service is disabled the command will return the following output: 

rdisc is stopped


If the service is running, this is a finding.",1.0,98977902,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38650] [low] The rdisc service must not be running.,,[],956238,81882,feature,2015-07-14T00:54:03Z,https://www.pivotaltracker.com/story/show/98977902
,2015-07-13T19:42:35Z,unscheduled,,"The ""oddjobd"" service may provide necessary functionality in some environments but it can be disabled if it is not needed. Execution of tasks by privileged programs, on behalf of unprivileged ones, has traditionally been a source of privilege escalation security issues.
---
None
---
SV-50447r2_rule
---
F-43595r2_fix
---
The ""oddjobd"" service exists to provide an interface and access control mechanism through which specified privileged tasks can run tasks for unprivileged client applications. Communication with ""oddjobd"" is through the system message bus. The ""oddjobd"" service can be disabled with the following commands: 

# chkconfig oddjobd off
# service oddjobd stop
---
C-46206r2_chk
---
To check that the ""oddjobd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""oddjobd"" --list

Output should indicate the ""oddjobd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""oddjobd"" --list
""oddjobd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""oddjobd"" is disabled through current runtime configuration: 

# service oddjobd status

If the service is disabled the command will return the following output: 

oddjobd is stopped


If the service is running, this is a finding.",1.0,98977884,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38646] [low] The oddjobd service must not be running.,,[],956238,81882,feature,2015-07-14T00:53:55Z,https://www.pivotaltracker.com/story/show/98977884
,2015-07-13T19:42:35Z,unscheduled,,"The ""ntpdate"" service may only be suitable for systems which are rebooted frequently enough that clock drift does not cause problems between reboots. In any event, the functionality of the ntpdate service is now available in the ntpd program and should be considered deprecated.
---
None
---
SV-50445r2_rule
---
F-43593r2_fix
---
The ntpdate service sets the local hardware clock by polling NTP servers when the system boots. It synchronizes to the NTP servers listed in ""/etc/ntp/step-tickers"" or ""/etc/ntp.conf"" and then sets the local hardware clock to the newly synchronized system time. The ""ntpdate"" service can be disabled with the following commands: 

# chkconfig ntpdate off
# service ntpdate stop
---
C-46204r1_chk
---
To check that the ""ntpdate"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""ntpdate"" --list

Output should indicate the ""ntpdate"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""ntpdate"" --list
""ntpdate"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""ntpdate"" is disabled through current runtime configuration: 

# service ntpdate status

If the service is disabled the command will return the following output: 

ntpdate is stopped


If the service is running, this is a finding.",1.0,98977888,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38644] [low] The ntpdate service must not be running.,,[],956238,81882,feature,2015-07-14T00:53:56Z,https://www.pivotaltracker.com/story/show/98977888
,2015-07-13T19:42:35Z,unscheduled,,"Mishandling crash data could expose sensitive information about vulnerabilities in software executing on the local machine, as well as sensitive information from within a process's address space or registers.
---
None
---
SV-50441r2_rule
---
F-43589r2_fix
---
The Automatic Bug Reporting Tool (""abrtd"") daemon collects and reports crash data when an application crash is detected. Using a variety of plugins, abrtd can email crash reports to system administrators, log crash reports to files, or forward crash reports to a centralized issue tracking system such as RHTSupport. The ""abrtd"" service can be disabled with the following commands: 

# chkconfig abrtd off
# service abrtd stop
---
C-46200r1_chk
---
To check that the ""abrtd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""abrtd"" --list

Output should indicate the ""abrtd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""abrtd"" --list
""abrtd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""abrtd"" is disabled through current runtime configuration: 

# service abrtd status

If the service is disabled the command will return the following output: 

abrtd is stopped


If the service is running, this is a finding.",1.0,98977880,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38640] [low] The Automatic Bug Reporting Tool (abrtd) service must not be running.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977880
,2015-07-13T19:42:35Z,unscheduled,,"The umask value influences the permissions assigned to files when they are created. A misconfigured umask value could result in files with excessive permissions that can be read and/or written to by unauthorized users.
---
None
---
SV-50448r1_rule
---
F-43596r1_fix
---
To ensure the default umask controlled by ""/etc/profile"" is set properly, add or correct the ""umask"" setting in ""/etc/profile"" to read as follows: 

umask 077
---
C-46207r1_chk
---
Verify the ""umask"" setting is configured correctly in the ""/etc/profile"" file by running the following command: 

# grep ""umask"" /etc/profile

All output must show the value of ""umask"" set to 077, as shown in the below: 

# grep ""umask"" /etc/profile
umask 077


If the above command returns no output, or if the umask is configured incorrectly, this is a finding.",1.0,98977882,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38647] [low] The system default umask in /etc/profile must be 077.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977882
,2015-07-13T19:42:35Z,unscheduled,,"The umask value influences the permissions assigned to files when they are created. A misconfigured umask value could result in files with excessive permissions that can be read and/or written to by unauthorized users.
---
None
---
SV-50446r1_rule
---
F-43594r1_fix
---
To ensure the default umask controlled by ""/etc/login.defs"" is set properly, add or correct the ""umask"" setting in ""/etc/login.defs"" to read as follows: 

UMASK 077
---
C-46205r1_chk
---
Verify the ""umask"" setting is configured correctly in the ""/etc/login.defs"" file by running the following command: 

# grep -i ""umask"" /etc/login.defs

All output must show the value of ""umask"" set to 077, as shown in the below: 

# grep -i ""umask"" /etc/login.defs
UMASK 077


If the above command returns no output, or if the umask is configured incorrectly, this is a finding.",1.0,98977886,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38645] [low] The system default umask in /etc/login.defs must be 077.,,[],956238,81882,feature,2015-07-14T00:53:52Z,https://www.pivotaltracker.com/story/show/98977886
,2015-07-13T19:42:35Z,unscheduled,,"The umask value influences the permissions assigned to files when they are created. A misconfigured umask value could result in files with excessive permissions that can be read and/or written to by unauthorized users.
---
None
---
SV-50452r1_rule
---
F-43600r1_fix
---
To ensure the default umask for users of the Bash shell is set properly, add or correct the ""umask"" setting in ""/etc/bashrc"" to read as follows: 

umask 077
---
C-46211r1_chk
---
Verify the ""umask"" setting is configured correctly in the ""/etc/bashrc"" file by running the following command: 

# grep ""umask"" /etc/bashrc

All output must show the value of ""umask"" set to 077, as shown below: 

# grep ""umask"" /etc/bashrc
umask 077
umask 077


If the above command returns no output, or if the umask is configured incorrectly, this is a finding.",1.0,98977904,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38651] [low] The system default umask for the bash shell must be 077.,,[],956238,81882,feature,2015-07-14T00:54:07Z,https://www.pivotaltracker.com/story/show/98977904
,2015-07-13T19:42:35Z,unscheduled,,"Setting the SELinux policy to ""targeted"" or a more specialized policy ensures the system will confine processes that are likely to be targeted for exploitation, such as network or system services. 
---
None
---
SV-65579r1_rule
---
F-56171r1_fix
---
The SELinux ""targeted"" policy is appropriate for general-purpose desktops and servers, as well as systems in many other roles. To configure the system to use this policy, add or correct the following line in ""/etc/selinux/config"":

SELINUXTYPE=targeted

Other policies, such as ""mls"", provide additional security labeling and greater confinement but are not compatible with many general-purpose use cases. 
---
C-53711r1_chk
---
Check the file ""/etc/selinux/config"" and ensure the following line appears:

SELINUXTYPE=targeted

If it does not, this is a finding. ",1.0,98977890,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-51369] [low] The system must use a Linux Security Module configured to limit the privileges of system services.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977890
,2015-07-13T19:42:35Z,unscheduled,,"Permissions on system binaries and configuration files that are too generous could allow an unauthorized user to gain privileges that they should not have. The permissions set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50252r1_rule
---
F-43398r1_fix
---
The RPM package management system can restore file access permissions of package files and directories. The following command will update permissions on files and directories with permissions different from what is expected by the RPM database: 

# rpm --setperms [package]
---
C-46008r1_chk
---
The following command will list which files and directories on the system have permissions different from what is expected by the RPM database: 

# rpm -Va  | grep '^.M'

If there is any output, for each file or directory found, find the associated RPM package and compare the RPM-expected permissions with the actual permissions on the file or directory:

# rpm -qf [file or directory name]
# rpm -q --queryformat ""[%{FILENAMES} %{FILEMODES:perms}\n]"" [package] | grep  [filename]
# ls -lL [filename]

If the existing permissions are more permissive than those expected by RPM, this is a finding.",1.0,98977892,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38452] [low] The system package management tool must verify permissions on all files and directories associated with packages.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977892
,2015-07-13T19:42:35Z,unscheduled,,"Group-ownership of system binaries and configuration files that is incorrect could allow an unauthorized user to gain privileges that they should not have. The group-ownership set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50253r1_rule
---
F-43399r1_fix
---
The RPM package management system can restore group-ownership of the package files and directories. The following command will update files and directories with group-ownership different from what is expected by the RPM database: 

# rpm -qf [file or directory name]
# rpm --setugids [package]
---
C-46009r1_chk
---
The following command will list which files on the system have group-ownership different from what is expected by the RPM database: 

# rpm -Va | grep '^......G'


If there is output, this is a finding.",1.0,98977894,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38453] [low] The system package management tool must verify group-ownership on all files and directories associated with packages.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977894
,2015-07-13T19:42:35Z,unscheduled,,"The hash on important files like system executables should match the information given by the RPM database. Executables with erroneous hashes could be a sign of nefarious activity on the system.
---
None
---
SV-50247r2_rule
---
F-43392r1_fix
---
The RPM package management system can check the hashes of installed software packages, including many that are important to system security. Run the following command to list which files on the system have hashes that differ from what is expected by the RPM database: 

# rpm -Va | grep '^..5'

A ""c"" in the second column indicates that a file is a configuration file, which may appropriately be expected to change. If the file that has changed was not expected to then refresh from distribution media or online repositories. 

rpm -Uvh [affected_package]

OR 

yum reinstall [affected_package]
---
C-46002r3_chk
---
The following command will list which files on the system have file hashes different from what is expected by the RPM database. 

# rpm -Va | awk '$1 ~ /..5/ && $2 != ""c""'


If there is output, this is a finding.",1.0,98977898,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38447] [low] The system package management tool must verify contents of all files associated with packages.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977898
,2015-07-13T19:42:35Z,unscheduled,,"The risk of a system's physical compromise, particularly mobile systems such as laptops, places its data at risk of compromise. Encrypting this data mitigates the risk of its loss if the system is lost.
---
None
---
SV-50460r1_rule
---
F-43609r1_fix
---
Red Hat Enterprise Linux 6 natively supports partition encryption through the Linux Unified Key Setup-on-disk-format (LUKS) technology. The easiest way to encrypt a partition is during installation time. 

For manual installations, select the ""Encrypt"" checkbox during partition creation to encrypt the partition. When this option is selected the system will prompt for a passphrase to use in decrypting the partition. The passphrase will subsequently need to be entered manually every time the system boots. 

For automated/unattended installations, it is possible to use Kickstart by adding the ""--encrypted"" and ""--passphrase="" options to the definition of each partition to be encrypted. For example, the following line would encrypt the root partition: 

part / --fstype=ext3 --size=100 --onpart=hda1 --encrypted --passphrase=[PASSPHRASE]

Any [PASSPHRASE] is stored in the Kickstart in plaintext, and the Kickstart must then be protected accordingly. Omitting the ""--passphrase="" option from the partition definition will cause the installer to pause and interactively ask for the passphrase during installation. 

Detailed information on encrypting partitions using LUKS can be found on the Red Had Documentation web site:
https://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/sect-Security_Guide-LUKS_Disk_Encryption.html
---
C-46220r1_chk
---
Determine if encryption must be used to protect data on the system. 
If encryption must be used and is not employed, this is a finding.",1.0,98977900,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38659] [low] The operating system must employ cryptographic mechanisms to protect information in storage.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977900
,2015-07-13T19:42:35Z,unscheduled,,"Packet signing can prevent man-in-the-middle attacks which modify SMB packets in transit.
---
None
---
SV-50457r1_rule
---
F-43606r1_fix
---
To require samba clients running ""smbclient"" to use packet signing, add the following to the ""[global]"" section of the Samba configuration file in ""/etc/samba/smb.conf"": 

client signing = mandatory

Requiring samba clients such as ""smbclient"" to use packet signing ensures they can only communicate with servers that support packet signing.
---
C-46217r1_chk
---
To verify that Samba clients running smbclient must use packet signing, run the following command: 

# grep signing /etc/samba/smb.conf

The output should show: 

client signing = mandatory


If it is not, this is a finding.",1.0,98977906,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38656] [low] The system must use SMB client signing for connecting to samba servers using smbclient.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977906
,2015-07-13T19:42:35Z,unscheduled,,"Packet signing can prevent man-in-the-middle attacks which modify SMB packets in transit.
---
None
---
SV-50458r2_rule
---
F-43607r1_fix
---
Require packet signing of clients who mount Samba shares using the ""mount.cifs"" program (e.g., those who specify shares in ""/etc/fstab""). To do so, ensure signing options (either ""sec=krb5i"" or ""sec=ntlmv2i"") are used. 

See the ""mount.cifs(8)"" man page for more information. A Samba client should only communicate with servers who can support SMB packet signing.
---
C-46218r4_chk
---
If Samba is not in use, this is not applicable.

To verify that Samba clients using mount.cifs must use packet signing, run the following command: 

# grep sec /etc/fstab /etc/mtab

The output should show either ""krb5i"" or ""ntlmv2i"" in use. 
If it does not, this is a finding.",1.0,98977908,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38657] [low] The system must use SMB client signing for connecting to samba servers using mount.cifs.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977908
,2015-07-13T19:42:35Z,unscheduled,,"All filesystems that are required for the successful operation of the system should be explicitly listed in ""/etc/fstab"" by an administrator. New filesystems should not be arbitrarily introduced via the automounter.

The ""autofs"" daemon mounts and unmounts filesystems, such as user home directories shared via NFS, on demand. In addition, autofs can be used to handle removable media, and the default configuration provides the cdrom device as ""/misc/cd"". However, this method of providing access to removable media is not common, so autofs can almost always be disabled if NFS is not in use. Even if NFS is required, it is almost always possible to configure filesystem mounts statically by editing ""/etc/fstab"" rather than relying on the automounter. 
---
None
---
SV-50237r1_rule
---
F-43381r1_fix
---
If the ""autofs"" service is not needed to dynamically mount NFS filesystems or removable media, disable the service for all runlevels: 

# chkconfig --level 0123456 autofs off

Stop the service if it is already running: 

# service autofs stop
---
C-45991r1_chk
---
To verify the ""autofs"" service is disabled, run the following command: 

chkconfig --list autofs

If properly configured, the output should be the following: 

autofs 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Verify the ""autofs"" service is not running:

# service autofs status

If the autofs service is enabled or running, this is a finding.",1.0,98977910,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38437] [low] Automated file system mounting tools must not be enabled unless needed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977910
,2015-07-13T19:42:35Z,unscheduled,,"Ensuring all packages' cryptographic signatures are valid prior to installation ensures the provenance of the software and protects against malicious tampering.
---
None
---
SV-50288r1_rule
---
F-43433r1_fix
---
To ensure signature checking is not disabled for any repos, remove any lines from files in ""/etc/yum.repos.d"" of the form: 

gpgcheck=0
---
C-46043r1_chk
---
To determine whether ""yum"" has been configured to disable ""gpgcheck"" for any repos, inspect all files in ""/etc/yum.repos.d"" and ensure the following does not appear in any sections: 

gpgcheck=0

A value of ""0"" indicates that ""gpgcheck"" has been disabled for that repo. 
If GPG checking is disabled, this is a finding.

If the ""yum"" system package management tool is not used to update the system, verify with the SA that installed packages are cryptographically signed.",1.0,98977920,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38487] [low] The system package management tool must cryptographically verify the authenticity of all software packages during installation.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977920
,2015-07-13T19:42:35Z,unscheduled,,"Setting the password warning age enables users to make the change at a practical time.
---
None
---
SV-50280r1_rule
---
F-43425r1_fix
---
To specify how many days prior to password expiration that a warning will be issued to users, edit the file ""/etc/login.defs"" and add or correct the following line, replacing [DAYS] appropriately: 

PASS_WARN_AGE [DAYS]

The DoD requirement is 7.
---
C-46035r1_chk
---
To check the password warning age, run the command: 

$ grep PASS_WARN_AGE /etc/login.defs

The DoD requirement is 7. 
If it is not set to the required value, this is a finding.",1.0,98977922,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38480] [low] Users must be warned 7 days in advance of password expiration.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977922
,2015-07-13T19:42:36Z,unscheduled,,"The presence of ""martian"" packets (which have impossible addresses) as well as spoofed packets, source-routed packets, and redirects could be a sign of nefarious network activity. Logging these packets enables this activity to be detected.
---
None
---
SV-50329r2_rule
---
F-43476r1_fix
---
To set the runtime status of the ""net.ipv4.conf.all.log_martians"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.all.log_martians=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.all.log_martians = 1
---
C-46086r3_chk
---
The status of the ""net.ipv4.conf.all.log_martians"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.all.log_martians

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.all.log_martians /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98977924,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38528] [low] The system must log Martian packets.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977924
,2015-07-13T19:42:36Z,unscheduled,,"The risk of a system's physical compromise, particularly mobile systems such as laptops, places its data at risk of compromise. Encrypting this data mitigates the risk of its loss if the system is lost.
---
None
---
SV-50463r1_rule
---
F-43611r1_fix
---
Red Hat Enterprise Linux 6 natively supports partition encryption through the Linux Unified Key Setup-on-disk-format (LUKS) technology. The easiest way to encrypt a partition is during installation time. 

For manual installations, select the ""Encrypt"" checkbox during partition creation to encrypt the partition. When this option is selected the system will prompt for a passphrase to use in decrypting the partition. The passphrase will subsequently need to be entered manually every time the system boots. 

For automated/unattended installations, it is possible to use Kickstart by adding the ""--encrypted"" and ""--passphrase="" options to the definition of each partition to be encrypted. For example, the following line would encrypt the root partition: 

part / --fstype=ext3 --size=100 --onpart=hda1 --encrypted --passphrase=[PASSPHRASE]

Any [PASSPHRASE] is stored in the Kickstart in plaintext, and the Kickstart must then be protected accordingly. Omitting the ""--passphrase="" option from the partition definition will cause the installer to pause and interactively ask for the passphrase during installation. 

Detailed information on encrypting partitions using LUKS can be found on the Red Had Documentation web site:
https://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/sect-Security_Guide-LUKS_Disk_Encryption.html
---
C-46222r1_chk
---
Determine if encryption must be used to protect data on the system. 
If encryption must be used and is not employed, this is a finding.",1.0,98977928,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38662] [low] The operating system must employ cryptographic mechanisms to prevent unauthorized disclosure of data at rest unless otherwise protected by alternative physical measures.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977928
,2015-07-13T19:42:36Z,unscheduled,,"Local mail delivery is essential to some system maintenance and notification tasks.
---
None
---
SV-50470r1_rule
---
F-43618r1_fix
---
The Postfix mail transfer agent is used for local mail delivery within the system. The default configuration only listens for connections to the default SMTP port (port 25) on the loopback interface (127.0.0.1). It is recommended to leave this service enabled for local mail delivery. The ""postfix"" service can be enabled with the following command: 

# chkconfig postfix on
# service postfix start
---
C-46230r1_chk
---
Run the following command to determine the current status of the ""postfix"" service:

# service postfix status

If the service is enabled, it should return the following:

postfix is running...

If the service is not enabled, this is a finding.",1.0,98977930,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38669] [low] The postfix service must be enabled for mail delivery.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977930
,2015-07-13T19:42:36Z,unscheduled,,"Placing ""/var/log"" in its own partition enables better separation between log files and other files in ""/var/"".
---
None
---
SV-50263r1_rule
---
F-43408r1_fix
---
System logs are stored in the ""/var/log"" directory. Ensure that it has its own partition or logical volume at installation time, or migrate it using LVM.
---
C-46018r1_chk
---
Run the following command to determine if ""/var/log"" is on its own partition or logical volume: 

$ mount | grep ""on /var/log ""

If ""/var/log"" has its own partition or volume group, a line will be returned. 
If no line is returned, this is a finding.",1.0,98977934,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38463] [low] The system must use a separate file system for /var/log.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977934
,2015-07-13T19:42:36Z,unscheduled,,"The ""all_squash"" option maps all client requests to a single anonymous uid/gid on the NFS server, negating the ability to track file access by user ID.
---
None
---
SV-50260r1_rule
---
F-43405r1_fix
---
Remove any instances of the ""all_squash"" option from the file ""/etc/exports"".  Restart the NFS daemon for the changes to take effect.

# service nfs restart
---
C-46016r1_chk
---
If the NFS server is read-only, in support of unrestricted access to organizational content, this is not applicable.

The related ""root_squash"" option provides protection against remote administrator-level access to NFS server content.  Its use is not a finding.

To verify the ""all_squash"" option has been disabled, run the following command:

# grep all_squash /etc/exports


If there is output, this is a finding.",1.0,98977936,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38460] [low] The NFS server must not have the all_squash option enabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977936
,2015-07-13T19:42:36Z,unscheduled,,"To trace malicious activity facilitated by the FTP service, it must be configured to ensure that all commands sent to the ftp server are logged using the verbose vsftpd log format. The default vsftpd log file is /var/log/vsftpd.log.
---
None
---
SV-50503r1_rule
---
F-43651r1_fix
---
Add or correct the following configuration options within the ""vsftpd"" configuration file, located at ""/etc/vsftpd/vsftpd.conf"". 

xferlog_enable=YES
xferlog_std_format=NO
log_ftp_protocol=YES
---
C-46264r1_chk
---
Find if logging is applied to the ftp daemon. 

Procedures: 

If vsftpd is started by xinetd the following command will indicate the xinetd.d startup file. 

# grep vsftpd /etc/xinetd.d/*



# grep server_args [vsftpd xinetd.d startup file]

This will indicate the vsftpd config file used when starting through xinetd. If the [server_args]line is missing or does not include the vsftpd configuration file, then the default config file (/etc/vsftpd/vsftpd.conf) is used. 

# grep xferlog_enable [vsftpd config file]


If xferlog_enable is missing, or is not set to yes, this is a finding.",1.0,98977938,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38702] [low] The FTP daemon must be configured for logging or verbose mode.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977938
,2015-07-13T19:42:36Z,unscheduled,,"Preventing direct root login to serial port interfaces helps ensure accountability for actions taken on the systems using the root account.
---
None
---
SV-50295r1_rule
---
F-43441r1_fix
---
To restrict root logins on serial ports, ensure lines of this form do not appear in ""/etc/securetty"": 

ttyS0
ttyS1

Note:  Serial port entries are not limited to those listed above.  Any lines starting with ""ttyS"" followed by numerals should be removed
---
C-46051r1_chk
---
To check for serial port entries which permit root login, run the following command: 

# grep '^ttyS[0-9]' /etc/securetty

If any output is returned, then root login over serial ports is permitted. 
If root login over serial ports is permitted, this is a finding.",1.0,98977956,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38494] [low] The system must prevent the root account from logging in from serial consoles.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977956
,2015-07-13T19:42:36Z,unscheduled,,"The ""netconsole"" service is not necessary unless there is a need to debug kernel panics, which is not common.
---
None
---
SV-50473r2_rule
---
F-43622r2_fix
---
The ""netconsole"" service is responsible for loading the netconsole kernel module, which logs kernel printk messages over UDP to a syslog server. This allows debugging of problems where disk logging fails and serial consoles are impractical. The ""netconsole"" service can be disabled with the following commands: 

# chkconfig netconsole off
# service netconsole stop
---
C-46233r1_chk
---
To check that the ""netconsole"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""netconsole"" --list

Output should indicate the ""netconsole"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""netconsole"" --list
""netconsole"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""netconsole"" is disabled through current runtime configuration: 

# service netconsole status

If the service is disabled the command will return the following output: 

netconsole is stopped


If the service is running, this is a finding.",1.0,98977958,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38672] [low] The netconsole service must be disabled unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977958
,2015-07-13T19:42:36Z,unscheduled,,"Unnecessary packages should not be installed to decrease the attack surface of the system.
---
None
---
SV-50477r1_rule
---
F-43625r1_fix
---
Removing all packages which constitute the X Window System ensures users or malicious software cannot start X. To do so, run the following command: 

# yum groupremove ""X Window System""
---
C-46236r1_chk
---
To ensure the X Windows package group is removed, run the following command: 

$ rpm -qi xorg-x11-server-common

The output should be: 

package xorg-x11-server-common is not installed


If it is not, this is a finding.",1.0,98977960,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38676] [low] The xorg-x11-server-common (X Windows) package must not be installed, unless required.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977960
,2015-07-13T19:42:36Z,unscheduled,,"A core dump includes a memory image taken at the time the operating system terminates an application. The memory image could contain sensitive data and is generally useful only for developers trying to debug problems.
---
None
---
SV-50476r2_rule
---
F-43624r1_fix
---
To disable core dumps for all users, add the following line to ""/etc/security/limits.conf"": 

* hard core 0
---
C-46235r2_chk
---
To verify that core dumps are disabled for all users, run the following command:

$ grep core /etc/security/limits.conf /etc/security/limits.d/*.conf

The output should be:

* hard core 0

If it is not, this is a finding. ",1.0,98977962,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38675] [low] Process core dumps must be disabled unless needed.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977962
,2015-07-13T19:42:37Z,unscheduled,,"The ability to lock graphical desktop sessions manually allows users to easily secure their accounts should they need to depart from their workstations temporarily.
---
None
---
SV-50274r2_rule
---
F-43420r1_fix
---
Run the following command to set the Gnome desktop keybinding for locking the screen:

# gconftool-2
--direct \
--config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory \
--type string \
--set /apps/gnome_settings_daemon/keybindings/screensaver ""<Control><Alt>l""

Another keyboard sequence may be substituted for ""<Control><Alt>l"", which is the default for the Gnome desktop.
---
C-46030r2_chk
---
If the GConf2 package is not installed, this is not applicable.

Verify the keybindings for the Gnome screensaver:

# gconftool-2 --direct --config-source xml:readwrite:/etc/gconf/gconf.xml.mandatory --get /apps/gnome_settings_daemon/keybindings/screensaver

If no output is visible, this is a finding.",1.0,98977964,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38474] [low] The system must allow locking of graphical desktop sessions.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977964
,2015-07-13T19:42:37Z,unscheduled,,"Ensuring that ""/home"" is mounted on its own partition enables the setting of more restrictive mount options, and also helps ensure that users cannot trivially fill partitions used for log or audit data storage.
---
None
---
SV-50273r1_rule
---
F-43418r1_fix
---
If user home directories will be stored locally, create a separate partition for ""/home"" at installation time (or migrate it later using LVM). If ""/home"" will be mounted from another system such as an NFS server, then creating a separate partition is not necessary at installation time, and the mountpoint can instead be configured later.
---
C-46028r1_chk
---
Run the following command to determine if ""/home"" is on its own partition or logical volume: 

$ mount | grep ""on /home ""

If ""/home"" has its own partition or volume group, a line will be returned. 
If no line is returned, this is a finding.",1.0,98977968,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38473] [low] The system must use a separate file system for user home directories.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977968
,2015-07-13T19:42:37Z,unscheduled,,"Although systems management and patching is extremely important to system security, management by a system outside the enterprise enclave is not desirable for some environments. However, if the system is being managed by RHN or RHN Satellite Server the ""rhnsd"" daemon can remain on.
---
None
---
SV-50278r2_rule
---
F-43423r2_fix
---
The Red Hat Network service automatically queries Red Hat Network servers to determine whether there are any actions that should be executed, such as package updates. This only occurs if the system was registered to an RHN server or satellite and managed as such. The ""rhnsd"" service can be disabled with the following commands: 

# chkconfig rhnsd off
# service rhnsd stop
---
C-46033r2_chk
---
If the system uses RHN or an RHN Satellite, this is not applicable.

To check that the ""rhnsd"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""rhnsd"" --list

Output should indicate the ""rhnsd"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""rhnsd"" --list
""rhnsd"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""rhnsd"" is disabled through current runtime configuration: 

# service rhnsd status

If the service is disabled the command will return the following output: 

rhnsd is stopped


If the service is running, this is a finding.",1.0,98977972,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38478] [low] The Red Hat Network Service (rhnsd) service must not be running, unless using RHN or an RHN Satellite.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977972
,2015-07-13T19:42:37Z,unscheduled,,"Requiring digits makes password guessing attacks more difficult by ensuring a larger search space.
---
None
---
SV-50282r1_rule
---
F-43427r1_fix
---
The pam_cracklib module's ""dcredit"" parameter controls requirements for usage of digits in a password. When set to a negative number, any password will be required to contain that many digits. When set to a positive number, pam_cracklib will grant +1 additional length credit for each digit. Add ""dcredit=-1"" after pam_cracklib.so to require use of a digit in passwords.
---
C-46037r1_chk
---
To check how many digits are required in a password, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

The ""dcredit"" parameter (as a negative number) will indicate how many digits are required. The DoD requires at least one digit in a password. This would appear as ""dcredit=-1"". 
If dcredit is not found or not set to the required value, this is a finding.",1.0,98977982,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38482] [low] The system must require passwords to contain at least one numeric character.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977982
,2015-07-13T19:42:37Z,unscheduled,,"Ownership of system binaries and configuration files that is incorrect could allow an unauthorized user to gain privileges that they should not have. The ownership set by the vendor should be maintained. Any deviations from this baseline should be investigated.
---
None
---
SV-50254r1_rule
---
F-43400r1_fix
---
The RPM package management system can restore ownership of package files and directories. The following command will update files and directories with ownership different from what is expected by the RPM database: 

# rpm -qf [file or directory name]
# rpm --setugids [package]
---
C-46010r1_chk
---
The following command will list which files on the system have ownership different from what is expected by the RPM database: 

# rpm -Va | grep '^.....U'


If there is output, this is a finding.",1.0,98977984,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38454] [low] The system package management tool must verify ownership on all files and directories associated with packages.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977984
,2015-07-13T19:42:37Z,unscheduled,,"Providing the ability for remote users or systems to initiate a secure VPN connection protects information when it is transmitted over a wide area network.
---
None
---
SV-50488r2_rule
---
F-43636r1_fix
---
The Openswan package provides an implementation of IPsec and IKE, which permits the creation of secure tunnels over untrusted networks. The ""openswan"" package can be installed with the following command: 

# yum install openswan
---
C-46249r2_chk
---
If the system does not communicate over untrusted networks, this is not applicable.

Run the following command to determine if the ""openswan"" package is installed: 

# rpm -q openswan


If the package is not installed, this is a finding.",1.0,98977986,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38687] [low] The system must provide VPN connectivity for communications over untrusted networks.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977986
,2015-07-13T19:42:37Z,unscheduled,,"When temporary accounts are created, there is a risk they may remain in place and active after the need for them no longer exists. Account expiration greatly reduces the risk of accounts being misused or hijacked.
---
None
---
SV-50486r1_rule
---
F-43634r1_fix
---
In the event temporary accounts are required, configure the system to terminate them after a documented time period. For every temporary account, run the following command to set an expiration date on it, substituting ""[USER]"" and ""[YYYY-MM-DD]"" appropriately: 

# chage -E [YYYY-MM-DD] [USER]

""[YYYY-MM-DD]"" indicates the documented expiration date for the account.
---
C-46247r1_chk
---
For every temporary account, run the following command to obtain its account aging and expiration information: 

# chage -l [USER]

Verify each of these accounts has an expiration date set as documented. 
If any temporary accounts have no expiration date set or do not expire within a documented time frame, this is a finding.",1.0,98977988,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38685] [low] Temporary accounts must be provisioned with an expiration date.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977988
,2015-07-13T19:42:37Z,unscheduled,,"Limiting simultaneous user logins can insulate the system from denial of service problems caused by excessive logins. Automated login processes operating improperly or maliciously may result in an exceptional number of simultaneous login sessions.
---
None
---
SV-50485r2_rule
---
F-43633r1_fix
---
Limiting the number of allowed users and sessions per user can limit risks related to denial of service attacks. This addresses concurrent sessions for a single account and does not address concurrent sessions by a single user via multiple accounts. To set the number of concurrent sessions per user add the following line in ""/etc/security/limits.conf"": 

* hard maxlogins 10

A documented site-defined number may be substituted for 10 in the above.
---
C-46246r2_chk
---
Run the following command to ensure the ""maxlogins"" value is configured for all users on the system:

$ grep ""maxlogins"" /etc/security/limits.conf /etc/security/limits.d/*.conf

You should receive output similar to the following:

* hard maxlogins 10

If it is not similar, this is a finding. ",1.0,98977990,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38684] [low] The system must limit users to 10 simultaneous system logins, or a site-defined number, in accordance with operational requirements.",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977990
,2015-07-13T19:42:37Z,unscheduled,,"Unique usernames allow for accountability on the system.
---
None
---
SV-50484r1_rule
---
F-43632r1_fix
---
Change usernames, or delete accounts, so each has a unique name.
---
C-46245r1_chk
---
Run the following command to check for duplicate account names: 

# pwck -rq

If there are no duplicate names, no line will be returned. 
If a line is returned, this is a finding.",1.0,98977992,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38683] [low] All accounts on the system must have unique user or account names,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977992
,2015-07-13T19:42:37Z,unscheduled,,"Inconsistency in GIDs between /etc/passwd and /etc/group could lead to a user having unintended rights.
---
None
---
SV-50482r2_rule
---
F-43630r1_fix
---
Add a group to the system for each GID referenced without a corresponding group.
---
C-46243r2_chk
---
To ensure all GIDs referenced in /etc/passwd are defined in /etc/group, run the following command: 

# pwck -r | grep 'no group'

There should be no output. 
If there is output, this is a finding.",1.0,98977994,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38681] [low] All GIDs referenced in /etc/passwd must be defined in /etc/group,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98977994
,2015-07-13T19:42:37Z,unscheduled,,"Requiring a minimum number of lowercase characters makes password guessing attacks more difficult by ensuring a larger search space.
---
None
---
SV-50372r1_rule
---
F-43519r1_fix
---
The pam_cracklib module's ""lcredit="" parameter controls requirements for usage of lowercase letters in a password. When set to a negative number, any password will be required to contain that many lowercase characters. When set to a positive number, pam_cracklib will grant +1 additional length credit for each lowercase character. Add ""lcredit=-1"" after pam_cracklib.so to require use of a lowercase character in passwords.
---
C-46129r1_chk
---
To check how many lowercase characters are required in a password, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

The ""lcredit"" parameter (as a negative number) will indicate how many special characters are required. The DoD requires at least one lowercase character in a password. This would appear as ""lcredit=-1"". 
If lcredit is not found or not set to the required value, this is a finding.",1.0,98978004,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38571] [low] The system must require passwords to contain at least one lowercase alphabetic character.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978004
,2015-07-13T19:42:37Z,unscheduled,,"Requiring a minimum number of different characters during password changes ensures that newly changed passwords should not resemble previously compromised ones. Note that passwords which are changed on compromised systems will still be compromised, however.
---
None
---
SV-50373r1_rule
---
F-43520r1_fix
---
The pam_cracklib module's ""difok"" parameter controls requirements for usage of different characters during a password change. Add ""difok=[NUM]"" after pam_cracklib.so to require differing characters when changing passwords, substituting [NUM] appropriately. The DoD requirement is 4.
---
C-46130r1_chk
---
To check how many characters must differ during a password change, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

The ""difok"" parameter will indicate how many characters must differ. The DoD requires four characters differ during a password change. This would appear as ""difok=4"". 
If difok is not found or not set to the required value, this is a finding.",1.0,98978006,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38572] [low] The system must require at least four characters be changed between the old and new passwords during a password change.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978006
,2015-07-13T19:42:37Z,unscheduled,,"Allowing a user account to own a world-writable directory is undesirable because it allows the owner of that directory to remove or replace any files that may be placed in the directory by other users.
---
None
---
SV-50500r2_rule
---
F-43648r1_fix
---
All directories in local partitions which are world-writable should be owned by root or another system account. If any world-writable directories are not owned by a system account, this should be investigated. Following this, the files should be deleted or assigned to an appropriate group.
---
C-46260r3_chk
---
The following command will discover and print world-writable directories that are not owned by a system account, given the assumption that only system accounts have a uid lower than 500. Run it once for each local partition [PART]: 

# find [PART] -xdev -type d -perm -0002 -uid +499 -print


If there is output, this is a finding.",1.0,98978008,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38699] [low] All public directories must be owned by a system account.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978008
,2015-07-13T19:42:37Z,unscheduled,,"Disabling RDS protects the system against exploitation of any flaws in its implementation.
---
None
---
SV-50317r3_rule
---
F-43463r4_fix
---
The Reliable Datagram Sockets (RDS) protocol is a transport layer protocol designed to provide reliable high-bandwidth, low-latency communications between nodes in a cluster. To configure the system to prevent the ""rds"" kernel module from being loaded, add the following line to a file in the directory ""/etc/modprobe.d"": 

install rds /bin/true
---
C-46073r3_chk
---
If the system is configured to prevent the loading of the ""rds"" kernel module, it will contain lines inside any file in ""/etc/modprobe.d"" or the deprecated ""/etc/modprobe.conf"". These lines instruct the module loading system to run another program (such as ""/bin/true"") upon a module ""install"" event. Run the following command to search for such lines in all files in ""/etc/modprobe.d"" and the deprecated ""/etc/modprobe.conf"": 

$ grep -r rds /etc/modprobe.conf /etc/modprobe.d

If no line is returned, this is a finding.",1.0,98978010,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38516] [low] The Reliable Datagram Sockets (RDS) protocol must be disabled unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978010
,2015-07-13T19:42:37Z,unscheduled,,"When emergency accounts are created, there is a risk they may remain in place and active after the need for them no longer exists. Account expiration greatly reduces the risk of accounts being misused or hijacked.
---
None
---
SV-50491r1_rule
---
F-43639r1_fix
---
In the event emergency accounts are required, configure the system to terminate them after a documented time period. For every emergency account, run the following command to set an expiration date on it, substituting ""[USER]"" and ""[YYYY-MM-DD]"" appropriately: 

# chage -E [YYYY-MM-DD] [USER]

""[YYYY-MM-DD]"" indicates the documented expiration date for the account.
---
C-46251r1_chk
---
For every emergency account, run the following command to obtain its account aging and expiration information: 

# chage -l [USER]

Verify each of these accounts has an expiration date set as documented. 
If any emergency accounts have no expiration date set or do not expire within a documented time frame, this is a finding.",1.0,98978012,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]","[V-38690] [low] Emergency accounts must be provisioned with an expiration date.
",,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978012
,2015-07-13T19:42:38Z,unscheduled,,"Passwords with excessive repeating characters may be more vulnerable to password-guessing attacks.
---
None
---
SV-50494r2_rule
---
F-43642r2_fix
---
The pam_cracklib module's ""maxrepeat"" parameter controls requirements for consecutive repeating characters. When set to a positive number, it will reject passwords which contain more than that number of consecutive characters. Add ""maxrepeat=3"" after pam_cracklib.so to prevent a run of (3 + 1) or more identical characters. 

password required pam_cracklib.so maxrepeat=3 
---
C-46255r1_chk
---
To check the maximum value for consecutive repeating characters, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

Look for the value of the ""maxrepeat"" parameter. The DoD requirement is 3. 
If maxrepeat is not found or not set to the required value, this is a finding.",1.0,98978014,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38693] [low] The system must require passwords to contain no more than three consecutive repeating characters.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978014
,2015-07-13T19:42:38Z,unscheduled,,"Installing ""screen"" ensures a console locking capability is available for users who may need to suspend console logins.
---
None
---
SV-50391r1_rule
---
F-43538r1_fix
---
To enable console screen locking when in text mode, install the ""screen"" package: 

# yum install screen

Instruct users to begin new terminal sessions with the following command: 

$ screen

The console can now be locked with the following key combination: 

ctrl+a x
---
C-46148r1_chk
---
Run the following command to determine if the ""screen"" package is installed: 

# rpm -q screen


If the package is not installed, this is a finding.",1.0,98978016,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38590] [low] The system must allow locking of the console screen in text mode.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978016
,2015-07-13T19:42:38Z,unscheduled,,"Ensuring that ""/var"" is mounted on its own partition enables the setting of more restrictive mount options. This helps protect system services such as daemons or other programs which use it. It is not uncommon for the ""/var"" directory to contain world-writable directories, installed by other software packages.
---
None
---
SV-50256r1_rule
---
F-43401r2_fix
---
The ""/var"" directory is used by daemons and other system services to store frequently-changing data. Ensure that ""/var"" has its own partition or logical volume at installation time, or migrate it using LVM.
---
C-46011r2_chk
---
Run the following command to determine if ""/var"" is on its own partition or logical volume: 

$ mount | grep ""on /var ""

If ""/var"" has its own partition or volume group, a line will be returned. 
If no line is returned, this is a finding.",1.0,98978018,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38456] [low] The system must use a separate file system for /var.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978018
,2015-07-13T19:42:38Z,unscheduled,,"Because the Avahi daemon service keeps an open network port, it is subject to network attacks. Its functionality is convenient but is only appropriate if the local network can be trusted.
---
None
---
SV-50419r2_rule
---
F-43567r2_fix
---
The ""avahi-daemon"" service can be disabled with the following commands: 

# chkconfig avahi-daemon off
# service avahi-daemon stop
---
C-46177r1_chk
---
To check that the ""avahi-daemon"" service is disabled in system boot configuration, run the following command: 

# chkconfig ""avahi-daemon"" --list

Output should indicate the ""avahi-daemon"" service has either not been installed, or has been disabled at all runlevels, as shown in the example below: 

# chkconfig ""avahi-daemon"" --list
""avahi-daemon"" 0:off 1:off 2:off 3:off 4:off 5:off 6:off

Run the following command to verify ""avahi-daemon"" is disabled through current runtime configuration: 

# service avahi-daemon status

If the service is disabled the command will return the following output: 

avahi-daemon is stopped


If the service is running, this is a finding.",1.0,98978022,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38618] [low] The avahi service must be disabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978022
,2015-07-13T19:42:38Z,unscheduled,,"Requiring a minimum number of special characters makes password guessing attacks more difficult by ensuring a larger search space.
---
None
---
SV-50371r1_rule
---
F-43518r1_fix
---
The pam_cracklib module's ""ocredit="" parameter controls requirements for usage of special (or ``other'') characters in a password. When set to a negative number, any password will be required to contain that many special characters. When set to a positive number, pam_cracklib will grant +1 additional length credit for each special character. Add ""ocredit=-1"" after pam_cracklib.so to require use of a special character in passwords.
---
C-46128r1_chk
---
To check how many special characters are required in a password, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

The ""ocredit"" parameter (as a negative number) will indicate how many special characters are required. The DoD requires at least one special character in a password. This would appear as ""ocredit=-1"". 
If ocredit is not found or not set to the required value, this is a finding.",1.0,98978024,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38570] [low] The system must require passwords to contain at least one special character.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978024
,2015-07-13T19:42:38Z,unscheduled,,"Requiring a minimum number of uppercase characters makes password guessing attacks more difficult by ensuring a larger search space.
---
None
---
SV-50370r1_rule
---
F-43517r1_fix
---
The pam_cracklib module's ""ucredit="" parameter controls requirements for usage of uppercase letters in a password. When set to a negative number, any password will be required to contain that many uppercase characters. When set to a positive number, pam_cracklib will grant +1 additional length credit for each uppercase character. Add ""ucredit=-1"" after pam_cracklib.so to require use of an uppercase character in passwords.
---
C-46127r1_chk
---
To check how many uppercase characters are required in a password, run the following command: 

$ grep pam_cracklib /etc/pam.d/system-auth

The ""ucredit"" parameter (as a negative number) will indicate how many uppercase characters are required. The DoD requires at least one uppercase character in a password. This would appear as ""ucredit=-1"". 
If ucredit is not found or not set to the required value, this is a finding.",1.0,98978028,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38569] [low] The system must require passwords to contain at least one uppercase alphabetic character.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978028
,2015-07-13T19:42:38Z,unscheduled,,"Ignoring bogus ICMP error responses reduces log size, although some activity would not be logged.
---
None
---
SV-50338r2_rule
---
F-43485r1_fix
---
To set the runtime status of the ""net.ipv4.icmp_ignore_bogus_error_responses"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.icmp_ignore_bogus_error_responses = 1
---
C-46095r2_chk
---
The status of the ""net.ipv4.icmp_ignore_bogus_error_responses"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.icmp_ignore_bogus_error_responses

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.icmp_ignore_bogus_error_responses /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978038,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38537] [low] The system must ignore ICMPv4 bogus error responses.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978038
,2015-07-13T19:42:38Z,unscheduled,,"Log files that are not properly rotated run the risk of growing so large that they fill up the /var/log partition. Valuable logging information could be lost if the /var/log partition becomes full.
---
None
---
SV-50425r1_rule
---
F-43573r1_fix
---
The ""logrotate"" service should be installed or reinstalled if it is not installed and operating properly, by running the following command:

# yum reinstall logrotate
---
C-46183r1_chk
---
Run the following commands to determine the current status of the ""logrotate"" service: 

# grep logrotate /var/log/cron*

If the logrotate service is not run on a daily basis by cron, this is a finding.",1.0,98978040,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38624] [low] System logs must be rotated daily.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978040
,2015-07-13T19:42:38Z,unscheduled,,"Unnecessary packages should not be installed to decrease the attack surface of the system.
---
None
---
SV-50428r1_rule
---
F-43577r1_fix
---
The ""openldap-servers"" package should be removed if not in use. Is this machine the OpenLDAP server? If not, remove the package. 

# yum erase openldap-servers

The openldap-servers RPM is not installed by default on RHEL6 machines. It is needed only by the OpenLDAP server, not by the clients which use LDAP for authentication. If the system is not intended for use as an LDAP Server it should be removed.
---
C-46187r1_chk
---
To verify the ""openldap-servers"" package is not installed, run the following command: 

$ rpm -q openldap-servers

The output should show the following. 

package openldap-servers is not installed


If it does not, this is a finding.",1.0,98978042,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38627] [low] The openldap-servers package must not be installed unless required.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978042
,2015-07-13T19:42:38Z,unscheduled,,"Removing the ""xinetd"" package decreases the risk of the xinetd service's accidental (or intentional) activation.
---
None
---
SV-50385r1_rule
---
F-43532r1_fix
---
The ""xinetd"" package can be uninstalled with the following command: 

# yum erase xinetd
---
C-46142r1_chk
---
If network services are using the xinetd service, this is not applicable.

Run the following command to determine if the ""xinetd"" package is installed: 

# rpm -q xinetd


If the package is installed, this is a finding.",1.0,98978044,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38584] [low] The xinetd service must be uninstalled if no network services utilizing it are enabled.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978044
,2015-07-13T19:42:38Z,unscheduled,,"Disabling inactive accounts ensures that accounts which may not have been responsibly removed are not available to attackers who may have compromised their credentials.
---
None
---
SV-50495r1_rule
---
F-43643r2_fix
---
To specify the number of days after a password expires (which signifies inactivity) until an account is permanently disabled, add or correct the following lines in ""/etc/default/useradd"", substituting ""[NUM_DAYS]"" appropriately: 

INACTIVE=[NUM_DAYS]

A value of 35 is recommended. If a password is currently on the verge of expiration, then 35 days remain until the account is automatically disabled. However, if the password will not expire for another 60 days, then 95 days could elapse until the account would be automatically disabled. See the ""useradd"" man page for more information. Determining the inactivity timeout must be done with careful consideration of the length of a ""normal"" period of inactivity for users in the particular environment. Setting the timeout too low incurs support costs and also has the potential to impact availability of the system to legitimate users.
---
C-46256r1_chk
---
To verify the ""INACTIVE"" setting, run the following command: 

grep ""INACTIVE"" /etc/default/useradd

The output should indicate the ""INACTIVE"" configuration option is set to an appropriate integer as shown in the example below: 

# grep ""INACTIVE"" /etc/default/useradd
INACTIVE=35

If it does not, this is a finding.",1.0,98978046,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38694] [low] The operating system must manage information system identifiers for users and devices by disabling the user identifier after an organization defined time period of inactivity.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978046
,2015-07-13T19:42:38Z,unscheduled,,"Failing to set the sticky bit on public directories allows unauthorized users to delete files in the directory structure. 

The only authorized public directories are those temporary directories supplied with the system, or those designed to be temporary file repositories. The setting is normally reserved for directories used by the system, and by users for temporary file storage - such as /tmp - and for directories requiring global read/write access.
---
None
---
SV-50498r2_rule
---
F-43646r1_fix
---
When the so-called 'sticky bit' is set on a directory, only the owner of a given file may remove that file from the directory. Without the sticky bit, any user with write access to a directory may remove any file in the directory. Setting the sticky bit prevents users from removing each other's files. In cases where there is no reason for a directory to be world-writable, a better solution is to remove that permission rather than to set the sticky bit. However, if a directory is used by a particular application, consult that application's documentation instead of blindly changing modes. 
To set the sticky bit on a world-writable directory [DIR], run the following command: 

# chmod +t [DIR]
---
C-46259r4_chk
---
To find world-writable directories that lack the sticky bit, run the following command for each local partition [PART]: 

# find [PART] -xdev -type d -perm -002 \! -perm -1000


If any world-writable directories are missing the sticky bit, this is a finding.",1.0,98978052,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38697] [low] The sticky bit must be set on all public directories.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978052
,2015-07-13T19:42:39Z,unscheduled,,"This feature of the IPv4 protocol has few legitimate uses. It should be disabled unless it is absolutely required.
---
None
---
SV-50334r3_rule
---
F-43481r1_fix
---
To set the runtime status of the ""net.ipv4.conf.default.accept_redirects"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.conf.default.accept_redirects=0

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.conf.default.accept_redirects = 0
---
C-46091r2_chk
---
The status of the ""net.ipv4.conf.default.accept_redirects"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.conf.default.accept_redirects

The output of the command should indicate a value of ""0"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.conf.default.accept_redirects /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978056,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38533] [low] The system must ignore ICMPv4 redirect messages by default.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978056
,2015-07-13T19:42:39Z,unscheduled,,"Ignoring ICMP echo requests (pings) sent to broadcast or multicast addresses makes the system slightly more difficult to enumerate on the network.
---
None
---
SV-50336r2_rule
---
F-43483r1_fix
---
To set the runtime status of the ""net.ipv4.icmp_echo_ignore_broadcasts"" kernel parameter, run the following command: 

# sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1

If this is not the system's default value, add the following line to ""/etc/sysctl.conf"": 

net.ipv4.icmp_echo_ignore_broadcasts = 1
---
C-46093r2_chk
---
The status of the ""net.ipv4.icmp_echo_ignore_broadcasts"" kernel parameter can be queried by running the following command:

$ sysctl net.ipv4.icmp_echo_ignore_broadcasts

The output of the command should indicate a value of ""1"". If this value is not the default value, investigate how it could have been adjusted at runtime, and verify it is not set improperly in ""/etc/sysctl.conf"".

$ grep net.ipv4.icmp_echo_ignore_broadcasts /etc/sysctl.conf

If the correct value is not returned, this is a finding. ",1.0,98978058,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38535] [low] The system must not respond to ICMPv4 sent to a broadcast address.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978058
,2015-07-13T19:42:39Z,unscheduled,,"Allowing users to execute binaries from removable media such as USB keys exposes the system to potential compromise.
---
None
---
SV-50456r1_rule
---
F-43605r1_fix
---
The ""noexec"" mount option prevents the direct execution of binaries on the mounted filesystem. Users should not be allowed to execute binaries that exist on partitions mounted from removable media (such as a USB key). The ""noexec"" option prevents code from being executed directly from the media itself, and may therefore provide a line of defense against certain types of worms or malicious code. Add the ""noexec"" option to the fourth column of ""/etc/fstab"" for the line which controls mounting of any removable media partitions.
---
C-46216r1_chk
---
To verify that binaries cannot be directly executed from removable media, run the following command: 

# grep noexec /etc/fstab

The output should show ""noexec"" in use. 
If it does not, this is a finding.",1.0,98978062,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38655] [low] The noexec option must be added to removable media partitions.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978062
,2015-07-13T19:42:39Z,unscheduled,,"Disabling inactive accounts ensures that accounts which may not have been responsibly removed are not available to attackers who may have compromised their credentials.
---
None
---
SV-50493r1_rule
---
F-43641r2_fix
---
To specify the number of days after a password expires (which signifies inactivity) until an account is permanently disabled, add or correct the following lines in ""/etc/default/useradd"", substituting ""[NUM_DAYS]"" appropriately: 

INACTIVE=[NUM_DAYS]

A value of 35 is recommended. If a password is currently on the verge of expiration, then 35 days remain until the account is automatically disabled. However, if the password will not expire for another 60 days, then 95 days could elapse until the account would be automatically disabled. See the ""useradd"" man page for more information. Determining the inactivity timeout must be done with careful consideration of the length of a ""normal"" period of inactivity for users in the particular environment. Setting the timeout too low incurs support costs and also has the potential to impact availability of the system to legitimate users.
---
C-46254r2_chk
---
To verify the ""INACTIVE"" setting, run the following command: 

grep ""INACTIVE"" /etc/default/useradd

The output should indicate the ""INACTIVE"" configuration option is set to an appropriate integer as shown in the example below: 

# grep ""INACTIVE"" /etc/default/useradd
INACTIVE=35

If it does not, this is a finding.",1.0,98978066,story,"[{'name': 'sec', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-12-18T00:45:50Z', 'id': 10289238, 'updated_at': '2014-12-18T00:45:50Z'}, {'name': 'stemcell', 'project_id': 956238, 'kind': 'label', 'created_at': '2014-06-23T19:14:08Z', 'id': 8781890, 'updated_at': '2014-06-23T19:14:08Z'}, {'name': 'stig', 'project_id': 956238, 'kind': 'label', 'created_at': '2015-07-14T00:01:41Z', 'id': 12209038, 'updated_at': '2015-07-14T00:01:41Z'}]",[V-38692] [low] Accounts must be locked upon 35 days of inactivity.,,[],956238,81882,feature,2015-07-14T00:02:37Z,https://www.pivotaltracker.com/story/show/98978066
